window.pdocSearch = (function(){
/** elasticlunr - http://weixsong.github.io * Copyright (C) 2017 Oliver Nightingale * Copyright (C) 2017 Wei Song * MIT Licensed */!function(){function e(e){if(null===e||"object"!=typeof e)return e;var t=e.constructor();for(var n in e)e.hasOwnProperty(n)&&(t[n]=e[n]);return t}var t=function(e){var n=new t.Index;return n.pipeline.add(t.trimmer,t.stopWordFilter,t.stemmer),e&&e.call(n,n),n};t.version="0.9.5",lunr=t,t.utils={},t.utils.warn=function(e){return function(t){e.console&&console.warn&&console.warn(t)}}(this),t.utils.toString=function(e){return void 0===e||null===e?"":e.toString()},t.EventEmitter=function(){this.events={}},t.EventEmitter.prototype.addListener=function(){var e=Array.prototype.slice.call(arguments),t=e.pop(),n=e;if("function"!=typeof t)throw new TypeError("last argument must be a function");n.forEach(function(e){this.hasHandler(e)||(this.events[e]=[]),this.events[e].push(t)},this)},t.EventEmitter.prototype.removeListener=function(e,t){if(this.hasHandler(e)){var n=this.events[e].indexOf(t);-1!==n&&(this.events[e].splice(n,1),0==this.events[e].length&&delete this.events[e])}},t.EventEmitter.prototype.emit=function(e){if(this.hasHandler(e)){var t=Array.prototype.slice.call(arguments,1);this.events[e].forEach(function(e){e.apply(void 0,t)},this)}},t.EventEmitter.prototype.hasHandler=function(e){return e in this.events},t.tokenizer=function(e){if(!arguments.length||null===e||void 0===e)return[];if(Array.isArray(e)){var n=e.filter(function(e){return null===e||void 0===e?!1:!0});n=n.map(function(e){return t.utils.toString(e).toLowerCase()});var i=[];return n.forEach(function(e){var n=e.split(t.tokenizer.seperator);i=i.concat(n)},this),i}return e.toString().trim().toLowerCase().split(t.tokenizer.seperator)},t.tokenizer.defaultSeperator=/[\s\-]+/,t.tokenizer.seperator=t.tokenizer.defaultSeperator,t.tokenizer.setSeperator=function(e){null!==e&&void 0!==e&&"object"==typeof e&&(t.tokenizer.seperator=e)},t.tokenizer.resetSeperator=function(){t.tokenizer.seperator=t.tokenizer.defaultSeperator},t.tokenizer.getSeperator=function(){return t.tokenizer.seperator},t.Pipeline=function(){this._queue=[]},t.Pipeline.registeredFunctions={},t.Pipeline.registerFunction=function(e,n){n in t.Pipeline.registeredFunctions&&t.utils.warn("Overwriting existing registered function: "+n),e.label=n,t.Pipeline.registeredFunctions[n]=e},t.Pipeline.getRegisteredFunction=function(e){return e in t.Pipeline.registeredFunctions!=!0?null:t.Pipeline.registeredFunctions[e]},t.Pipeline.warnIfFunctionNotRegistered=function(e){var n=e.label&&e.label in this.registeredFunctions;n||t.utils.warn("Function is not registered with pipeline. This may cause problems when serialising the index.\n",e)},t.Pipeline.load=function(e){var n=new t.Pipeline;return e.forEach(function(e){var i=t.Pipeline.getRegisteredFunction(e);if(!i)throw new Error("Cannot load un-registered function: "+e);n.add(i)}),n},t.Pipeline.prototype.add=function(){var e=Array.prototype.slice.call(arguments);e.forEach(function(e){t.Pipeline.warnIfFunctionNotRegistered(e),this._queue.push(e)},this)},t.Pipeline.prototype.after=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i+1,0,n)},t.Pipeline.prototype.before=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i,0,n)},t.Pipeline.prototype.remove=function(e){var t=this._queue.indexOf(e);-1!==t&&this._queue.splice(t,1)},t.Pipeline.prototype.run=function(e){for(var t=[],n=e.length,i=this._queue.length,o=0;n>o;o++){for(var r=e[o],s=0;i>s&&(r=this._queue[s](r,o,e),void 0!==r&&null!==r);s++);void 0!==r&&null!==r&&t.push(r)}return t},t.Pipeline.prototype.reset=function(){this._queue=[]},t.Pipeline.prototype.get=function(){return this._queue},t.Pipeline.prototype.toJSON=function(){return this._queue.map(function(e){return t.Pipeline.warnIfFunctionNotRegistered(e),e.label})},t.Index=function(){this._fields=[],this._ref="id",this.pipeline=new t.Pipeline,this.documentStore=new t.DocumentStore,this.index={},this.eventEmitter=new t.EventEmitter,this._idfCache={},this.on("add","remove","update",function(){this._idfCache={}}.bind(this))},t.Index.prototype.on=function(){var e=Array.prototype.slice.call(arguments);return this.eventEmitter.addListener.apply(this.eventEmitter,e)},t.Index.prototype.off=function(e,t){return this.eventEmitter.removeListener(e,t)},t.Index.load=function(e){e.version!==t.version&&t.utils.warn("version mismatch: current "+t.version+" importing "+e.version);var n=new this;n._fields=e.fields,n._ref=e.ref,n.documentStore=t.DocumentStore.load(e.documentStore),n.pipeline=t.Pipeline.load(e.pipeline),n.index={};for(var i in e.index)n.index[i]=t.InvertedIndex.load(e.index[i]);return n},t.Index.prototype.addField=function(e){return this._fields.push(e),this.index[e]=new t.InvertedIndex,this},t.Index.prototype.setRef=function(e){return this._ref=e,this},t.Index.prototype.saveDocument=function(e){return this.documentStore=new t.DocumentStore(e),this},t.Index.prototype.addDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.addDoc(i,e),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));this.documentStore.addFieldLength(i,n,o.length);var r={};o.forEach(function(e){e in r?r[e]+=1:r[e]=1},this);for(var s in r){var u=r[s];u=Math.sqrt(u),this.index[n].addToken(s,{ref:i,tf:u})}},this),n&&this.eventEmitter.emit("add",e,this)}},t.Index.prototype.removeDocByRef=function(e){if(e&&this.documentStore.isDocStored()!==!1&&this.documentStore.hasDoc(e)){var t=this.documentStore.getDoc(e);this.removeDoc(t,!1)}},t.Index.prototype.removeDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.hasDoc(i)&&(this.documentStore.removeDoc(i),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));o.forEach(function(e){this.index[n].removeToken(e,i)},this)},this),n&&this.eventEmitter.emit("remove",e,this))}},t.Index.prototype.updateDoc=function(e,t){var t=void 0===t?!0:t;this.removeDocByRef(e[this._ref],!1),this.addDoc(e,!1),t&&this.eventEmitter.emit("update",e,this)},t.Index.prototype.idf=function(e,t){var n="@"+t+"/"+e;if(Object.prototype.hasOwnProperty.call(this._idfCache,n))return this._idfCache[n];var i=this.index[t].getDocFreq(e),o=1+Math.log(this.documentStore.length/(i+1));return this._idfCache[n]=o,o},t.Index.prototype.getFields=function(){return this._fields.slice()},t.Index.prototype.search=function(e,n){if(!e)return[];e="string"==typeof e?{any:e}:JSON.parse(JSON.stringify(e));var i=null;null!=n&&(i=JSON.stringify(n));for(var o=new t.Configuration(i,this.getFields()).get(),r={},s=Object.keys(e),u=0;u<s.length;u++){var a=s[u];r[a]=this.pipeline.run(t.tokenizer(e[a]))}var l={};for(var c in o){var d=r[c]||r.any;if(d){var f=this.fieldSearch(d,c,o),h=o[c].boost;for(var p in f)f[p]=f[p]*h;for(var p in f)p in l?l[p]+=f[p]:l[p]=f[p]}}var v,g=[];for(var p in l)v={ref:p,score:l[p]},this.documentStore.hasDoc(p)&&(v.doc=this.documentStore.getDoc(p)),g.push(v);return g.sort(function(e,t){return t.score-e.score}),g},t.Index.prototype.fieldSearch=function(e,t,n){var i=n[t].bool,o=n[t].expand,r=n[t].boost,s=null,u={};return 0!==r?(e.forEach(function(e){var n=[e];1==o&&(n=this.index[t].expandToken(e));var r={};n.forEach(function(n){var o=this.index[t].getDocs(n),a=this.idf(n,t);if(s&&"AND"==i){var l={};for(var c in s)c in o&&(l[c]=o[c]);o=l}n==e&&this.fieldSearchStats(u,n,o);for(var c in o){var d=this.index[t].getTermFrequency(n,c),f=this.documentStore.getFieldLength(c,t),h=1;0!=f&&(h=1/Math.sqrt(f));var p=1;n!=e&&(p=.15*(1-(n.length-e.length)/n.length));var v=d*a*h*p;c in r?r[c]+=v:r[c]=v}},this),s=this.mergeScores(s,r,i)},this),s=this.coordNorm(s,u,e.length)):void 0},t.Index.prototype.mergeScores=function(e,t,n){if(!e)return t;if("AND"==n){var i={};for(var o in t)o in e&&(i[o]=e[o]+t[o]);return i}for(var o in t)o in e?e[o]+=t[o]:e[o]=t[o];return e},t.Index.prototype.fieldSearchStats=function(e,t,n){for(var i in n)i in e?e[i].push(t):e[i]=[t]},t.Index.prototype.coordNorm=function(e,t,n){for(var i in e)if(i in t){var o=t[i].length;e[i]=e[i]*o/n}return e},t.Index.prototype.toJSON=function(){var e={};return this._fields.forEach(function(t){e[t]=this.index[t].toJSON()},this),{version:t.version,fields:this._fields,ref:this._ref,documentStore:this.documentStore.toJSON(),index:e,pipeline:this.pipeline.toJSON()}},t.Index.prototype.use=function(e){var t=Array.prototype.slice.call(arguments,1);t.unshift(this),e.apply(this,t)},t.DocumentStore=function(e){this._save=null===e||void 0===e?!0:e,this.docs={},this.docInfo={},this.length=0},t.DocumentStore.load=function(e){var t=new this;return t.length=e.length,t.docs=e.docs,t.docInfo=e.docInfo,t._save=e.save,t},t.DocumentStore.prototype.isDocStored=function(){return this._save},t.DocumentStore.prototype.addDoc=function(t,n){this.hasDoc(t)||this.length++,this.docs[t]=this._save===!0?e(n):null},t.DocumentStore.prototype.getDoc=function(e){return this.hasDoc(e)===!1?null:this.docs[e]},t.DocumentStore.prototype.hasDoc=function(e){return e in this.docs},t.DocumentStore.prototype.removeDoc=function(e){this.hasDoc(e)&&(delete this.docs[e],delete this.docInfo[e],this.length--)},t.DocumentStore.prototype.addFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&(this.docInfo[e]||(this.docInfo[e]={}),this.docInfo[e][t]=n)},t.DocumentStore.prototype.updateFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&this.addFieldLength(e,t,n)},t.DocumentStore.prototype.getFieldLength=function(e,t){return null===e||void 0===e?0:e in this.docs&&t in this.docInfo[e]?this.docInfo[e][t]:0},t.DocumentStore.prototype.toJSON=function(){return{docs:this.docs,docInfo:this.docInfo,length:this.length,save:this._save}},t.stemmer=function(){var e={ational:"ate",tional:"tion",enci:"ence",anci:"ance",izer:"ize",bli:"ble",alli:"al",entli:"ent",eli:"e",ousli:"ous",ization:"ize",ation:"ate",ator:"ate",alism:"al",iveness:"ive",fulness:"ful",ousness:"ous",aliti:"al",iviti:"ive",biliti:"ble",logi:"log"},t={icate:"ic",ative:"",alize:"al",iciti:"ic",ical:"ic",ful:"",ness:""},n="[^aeiou]",i="[aeiouy]",o=n+"[^aeiouy]*",r=i+"[aeiou]*",s="^("+o+")?"+r+o,u="^("+o+")?"+r+o+"("+r+")?$",a="^("+o+")?"+r+o+r+o,l="^("+o+")?"+i,c=new RegExp(s),d=new RegExp(a),f=new RegExp(u),h=new RegExp(l),p=/^(.+?)(ss|i)es$/,v=/^(.+?)([^s])s$/,g=/^(.+?)eed$/,m=/^(.+?)(ed|ing)$/,y=/.$/,S=/(at|bl|iz)$/,x=new RegExp("([^aeiouylsz])\\1$"),w=new RegExp("^"+o+i+"[^aeiouwxy]$"),I=/^(.+?[^aeiou])y$/,b=/^(.+?)(ational|tional|enci|anci|izer|bli|alli|entli|eli|ousli|ization|ation|ator|alism|iveness|fulness|ousness|aliti|iviti|biliti|logi)$/,E=/^(.+?)(icate|ative|alize|iciti|ical|ful|ness)$/,D=/^(.+?)(al|ance|ence|er|ic|able|ible|ant|ement|ment|ent|ou|ism|ate|iti|ous|ive|ize)$/,F=/^(.+?)(s|t)(ion)$/,_=/^(.+?)e$/,P=/ll$/,k=new RegExp("^"+o+i+"[^aeiouwxy]$"),z=function(n){var i,o,r,s,u,a,l;if(n.length<3)return n;if(r=n.substr(0,1),"y"==r&&(n=r.toUpperCase()+n.substr(1)),s=p,u=v,s.test(n)?n=n.replace(s,"$1$2"):u.test(n)&&(n=n.replace(u,"$1$2")),s=g,u=m,s.test(n)){var z=s.exec(n);s=c,s.test(z[1])&&(s=y,n=n.replace(s,""))}else if(u.test(n)){var z=u.exec(n);i=z[1],u=h,u.test(i)&&(n=i,u=S,a=x,l=w,u.test(n)?n+="e":a.test(n)?(s=y,n=n.replace(s,"")):l.test(n)&&(n+="e"))}if(s=I,s.test(n)){var z=s.exec(n);i=z[1],n=i+"i"}if(s=b,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+e[o])}if(s=E,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+t[o])}if(s=D,u=F,s.test(n)){var z=s.exec(n);i=z[1],s=d,s.test(i)&&(n=i)}else if(u.test(n)){var z=u.exec(n);i=z[1]+z[2],u=d,u.test(i)&&(n=i)}if(s=_,s.test(n)){var z=s.exec(n);i=z[1],s=d,u=f,a=k,(s.test(i)||u.test(i)&&!a.test(i))&&(n=i)}return s=P,u=d,s.test(n)&&u.test(n)&&(s=y,n=n.replace(s,"")),"y"==r&&(n=r.toLowerCase()+n.substr(1)),n};return z}(),t.Pipeline.registerFunction(t.stemmer,"stemmer"),t.stopWordFilter=function(e){return e&&t.stopWordFilter.stopWords[e]!==!0?e:void 0},t.clearStopWords=function(){t.stopWordFilter.stopWords={}},t.addStopWords=function(e){null!=e&&Array.isArray(e)!==!1&&e.forEach(function(e){t.stopWordFilter.stopWords[e]=!0},this)},t.resetStopWords=function(){t.stopWordFilter.stopWords=t.defaultStopWords},t.defaultStopWords={"":!0,a:!0,able:!0,about:!0,across:!0,after:!0,all:!0,almost:!0,also:!0,am:!0,among:!0,an:!0,and:!0,any:!0,are:!0,as:!0,at:!0,be:!0,because:!0,been:!0,but:!0,by:!0,can:!0,cannot:!0,could:!0,dear:!0,did:!0,"do":!0,does:!0,either:!0,"else":!0,ever:!0,every:!0,"for":!0,from:!0,get:!0,got:!0,had:!0,has:!0,have:!0,he:!0,her:!0,hers:!0,him:!0,his:!0,how:!0,however:!0,i:!0,"if":!0,"in":!0,into:!0,is:!0,it:!0,its:!0,just:!0,least:!0,let:!0,like:!0,likely:!0,may:!0,me:!0,might:!0,most:!0,must:!0,my:!0,neither:!0,no:!0,nor:!0,not:!0,of:!0,off:!0,often:!0,on:!0,only:!0,or:!0,other:!0,our:!0,own:!0,rather:!0,said:!0,say:!0,says:!0,she:!0,should:!0,since:!0,so:!0,some:!0,than:!0,that:!0,the:!0,their:!0,them:!0,then:!0,there:!0,these:!0,they:!0,"this":!0,tis:!0,to:!0,too:!0,twas:!0,us:!0,wants:!0,was:!0,we:!0,were:!0,what:!0,when:!0,where:!0,which:!0,"while":!0,who:!0,whom:!0,why:!0,will:!0,"with":!0,would:!0,yet:!0,you:!0,your:!0},t.stopWordFilter.stopWords=t.defaultStopWords,t.Pipeline.registerFunction(t.stopWordFilter,"stopWordFilter"),t.trimmer=function(e){if(null===e||void 0===e)throw new Error("token should not be undefined");return e.replace(/^\W+/,"").replace(/\W+$/,"")},t.Pipeline.registerFunction(t.trimmer,"trimmer"),t.InvertedIndex=function(){this.root={docs:{},df:0}},t.InvertedIndex.load=function(e){var t=new this;return t.root=e.root,t},t.InvertedIndex.prototype.addToken=function(e,t,n){for(var n=n||this.root,i=0;i<=e.length-1;){var o=e[i];o in n||(n[o]={docs:{},df:0}),i+=1,n=n[o]}var r=t.ref;n.docs[r]?n.docs[r]={tf:t.tf}:(n.docs[r]={tf:t.tf},n.df+=1)},t.InvertedIndex.prototype.hasToken=function(e){if(!e)return!1;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return!1;t=t[e[n]]}return!0},t.InvertedIndex.prototype.getNode=function(e){if(!e)return null;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return null;t=t[e[n]]}return t},t.InvertedIndex.prototype.getDocs=function(e){var t=this.getNode(e);return null==t?{}:t.docs},t.InvertedIndex.prototype.getTermFrequency=function(e,t){var n=this.getNode(e);return null==n?0:t in n.docs?n.docs[t].tf:0},t.InvertedIndex.prototype.getDocFreq=function(e){var t=this.getNode(e);return null==t?0:t.df},t.InvertedIndex.prototype.removeToken=function(e,t){if(e){var n=this.getNode(e);null!=n&&t in n.docs&&(delete n.docs[t],n.df-=1)}},t.InvertedIndex.prototype.expandToken=function(e,t,n){if(null==e||""==e)return[];var t=t||[];if(void 0==n&&(n=this.getNode(e),null==n))return t;n.df>0&&t.push(e);for(var i in n)"docs"!==i&&"df"!==i&&this.expandToken(e+i,t,n[i]);return t},t.InvertedIndex.prototype.toJSON=function(){return{root:this.root}},t.Configuration=function(e,n){var e=e||"";if(void 0==n||null==n)throw new Error("fields should not be null");this.config={};var i;try{i=JSON.parse(e),this.buildUserConfig(i,n)}catch(o){t.utils.warn("user configuration parse failed, will use default configuration"),this.buildDefaultConfig(n)}},t.Configuration.prototype.buildDefaultConfig=function(e){this.reset(),e.forEach(function(e){this.config[e]={boost:1,bool:"OR",expand:!1}},this)},t.Configuration.prototype.buildUserConfig=function(e,n){var i="OR",o=!1;if(this.reset(),"bool"in e&&(i=e.bool||i),"expand"in e&&(o=e.expand||o),"fields"in e)for(var r in e.fields)if(n.indexOf(r)>-1){var s=e.fields[r],u=o;void 0!=s.expand&&(u=s.expand),this.config[r]={boost:s.boost||0===s.boost?s.boost:1,bool:s.bool||i,expand:u}}else t.utils.warn("field name in user configuration not found in index instance fields");else this.addAllFields2UserConfig(i,o,n)},t.Configuration.prototype.addAllFields2UserConfig=function(e,t,n){n.forEach(function(n){this.config[n]={boost:1,bool:e,expand:t}},this)},t.Configuration.prototype.get=function(){return this.config},t.Configuration.prototype.reset=function(){this.config={}},lunr.SortedSet=function(){this.length=0,this.elements=[]},lunr.SortedSet.load=function(e){var t=new this;return t.elements=e,t.length=e.length,t},lunr.SortedSet.prototype.add=function(){var e,t;for(e=0;e<arguments.length;e++)t=arguments[e],~this.indexOf(t)||this.elements.splice(this.locationFor(t),0,t);this.length=this.elements.length},lunr.SortedSet.prototype.toArray=function(){return this.elements.slice()},lunr.SortedSet.prototype.map=function(e,t){return this.elements.map(e,t)},lunr.SortedSet.prototype.forEach=function(e,t){return this.elements.forEach(e,t)},lunr.SortedSet.prototype.indexOf=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;){if(r===e)return o;e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o]}return r===e?o:-1},lunr.SortedSet.prototype.locationFor=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;)e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o];return r>e?o:e>r?o+1:void 0},lunr.SortedSet.prototype.intersect=function(e){for(var t=new lunr.SortedSet,n=0,i=0,o=this.length,r=e.length,s=this.elements,u=e.elements;;){if(n>o-1||i>r-1)break;s[n]!==u[i]?s[n]<u[i]?n++:s[n]>u[i]&&i++:(t.add(s[n]),n++,i++)}return t},lunr.SortedSet.prototype.clone=function(){var e=new lunr.SortedSet;return e.elements=this.toArray(),e.length=e.elements.length,e},lunr.SortedSet.prototype.union=function(e){var t,n,i;this.length>=e.length?(t=this,n=e):(t=e,n=this),i=t.clone();for(var o=0,r=n.toArray();o<r.length;o++)i.add(r[o]);return i},lunr.SortedSet.prototype.toJSON=function(){return this.toArray()},function(e,t){"function"==typeof define&&define.amd?define(t):"object"==typeof exports?module.exports=t():e.elasticlunr=t()}(this,function(){return t})}();
    /** pdoc search index */const docs = [{"fullname": "clarena", "modulename": "clarena", "kind": "module", "doc": "<h1 id=\"welcome-to-clarena\">Welcome to CLArena</h1>\n\n<p><strong>CLArena (Continual Learning Arena)</strong> is a open-source Python package for Continual Learning (CL) research. In this package, we provide a integrated environment and various APIs to conduct CL experiments for research purposes, as well as implemented CL algorithms and datasets that you can give it a spin immediately.</p>\n\n<p>Please note that this is an API documantation providing detailed information about the available classes, functions, and modules in CLArena. Please refer to the main documentation and my beginners' guide to continual learning for more intuitive tutorials, examples, and guides on how to use CLArena:</p>\n\n<ul>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena\"><strong>Main Documentation</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/posts/continual-learning-beginners-guide\"><strong>A Beginners' Guide to Continual Learning</strong></a></li>\n</ul>\n\n<p>We provide various components of continual learning system in the submodules:</p>\n\n<ul>\n<li><code>clarena.cl_datasets</code>: Continual learning datasets.</li>\n<li><code>clarena.backbones</code>: Neural network architectures used as backbones for CL algorithms.</li>\n<li><code>clarena.cl_heads</code>: Multi-head classifiers for continual learning outputs. Task-Incremental Learning (TIL) head and Class-Incremental Learning (CIL) head are included.</li>\n<li><code>clarena.cl_algorithms</code>: Implementation of various continual learning algorithms.</li>\n<li><code>clarena.callbacks</code>: Extra actions added in the continual learning process.</li>\n<li><code>utils</code>: Utility functions for continual learning experiments.</li>\n</ul>\n\n<p>As well as the base class in the outmost directory of the package:</p>\n\n<ul>\n<li><code>CLExperiment</code>: The base class for continual learning experiments.</li>\n</ul>\n"}, {"fullname": "clarena.CLExperiment", "modulename": "clarena", "qualname": "CLExperiment", "kind": "class", "doc": "<p>The base class for continual learning experiments.</p>\n"}, {"fullname": "clarena.CLExperiment.__init__", "modulename": "clarena", "qualname": "CLExperiment.__init__", "kind": "function", "doc": "<p>Initializes the CL experiment object with a complete configuration.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>cfg</strong> (<code>DictConfig</code>): the complete config dict for the CL experiment.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span>)</span>"}, {"fullname": "clarena.CLExperiment.cfg", "modulename": "clarena", "qualname": "CLExperiment.cfg", "kind": "variable", "doc": "<p>Store the complete config dict for any future reference.</p>\n", "annotation": ": omegaconf.dictconfig.DictConfig"}, {"fullname": "clarena.CLExperiment.cl_paradigm", "modulename": "clarena", "qualname": "CLExperiment.cl_paradigm", "kind": "variable", "doc": "<p>Store the continual learning paradigm, either 'TIL' (Task-Incremental Learning) or 'CIL' (Class-Incremental Learning). Parsed from config and used to instantiate the correct heads object and set up CL dataset.</p>\n", "annotation": ": str"}, {"fullname": "clarena.CLExperiment.num_tasks", "modulename": "clarena", "qualname": "CLExperiment.num_tasks", "kind": "variable", "doc": "<p>Store the number of tasks to be conducted in this experiment. Parsed from config and used in the tasks loop.</p>\n", "annotation": ": int"}, {"fullname": "clarena.CLExperiment.global_seed", "modulename": "clarena", "qualname": "CLExperiment.global_seed", "kind": "variable", "doc": "<p>Store the global seed for the entire experiment. Parsed from config and used to seed all random number generators.</p>\n", "annotation": ": int"}, {"fullname": "clarena.CLExperiment.test", "modulename": "clarena", "qualname": "CLExperiment.test", "kind": "variable", "doc": "<p>Store whether to test the model after training and validation. Parsed from config and used in the tasks loop.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.CLExperiment.output_dir_name", "modulename": "clarena", "qualname": "CLExperiment.output_dir_name", "kind": "variable", "doc": "<p>Store the name of the output directory to store the logs and checkpoints. Parsed from config and help any output operation to locate the correct directory.</p>\n", "annotation": ": str"}, {"fullname": "clarena.CLExperiment.task_id", "modulename": "clarena", "qualname": "CLExperiment.task_id", "kind": "variable", "doc": "<p>Task ID counter indicating which task is being processed. Self updated during the task loop.</p>\n", "annotation": ": int"}, {"fullname": "clarena.CLExperiment.cl_dataset", "modulename": "clarena", "qualname": "CLExperiment.cl_dataset", "kind": "variable", "doc": "<p>CL dataset object. Instantiate in <code>instantiate_cl_dataset()</code>.</p>\n", "annotation": ": clarena.cl_datasets.base.CLDataset"}, {"fullname": "clarena.CLExperiment.backbone", "modulename": "clarena", "qualname": "CLExperiment.backbone", "kind": "variable", "doc": "<p>Backbone network object. Instantiate in <code>instantiate_backbone()</code>.</p>\n", "annotation": ": clarena.backbones.base.CLBackbone"}, {"fullname": "clarena.CLExperiment.heads", "modulename": "clarena", "qualname": "CLExperiment.heads", "kind": "variable", "doc": "<p>CL output heads object. Instantiate in <code>instantiate_heads()</code>.</p>\n", "annotation": ": clarena.cl_heads.heads_til.HeadsTIL | clarena.cl_heads.heads_cil.HeadsCIL"}, {"fullname": "clarena.CLExperiment.model", "modulename": "clarena", "qualname": "CLExperiment.model", "kind": "variable", "doc": "<p>CL model object. Instantiate in <code>instantiate_cl_algorithm()</code>.</p>\n", "annotation": ": clarena.cl_algorithms.base.CLAlgorithm"}, {"fullname": "clarena.CLExperiment.optimizer", "modulename": "clarena", "qualname": "CLExperiment.optimizer", "kind": "variable", "doc": "<p>Optimizer object for current task <code>self.task_id</code>. Instantiate in <code>instantiate_optimizer()</code>.</p>\n", "annotation": ": torch.optim.optimizer.Optimizer"}, {"fullname": "clarena.CLExperiment.trainer", "modulename": "clarena", "qualname": "CLExperiment.trainer", "kind": "variable", "doc": "<p>Trainer object for current task <code>self.task_id</code>. Instantiate in <code>instantiate_trainer()</code>.</p>\n", "annotation": ": lightning.pytorch.trainer.trainer.Trainer"}, {"fullname": "clarena.CLExperiment.lightning_loggers", "modulename": "clarena", "qualname": "CLExperiment.lightning_loggers", "kind": "variable", "doc": "<p>The list of initialised lightning loggers objects for current task <code>self.task_id</code>. Instantiate in <code>instantiate_lightning_loggers()</code>.</p>\n", "annotation": ": list[lightning.pytorch.loggers.logger.Logger]"}, {"fullname": "clarena.CLExperiment.callbacks", "modulename": "clarena", "qualname": "CLExperiment.callbacks", "kind": "variable", "doc": "<p>The list of initialised callbacks objects for current task <code>self.task_id</code>. Instantiate in <code>instantiate_callbacks()</code>.</p>\n", "annotation": ": list[lightning.pytorch.callbacks.callback.Callback]"}, {"fullname": "clarena.CLExperiment.sanity_check", "modulename": "clarena", "qualname": "CLExperiment.sanity_check", "kind": "function", "doc": "<p>Check the sanity of the config dict <code>self.cfg</code>.</p>\n\n<p><strong>Raises:</strong></p>\n\n<ul>\n<li><strong>KeyError</strong>: when required fields in experiment config are missing, including <code>cl_paradigm</code>, <code>num_tasks</code>, <code>test</code>, <code>output_dir_name</code>.</li>\n<li><strong>ValueError</strong>: when the value of <code>cl_paradigm</code> is not 'TIL' or 'CIL', or when the number of tasks is larger than the number of tasks in the CL dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.CLExperiment.instantiate_cl_dataset", "modulename": "clarena", "qualname": "CLExperiment.instantiate_cl_dataset", "kind": "function", "doc": "<p>Instantiate the CL dataset object from cl_dataset config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>cl_dataset_cfg</strong> (<code>DictConfig</code>): the cl_dataset config dict.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">cl_dataset_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.CLExperiment.instantiate_backbone", "modulename": "clarena", "qualname": "CLExperiment.instantiate_backbone", "kind": "function", "doc": "<p>Instantiate the CL backbone network object from backbone config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone_cfg</strong> (<code>DictConfig</code>): the backbone config dict.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">backbone_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.CLExperiment.instantiate_heads", "modulename": "clarena", "qualname": "CLExperiment.instantiate_heads", "kind": "function", "doc": "<p>Instantiate the CL output heads object according to field <code>cl_paradigm</code> and backbone <code>output_dim</code> in the config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>cl_paradigm</strong> (<code>str</code>): the CL paradigm, either 'TIL' or 'CIL'.</li>\n<li><strong>input_dim</strong> (<code>int</code>): the input dimension of the heads. Must be equal to the <code>output_dim</code> of the connected backbone.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">cl_paradigm</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">input_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.CLExperiment.instantiate_cl_algorithm", "modulename": "clarena", "qualname": "CLExperiment.instantiate_cl_algorithm", "kind": "function", "doc": "<p>Instantiate the cl_algorithm object from cl_algorithm config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>cl_algorithm_cfg</strong> (<code>DictConfig</code>): the cl_algorithm config dict.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">cl_algorithm_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.CLExperiment.instantiate_optimizer", "modulename": "clarena", "qualname": "CLExperiment.instantiate_optimizer", "kind": "function", "doc": "<p>Instantiate the optimizer object for task <code>task_id</code> from optimizer config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>optimizer_cfg</strong> (<code>DictConfig</code> or <code>ListConfig</code>): the optimizer config dict. If it's a <code>ListConfig</code>, it should contain optimizer config for each task; otherwise, it's an uniform optimizer config for all tasks.</li>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">optimizer_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span> <span class=\"o\">|</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">listconfig</span><span class=\"o\">.</span><span class=\"n\">ListConfig</span>,</span><span class=\"param\">\t<span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.CLExperiment.instantiate_trainer", "modulename": "clarena", "qualname": "CLExperiment.instantiate_trainer", "kind": "function", "doc": "<p>Instantiate the trainer object for task <code>task_id</code> from trainer config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>trainer_cfg</strong> (<code>DictConfig</code>): the trainer config dict. All tasks share the same trainer config but different objects.</li>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">trainer_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.CLExperiment.instantiate_lightning_loggers", "modulename": "clarena", "qualname": "CLExperiment.instantiate_lightning_loggers", "kind": "function", "doc": "<p>Instantiate the list of lightning loggers objects for task <code>task_id</code> from lightning_loggers config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>lightning_loggers_cfg</strong> (<code>DictConfig</code>): the lightning_loggers config dict. All tasks share the same lightning_loggers config but different objects.</li>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">lightning_loggers_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span>,</span><span class=\"param\">\t<span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.CLExperiment.instantiate_callbacks", "modulename": "clarena", "qualname": "CLExperiment.instantiate_callbacks", "kind": "function", "doc": "<p>Instantiate the list of callbacks objects for task <code>task_id</code> from callbacks config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>callbacks_cfg</strong> (<code>DictConfig</code>): the callbacks config dict. All tasks share the same callbacks config but different objects.</li>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">callbacks_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span>,</span><span class=\"param\">\t<span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.CLExperiment.set_global_seed", "modulename": "clarena", "qualname": "CLExperiment.set_global_seed", "kind": "function", "doc": "<p>Set the global seed for the entire experiment.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.CLExperiment.setup_task_id", "modulename": "clarena", "qualname": "CLExperiment.setup_task_id", "kind": "function", "doc": "<p>Set up current task_id in the beginning of the continual learning process of a new task.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): current task_id.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.CLExperiment.instantiate_global", "modulename": "clarena", "qualname": "CLExperiment.instantiate_global", "kind": "function", "doc": "<p>Instantiate global components for the entire CL experiment from <code>self.cfg</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.CLExperiment.setup_global", "modulename": "clarena", "qualname": "CLExperiment.setup_global", "kind": "function", "doc": "<p>Let CL dataset know the CL paradigm to define its CL class map.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.CLExperiment.instantiate_task_specific", "modulename": "clarena", "qualname": "CLExperiment.instantiate_task_specific", "kind": "function", "doc": "<p>Instantiate task-specific components for the current task <code>self.task_id</code> from <code>self.cfg</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.CLExperiment.setup_task_specific", "modulename": "clarena", "qualname": "CLExperiment.setup_task_specific", "kind": "function", "doc": "<p>Setup task-specific components to get ready for the current task <code>self.task_id</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.CLExperiment.run_task", "modulename": "clarena", "qualname": "CLExperiment.run_task", "kind": "function", "doc": "<p>Fit the model on the current task <code>self.task_id</code>. Also test the model if <code>self.test</code> is set to True.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.CLExperiment.run", "modulename": "clarena", "qualname": "CLExperiment.run", "kind": "function", "doc": "<p>The main method to run the continual learning experiment.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones", "modulename": "clarena.backbones", "kind": "module", "doc": "<h1 id=\"backbone-networks-for-continual-learning\">Backbone Networks for Continual Learning</h1>\n\n<p>This submodule provides the <strong>backbone neural network architectures for continual learning</strong>. </p>\n\n<p>Please note that this is an API documentation. Please refer to the main documentation pages for more information about the backbone networks and how to \nconfigure and implement them:</p>\n\n<ul>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/configure-your-experiment/backbone-network\"><strong>Configure Backbone Network</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/implement-your-CL-modules/backbone-network\"><strong>Implement Your CL Backbone Class</strong></a></li>\n</ul>\n\n<p>The backbones are implemented as subclasses of <code>CLBackbone</code> classes, which are the base class for all continual learning backbones in CLArena.</p>\n\n<ul>\n<li><code>CLBackbone</code>: The base class for continual learning backbones.</li>\n<li><code>HATMaskBackbone</code>: The base class for backbones used in <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task) algorithm</a>. A child class of <code>CLBackbone</code>.</li>\n</ul>\n"}, {"fullname": "clarena.backbones.CLBackbone", "modulename": "clarena.backbones", "qualname": "CLBackbone", "kind": "class", "doc": "<p>The base class of continual learning backbone networks, inherited from <code>nn.Module</code>.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "clarena.backbones.CLBackbone.__init__", "modulename": "clarena.backbones", "qualname": "CLBackbone.__init__", "kind": "function", "doc": "<p>Initialise the CL backbone network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>output_dim</strong> (<code>int</code> | <code>None</code>): The output dimension which connects to CL output heads. The <code>input_dim</code> of output heads are expected to be the same as this <code>output_dim</code>. In some cases, this class is used for a block in the backbone network, which doesn't have the output dimension. In this case, it can be <code>None</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.backbones.CLBackbone.output_dim", "modulename": "clarena.backbones", "qualname": "CLBackbone.output_dim", "kind": "variable", "doc": "<p>Store the output dimension of the backbone network.</p>\n", "annotation": ": int"}, {"fullname": "clarena.backbones.CLBackbone.weighted_layer_names", "modulename": "clarena.backbones", "qualname": "CLBackbone.weighted_layer_names", "kind": "variable", "doc": "<p>Maintain a list of the weighted layer names. Weighted layer has weights connecting to other weighted layer. They are the main part of neural networks. <strong>It must be provided in subclasses.</strong></p>\n\n<p>The names are following the <code>nn.Module</code> internal naming mechanism. For example, if the a layer is assigned to <code>self.conv1</code>, the name becomes <code>conv1</code>. If the <code>nn.Sequential</code> is used, the name becomes the index of the layer in the sequence, such as <code>0</code>, <code>1</code>, etc. If hierarchical structure is used, for example, a <code>nn.Module</code> is assigned to <code>self.block</code> which has <code>self.conv1</code>, the name becomes <code>block/conv1</code>. Note that it should be <code>block.conv1</code> according to <code>nn.Module</code> internal mechanism, but we use '/' instead of '.' to avoid the error of using '.' in the key of <code>ModuleDict</code>.</p>\n\n<p>In HAT architecture, it's also the layer names with task embedding masking in the order of forward pass. HAT gives task embedding to every possible weighted layer.</p>\n", "annotation": ": list[str]"}, {"fullname": "clarena.backbones.CLBackbone.task_id", "modulename": "clarena.backbones", "qualname": "CLBackbone.task_id", "kind": "variable", "doc": "<p>Task ID counter indicating which task is being processed. Self updated during the task loop.</p>\n", "annotation": ": int"}, {"fullname": "clarena.backbones.CLBackbone.setup_task_id", "modulename": "clarena.backbones", "qualname": "CLBackbone.setup_task_id", "kind": "function", "doc": "<p>Set up which task's dataset the CL experiment is on. This must be done before <code>forward()</code> method is called.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.CLBackbone.get_layer_by_name", "modulename": "clarena.backbones", "qualname": "CLBackbone.get_layer_by_name", "kind": "function", "doc": "<p>Get the layer by its name.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>layer_name</strong> (<code>str</code>): the name of the layer. Note that the name is the name substituting the '.' with '/', like <code>block/conv1</code>, rather than <code>block.conv1</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>layer</strong> (<code>nn.Module</code>): the layer.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.CLBackbone.preceding_layer_name", "modulename": "clarena.backbones", "qualname": "CLBackbone.preceding_layer_name", "kind": "function", "doc": "<p>Get the name of the preceding layer of the given layer from the stored <code>self.masked_layer_order</code>. If the given layer is the first layer, return <code>None</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>layer_name</strong> (<code>str</code>): the name of the layer.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>preceding_layer_name</strong> (<code>str</code>): the name of the preceding layer.</li>\n</ul>\n\n<p><strong>Raises:</strong></p>\n\n<ul>\n<li><strong>ValueError</strong>: if <code>layer_name</code> is not in the weighted layer order.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.CLBackbone.forward", "modulename": "clarena.backbones", "qualname": "CLBackbone.forward", "kind": "function", "doc": "<p>The forward pass for data from task <code>task_id</code>. In some backbones, the forward pass might be different for different tasks. <strong>It must be implemented by subclasses.</strong></p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): The input tensor from data.</li>\n<li><strong>stage</strong> (<code>str</code>): the stage of the forward pass, should be one of the following:\n<ol>\n<li>'train': training stage.</li>\n<li>'validation': validation stage.</li>\n<li>'test': testing stage.</li>\n</ol></li>\n<li><strong>task_id</strong> (<code>int</code> | <code>None</code>): the task ID where the data are from. If stage is 'train' or 'validation', it is usually from the current task <code>self.task_id</code>. If stage is 'test', it could be from any seen task. In TIL, the task IDs of test data are provided thus this argument can be used. In CIL, they are not provided, so it is just a placeholder for API consistence but never used, and best practices are not to provide this argument and leave it as the default value.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>output_feature</strong> (<code>Tensor</code>): the output feature tensor to be passed into heads. This is the main target of backpropagation.</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): the hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name, value (<code>Tensor</code>) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.HATMaskBackbone", "modulename": "clarena.backbones", "qualname": "HATMaskBackbone", "kind": "class", "doc": "<p>The backbone network for HAT-based algorithms with learnable hard attention masks.</p>\n\n<p>HAT-based algorithms:</p>\n\n<ul>\n<li><a href=\"http://proceedings.mlr.press/v80/serra18a\"><strong>HAT (Hard Attention to the Task, 2018)</strong></a> is an architecture-based continual learning approach that uses learnable hard attention masks to select the task-specific parameters.</li>\n<li><a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\"><strong>Adaptive HAT (Adaptive Hard Attention to the Task, 2024)</strong></a> is an architecture-based continual learning approach that improves <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task, 2018)</a> by introducing new adaptive soft gradient clipping based on parameter importance and network sparsity.</li>\n<li><strong>CBPHAT</strong> is what I am working on, trying combining HAT (Hard Attention to the Task) algorithm with Continual Backpropagation (CBP) by leveraging the contribution utility as the parameter importance like in AdaHAT (Adaptive Hard Attention to the Task) algorithm.</li>\n</ul>\n", "bases": "clarena.backbones.base.CLBackbone"}, {"fullname": "clarena.backbones.HATMaskBackbone.__init__", "modulename": "clarena.backbones", "qualname": "HATMaskBackbone.__init__", "kind": "function", "doc": "<p>Initialise the HAT mask backbone network with task embeddings and masks.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>output_dim</strong> (<code>int</code>): The output dimension which connects to CL output heads. The <code>input_dim</code> of output heads are expected to be the same as this <code>output_dim</code>. In some cases, this class is used for a block in the backbone network, which doesn't have the output dimension. In this case, it can be <code>None</code>.</li>\n<li><strong>gate</strong> (<code>str</code>): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:\n<ul>\n<li><code>sigmoid</code>: the sigmoid function.</li>\n</ul></li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">gate</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span>)</span>"}, {"fullname": "clarena.backbones.HATMaskBackbone.register_hat_mask_module_explicitly", "modulename": "clarena.backbones", "qualname": "HATMaskBackbone.register_hat_mask_module_explicitly", "kind": "function", "doc": "<p>Register all <code>nn.Module</code>s explicitly in this method. For <code>HATMaskBackbone</code>, they are task embedding for the current task and the masks.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>gate</strong> (<code>str</code>): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:\n<ul>\n<li><code>sigmoid</code>: the sigmoid function.</li>\n</ul></li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">gate</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.HATMaskBackbone.initialise_task_embedding", "modulename": "clarena.backbones", "qualname": "HATMaskBackbone.initialise_task_embedding", "kind": "function", "doc": "<p>Initialise the task embedding for the current task.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>mode</strong> (<code>str</code>): the initialisation mode for task embeddings, should be one of the following:\n<ol>\n<li>'N01' (default): standard normal distribution $N(0, 1)$.</li>\n<li>'U-11': uniform distribution $U(-1, 1)$.</li>\n<li>'U01': uniform distribution $U(0, 1)$.</li>\n<li>'U-10': uniform distribution $U(-1, 0)$.</li>\n<li>'last': inherit task embedding from last task.</li>\n</ol></li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.HATMaskBackbone.sanity_check", "modulename": "clarena.backbones", "qualname": "HATMaskBackbone.sanity_check", "kind": "function", "doc": "<p>Check the sanity of the arguments.</p>\n\n<p><strong>Raises:</strong></p>\n\n<ul>\n<li><strong>ValueError</strong>: when the <code>gate</code> is not one of the valid options.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.HATMaskBackbone.get_mask", "modulename": "clarena.backbones", "qualname": "HATMaskBackbone.get_mask", "kind": "function", "doc": "<p>Get the hard attention mask used in <code>forward()</code> method for different stages.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>stage</strong> (<code>str</code>): the stage when applying the conversion, should be one of the following:\n<ol>\n<li>'train': training stage. If stage is 'train', get the mask from task embedding of current task through the gate function, which is scaled by an annealed scalar. See chapter 2.4 \"Hard Attention Training\" in <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</li>\n<li>\u2018validation': validation stage. If stage is 'validation', get the mask from task embedding of current task through the gate function, which is scaled by <code>s_max</code>. (Note that in this stage, the binary mask hasn't been stored yet as the training is not over.)</li>\n<li>'test': testing stage. If stage is 'test', apply the mask gate function is scaled by <code>s_max</code>, the large scaling making masks nearly binary.</li>\n</ol></li>\n<li><strong>s_max</strong> (<code>float</code>): the maximum scaling factor in the gate function. Doesn't apply to testing stage. See chapter 2.4 \"Hard Attention Training\" in <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</li>\n<li><strong>batch_idx</strong> (<code>int</code> | <code>None</code>): the current batch index. Applies only to training stage. For other stages, it is default <code>None</code>.</li>\n<li><strong>num_batches</strong> (<code>int</code> | <code>None</code>): the total number of batches. Applies only to training stage. For other stages, it is default <code>None</code>.</li>\n<li><strong>test_mask</strong> (<code>dict[str, Tensor]</code> | <code>None</code>): the binary mask used for test. Applies only to testing stage. For other stages, it is default <code>None</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>mask</strong> (<code>dict[str, Tensor]</code>): the hard attention (whose values are 0 or 1) mask. Key (<code>str</code>) is layer name, value (<code>Tensor</code>) is the mask tensor. The mask tensor has size (number of units).</li>\n</ul>\n\n<p><strong>Raises:</strong></p>\n\n<ul>\n<li><strong>ValueError</strong>: if the <code>batch_idx</code> and <code>batch_num</code> are not provided in 'train' stage; if the <code>s_max</code> is not provided in 'validation' stage; if the <code>task_id</code> is not provided in 'test' stage.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">s_max</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">test_mask</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.HATMaskBackbone.get_cumulative_mask", "modulename": "clarena.backbones", "qualname": "HATMaskBackbone.get_cumulative_mask", "kind": "function", "doc": "<p>Get the cumulative mask till current task.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>cumulative_mask</strong> (<code>dict[str, Tensor]</code>): the cumulative mask. Key (<code>str</code>) is layer name, value (<code>Tensor</code>) is the mask tensor. The mask tensor has size (number of units).</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.HATMaskBackbone.get_summative_mask", "modulename": "clarena.backbones", "qualname": "HATMaskBackbone.get_summative_mask", "kind": "function", "doc": "<p>Get the summative mask till current task.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>summative_mask</strong> (<code>dict[str, Tensor]</code>): the summative mask tensor. Key (<code>str</code>) is layer name, value (<code>Tensor</code>) is the mask tensor. The mask tensor has size (number of units).</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.HATMaskBackbone.get_layer_measure_parameter_wise", "modulename": "clarena.backbones", "qualname": "HATMaskBackbone.get_layer_measure_parameter_wise", "kind": "function", "doc": "<p>Get the parameter-wise measure on the parameters right before the given layer.</p>\n\n<p>It is calculated from the given unit-wise measure. It aggregates two feature-sized vectors (corresponding the given layer and preceding layer) into a weight-wise matrix (corresponding the weights in between) and bias-wise vector (corresponding the bias of the given layer), using the given aggregation method. For example, given two feature-sized measure $m_{l,i}$ and $m_{l-1,j}$ and 'min' aggregation, the parameter-wise measure is then $\\min \\left(a_{l,i}, a_{l-1,j}\\right)$, a matrix with respect to $i, j$.</p>\n\n<p>Note that if the given layer is the first layer with no preceding layer, we will get parameter-wise measure directly broadcasted from the unit-wise measure of given layer.</p>\n\n<p>This method is used in the calculation of parameter-wise measure in various HAT-based algorithms:</p>\n\n<ul>\n<li><strong>HAT</strong>: the parameter-wise measure is the binary mask for previous tasks from the unit-wise cumulative mask of previous tasks <code>self.cumulative_mask_for_previous_tasks</code>, which is $\\min \\left(a_{l,i}^{<t}, a_{l-1,j}^{<t}\\right)$ in equation (2) in <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</li>\n<li><strong>AdaHAT</strong>: the parameter-wise measure is the parameter importance for previous tasks from the unit-wise summative mask of previous tasks <code>self.summative_mask_for_previous_tasks</code>, which is $\\min \\left(m_{l,i}^{<t,\\text{sum}}, m_{l-1,j}^{<t,\\text{sum}}\\right)$ in equation (9) in <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>.</li>\n<li><strong>CBPHAT</strong>: the parameter-wise measure is the parameter importance for previous tasks from the unit-wise importance of previous tasks <code>self.unit_importance_for_previous_tasks</code> based on contribution utility, which is $\\min \\left(I_{l,i}^{(t-1)}, I_{l-1,j}^{(t-1)}\\right)$ in the adjustment rate formula in the paper draft.</li>\n</ul>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>unit_wise_measure</strong> (<code>dict[str, Tensor]</code>): the unit-wise measure. Key is layer name, value is the unit-wise measure tensor. The measure tensor has size (number of units).</li>\n<li><strong>layer_name</strong> (<code>str</code>): the name of given layer.</li>\n<li><strong>aggregation</strong> (<code>str</code>): the aggregation method turning two feature-wise measures into weight-wise matrix, should be one of the following:\n<ul>\n<li>'min': takes minimum of the two connected unit measures.</li>\n<li>'max': takes maximum of the two connected unit measures.</li>\n</ul></li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>weight_measure</strong> (<code>Tensor</code>): the weight measure matrix, same size as the corresponding weights.</li>\n<li><strong>bias_measure</strong> (<code>Tensor</code>): the bias measure vector, same size as the corresponding bias.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">unit_wise_measure</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">aggregation</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.HATMaskBackbone.forward", "modulename": "clarena.backbones", "qualname": "HATMaskBackbone.forward", "kind": "function", "doc": "<p>The forward pass for data from task <code>task_id</code>. Task-specific mask for <code>task_id</code> are applied to the units in each layer.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): The input tensor from data.</li>\n<li><strong>stage</strong> (<code>str</code>): the stage of the forward pass, should be one of the following:\n<ol>\n<li>'train': training stage.</li>\n<li>'validation': validation stage.</li>\n<li>'test': testing stage.</li>\n</ol></li>\n<li><strong>s_max</strong> (<code>float</code>): the maximum scaling factor in the gate function. See chapter 2.4 \"Hard Attention Training\" in <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</li>\n<li><strong>batch_idx</strong> (<code>int</code> | <code>None</code>): the current batch index. Applies only to training stage. For other stages, it is default <code>None</code>.</li>\n<li><strong>num_batches</strong> (<code>int</code> | <code>None</code>): the total number of batches. Applies only to training stage. For other stages, it is default <code>None</code>.</li>\n<li><strong>test_mask</strong> (<code>dict[str, Tensor]</code> | <code>None</code>): the binary mask used for test. Applies only to testing stage. For other stages, it is default <code>None</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>output_feature</strong> (<code>Tensor</code>): the output feature tensor to be passed into heads. This is the main target of backpropagation.</li>\n<li><strong>mask</strong> (<code>dict[str, Tensor]</code>): the mask for the current task. Key (<code>str</code>) is layer name, value (<code>Tensor</code>) is the mask tensor. The mask tensor has size (number of units).</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): the hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name, value (<code>Tensor</code>) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes. Although HAT algorithm does not need this, it is still provided for API consistence for other HAT-based algorithms inherited this <code>forward()</code> method of <code>HAT</code> class.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">s_max</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">test_mask</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">],</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.mlp", "modulename": "clarena.backbones.mlp", "kind": "module", "doc": "<p>The submodule in <code>backbones</code> for MLP backbone network.</p>\n"}, {"fullname": "clarena.backbones.mlp.MLP", "modulename": "clarena.backbones.mlp", "qualname": "MLP", "kind": "class", "doc": "<p><strong>Multi-layer perceptron (MLP)</strong> a.k.a. fully-connected network.</p>\n\n<p>MLP is an dense network architecture, which has several fully-connected layers, each followed by an activation function. The last layer connects to the CL output heads.</p>\n", "bases": "clarena.backbones.base.CLBackbone"}, {"fullname": "clarena.backbones.mlp.MLP.__init__", "modulename": "clarena.backbones.mlp", "qualname": "MLP.__init__", "kind": "function", "doc": "<p>Construct and initialise the MLP backbone network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_dim</strong> (<code>int</code>): the input dimension. Any data need to be flattened before going in MLP. Note that it is not required in convolutional networks.</li>\n<li><strong>hidden_dims</strong> (<code>list[int]</code>): list of hidden layer dimensions. It can be empty list which means single-layer MLP, and it can be as many layers as you want. Note that it doesn't include the last dimension which we take as output dimension.</li>\n<li><strong>output_dim</strong> (<code>int</code>): the output dimension which connects to CL output heads.</li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code> | <code>None</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>batch_normalisation</strong> (<code>bool</code>): whether to use batch normalisation after the fully-connected layers. Default <code>False</code>.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the linear layer. Default <code>True</code>.</li>\n<li><strong>dropout</strong> (<code>float</code> | <code>None</code>): the probability for the dropout layer, if <code>None</code> this layer won't be used. Default <code>None</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">hidden_dims</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">batch_normalisation</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">dropout</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.backbones.mlp.MLP.num_fc_layers", "modulename": "clarena.backbones.mlp", "qualname": "MLP.num_fc_layers", "kind": "variable", "doc": "<p>Store the number of fully-connected layers in the MLP backbone network, which helps form the loops in constructing layers and forward pass.</p>\n", "annotation": ": int"}, {"fullname": "clarena.backbones.mlp.MLP.batch_normalisation", "modulename": "clarena.backbones.mlp", "qualname": "MLP.batch_normalisation", "kind": "variable", "doc": "<p>Store whether to use batch normalisation after the fully-connected layers.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.backbones.mlp.MLP.activation", "modulename": "clarena.backbones.mlp", "qualname": "MLP.activation", "kind": "variable", "doc": "<p>Store whether to use activation function after the fully-connected layers.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.backbones.mlp.MLP.dropout", "modulename": "clarena.backbones.mlp", "qualname": "MLP.dropout", "kind": "variable", "doc": "<p>Store whether to use dropout after the fully-connected layers.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.backbones.mlp.MLP.fc", "modulename": "clarena.backbones.mlp", "qualname": "MLP.fc", "kind": "variable", "doc": "<p>The list of fully-connected (<code>nn.Linear</code>) layers.</p>\n", "annotation": ": torch.nn.modules.container.ModuleList"}, {"fullname": "clarena.backbones.mlp.MLP.forward", "modulename": "clarena.backbones.mlp", "qualname": "MLP.forward", "kind": "function", "doc": "<p>The forward pass for data. It is the same for all tasks.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): the input tensor from data.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>output_feature</strong> (<code>Tensor</code>): the output feature tensor to be passed into heads. This is the main target of backpropagation.</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): the hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name, value (<code>Tensor</code>) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.mlp.HATMaskMLP", "modulename": "clarena.backbones.mlp", "qualname": "HATMaskMLP", "kind": "class", "doc": "<p>HAT masked multi-Layer perceptron (MLP).</p>\n\n<p><a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task, 2018)</a> is an architecture-based continual learning approach that uses learnable hard attention masks to select the task-specific parameters.</p>\n\n<p>MLP is a dense network architecture, which has several fully-connected layers, each followed by an activation function. The last layer connects to the CL output heads.</p>\n\n<p>Mask is applied to the units which are neurons in each fully-connected layer. The mask is generated from the unit-wise task embedding and gate function.</p>\n", "bases": "MLP, clarena.backbones.base.HATMaskBackbone"}, {"fullname": "clarena.backbones.mlp.HATMaskMLP.__init__", "modulename": "clarena.backbones.mlp", "qualname": "HATMaskMLP.__init__", "kind": "function", "doc": "<p>Construct and initialise the HAT masked MLP backbone network with task embedding. Note that batch normalisation is incompatible with HAT mechanism.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_dim</strong> (<code>int</code>): the input dimension. Any data need to be flattened before going in MLP.</li>\n<li><strong>hidden_dims</strong> (<code>list[int]</code>): list of hidden layer dimensions. It can be empty list which means single-layer MLP, and it can be as many layers as you want. Note that it doesn't include the last dimension which we take as output dimension.</li>\n<li><strong>output_dim</strong> (<code>int</code>): the output dimension which connects to CL output heads.</li>\n<li><strong>gate</strong> (<code>str</code>): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:\n<ul>\n<li><code>sigmoid</code>: the sigmoid function.</li>\n</ul></li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code> | <code>None</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the linear layer. Default <code>True</code>.</li>\n<li><strong>dropout</strong> (<code>float</code> | <code>None</code>): the probability for the dropout layer, if <code>None</code> this layer won't be used. Default <code>None</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">hidden_dims</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">gate</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">dropout</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.backbones.mlp.HATMaskMLP.forward", "modulename": "clarena.backbones.mlp", "qualname": "HATMaskMLP.forward", "kind": "function", "doc": "<p>The forward pass for data from task <code>task_id</code>. Task-specific mask for <code>task_id</code> are applied to the units which are neurons in each fully-connected layer.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): the input tensor from data.</li>\n<li><strong>stage</strong> (<code>str</code>): the stage of the forward pass, should be one of the following:\n<ol>\n<li>'train': training stage.</li>\n<li>'validation': validation stage.</li>\n<li>'test': testing stage.</li>\n</ol></li>\n<li><strong>s_max</strong> (<code>float</code> | <code>None</code>): the maximum scaling factor in the gate function. Doesn't apply to testing stage. See chapter 2.4 \"Hard Attention Training\" in <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</li>\n<li><strong>batch_idx</strong> (<code>int</code> | <code>None</code>): the current batch index. Applies only to training stage. For other stages, it is default <code>None</code>.</li>\n<li><strong>num_batches</strong> (<code>int</code> | <code>None</code>): the total number of batches. Applies only to training stage. For other stages, it is default <code>None</code>.</li>\n<li><strong>test_mask</strong> (<code>dict[str, Tensor]</code> | <code>None</code>): the binary mask used for test. Applies only to testing stage. For other stages, it is default <code>None</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>output_feature</strong> (<code>Tensor</code>): the output feature tensor to be passed into heads. This is the main target of backpropagation.</li>\n<li><strong>mask</strong> (<code>dict[str, Tensor]</code>): the mask for the current task. Key (<code>str</code>) is layer name, value (<code>Tensor</code>) is the mask tensor. The mask tensor has size (number of units).</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): the hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name, value (<code>Tensor</code>) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes. Although HAT algorithm does not need this, it is still provided for API consistence for other HAT-based algorithms inherited this <code>forward()</code> method of <code>HAT</code> class.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">s_max</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">test_mask</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">],</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.resnet", "modulename": "clarena.backbones.resnet", "kind": "module", "doc": "<p>The submodule in <code>backbones</code> for ResNet backbone network.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBlockSmall", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockSmall", "kind": "class", "doc": "<p>The smaller building block for ResNet-18/34.</p>\n\n<p>It consists of 2 weight convolutional layers, each followed by an activation function. See Table 1 or Figure 5 (left) in the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a>.</p>\n", "bases": "clarena.backbones.base.CLBackbone"}, {"fullname": "clarena.backbones.resnet.ResNetBlockSmall.__init__", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockSmall.__init__", "kind": "function", "doc": "<p>Construct and initialise the smaller building block.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outer_layer_name</strong> (<code>str</code>): pass the name of the multi-building-block layer that contains this building block to construct the full name of each weighted convolutional layer.</li>\n<li><strong>block_idx</strong> (<code>int</code>): the index of the building blocks in the multi-building-block layer to construct the full name of each weighted convolutional layer.</li>\n<li><strong>preceding_output_channels</strong> (<code>int</code>): the number of channels of preceding output of this particular building block.</li>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block.</li>\n<li><strong>overall_stride</strong> (<code>int</code>): the overall stride of this building block. This stride is performed at 2nd (last) convolutional layer where the 1st convolutional layer remain stride of 1.</li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>batch_normalisation</strong> (<code>bool</code>): whether to use batch normalisation after the weight convolutional layers. Default <code>True</code>, same as what the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a> does.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>, because batch normalisation are doing the similar thing with bias.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">outer_layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">block_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">preceding_output_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">overall_stride</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">batch_normalisation</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.ResNetBlockSmall.batch_normalisation", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockSmall.batch_normalisation", "kind": "variable", "doc": "<p>Store whether to use batch normalisation after the fully-connected layers.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.backbones.resnet.ResNetBlockSmall.activation", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockSmall.activation", "kind": "variable", "doc": "<p>Store whether to use activation function after the fully-connected layers.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.backbones.resnet.ResNetBlockSmall.full_1st_layer_name", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockSmall.full_1st_layer_name", "kind": "variable", "doc": "<p>Format and store full name of the 1st weighted convolutional layer.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBlockSmall.full_2nd_layer_name", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockSmall.full_2nd_layer_name", "kind": "variable", "doc": "<p>Format and store full name of the 2nd weighted convolutional layer.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBlockSmall.conv1", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockSmall.conv1", "kind": "variable", "doc": "<p>The 1st weight convolutional layer of the smaller building block.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBlockSmall.conv2", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockSmall.conv2", "kind": "variable", "doc": "<p>The 2nd weight convolutional layer of the smaller building block.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBlockSmall.identity_downsample", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockSmall.identity_downsample", "kind": "variable", "doc": "<p>The convolutional layer for downsampling identity in the shortcut connection if the dimension of identity from input doesn't match the output's. This case only happens when the number of input channels doesn't equal to the number of preceding output channels or a layer with stride &gt; 1 exists.</p>\n", "annotation": ": torch.nn.modules.module.Module"}, {"fullname": "clarena.backbones.resnet.ResNetBlockSmall.forward", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockSmall.forward", "kind": "function", "doc": "<p>The forward pass for data.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): the input feature maps.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>output_feature</strong> (<code>Tensor</code>): the output feature maps.</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): the hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name, value (<code>Tensor</code>) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.resnet.ResNetBlockLarge", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockLarge", "kind": "class", "doc": "<p>The larger building block for ResNet-50/101/152. It is referred to \"bottleneck\" building block in the paper.</p>\n\n<p>It consists of 3 weight convolutional layers, each followed by an activation function. See Table 1 or Figure 5 (right) in the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a>.</p>\n", "bases": "clarena.backbones.base.CLBackbone"}, {"fullname": "clarena.backbones.resnet.ResNetBlockLarge.__init__", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockLarge.__init__", "kind": "function", "doc": "<p>Construct and initialise the larger building block.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outer_layer_name</strong> (<code>str</code>): pass the name of the multi-building-block layer that contains this building block to construct the full name of each weighted convolutional layer.</li>\n<li><strong>block_idx</strong> (<code>int</code>): the index of the building blocks in the multi-building-block layer to construct the full name of each weighted convolutional layer.</li>\n<li><strong>preceding_output_channels</strong> (<code>int</code>): the number of channels of preceding output of this particular building block.</li>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block.</li>\n<li><strong>overall_stride</strong> (<code>int</code>): the overall stride of this building block. This stride is performed at 2nd (middle) convolutional layer where 1st and 3rd convolutional layers remain stride of 1.</li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>batch_normalisation</strong> (<code>bool</code>): whether to use batch normalisation after the weight convolutional layers. Default <code>True</code>, same as what the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a> does.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>, because batch normalisation are doing the similar thing with bias.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">outer_layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">block_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">preceding_output_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">overall_stride</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">batch_normalisation</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.ResNetBlockLarge.batch_normalisation", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockLarge.batch_normalisation", "kind": "variable", "doc": "<p>Store whether to use batch normalisation after the fully-connected layers.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.backbones.resnet.ResNetBlockLarge.activation", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockLarge.activation", "kind": "variable", "doc": "<p>Store whether to use activation function after the fully-connected layers.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.backbones.resnet.ResNetBlockLarge.full_1st_layer_name", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockLarge.full_1st_layer_name", "kind": "variable", "doc": "<p>Format and store full name of the 1st weighted convolutional layer.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBlockLarge.full_2nd_layer_name", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockLarge.full_2nd_layer_name", "kind": "variable", "doc": "<p>Format and store full name of the 2nd weighted convolutional layer.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBlockLarge.full_3rd_layer_name", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockLarge.full_3rd_layer_name", "kind": "variable", "doc": "<p>Format and store full name of the 3rd weighted convolutional layer.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBlockLarge.conv1", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockLarge.conv1", "kind": "variable", "doc": "<p>The 1st weight convolutional layer of the larger building block.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBlockLarge.conv2", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockLarge.conv2", "kind": "variable", "doc": "<p>The 2nd weight convolutional layer of the larger building block.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBlockLarge.conv3", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockLarge.conv3", "kind": "variable", "doc": "<p>The 3rd weight convolutional layer of the larger building block.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBlockLarge.identity_downsample", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockLarge.identity_downsample", "kind": "variable", "doc": "<p>The convolutional layer for downsampling identity in the shortcut connection if the dimension of identity from input doesn't match the output's. This case only happens when the number of input channels doesn't equal to the number of preceding output channels or a layer with stride &gt; 1 exists.</p>\n", "annotation": ": torch.nn.modules.module.Module"}, {"fullname": "clarena.backbones.resnet.ResNetBlockLarge.forward", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockLarge.forward", "kind": "function", "doc": "<p>The forward pass for data.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): the input feature maps.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>output_feature</strong> (<code>Tensor</code>): the output feature maps.</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): the hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name, value (<code>Tensor</code>) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.resnet.ResNetBase", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBase", "kind": "class", "doc": "<p>The base class of <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">residual network (ResNet)</a>.</p>\n\n<p>ResNet is a convolutional network architecture, which has 1st convolutional parameter layer and a maxpooling layer, connecting to 4 convolutional layers which contains multiple convolutional parameter layer. Each layer of the 4 are constructed from basic building blocks which are either small (<code>ResNetBlockSmall</code>) or large (<code>ResNetBlockLarge</code>). Each building block contains several convolutional parameter layers. The building blocks are connected by a skip connection which is a direct connection from the input of the block to the output of the block, and this is why it's called residual (find \"shortcut connections\" in the paper for more details). After the 5th convolutional layer, there are average pooling layer and a fully connected layer which connects to the CL output heads.</p>\n", "bases": "clarena.backbones.base.CLBackbone"}, {"fullname": "clarena.backbones.resnet.ResNetBase.__init__", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBase.__init__", "kind": "function", "doc": "<p>Construct and initialise the ResNet backbone network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input. Image data are kept channels when going in ResNet. Note that convolutional networks require number of input channels instead of dimension.</li>\n<li><strong>building_block_type</strong> (<code>ResNetBlockSmall</code> | <code>ResNetBlockLarge</code>): the type of building block used in the ResNet.</li>\n<li><strong>building_block_nums</strong> (<code>tuple[int, int, int, int]</code>): the number of building blocks in the 2-5 convolutional layer correspondingly.</li>\n<li><strong>building_block_preceding_output_channels</strong> (<code>tuple[int, int, int, int]</code>): the number of channels of preceding output of each building block in the 2-5 convolutional layer correspondingly.</li>\n<li><strong>building_block_input_channels</strong> (<code>tuple[int, int, int, int]</code>): the number of channels of input of each building block in the 2-5 convolutional layer correspondingly.</li>\n<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>batch_normalisation</strong> (<code>bool</code>): whether to use batch normalisation after the weight convolutional layers. Default <code>True</code>, same as what the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a> does.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>, because batch normalisation are doing the similar thing with bias.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">building_block_type</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">resnet</span><span class=\"o\">.</span><span class=\"n\">ResNetBlockSmall</span> <span class=\"o\">|</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">resnet</span><span class=\"o\">.</span><span class=\"n\">ResNetBlockLarge</span>,</span><span class=\"param\">\t<span class=\"n\">building_block_nums</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">building_block_preceding_output_channels</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">building_block_input_channels</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">batch_normalisation</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.ResNetBase.batch_normalisation", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBase.batch_normalisation", "kind": "variable", "doc": "<p>Store whether to use batch normalisation after the fully-connected layers.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.backbones.resnet.ResNetBase.activation", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBase.activation", "kind": "variable", "doc": "<p>Store whether to use activation function after the fully-connected layers.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.backbones.resnet.ResNetBase.conv1", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBase.conv1", "kind": "variable", "doc": "<p>The 1st weight convolutional layer of the entire ResNet. It  is always with fixed kernel size 7x7, stride 2, and padding 3.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBase.maxpool", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBase.maxpool", "kind": "variable", "doc": "<p>The max pooling layer which is laid in between 1st and 2nd convolutional layers with kernel size 3x3, stride 2.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBase.conv2x", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBase.conv2x", "kind": "variable", "doc": "<p>The 2nd convolutional layer of the ResNet, which contains multiple blocks.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBase.conv3x", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBase.conv3x", "kind": "variable", "doc": "<p>The 3rd convolutional layer of the ResNet, which contains multiple blocks.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBase.conv4x", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBase.conv4x", "kind": "variable", "doc": "<p>The 4th convolutional layer of the ResNet, which contains multiple blocks.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBase.conv5x", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBase.conv5x", "kind": "variable", "doc": "<p>The 5th convolutional layer of the ResNet, which contains multiple blocks.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBase.avepool", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBase.avepool", "kind": "variable", "doc": "<p>The average pooling layer which is laid after the convolutional layers and before feature maps are flattened.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBase.forward", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBase.forward", "kind": "function", "doc": "<p>The forward pass for data. It is the same for all tasks.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): the input tensor from data.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>output_feature</strong> (<code>Tensor</code>): the output feature tensor to be passed into heads. This is the main target of backpropagation.</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): the hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name, value (<code>Tensor</code>) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.resnet.ResNet18", "modulename": "clarena.backbones.resnet", "qualname": "ResNet18", "kind": "class", "doc": "<p>ResNet-18 backbone network.</p>\n\n<p>This is a smaller architecture proposed in the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a>. It consists of 18 weight convolutional layers in total. See Table 1 in the paper for details.</p>\n", "bases": "ResNetBase"}, {"fullname": "clarena.backbones.resnet.ResNet18.__init__", "modulename": "clarena.backbones.resnet", "qualname": "ResNet18.__init__", "kind": "function", "doc": "<p>Construct and initialise the ResNet-18 backbone network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</li>\n<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>batch_normalisation</strong> (<code>bool</code>): whether to use batch normalisation after the weight convolutional layers. Default <code>True</code>, same as what the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a> does.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>, because batch normalisation are doing the similar thing with bias.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">batch_normalisation</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.ResNet34", "modulename": "clarena.backbones.resnet", "qualname": "ResNet34", "kind": "class", "doc": "<p>ResNet-34 backbone network.</p>\n\n<p>This is a smaller architecture proposed in the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a>. It consists of 34 weight convolutional layers in total. See Table 1 in the paper for details.</p>\n", "bases": "ResNetBase"}, {"fullname": "clarena.backbones.resnet.ResNet34.__init__", "modulename": "clarena.backbones.resnet", "qualname": "ResNet34.__init__", "kind": "function", "doc": "<p>Construct and initialise the ResNet-34 backbone network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</li>\n<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>batch_normalisation</strong> (<code>bool</code>): whether to use batch normalisation after the weight convolutional layers. Default <code>True</code>, same as what the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a> does.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>, because batch normalisation are doing the similar thing with bias.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">batch_normalisation</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.ResNet50", "modulename": "clarena.backbones.resnet", "qualname": "ResNet50", "kind": "class", "doc": "<p>ResNet-50 backbone network.</p>\n\n<p>This is a larger architecture proposed in the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a>. It consists of 50 weight convolutional layers in total. See Table 1 in the paper for details.</p>\n", "bases": "ResNetBase"}, {"fullname": "clarena.backbones.resnet.ResNet50.__init__", "modulename": "clarena.backbones.resnet", "qualname": "ResNet50.__init__", "kind": "function", "doc": "<p>Construct and initialise the ResNet-50 backbone network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</li>\n<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>batch_normalisation</strong> (<code>bool</code>): whether to use batch normalisation after the weight convolutional layers. Default <code>True</code>, same as what the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a> does.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>, because batch normalisation are doing the similar thing with bias.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">batch_normalisation</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.ResNet101", "modulename": "clarena.backbones.resnet", "qualname": "ResNet101", "kind": "class", "doc": "<p>ResNet-101 backbone network.</p>\n\n<p>This is a larger architecture proposed in the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a>. It consists of 101 weight convolutional layers in total. See Table 1 in the paper for details.</p>\n", "bases": "ResNetBase"}, {"fullname": "clarena.backbones.resnet.ResNet101.__init__", "modulename": "clarena.backbones.resnet", "qualname": "ResNet101.__init__", "kind": "function", "doc": "<p>Construct and initialise the ResNet-101 backbone network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</li>\n<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>batch_normalisation</strong> (<code>bool</code>): whether to use batch normalisation after the weight convolutional layers. Default <code>True</code>, same as what the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a> does.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>, because batch normalisation are doing the similar thing with bias.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">batch_normalisation</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.ResNet152", "modulename": "clarena.backbones.resnet", "qualname": "ResNet152", "kind": "class", "doc": "<p>ResNet-152 backbone network.</p>\n\n<p>This is the largest architecture proposed in the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a>. It consists of 152 weight convolutional layers in total. See Table 1 in the paper for details.</p>\n", "bases": "ResNetBase"}, {"fullname": "clarena.backbones.resnet.ResNet152.__init__", "modulename": "clarena.backbones.resnet", "qualname": "ResNet152.__init__", "kind": "function", "doc": "<p>Construct and initialise the ResNet-50 backbone network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</li>\n<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>batch_normalisation</strong> (<code>bool</code>): whether to use batch normalisation after the weight convolutional layers. Default <code>True</code>, same as what the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a> does.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>, because batch normalisation are doing the similar thing with bias.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">batch_normalisation</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.HATMaskResNetBlockSmall", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNetBlockSmall", "kind": "class", "doc": "<p>The smaller building block for HAT masked ResNet-18/34.</p>\n\n<p>It consists of 2 weight convolutional layers, each followed by an activation function. See Table 1 or Figure 5 (left) in the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a>.</p>\n\n<p>Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the unit-wise task embedding and gate function.</p>\n", "bases": "clarena.backbones.base.HATMaskBackbone, ResNetBlockSmall"}, {"fullname": "clarena.backbones.resnet.HATMaskResNetBlockSmall.__init__", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNetBlockSmall.__init__", "kind": "function", "doc": "<p>Construct and initialise the smaller building block with task embedding.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outer_layer_name</strong> (<code>str</code>): pass the name of the multi-building-block layer that contains this building block to construct the full name of each weighted convolutional layer.</li>\n<li><strong>block_idx</strong> (<code>int</code>): the index of the building blocks in the multi-building-block layer to construct the full name of each weighted convolutional layer.</li>\n<li><strong>preceding_output_channels</strong> (<code>int</code>): the number of channels of preceding output of this particular building block.</li>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block.</li>\n<li><strong>overall_stride</strong> (<code>int</code>): the overall stride of this building block. This stride is performed at 2nd (last) convolutional layer where the 1st convolutional layer remain stride of 1.</li>\n<li><strong>gate</strong> (<code>str</code>): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:\n<ul>\n<li><code>sigmoid</code>: the sigmoid function.</li>\n</ul></li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">outer_layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">block_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">preceding_output_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">overall_stride</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">gate</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.HATMaskResNetBlockSmall.forward", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNetBlockSmall.forward", "kind": "function", "doc": "<p>The forward pass for data from task <code>task_id</code>. Task-specific mask for <code>task_id</code> are applied to the units which are channels in each weighted convolutional layer.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): The input tensor from data.</li>\n<li><strong>stage</strong> (<code>str</code>): the stage of the forward pass, should be one of the following:\n<ol>\n<li>'train': training stage.</li>\n<li>'validation': validation stage.</li>\n<li>'test': testing stage.</li>\n</ol></li>\n<li><strong>s_max</strong> (<code>float</code> | <code>None</code>): the maximum scaling factor in the gate function. Doesn't apply to testing stage. See chapter 2.4 \"Hard Attention Training\" in <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</li>\n<li><strong>batch_idx</strong> (<code>int</code> | <code>None</code>): the current batch index. Applies only to training stage. For other stages, it is default <code>None</code>.</li>\n<li><strong>num_batches</strong> (<code>int</code> | <code>None</code>): the total number of batches. Applies only to training stage. For other stages, it is default <code>None</code>.</li>\n<li><strong>test_mask</strong> (<code>dict[str, Tensor]</code> | <code>None</code>): the binary mask used for test. Applies only to testing stage. For other stages, it is default <code>None</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>output_feature</strong> (<code>Tensor</code>): the output feature maps.</li>\n<li><strong>mask</strong> (<code>dict[str, Tensor]</code>): the mask for the current task. Key (<code>str</code>) is layer name, value (<code>Tensor</code>) is the mask tensor. The mask tensor has size (number of units).</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): the hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name, value (<code>Tensor</code>) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes. Although HAT algorithm does not need this, it is still provided for API consistence for other HAT-based algorithms inherited this <code>forward()</code> method of <code>HAT</code> class.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">s_max</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">test_mask</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">],</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.resnet.HATMaskResNetBlockLarge", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNetBlockLarge", "kind": "class", "doc": "<p>The larger building block for ResNet-50/101/152. It is referred to \"bottleneck\" building block in the ResNet paper.</p>\n\n<p>It consists of 3 weight convolutional layers, each followed by an activation function. See Table 1 or Figure 5 (right) in the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a>.</p>\n\n<p>Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the unit-wise task embedding and gate function.</p>\n", "bases": "clarena.backbones.base.HATMaskBackbone, ResNetBlockLarge"}, {"fullname": "clarena.backbones.resnet.HATMaskResNetBlockLarge.__init__", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNetBlockLarge.__init__", "kind": "function", "doc": "<p>Construct and initialise the larger building block with task embedding.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outer_layer_name</strong> (<code>str</code>): pass the name of the multi-building-block layer that contains this building block to construct the full name of each weighted convolutional layer.</li>\n<li><strong>block_idx</strong> (<code>int</code>): the index of the building blocks in the multi-building-block layer to construct the full name of each weighted convolutional layer.</li>\n<li><strong>preceding_output_channels</strong> (<code>int</code>): the number of channels of preceding output of this particular building block.</li>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block.</li>\n<li><strong>overall_stride</strong> (<code>int</code>): the overall stride of this building block. This stride is performed at 2nd (middle) convolutional layer where 1st and 3rd convolutional layers remain stride of 1.</li>\n<li><strong>gate</strong> (<code>str</code>): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:\n<ul>\n<li><code>sigmoid</code>: the sigmoid function.</li>\n</ul></li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">outer_layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">block_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">preceding_output_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">overall_stride</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">gate</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.HATMaskResNetBlockLarge.forward", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNetBlockLarge.forward", "kind": "function", "doc": "<p>The forward pass for data from task <code>task_id</code>. Task-specific mask for <code>task_id</code> are applied to the units which are channels in each weighted convolutional layer.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): The input tensor from data.</li>\n<li><strong>stage</strong> (<code>str</code>): the stage of the forward pass, should be one of the following:\n<ol>\n<li>'train': training stage.</li>\n<li>'validation': validation stage.</li>\n<li>'test': testing stage.</li>\n</ol></li>\n<li><strong>s_max</strong> (<code>float</code> | <code>None</code>): the maximum scaling factor in the gate function. Doesn't apply to testing stage. See chapter 2.4 \"Hard Attention Training\" in <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</li>\n<li><strong>batch_idx</strong> (<code>int</code> | <code>None</code>): the current batch index. Applies only to training stage. For other stages, it is default <code>None</code>.</li>\n<li><strong>num_batches</strong> (<code>int</code> | <code>None</code>): the total number of batches. Applies only to training stage. For other stages, it is default <code>None</code>.</li>\n<li><strong>test_mask</strong> (<code>dict[str, Tensor]</code> | <code>None</code>): the binary mask used for test. Applies only to testing stage. For other stages, it is default <code>None</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>output_feature</strong> (<code>Tensor</code>): the output feature maps.</li>\n<li><strong>mask</strong> (<code>dict[str, Tensor]</code>): the mask for the current task. Key (<code>str</code>) is layer name, value (<code>Tensor</code>) is the mask tensor. The mask tensor has size (number of units).</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): the hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name, value (<code>Tensor</code>) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes. Although HAT algorithm does not need this, it is still provided for API consistence for other HAT-based algorithms inherited this <code>forward()</code> method of <code>HAT</code> class.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">s_max</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">test_mask</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">],</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.resnet.HATMaskResNetBase", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNetBase", "kind": "class", "doc": "<p>The base class of HAT masked <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">residual network (ResNet)</a>.</p>\n\n<p><a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task, 2018)</a> is an architecture-based continual learning approach that uses learnable hard attention masks to select the task-specific parameters.</p>\n\n<p>ResNet is a convolutional network architecture, which has 1st convolutional parameter layer and a maxpooling layer, connecting to 4 convolutional layers which contains multiple convolutional parameter layer. Each layer of the 4 are constructed from basic building blocks which are either small (<code>ResNetBlockSmall</code>) or large (<code>ResNetBlockLarge</code>). Each building block contains several convolutional parameter layers. The building blocks are connected by a skip connection which is a direct connection from the input of the block to the output of the block, and this is why it's called residual (find \"shortcut connections\" in the paper for more details). After the 5th convolutional layer, there are average pooling layer and a fully connected layer which connects to the CL output heads.</p>\n\n<p>Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the unit-wise task embedding and gate function.</p>\n", "bases": "ResNetBase, clarena.backbones.base.HATMaskBackbone"}, {"fullname": "clarena.backbones.resnet.HATMaskResNetBase.__init__", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNetBase.__init__", "kind": "function", "doc": "<p>Construct and initialise the HAT masked ResNet backbone network with task embedding. Note that batch normalisation is incompatible with HAT mechanism.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input. Image data are kept channels when going in ResNet. Note that convolutional networks require number of input channels instead of dimension.</li>\n<li><strong>building_block_type</strong> (<code>HATMaskResNetBlockSmall</code> | <code>HATMaskResNetBlockLarge</code>): the type of building block used in the ResNet.</li>\n<li><strong>building_block_nums</strong> (<code>tuple[int, int, int, int]</code>): the number of building blocks in the 2-5 convolutional layer correspondingly.</li>\n<li><strong>building_block_preceding_output_channels</strong> (<code>tuple[int, int, int, int]</code>): the number of channels of preceding output of each building block in the 2-5 convolutional layer correspondingly.</li>\n<li><strong>building_block_input_channels</strong> (<code>tuple[int, int, int, int]</code>): the number of channels of input of each building block in the 2-5 convolutional layer correspondingly.</li>\n<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>\n<li><strong>gate</strong> (<code>str</code>): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:\n<ul>\n<li><code>sigmoid</code>: the sigmoid function.</li>\n</ul></li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>, because batch normalisation are doing the similar thing with bias.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">building_block_type</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">resnet</span><span class=\"o\">.</span><span class=\"n\">HATMaskResNetBlockSmall</span> <span class=\"o\">|</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">resnet</span><span class=\"o\">.</span><span class=\"n\">HATMaskResNetBlockLarge</span>,</span><span class=\"param\">\t<span class=\"n\">building_block_nums</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">building_block_preceding_output_channels</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">building_block_input_channels</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">gate</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.HATMaskResNetBase.update_multiple_blocks_task_embedding", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNetBase.update_multiple_blocks_task_embedding", "kind": "function", "doc": "<p>Collect the task embeddings in the multiple building blocks (2-5 convolutional layers) and sync to the weighted layer names list in the outer network.</p>\n\n<p>This should only be called explicitly after the <code>__init__()</code> method, just because task embedding as <code>nn.Module</code> instance was wiped out at the beginning of it.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.resnet.HATMaskResNetBase.forward", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNetBase.forward", "kind": "function", "doc": "<p>The forward pass for data from task <code>task_id</code>. Task-specific mask for <code>task_id</code> are applied to the units which are channels in each weighted convolutional layer.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): the input tensor from data.</li>\n<li><strong>stage</strong> (<code>str</code>): the stage of the forward pass, should be one of the following:\n<ol>\n<li>'train': training stage.</li>\n<li>'validation': validation stage.</li>\n<li>'test': testing stage.</li>\n</ol></li>\n<li><strong>s_max</strong> (<code>float</code> | <code>None</code>): the maximum scaling factor in the gate function. Doesn't apply to testing stage. See chapter 2.4 \"Hard Attention Training\" in <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</li>\n<li><strong>batch_idx</strong> (<code>int</code> | <code>None</code>): the current batch index. Applies only to training stage. For other stages, it is default <code>None</code>.</li>\n<li><strong>num_batches</strong> (<code>int</code> | <code>None</code>): the total number of batches. Applies only to training stage. For other stages, it is default <code>None</code>.</li>\n<li><strong>test_mask</strong> (<code>dict[str, Tensor]</code> | <code>None</code>): the binary mask used for test. Applies only to testing stage. For other stages, it is default <code>None</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>output_feature</strong> (<code>Tensor</code>): the output feature tensor to be passed to the heads.</li>\n<li><strong>mask</strong> (<code>dict[str, Tensor]</code>): the mask for the current task. Key (<code>str</code>) is layer name, value (<code>Tensor</code>) is the mask tensor. The mask tensor has size (number of units).</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): the hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name, value (<code>Tensor</code>) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes. Although HAT algorithm does not need this, it is still provided for API consistence for other HAT-based algorithms inherited this <code>forward()</code> method of <code>HAT</code> class.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">s_max</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">test_mask</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">],</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.resnet.HATMaskResNet18", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNet18", "kind": "class", "doc": "<p>HAT masked ResNet-18 backbone network.</p>\n\n<p><a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task, 2018)</a> is an architecture-based continual learning approach that uses learnable hard attention masks to select the task-specific parameters.</p>\n\n<p>ResNet-18 is a smaller architecture proposed in the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a>. It consists of 18 weight convolutional layers in total. See Table 1 in the paper for details.</p>\n\n<p>Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the unit-wise task embedding and gate function.</p>\n", "bases": "HATMaskResNetBase"}, {"fullname": "clarena.backbones.resnet.HATMaskResNet18.__init__", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNet18.__init__", "kind": "function", "doc": "<p>Construct and initialise the ResNet-18 backbone network with task embedding. Note that batch normalisation is incompatible with HAT mechanism.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</li>\n<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>\n<li><strong>gate</strong> (<code>str</code>): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:\n<ul>\n<li><code>sigmoid</code>: the sigmoid function.</li>\n</ul></li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">gate</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.HATMaskResNet34", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNet34", "kind": "class", "doc": "<p>HAT masked ResNet-34 backbone network.</p>\n\n<p><a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task, 2018)</a> is an architecture-based continual learning approach that uses learnable hard attention masks to select the task-specific parameters.</p>\n\n<p>ResNet-34 is a smaller architecture proposed in the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a>. It consists of 34 weight convolutional layers in total. See Table 1 in the paper for details.</p>\n\n<p>Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the unit-wise task embedding and gate function.</p>\n", "bases": "HATMaskResNetBase"}, {"fullname": "clarena.backbones.resnet.HATMaskResNet34.__init__", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNet34.__init__", "kind": "function", "doc": "<p>Construct and initialise the ResNet-34 backbone network with task embedding. Note that batch normalisation is incompatible with HAT mechanism.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</li>\n<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>\n<li><strong>gate</strong> (<code>str</code>): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:\n<ul>\n<li><code>sigmoid</code>: the sigmoid function.</li>\n</ul></li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">gate</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.HATMaskResNet50", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNet50", "kind": "class", "doc": "<p>HAT masked ResNet-50 backbone network.</p>\n\n<p><a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task, 2018)</a> is an architecture-based continual learning approach that uses learnable hard attention masks to select the task-specific parameters.</p>\n\n<p>ResNet-50 is a larger architecture proposed in the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a>. It consists of 50 weight convolutional layers in total. See Table 1 in the paper for details.</p>\n\n<p>Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the unit-wise task embedding and gate function.</p>\n", "bases": "HATMaskResNetBase"}, {"fullname": "clarena.backbones.resnet.HATMaskResNet50.__init__", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNet50.__init__", "kind": "function", "doc": "<p>Construct and initialise the ResNet-50 backbone network with task embedding. Note that batch normalisation is incompatible with HAT mechanism.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</li>\n<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>\n<li><strong>gate</strong> (<code>str</code>): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:\n<ul>\n<li><code>sigmoid</code>: the sigmoid function.</li>\n</ul></li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">gate</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.HATMaskResNet101", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNet101", "kind": "class", "doc": "<p>HAT masked ResNet-101 backbone network.</p>\n\n<p><a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task, 2018)</a> is an architecture-based continual learning approach that uses learnable hard attention masks to select the task-specific parameters.</p>\n\n<p>ResNet-101 is a larger architecture proposed in the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a>. It consists of 101 weight convolutional layers in total. See Table 1 in the paper for details.</p>\n\n<p>Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the unit-wise task embedding and gate function.</p>\n", "bases": "HATMaskResNetBase"}, {"fullname": "clarena.backbones.resnet.HATMaskResNet101.__init__", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNet101.__init__", "kind": "function", "doc": "<p>Construct and initialise the ResNet-101 backbone network with task embedding. Note that batch normalisation is incompatible with HAT mechanism.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</li>\n<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>\n<li><strong>gate</strong> (<code>str</code>): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:\n<ul>\n<li><code>sigmoid</code>: the sigmoid function.</li>\n</ul></li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">gate</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.HATMaskResNet152", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNet152", "kind": "class", "doc": "<p>HAT masked ResNet-152 backbone network.</p>\n\n<p><a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task, 2018)</a> is an architecture-based continual learning approach that uses learnable hard attention masks to select the task-specific parameters.</p>\n\n<p>ResNet-152 is the largest architecture proposed in the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a>. It consists of 152 weight convolutional layers in total. See Table 1 in the paper for details.</p>\n\n<p>Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the unit-wise task embedding and gate function.</p>\n", "bases": "HATMaskResNetBase"}, {"fullname": "clarena.backbones.resnet.HATMaskResNet152.__init__", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNet152.__init__", "kind": "function", "doc": "<p>Construct and initialise the ResNet-152 backbone network with task embedding. Note that batch normalisation is incompatible with HAT mechanism.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</li>\n<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>\n<li><strong>gate</strong> (<code>str</code>): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:\n<ul>\n<li><code>sigmoid</code>: the sigmoid function.</li>\n</ul></li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">gate</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.callbacks", "modulename": "clarena.callbacks", "kind": "module", "doc": "<h1 id=\"callbacks\">Callbacks</h1>\n\n<p>This submodule provides <strong>callbacks</strong> that can be used in CLArena. </p>\n\n<p>Please note that this is an API documantation. Please refer to the main documentation pages for more information about the callbacks and how to configure and implement them:</p>\n\n<ul>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/configure-your-experiment/callbacks\"><strong>Configure Callbacks</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/implement-your-cl-modules/callback\"><strong>Implement Your Callbacks</strong></a></li>\n</ul>\n\n<p>The callbacks are implemented as subclasses of <code>lightning.Callback</code>.</p>\n"}, {"fullname": "clarena.callbacks.cl_metrics", "modulename": "clarena.callbacks.cl_metrics", "kind": "module", "doc": "<p>The submodule in <code>callbacks</code> for <code>CLMetricsCallback</code>.</p>\n"}, {"fullname": "clarena.callbacks.cl_metrics.CLMetricsCallback", "modulename": "clarena.callbacks.cl_metrics", "qualname": "CLMetricsCallback", "kind": "class", "doc": "<p>Provides all actions that are related to CL metrics, which include:</p>\n\n<ul>\n<li>Defining, initialising and recording metrics.</li>\n<li>Logging training and validation metrics to Lightning loggers in real time.</li>\n<li>Saving test metrics to files.</li>\n<li>Visualising test metrics as plots.</li>\n</ul>\n\n<p>Please refer to the <a href=\"https://pengxiang-wang.com/posts/continual-learning-metrics\">A Summary of Continual Learning Metrics</a> to learn what continual learning metrics mean.</p>\n\n<p>Lightning provides <code>self.log()</code> to log metrics in <code>LightningModule</code> where our <code>CLAlgorithm</code> based. You can put <code>self.log()</code> here if you don't want to mess up the <code>CLAlgorithm</code> with a huge amount of logging codes.</p>\n\n<p>The callback is able to produce the following outputs:</p>\n\n<ul>\n<li>CSV files for test accuracy and classification loss (lower triangular) matrix, average accuracy and classification loss. See <a href=\"https://pengxiang-wang.com/posts/continual-learning-metrics.html#sec-test-performance-of-previous-tasks\">here</a> for details.</li>\n<li>Coloured plot for test accuracy and classification loss (lower triangular) matrix. See <a href=\"https://pengxiang-wang.com/posts/continual-learning-metrics.html#sec-test-performance-of-previous-tasks\">here</a> for details.</li>\n<li>Curve plots for test average accuracy and classification loss over different training tasks. See <a href=\"https://pengxiang-wang.com/posts/continual-learning-metrics.html#sec-average-test-performance-over-tasks\">here</a> for details.</li>\n</ul>\n", "bases": "lightning.pytorch.callbacks.callback.Callback"}, {"fullname": "clarena.callbacks.cl_metrics.CLMetricsCallback.__init__", "modulename": "clarena.callbacks.cl_metrics", "qualname": "CLMetricsCallback.__init__", "kind": "function", "doc": "<p>Initialise the <code>CLMetricsCallback</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>save_dir</strong> (<code>str</code> | <code>None</code>): the directory to save the test metrics files and plots. Better inside the output folder.</li>\n<li><strong>test_acc_csv_name</strong> (<code>str</code> | <code>None</code>): file name to save test accuracy matrix and average accuracy as CSV file. If <code>None</code>, no file will be saved.</li>\n<li><strong>test_loss_cls_csv_name</strong>(<code>str</code> | <code>None</code>): file name to save classification loss matrix and average classification loss as CSV file. If <code>None</code>, no file will be saved.</li>\n<li><strong>test_acc_matrix_plot_name</strong> (<code>str</code> | <code>None</code>): file name to save accuracy matrix plot. If <code>None</code>, no file will be saved.</li>\n<li><strong>test_loss_cls_matrix_plot_name</strong> (<code>str</code> | <code>None</code>): file name to save classification loss matrix plot. If <code>None</code>, no file will be saved.</li>\n<li><strong>test_ave_acc_plot_name</strong> (<code>str</code> | <code>None</code>): file name to save average accuracy as curve plot over different training tasks. If <code>None</code>, no file will be saved.</li>\n<li><strong>test_ave_loss_cls_plot_name</strong> (<code>str</code> | <code>None</code>): file name to save average classification loss as curve plot over different training tasks. If <code>None</code>, no file will be saved.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">save_dir</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">test_acc_csv_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">test_loss_cls_csv_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">test_acc_matrix_plot_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">test_loss_cls_matrix_plot_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">test_ave_acc_plot_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">test_ave_loss_cls_plot_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.callbacks.cl_metrics.CLMetricsCallback.test_acc_csv_path", "modulename": "clarena.callbacks.cl_metrics", "qualname": "CLMetricsCallback.test_acc_csv_path", "kind": "variable", "doc": "<p>Store the path to save test accuracy matrix and average accuracy CSV file.</p>\n", "annotation": ": str"}, {"fullname": "clarena.callbacks.cl_metrics.CLMetricsCallback.test_loss_cls_csv_path", "modulename": "clarena.callbacks.cl_metrics", "qualname": "CLMetricsCallback.test_loss_cls_csv_path", "kind": "variable", "doc": "<p>Store the path to save test classification loss and average accuracy CSV file.</p>\n", "annotation": ": str"}, {"fullname": "clarena.callbacks.cl_metrics.CLMetricsCallback.test_acc_matrix_plot_path", "modulename": "clarena.callbacks.cl_metrics", "qualname": "CLMetricsCallback.test_acc_matrix_plot_path", "kind": "variable", "doc": "<p>Store the path to save test accuracy matrix plot.</p>\n", "annotation": ": str"}, {"fullname": "clarena.callbacks.cl_metrics.CLMetricsCallback.test_loss_cls_matrix_plot_path", "modulename": "clarena.callbacks.cl_metrics", "qualname": "CLMetricsCallback.test_loss_cls_matrix_plot_path", "kind": "variable", "doc": "<p>Store the path to save test classification loss matrix plot.</p>\n", "annotation": ": str"}, {"fullname": "clarena.callbacks.cl_metrics.CLMetricsCallback.test_ave_acc_plot_path", "modulename": "clarena.callbacks.cl_metrics", "qualname": "CLMetricsCallback.test_ave_acc_plot_path", "kind": "variable", "doc": "<p>Store the path to save test average accuracy curve plot.</p>\n", "annotation": ": str"}, {"fullname": "clarena.callbacks.cl_metrics.CLMetricsCallback.test_ave_loss_cls_plot_path", "modulename": "clarena.callbacks.cl_metrics", "qualname": "CLMetricsCallback.test_ave_loss_cls_plot_path", "kind": "variable", "doc": "<p>Store the path to save test average classification loss curve plot.</p>\n", "annotation": ": str"}, {"fullname": "clarena.callbacks.cl_metrics.CLMetricsCallback.acc_training_epoch", "modulename": "clarena.callbacks.cl_metrics", "qualname": "CLMetricsCallback.acc_training_epoch", "kind": "variable", "doc": "<p>Classification accuracy of training epoch. Accumulated and calculated from the training batches. See <a href=\"https://pengxiang-wang.com/posts/continual-learning-metrics.html#sec-performance-of-training-epoch\">here</a> for details.</p>\n", "annotation": ": clarena.utils.metrics.MeanMetricBatch"}, {"fullname": "clarena.callbacks.cl_metrics.CLMetricsCallback.loss_cls_training_epoch", "modulename": "clarena.callbacks.cl_metrics", "qualname": "CLMetricsCallback.loss_cls_training_epoch", "kind": "variable", "doc": "<p>Classification loss of training epoch. Accumulated and calculated from the training batches. See <a href=\"https://pengxiang-wang.com/posts/continual-learning-metrics.html#sec-performance-of-training-epoch\">here</a> for details.</p>\n", "annotation": ": clarena.utils.metrics.MeanMetricBatch"}, {"fullname": "clarena.callbacks.cl_metrics.CLMetricsCallback.loss_training_epoch", "modulename": "clarena.callbacks.cl_metrics", "qualname": "CLMetricsCallback.loss_training_epoch", "kind": "variable", "doc": "<p>Total loss of training epoch. Accumulated and calculated from the training batches. See <a href=\"https://pengxiang-wang.com/posts/continual-learning-metrics.html#sec-performance-of-training-epoch\">here</a> for details.</p>\n", "annotation": ": clarena.utils.metrics.MeanMetricBatch"}, {"fullname": "clarena.callbacks.cl_metrics.CLMetricsCallback.acc_val", "modulename": "clarena.callbacks.cl_metrics", "qualname": "CLMetricsCallback.acc_val", "kind": "variable", "doc": "<p>Validation classification accuracy of the model after training epoch. Accumulated and calculated from the validation batches. See <a href=\"https://pengxiang-wang.com/posts/continual-learning-metrics.html#sec-validation-performace\">here</a> for details.</p>\n", "annotation": ": clarena.utils.metrics.MeanMetricBatch"}, {"fullname": "clarena.callbacks.cl_metrics.CLMetricsCallback.loss_cls_val", "modulename": "clarena.callbacks.cl_metrics", "qualname": "CLMetricsCallback.loss_cls_val", "kind": "variable", "doc": "<p>Validation classification of the model loss after training epoch. Accumulated and calculated from the validation batches. See <a href=\"https://pengxiang-wang.com/posts/continual-learning-metrics.html#sec-validation-performace\">here</a> for details.</p>\n", "annotation": ": clarena.utils.metrics.MeanMetricBatch"}, {"fullname": "clarena.callbacks.cl_metrics.CLMetricsCallback.acc_test", "modulename": "clarena.callbacks.cl_metrics", "qualname": "CLMetricsCallback.acc_test", "kind": "variable", "doc": "<p>Test classification accuracy of the current model (<code>self.task_id</code>) on current and previous tasks. Accumulated and calculated from the test batches. Keys are task IDs (string type) and values are the corresponding metrics. It is the last row of the lower triangular matrix. See <a href=\"https://pengxiang-wang.com/posts/continual-learning-metrics.html#sec-test-performance-of-previous-tasks\">here</a> for details.</p>\n", "annotation": ": dict[str, clarena.utils.metrics.MeanMetricBatch]"}, {"fullname": "clarena.callbacks.cl_metrics.CLMetricsCallback.loss_cls_test", "modulename": "clarena.callbacks.cl_metrics", "qualname": "CLMetricsCallback.loss_cls_test", "kind": "variable", "doc": "<p>Test classification loss of the current model (<code>self.task_id</code>) on current and previous tasks. Accumulated and calculated from the test batches. Keys are task IDs (string type) and values are the corresponding metrics. It is the last row of the lower triangular matrix. See <a href=\"https://pengxiang-wang.com/posts/continual-learning-metrics.html#sec-test-performance-of-previous-tasks\">here</a> for details.</p>\n", "annotation": ": dict[str, clarena.utils.metrics.MeanMetricBatch]"}, {"fullname": "clarena.callbacks.cl_metrics.CLMetricsCallback.task_id", "modulename": "clarena.callbacks.cl_metrics", "qualname": "CLMetricsCallback.task_id", "kind": "variable", "doc": "<p>Task ID counter indicating which task is being processed. Self updated during the task loop.</p>\n", "annotation": ": int"}, {"fullname": "clarena.callbacks.cl_metrics.CLMetricsCallback.on_fit_start", "modulename": "clarena.callbacks.cl_metrics", "qualname": "CLMetricsCallback.on_fit_start", "kind": "function", "doc": "<p>Initialise training and validation metrics.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.cl_metrics.CLMetricsCallback.on_train_batch_end", "modulename": "clarena.callbacks.cl_metrics", "qualname": "CLMetricsCallback.on_train_batch_end", "kind": "function", "doc": "<p>Record training metrics from training batch, log metrics of training batch and accumulated metrics of the epoch to Lightning loggers.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the training step, the returns of the <code>training_step()</code> method in the <code>CLAlgorithm</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the training data batch.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.cl_metrics.CLMetricsCallback.on_train_epoch_end", "modulename": "clarena.callbacks.cl_metrics", "qualname": "CLMetricsCallback.on_train_epoch_end", "kind": "function", "doc": "<p>Log metrics of training epoch to plot learning curves and reset the metrics accumulation at the end of training epoch.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.cl_metrics.CLMetricsCallback.on_validation_batch_end", "modulename": "clarena.callbacks.cl_metrics", "qualname": "CLMetricsCallback.on_validation_batch_end", "kind": "function", "doc": "<p>Accumulating metrics from validation batch. We don't need to log and monitor the metrics of validation batches.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the validation step, which is the returns of the <code>validation_step()</code> method in the <code>CLAlgorithm</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the validation data batch.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.cl_metrics.CLMetricsCallback.on_validation_epoch_end", "modulename": "clarena.callbacks.cl_metrics", "qualname": "CLMetricsCallback.on_validation_epoch_end", "kind": "function", "doc": "<p>Log validation metrics to plot learning curves.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.cl_metrics.CLMetricsCallback.on_test_start", "modulename": "clarena.callbacks.cl_metrics", "qualname": "CLMetricsCallback.on_test_start", "kind": "function", "doc": "<p>Initialise the metrics for testing each seen task in the beginning of a task's testing.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.cl_metrics.CLMetricsCallback.on_test_batch_end", "modulename": "clarena.callbacks.cl_metrics", "qualname": "CLMetricsCallback.on_test_batch_end", "kind": "function", "doc": "<p>Accumulating metrics from test batch. We don't need to log and monitor the metrics of test batches.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the test step, which is the returns of the <code>test_step()</code> method in the <code>CLAlgorithm</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the validation data batch.</li>\n<li><strong>dataloader_idx</strong> (<code>int</code>): the task ID of seen tasks to be tested. A default value of 0 is given otherwise the LightningModule will raise a <code>RuntimeError</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">dataloader_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.cl_metrics.CLMetricsCallback.on_test_epoch_end", "modulename": "clarena.callbacks.cl_metrics", "qualname": "CLMetricsCallback.on_test_epoch_end", "kind": "function", "doc": "<p>Save and plot test metrics at the end of test.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.cl_rich_progress_bar", "modulename": "clarena.callbacks.cl_rich_progress_bar", "kind": "module", "doc": "<p>The submodule in <code>callbacks</code> for <code>CLRichProgressBar</code>.</p>\n"}, {"fullname": "clarena.callbacks.cl_rich_progress_bar.CLRichProgressBar", "modulename": "clarena.callbacks.cl_rich_progress_bar", "qualname": "CLRichProgressBar", "kind": "class", "doc": "<p>Customised <code>RichProgressBar</code> for continual learning.</p>\n", "bases": "lightning.pytorch.callbacks.progress.rich_progress.RichProgressBar"}, {"fullname": "clarena.callbacks.cl_rich_progress_bar.CLRichProgressBar.get_metrics", "modulename": "clarena.callbacks.cl_rich_progress_bar", "qualname": "CLRichProgressBar.get_metrics", "kind": "function", "doc": "<p>Filter out the version number from the metrics displayed in the progress bar.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"o\">*</span><span class=\"n\">args</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.hat_metrics", "modulename": "clarena.callbacks.hat_metrics", "kind": "module", "doc": "<p>The submodule in <code>callbacks</code> for <code>HATMetricsCallback</code>.</p>\n"}, {"fullname": "clarena.callbacks.hat_metrics.HATMetricsCallback", "modulename": "clarena.callbacks.hat_metrics", "qualname": "HATMetricsCallback", "kind": "class", "doc": "<p>Provides all actions that are related to metrics used for <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task)</a> algorithm, which include:</p>\n\n<ul>\n<li>Visualising mask and cumulative mask figures during training and testing as figures.</li>\n<li>Logging network capacity during training. See the \"Evaluation Metrics\" section in chapter 4.1 in <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a> for more details about network capacity.</li>\n</ul>\n\n<p>Lightning provides <code>self.log()</code> to log metrics in <code>LightningModule</code> where our <code>CLAlgorithm</code> based. You can put <code>self.log()</code> here if you don't want to mess up the <code>CLAlgorithm</code> with a huge amount of logging codes.</p>\n\n<p>The callback is able to produce the following outputs:</p>\n\n<ul>\n<li><strong>Mask Figures</strong>: both training and test, masks and cumulative masks.</li>\n</ul>\n", "bases": "lightning.pytorch.callbacks.callback.Callback"}, {"fullname": "clarena.callbacks.hat_metrics.HATMetricsCallback.__init__", "modulename": "clarena.callbacks.hat_metrics", "qualname": "HATMetricsCallback.__init__", "kind": "function", "doc": "<p>Initialise the <code>HATMetricsCallback</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>test_masks_plot_dir</strong> (<code>str</code> | <code>None</code>): the directory to save the test mask figures. If <code>None</code>, no file will be saved.</li>\n<li><strong>test_cumulative_masks_plot_dir</strong> (<code>str</code> | <code>None</code>): the directory to save the test cumulative mask figures. If <code>None</code>, no file will be saved.</li>\n<li><strong>training_masks_plot_dir</strong> (<code>str</code> | <code>None</code>): the directory to save the training mask figures. If <code>None</code>, no file will be saved.</li>\n<li><strong>plot_training_mask_every_n_steps</strong> (<code>int</code>): the frequency of plotting training mask figures in terms of number of batches during training. Only applies when <code>training_masks_plot_dir</code> is not <code>None</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">test_masks_plot_dir</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">test_cumulative_masks_plot_dir</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">training_masks_plot_dir</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">plot_training_mask_every_n_steps</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.callbacks.hat_metrics.HATMetricsCallback.test_masks_plot_dir", "modulename": "clarena.callbacks.hat_metrics", "qualname": "HATMetricsCallback.test_masks_plot_dir", "kind": "variable", "doc": "<p>Store the directory to save the test mask figures.</p>\n", "annotation": ": str"}, {"fullname": "clarena.callbacks.hat_metrics.HATMetricsCallback.test_cumulative_masks_plot_dir", "modulename": "clarena.callbacks.hat_metrics", "qualname": "HATMetricsCallback.test_cumulative_masks_plot_dir", "kind": "variable", "doc": "<p>Store the directory to save the test cumulative mask figures.</p>\n", "annotation": ": str"}, {"fullname": "clarena.callbacks.hat_metrics.HATMetricsCallback.training_masks_plot_dir", "modulename": "clarena.callbacks.hat_metrics", "qualname": "HATMetricsCallback.training_masks_plot_dir", "kind": "variable", "doc": "<p>Store the directory to save the training mask figures.</p>\n", "annotation": ": str"}, {"fullname": "clarena.callbacks.hat_metrics.HATMetricsCallback.plot_training_mask_every_n_steps", "modulename": "clarena.callbacks.hat_metrics", "qualname": "HATMetricsCallback.plot_training_mask_every_n_steps", "kind": "variable", "doc": "<p>Store the frequency of plotting training masks in terms of number of batches.</p>\n", "annotation": ": int"}, {"fullname": "clarena.callbacks.hat_metrics.HATMetricsCallback.task_id", "modulename": "clarena.callbacks.hat_metrics", "qualname": "HATMetricsCallback.task_id", "kind": "variable", "doc": "<p>Task ID counter indicating which task is being processed. Self updated during the task loop.</p>\n", "annotation": ": int"}, {"fullname": "clarena.callbacks.hat_metrics.HATMetricsCallback.on_fit_start", "modulename": "clarena.callbacks.hat_metrics", "qualname": "HATMetricsCallback.on_fit_start", "kind": "function", "doc": "<p>Get the current task ID in the beginning of a task's fitting (training and validation). Sanity check the <code>pl_module</code> to be <code>HAT</code> or <code>AdaHAT</code>.</p>\n\n<p><strong>Raises:</strong>\n-<strong>TypeError</strong>: when the <code>pl_module</code> is not <code>HAT</code> or <code>AdaHAT</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.hat_metrics.HATMetricsCallback.on_train_batch_end", "modulename": "clarena.callbacks.hat_metrics", "qualname": "HATMetricsCallback.on_train_batch_end", "kind": "function", "doc": "<p>Plot training mask and log network capacity after training batch.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the training step, which is the returns of the <code>training_step()</code> method in the <code>CLAlgorithm</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the training data batch.</li>\n<li><strong>batch_idx</strong> (<code>int</code>): the index of the current batch. This is for the file name of mask figures.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.hat_metrics.HATMetricsCallback.on_test_start", "modulename": "clarena.callbacks.hat_metrics", "qualname": "HATMetricsCallback.on_test_start", "kind": "function", "doc": "<p>Plot test mask and cumulative mask figures.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.pylogger", "modulename": "clarena.callbacks.pylogger", "kind": "module", "doc": "<p>The submodule in <code>callbacks</code> for <code>PyloggerCallback</code>.</p>\n"}, {"fullname": "clarena.callbacks.pylogger.PyloggerCallback", "modulename": "clarena.callbacks.pylogger", "qualname": "PyloggerCallback", "kind": "class", "doc": "<p>Pylogger Callback provides additional logging for during continual learning progress.</p>\n\n<p>Put logging messages here if you don't want to mess up the <code>CLAlgorithm</code> (<code>LightningModule</code>) with a huge amount of logging codes.</p>\n", "bases": "lightning.pytorch.callbacks.callback.Callback"}, {"fullname": "clarena.callbacks.pylogger.PyloggerCallback.on_fit_start", "modulename": "clarena.callbacks.pylogger", "qualname": "PyloggerCallback.on_fit_start", "kind": "function", "doc": "<p>Log messages for the start of training task.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.pylogger.PyloggerCallback.on_train_end", "modulename": "clarena.callbacks.pylogger", "qualname": "PyloggerCallback.on_train_end", "kind": "function", "doc": "<p>Log messages for the end of training task.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.pylogger.PyloggerCallback.on_test_start", "modulename": "clarena.callbacks.pylogger", "qualname": "PyloggerCallback.on_test_start", "kind": "function", "doc": "<p>Log messages for the start of testing task.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.pylogger.PyloggerCallback.on_test_end", "modulename": "clarena.callbacks.pylogger", "qualname": "PyloggerCallback.on_test_end", "kind": "function", "doc": "<p>Log messages for the end of testing task.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.save_first_batch_images", "modulename": "clarena.callbacks.save_first_batch_images", "kind": "module", "doc": "<p>The submodule in <code>callbacks</code> for <code>SaveFirstBatchImagesCallback</code>.</p>\n"}, {"fullname": "clarena.callbacks.save_first_batch_images.SaveFirstBatchImagesCallback", "modulename": "clarena.callbacks.save_first_batch_images", "qualname": "SaveFirstBatchImagesCallback", "kind": "class", "doc": "<p>Saves images and labels into files in the first batch of training data.</p>\n", "bases": "lightning.pytorch.callbacks.callback.Callback"}, {"fullname": "clarena.callbacks.save_first_batch_images.SaveFirstBatchImagesCallback.__init__", "modulename": "clarena.callbacks.save_first_batch_images", "qualname": "SaveFirstBatchImagesCallback.__init__", "kind": "function", "doc": "<p>Initialise the Image Show Callback.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>save_dir</strong> (<code>str</code>): the directory to save images and labels as documents. Better inside the output directory.</li>\n<li><strong>img_prefix</strong> (<code>str</code>): the prefix for image files.</li>\n<li><strong>labels_filename</strong> (<code>str</code>): the filename for the labels file.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">save_dir</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">img_prefix</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;sample&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">labels_filename</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;labels.txt&#39;</span></span>)</span>"}, {"fullname": "clarena.callbacks.save_first_batch_images.SaveFirstBatchImagesCallback.save_dir", "modulename": "clarena.callbacks.save_first_batch_images", "qualname": "SaveFirstBatchImagesCallback.save_dir", "kind": "variable", "doc": "<p>Store the directory to save images and labels as documents.</p>\n", "annotation": ": str"}, {"fullname": "clarena.callbacks.save_first_batch_images.SaveFirstBatchImagesCallback.img_prefix", "modulename": "clarena.callbacks.save_first_batch_images", "qualname": "SaveFirstBatchImagesCallback.img_prefix", "kind": "variable", "doc": "<p>Store the prefix for image files.</p>\n", "annotation": ": str"}, {"fullname": "clarena.callbacks.save_first_batch_images.SaveFirstBatchImagesCallback.labels_filename", "modulename": "clarena.callbacks.save_first_batch_images", "qualname": "SaveFirstBatchImagesCallback.labels_filename", "kind": "variable", "doc": "<p>Store the filename for the labels file.</p>\n", "annotation": ": str"}, {"fullname": "clarena.callbacks.save_first_batch_images.SaveFirstBatchImagesCallback.called", "modulename": "clarena.callbacks.save_first_batch_images", "qualname": "SaveFirstBatchImagesCallback.called", "kind": "variable", "doc": "<p>Flag to avoid calling the callback multiple times.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.callbacks.save_first_batch_images.SaveFirstBatchImagesCallback.on_train_start", "modulename": "clarena.callbacks.save_first_batch_images", "qualname": "SaveFirstBatchImagesCallback.on_train_start", "kind": "function", "doc": "<p>Save images and labels into files in the first batch of training data at the beginning of the training of the task.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms", "modulename": "clarena.cl_algorithms", "kind": "module", "doc": "<h1 id=\"continual-learning-algorithms\">Continual Learning Algorithms</h1>\n\n<p>This submodule provides the <strong>continual learning algorithms</strong> in CLArena. </p>\n\n<p>Please note that this is an API documantation. Please refer to the main documentation pages for more information about the backbone networks and how to configure and implement them:</p>\n\n<ul>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/configure-your-experiment/cl-algorithm\"><strong>Configure CL Algorithm</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/implement-your-cl-modules/cl-algorithm\"><strong>Implement Your CL Algorithm Class</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/posts/continual-learning-beginners-guide#sec-methodology\"><strong>A Beginners' Guide to Continual Learning (Methodology Overview)</strong></a></li>\n</ul>\n\n<p>The algorithms are implemented as subclasses of <code>CLAlgorithm</code>.</p>\n"}, {"fullname": "clarena.cl_algorithms.CLAlgorithm", "modulename": "clarena.cl_algorithms", "qualname": "CLAlgorithm", "kind": "class", "doc": "<p>The base class of continual learning algorithms, inherited from <code>LightningModule</code>.</p>\n", "bases": "lightning.pytorch.core.module.LightningModule"}, {"fullname": "clarena.cl_algorithms.CLAlgorithm.__init__", "modulename": "clarena.cl_algorithms", "qualname": "CLAlgorithm.__init__", "kind": "function", "doc": "<p>Initialise the CL algorithm with the network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone</strong> (<code>CLBackbone</code>): backbone network.</li>\n<li><strong>heads</strong> (<code>HeadsTIL</code> | <code>HeadsCIL</code>): output heads.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLBackbone</span>,</span><span class=\"param\">\t<span class=\"n\">heads</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_heads</span><span class=\"o\">.</span><span class=\"n\">heads_til</span><span class=\"o\">.</span><span class=\"n\">HeadsTIL</span> <span class=\"o\">|</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_heads</span><span class=\"o\">.</span><span class=\"n\">heads_cil</span><span class=\"o\">.</span><span class=\"n\">HeadsCIL</span></span>)</span>"}, {"fullname": "clarena.cl_algorithms.CLAlgorithm.backbone", "modulename": "clarena.cl_algorithms", "qualname": "CLAlgorithm.backbone", "kind": "variable", "doc": "<p>Store the backbone network.</p>\n", "annotation": ": clarena.backbones.base.CLBackbone"}, {"fullname": "clarena.cl_algorithms.CLAlgorithm.heads", "modulename": "clarena.cl_algorithms", "qualname": "CLAlgorithm.heads", "kind": "variable", "doc": "<p>Store the output heads.</p>\n", "annotation": ": clarena.cl_heads.heads_til.HeadsTIL | clarena.cl_heads.heads_cil.HeadsCIL"}, {"fullname": "clarena.cl_algorithms.CLAlgorithm.optimizer", "modulename": "clarena.cl_algorithms", "qualname": "CLAlgorithm.optimizer", "kind": "variable", "doc": "<p>Store the optimizer object (partially initialised) for the backpropagation of task <code>self.task_id</code>. Will be equipped with parameters in <code>configure_optimizers()</code>.</p>\n", "annotation": ": torch.optim.optimizer.Optimizer"}, {"fullname": "clarena.cl_algorithms.CLAlgorithm.criterion", "modulename": "clarena.cl_algorithms", "qualname": "CLAlgorithm.criterion", "kind": "variable", "doc": "<p>The loss function bewteen the output logits and the target labels. Default is cross-entropy loss.</p>\n"}, {"fullname": "clarena.cl_algorithms.CLAlgorithm.task_id", "modulename": "clarena.cl_algorithms", "qualname": "CLAlgorithm.task_id", "kind": "variable", "doc": "<p>Task ID counter indicating which task is being processed. Self updated during the task loop.</p>\n", "annotation": ": int"}, {"fullname": "clarena.cl_algorithms.CLAlgorithm.sanity_check", "modulename": "clarena.cl_algorithms", "qualname": "CLAlgorithm.sanity_check", "kind": "function", "doc": "<p>Check the sanity of the arguments.</p>\n\n<p><strong>Raises:</strong></p>\n\n<ul>\n<li><strong>ValueError</strong>: if the <code>output_dim</code> of backbone network is not equal to the <code>input_dim</code> of CL heads.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.CLAlgorithm.setup_task_id", "modulename": "clarena.cl_algorithms", "qualname": "CLAlgorithm.setup_task_id", "kind": "function", "doc": "<p>Set up which task's dataset the CL experiment is on. This must be done before <code>forward()</code> method is called.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID.</li>\n<li><strong>num_classes_t</strong> (<code>int</code>): the number of classes in the task.</li>\n<li><strong>optimizer</strong> (<code>Optimizer</code>): the optimizer object (partially initialised) for the task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">num_classes_t</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">optimizer</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">Optimizer</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.CLAlgorithm.configure_optimizers", "modulename": "clarena.cl_algorithms", "qualname": "CLAlgorithm.configure_optimizers", "kind": "function", "doc": "<p>Configure optimizer hooks by Lightning.\nSee <a href=\"https://lightning.ai/docs/pytorch/stable/common/lightning_module.html#configure-optimizers\">Lightning docs</a> for more details.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">Optimizer</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.adahat", "modulename": "clarena.cl_algorithms.adahat", "kind": "module", "doc": "<p>The submodule in <code>cl_algorithms</code> for <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT (Adaptive Hard Attention to the Task) algorithm</a>.</p>\n"}, {"fullname": "clarena.cl_algorithms.adahat.AdaHAT", "modulename": "clarena.cl_algorithms.adahat", "qualname": "AdaHAT", "kind": "class", "doc": "<p>AdaHAT (Adaptive Hard Attention to the Task) algorithm.</p>\n\n<p><a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">Adaptive HAT (Adaptive Hard Attention to the Task, 2024)</a> is an architecture-based continual learning approach that improves <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task, 2018)</a> by introducing new adaptive soft gradient clipping based on parameter importance and network sparsity.</p>\n\n<p>We implement AdaHAT as a subclass of HAT algorithm, as AdaHAT has the same  <code>forward()</code>, <code>compensate_task_embedding_gradients()</code>, <code>training_step()</code>, <code>on_train_end()</code>,<code>validation_step()</code>, <code>test_step()</code> method as <code>HAT</code> class.</p>\n", "bases": "clarena.cl_algorithms.hat.HAT"}, {"fullname": "clarena.cl_algorithms.adahat.AdaHAT.__init__", "modulename": "clarena.cl_algorithms.adahat", "qualname": "AdaHAT.__init__", "kind": "function", "doc": "<p>Initialise the AdaHAT algorithm with the network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone</strong> (<code>HATMaskBackbone</code>): must be a backbone network with HAT mask mechanism.</li>\n<li><strong>heads</strong> (<code>HeadsTIL</code> | <code>HeadsCIL</code>): output heads.</li>\n<li><strong>adjustment_mode</strong> (<code>str</code>): the strategy of adjustment i.e. the mode of gradient clipping, should be one of the following:\n<ol>\n<li>'adahat': set the gradients of parameters linking to masked units to a soft adjustment rate in the original AdaHAT approach. This is the way that AdaHAT does, which allowes the part of network for previous tasks to be updated slightly. See equation (8) and (9) chapter 3.1 in <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>.</li>\n<li>'adahat_no_sum': set the gradients of parameters linking to masked units to a soft adjustment rate in the original AdaHAT approach, but without considering the information of parameter importance i.e. summative mask. This is the way that one of the AdaHAT ablation study does. See chapter 4.3 in <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>.</li>\n<li>'adahat_no_reg': set the gradients of parameters linking to masked units to a soft adjustment rate in the original AdaHAT approach, but without considering the information of network sparsity i.e. mask sparsity regularisation value. This is the way that one of the AdaHAT ablation study does. See chapter 4.3 in <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>.</li>\n</ol></li>\n<li><strong>adjustment_intensity</strong> (<code>float</code>): hyperparameter, control the overall intensity of gradient adjustment. It's the $\\alpha$ in equation (9) in <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>.</li>\n<li><strong>s_max</strong> (<code>float</code>): hyperparameter, the maximum scaling factor in the gate function. See chapter 2.4 \"Hard Attention Training\" in <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</li>\n<li><strong>clamp_threshold</strong> (<code>float</code>): the threshold for task embedding gradient compensation. See chapter 2.5 \"Embedding Gradient Compensation\" in <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</li>\n<li><strong>mask_sparsity_reg_factor</strong> (<code>float</code>): hyperparameter, the regularisation factor for mask sparsity.</li>\n<li><strong>mask_sparsity_reg_mode</strong> (<code>str</code>): the mode of mask sparsity regularisation, should be one of the following:\n<ol>\n<li>'original' (default): the original mask sparsity regularisation in HAT paper.</li>\n<li>'cross': the cross version mask sparsity regularisation.</li>\n</ol></li>\n<li><strong>task_embedding_init_mode</strong> (<code>str</code>): the initialisation method for task embeddings, should be one of the following:\n<ol>\n<li>'N01' (default): standard normal distribution $N(0, 1)$.</li>\n<li>'U-11': uniform distribution $U(-1, 1)$.</li>\n<li>'U01': uniform distribution $U(0, 1)$.</li>\n<li>'U-10': uniform distribution $U(-1, 0)$.</li>\n<li>'last': inherit task embedding from last task.</li>\n</ol></li>\n<li><strong>epsilon</strong> (<code>float</code>): the value added to network sparsity to avoid division by zero appeared in equation (9) in <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">HATMaskBackbone</span>,</span><span class=\"param\">\t<span class=\"n\">heads</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_heads</span><span class=\"o\">.</span><span class=\"n\">heads_til</span><span class=\"o\">.</span><span class=\"n\">HeadsTIL</span> <span class=\"o\">|</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_heads</span><span class=\"o\">.</span><span class=\"n\">heads_cil</span><span class=\"o\">.</span><span class=\"n\">HeadsCIL</span>,</span><span class=\"param\">\t<span class=\"n\">adjustment_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">adjustment_intensity</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">s_max</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">clamp_threshold</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">mask_sparsity_reg_factor</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">mask_sparsity_reg_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;original&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">task_embedding_init_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;N01&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">epsilon</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span></span>)</span>"}, {"fullname": "clarena.cl_algorithms.adahat.AdaHAT.adjustment_intensity", "modulename": "clarena.cl_algorithms.adahat", "qualname": "AdaHAT.adjustment_intensity", "kind": "variable", "doc": "<p>Store the adjustment intensity in equation (9) in <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>.</p>\n"}, {"fullname": "clarena.cl_algorithms.adahat.AdaHAT.epsilon", "modulename": "clarena.cl_algorithms.adahat", "qualname": "AdaHAT.epsilon", "kind": "variable", "doc": "<p>Store the small value to avoid division by zero appeared in equation (9) in <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>.</p>\n"}, {"fullname": "clarena.cl_algorithms.adahat.AdaHAT.summative_mask_for_previous_tasks", "modulename": "clarena.cl_algorithms.adahat", "qualname": "AdaHAT.summative_mask_for_previous_tasks", "kind": "variable", "doc": "<p>Store the summative binary attention mask $\\mathrm{M}^{<t,\\text{sum}}$ previous tasks $1,\\cdots, t-1$, gated from the task embedding. Keys are task IDs and values are the corresponding summative mask. Each cumulative mask is a dict where keys are layer names and values are the binary mask tensor for the layer. The mask tensor has size (number of units).</p>\n", "annotation": ": dict[str, torch.Tensor]"}, {"fullname": "clarena.cl_algorithms.adahat.AdaHAT.automatic_optimization", "modulename": "clarena.cl_algorithms.adahat", "qualname": "AdaHAT.automatic_optimization", "kind": "variable", "doc": "<p>If set to <code>False</code> you are responsible for calling <code>.backward()</code>, <code>.step()</code>, <code>.zero_grad()</code>.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.cl_algorithms.adahat.AdaHAT.sanity_check", "modulename": "clarena.cl_algorithms.adahat", "qualname": "AdaHAT.sanity_check", "kind": "function", "doc": "<p>Check the sanity of the arguments.</p>\n\n<p><strong>Raises:</strong></p>\n\n<ul>\n<li><strong>ValueError</strong>: If the <code>adjustment_intensity</code> is not positive.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.adahat.AdaHAT.on_train_start", "modulename": "clarena.cl_algorithms.adahat", "qualname": "AdaHAT.on_train_start", "kind": "function", "doc": "<p>Additionally initialise the summative mask at the beginning of first task.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.adahat.AdaHAT.clip_grad_by_adjustment", "modulename": "clarena.cl_algorithms.adahat", "qualname": "AdaHAT.clip_grad_by_adjustment", "kind": "function", "doc": "<p>Clip the gradients by the adjustment rate.</p>\n\n<p>Note that as the task embedding fully covers every layer in the backbone network, no parameters are left out of this system. This applies not only the parameters in between layers with task embedding, but also those before the first layer. We designed it seperately in the codes.</p>\n\n<p>Network capacity is measured along with this method. Network capacity is defined as the average adjustment rate over all parameters. See chapter 4.1 in <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>network_sparsity</strong> (<code>dict[str, Tensor]</code> | <code>None</code>): The network sparsity i.e. the mask sparsity loss of each layer for the current task. It applies only to AdaHAT modes, as it is used to calculate the adjustment rate for the gradients.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>capacity</strong> (<code>Tensor</code>): the calculated network capacity.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">network_sparsity</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.adahat.AdaHAT.on_train_end", "modulename": "clarena.cl_algorithms.adahat", "qualname": "AdaHAT.on_train_end", "kind": "function", "doc": "<p>Additionally update summative mask after training the task.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.cbp", "modulename": "clarena.cl_algorithms.cbp", "kind": "module", "doc": "<p>The submodule in <code>cl_algorithms</code> for <a href=\"https://www.nature.com/articles/s41586-024-07711-7\">CBP (Continual Backpropagation)</a> algorithm.</p>\n"}, {"fullname": "clarena.cl_algorithms.cbp.CBP", "modulename": "clarena.cl_algorithms.cbp", "qualname": "CBP", "kind": "class", "doc": "<p>CBP (Continual Backpropagation) algorithm.</p>\n\n<p><a href=\"https://www.nature.com/articles/s41586-024-07711-7\">CBP (Continual Backpropagation, 2024)</a> is a continual learning approach that reinitialises a small number of units during training, using an utility measures to determine which units to reinitialise. It aims to address loss of plasticity problem for learning new tasks, yet not very well solve the catastrophic forgetting problem in continual learning.</p>\n\n<p>We implement CBP as a subclass of Finetuning algorithm, as CBP has the same <code>forward()</code>, <code>training_step()</code>, <code>validation_step()</code> and <code>test_step()</code> method as <code>Finetuning</code> class.</p>\n", "bases": "clarena.cl_algorithms.finetuning.Finetuning"}, {"fullname": "clarena.cl_algorithms.cbp.CBP.__init__", "modulename": "clarena.cl_algorithms.cbp", "qualname": "CBP.__init__", "kind": "function", "doc": "<p>Initialise the Finetuning algorithm with the network. It has no additional hyperparameters.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone</strong> (<code>CLBackbone</code>): backbone network.</li>\n<li><strong>heads</strong> (<code>HeadsTIL</code> | <code>HeadsCIL</code>): output heads.</li>\n<li><strong>replacement_rate</strong> (<code>float</code>): the replacement rate of units. It is the precentage of units to be reinitialised during training.</li>\n<li><strong>maturity_threshold</strong> (<code>int</code>): the maturity threshold of units. It is the number of training steps before a unit can be reinitialised.</li>\n<li><strong>utility_decay_rate</strong> (<code>float</code>): the utility decay rate of units. It is the rate at which the utility of a unit decays over time.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLBackbone</span>,</span><span class=\"param\">\t<span class=\"n\">heads</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_heads</span><span class=\"o\">.</span><span class=\"n\">heads_til</span><span class=\"o\">.</span><span class=\"n\">HeadsTIL</span> <span class=\"o\">|</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_heads</span><span class=\"o\">.</span><span class=\"n\">heads_cil</span><span class=\"o\">.</span><span class=\"n\">HeadsCIL</span>,</span><span class=\"param\">\t<span class=\"n\">replacement_rate</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">maturity_threshold</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">utility_decay_rate</span><span class=\"p\">:</span> <span class=\"nb\">float</span></span>)</span>"}, {"fullname": "clarena.cl_algorithms.cbp.CBP.replacement_rate", "modulename": "clarena.cl_algorithms.cbp", "qualname": "CBP.replacement_rate", "kind": "variable", "doc": "<p>Store the replacement rate of units.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_algorithms.cbp.CBP.maturity_threshold", "modulename": "clarena.cl_algorithms.cbp", "qualname": "CBP.maturity_threshold", "kind": "variable", "doc": "<p>Store the maturity threshold of units.</p>\n", "annotation": ": int"}, {"fullname": "clarena.cl_algorithms.cbp.CBP.utility_decay_rate", "modulename": "clarena.cl_algorithms.cbp", "qualname": "CBP.utility_decay_rate", "kind": "variable", "doc": "<p>Store the utility decay rate of units.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_algorithms.cbp.CBP.contribution_utility", "modulename": "clarena.cl_algorithms.cbp", "qualname": "CBP.contribution_utility", "kind": "variable", "doc": "<p>Store the contribution utility of units. See equation (1) in the <a href=\"https://www.nature.com/articles/s41586-024-07711-7\">continual backpropagation paper</a>. Keys are layer names and values are the utility tensor for the layer. The utility tensor is the same size as the feature tensor with size (number of units).</p>\n", "annotation": ": dict[str, torch.Tensor]"}, {"fullname": "clarena.cl_algorithms.cbp.CBP.num_replacements", "modulename": "clarena.cl_algorithms.cbp", "qualname": "CBP.num_replacements", "kind": "variable", "doc": "<p>Store the number of replacements of units in each layer. Keys are layer names and values are the number of replacements for the layer.</p>\n", "annotation": ": dict[str, int]"}, {"fullname": "clarena.cl_algorithms.cbp.CBP.age", "modulename": "clarena.cl_algorithms.cbp", "qualname": "CBP.age", "kind": "variable", "doc": "<p>Store the age of units. Keys are layer names and values are the age tensor for the layer. The age tensor is the same size as the feature tensor with size (1, number of units).</p>\n", "annotation": ": dict[str, torch.Tensor]"}, {"fullname": "clarena.cl_algorithms.cbp.CBP.on_train_start", "modulename": "clarena.cl_algorithms.cbp", "qualname": "CBP.on_train_start", "kind": "function", "doc": "<p>Initialise the utility, number of replacements and age for each layer as zeros.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.cbp.CBP.on_train_batch_end", "modulename": "clarena.cl_algorithms.cbp", "qualname": "CBP.on_train_batch_end", "kind": "function", "doc": "<p>Update the contribution utility and age of units after each training step, and conduct reinitialisation of units based on utility measures. This is the core of the CBP algorithm.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the training step, which is the returns of the <code>training_step()</code> method in the <code>CLAlgorithm</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the training data batch.</li>\n<li><strong>batch_idx</strong> (<code>int</code>): the index of the current batch. This is for the file name of mask figures.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>, </span><span class=\"param\"><span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.cbphat", "modulename": "clarena.cl_algorithms.cbphat", "kind": "module", "doc": "<p>The submodule in <code>cl_algorithms</code> for CBPHAT algorithm.</p>\n"}, {"fullname": "clarena.cl_algorithms.cbphat.CBPHAT", "modulename": "clarena.cl_algorithms.cbphat", "qualname": "CBPHAT", "kind": "class", "doc": "<p>CBPHAT algorithm.</p>\n\n<p>CBPHAT is what I am working on, trying combining HAT (Hard Attention to the Task) algorithm with Continual Backpropagation (CBP) by leveraging the contribution utility as the parameter importance like in AdaHAT (Adaptive Hard Attention to the Task) algorithm.</p>\n\n<p>We implement CBPHAT as a subclass of AdaHAT algorithm because CBPHAT adopt the similar idea as AdaHAT.</p>\n", "bases": "clarena.cl_algorithms.adahat.AdaHAT"}, {"fullname": "clarena.cl_algorithms.cbphat.CBPHAT.__init__", "modulename": "clarena.cl_algorithms.cbphat", "qualname": "CBPHAT.__init__", "kind": "function", "doc": "<p>Initialise the CBPHAT algorithm with the network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone</strong> (<code>HATMaskBackbone</code>): must be a backbone network with HAT mask mechanism.</li>\n<li><strong>heads</strong> (<code>HeadsTIL</code> | <code>HeadsCIL</code>): output heads.</li>\n<li><strong>adjustment_mode</strong> (<code>str</code>): the strategy of adjustment i.e. the mode of gradient clipping, should be one of the following:\n<ol>\n<li>'cbphat': our original CBP mode.</li>\n</ol></li>\n<li><strong>adjustment_intensity</strong> (<code>float</code>): hyperparameter, control the overall intensity of gradient adjustment. It's the $\\alpha$ in equation (9) in <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>.</li>\n<li><strong>utility_decay_rate</strong> (<code>float</code>): the utility decay rate of units. It is the rate at which the utility of a unit decays over time.</li>\n<li><strong>s_max</strong> (<code>float</code>): hyperparameter, the maximum scaling factor in the gate function. See chapter 2.4 \"Hard Attention Training\" in <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</li>\n<li><strong>clamp_threshold</strong> (<code>float</code>): the threshold for task embedding gradient compensation. See chapter 2.5 \"Embedding Gradient Compensation\" in <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</li>\n<li><strong>mask_sparsity_reg_factor</strong> (<code>float</code>): hyperparameter, the regularisation factor for mask sparsity.</li>\n<li><strong>mask_sparsity_reg_mode</strong> (<code>str</code>): the mode of mask sparsity regularisation, should be one of the following:\n<ol>\n<li>'original' (default): the original mask sparsity regularisation in HAT paper.</li>\n<li>'cross': the cross version mask sparsity regularisation.</li>\n</ol></li>\n<li><strong>task_embedding_init_mode</strong> (<code>str</code>): the initialisation method for task embeddings, should be one of the following:\n<ol>\n<li>'N01' (default): standard normal distribution $N(0, 1)$.</li>\n<li>'U-11': uniform distribution $U(-1, 1)$.</li>\n<li>'U01': uniform distribution $U(0, 1)$.</li>\n<li>'U-10': uniform distribution $U(-1, 0)$.</li>\n<li>'last': inherit task embedding from last task.</li>\n</ol></li>\n<li><strong>epsilon</strong> (<code>float</code>): the value added to network sparsity to avoid zero appeared in equation (9) in <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">HATMaskBackbone</span>,</span><span class=\"param\">\t<span class=\"n\">heads</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_heads</span><span class=\"o\">.</span><span class=\"n\">heads_til</span><span class=\"o\">.</span><span class=\"n\">HeadsTIL</span> <span class=\"o\">|</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_heads</span><span class=\"o\">.</span><span class=\"n\">heads_cil</span><span class=\"o\">.</span><span class=\"n\">HeadsCIL</span>,</span><span class=\"param\">\t<span class=\"n\">adjustment_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">adjustment_intensity</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">utility_decay_rate</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">s_max</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">clamp_threshold</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">mask_sparsity_reg_factor</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">mask_sparsity_reg_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;original&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">task_embedding_init_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;N01&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">epsilon</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span></span>)</span>"}, {"fullname": "clarena.cl_algorithms.cbphat.CBPHAT.utility_decay_rate", "modulename": "clarena.cl_algorithms.cbphat", "qualname": "CBPHAT.utility_decay_rate", "kind": "variable", "doc": "<p>Store the utility decay rate of units.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_algorithms.cbphat.CBPHAT.contribution_utility_t", "modulename": "clarena.cl_algorithms.cbphat", "qualname": "CBPHAT.contribution_utility_t", "kind": "variable", "doc": "<p>Store the summative min-max scaled contribution utility of units. See $U$ in the paper draft. Keys are layer names and values are the utility tensor for the layer. The utility tensor is the same size as the feature tensor with size (number of units).</p>\n", "annotation": ": dict[str, torch.Tensor]"}, {"fullname": "clarena.cl_algorithms.cbphat.CBPHAT.unit_importance_for_previous_tasks", "modulename": "clarena.cl_algorithms.cbphat", "qualname": "CBPHAT.unit_importance_for_previous_tasks", "kind": "variable", "doc": "<p>Store the unit importance values of units for previous tasks (1, \\cdots, self.task_id - 1). See the \"screenshot\" $I^{(t-1)}$ in the paper draft. Keys are layer names and values are the importance tensor for the layer. The importance tensor is the same size as the feature tensor with size (number of units).</p>\n", "annotation": ": dict[str, torch.Tensor]"}, {"fullname": "clarena.cl_algorithms.cbphat.CBPHAT.age_t", "modulename": "clarena.cl_algorithms.cbphat", "qualname": "CBPHAT.age_t", "kind": "variable", "doc": "<p>Store the age of units. Keys are layer names and values are the age tensor for the layer for current task. The age tensor is the same size as the feature tensor with size (number of units).</p>\n", "annotation": ": dict[str, torch.Tensor]"}, {"fullname": "clarena.cl_algorithms.cbphat.CBPHAT.automatic_optimization", "modulename": "clarena.cl_algorithms.cbphat", "qualname": "CBPHAT.automatic_optimization", "kind": "variable", "doc": "<p>If set to <code>False</code> you are responsible for calling <code>.backward()</code>, <code>.step()</code>, <code>.zero_grad()</code>.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.cl_algorithms.cbphat.CBPHAT.sanity_check", "modulename": "clarena.cl_algorithms.cbphat", "qualname": "CBPHAT.sanity_check", "kind": "function", "doc": "<p>Check the sanity of the arguments.</p>\n\n<p><strong>Raises:</strong></p>\n\n<ul>\n<li><strong>ValueError</strong>: If the utility decay rate is not in the range (0, 1].</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.cbphat.CBPHAT.on_train_start", "modulename": "clarena.cl_algorithms.cbphat", "qualname": "CBPHAT.on_train_start", "kind": "function", "doc": "<p>Additionally initialise the utility, age and the CBPHAT unit importance for each layer as zeros.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.cbphat.CBPHAT.clip_grad_by_adjustment", "modulename": "clarena.cl_algorithms.cbphat", "qualname": "CBPHAT.clip_grad_by_adjustment", "kind": "function", "doc": "<p>Clip the gradients by the adjustment rate.</p>\n\n<p>Note that as the task embedding fully covers every layer in the backbone network, no parameters are left out of this system. This applies not only the parameters in between layers with task embedding, but also those before the first layer. We designed it seperately in the codes.</p>\n\n<p>Network capacity is measured along with this method. Network capacity is defined as the average adjustment rate over all parameters. See chapter 4.1 in <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>network_sparsity</strong> (<code>dict[str, Tensor]</code> | <code>None</code>): The network sparsity i.e. the mask sparsity loss of each layer for the current task. It applies only to AdaHAT modes, as it is used to calculate the adjustment rate for the gradients.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>capacity</strong> (<code>Tensor</code>): the calculated network capacity.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">network_sparsity</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.cbphat.CBPHAT.on_train_batch_end", "modulename": "clarena.cl_algorithms.cbphat", "qualname": "CBPHAT.on_train_batch_end", "kind": "function", "doc": "<p>Update the contribution utility and age of units after each training step.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the training step, which is the returns of the <code>training_step()</code> method in the <code>CLAlgorithm</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the training data batch.</li>\n<li><strong>batch_idx</strong> (<code>int</code>): the index of the current batch. This is for the file name of mask figures.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>, </span><span class=\"param\"><span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.cbphat.CBPHAT.on_train_end", "modulename": "clarena.cl_algorithms.cbphat", "qualname": "CBPHAT.on_train_end", "kind": "function", "doc": "<p>Additionally convert the contribution utility into importance and store (take screenshot of) it as unit importance for previous tasks at the end of a task training.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.ewc", "modulename": "clarena.cl_algorithms.ewc", "kind": "module", "doc": "<p>The submodule in <code>cl_algorithms</code> for <a href=\"https://www.pnas.org/doi/10.1073/pnas.1611835114\">EWC (Elastic Weight Consolidation) algorithm</a>.</p>\n"}, {"fullname": "clarena.cl_algorithms.ewc.EWC", "modulename": "clarena.cl_algorithms.ewc", "qualname": "EWC", "kind": "class", "doc": "<p>EWC (Elastic Weight Consolidation) algorithm.</p>\n\n<p><a href=\"https://www.pnas.org/doi/10.1073/pnas.1611835114\">EWC (Elastic Weight Consolidation, 2017)</a> is a regularisation-based continual learning approach that calculates parameter importance for the previous tasks and penalises the current task loss with the importance of the parameters.</p>\n\n<p>We implement EWC as a subclass of Finetuning algorithm, as EWC has the same <code>forward()</code>, <code>validation_step()</code> and <code>test_step()</code> method as <code>Finetuning</code> class.</p>\n", "bases": "clarena.cl_algorithms.finetuning.Finetuning"}, {"fullname": "clarena.cl_algorithms.ewc.EWC.__init__", "modulename": "clarena.cl_algorithms.ewc", "qualname": "EWC.__init__", "kind": "function", "doc": "<p>Initialise the HAT algorithm with the network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone</strong> (<code>CLBackbone</code>): backbone network.</li>\n<li><strong>heads</strong> (<code>HeadsTIL</code> | <code>HeadsCIL</code>): output heads.</li>\n<li><strong>parameter_change_reg_factor</strong> (<code>float</code>): the parameter change regularisation factor. It controls the strength of preventing forgetting.</li>\n<li><strong>parameter_change_reg_p_norm</strong> (<code>float</code>): the norm of the distance of parameters between previous tasks and current task in the parameter change regularisation.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLBackbone</span>,</span><span class=\"param\">\t<span class=\"n\">heads</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_heads</span><span class=\"o\">.</span><span class=\"n\">heads_til</span><span class=\"o\">.</span><span class=\"n\">HeadsTIL</span> <span class=\"o\">|</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_heads</span><span class=\"o\">.</span><span class=\"n\">heads_cil</span><span class=\"o\">.</span><span class=\"n\">HeadsCIL</span>,</span><span class=\"param\">\t<span class=\"n\">parameter_change_reg_factor</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">parameter_change_reg_p_norm</span><span class=\"p\">:</span> <span class=\"nb\">float</span></span>)</span>"}, {"fullname": "clarena.cl_algorithms.ewc.EWC.parameter_importance", "modulename": "clarena.cl_algorithms.ewc", "qualname": "EWC.parameter_importance", "kind": "variable", "doc": "<p>Store the parameter importance of each previous task. Keys are task IDs (string type) and values are the corresponding importance. Each importance entity is a dict where keys are parameter names (named by <code>named_parameters()</code> of the <code>nn.Module</code>) and values are the importance tensor for the layer. It has the same shape as the parameters of the layer.</p>\n", "annotation": ": dict[str, dict[str, torch.Tensor]]"}, {"fullname": "clarena.cl_algorithms.ewc.EWC.previous_task_backbones", "modulename": "clarena.cl_algorithms.ewc", "qualname": "EWC.previous_task_backbones", "kind": "variable", "doc": "<p>Store the backbone models of the previous tasks. Keys are task IDs (string type) and values are the corresponding models. Each model is a <code>nn.Module</code> backbone after the corresponding previous task was trained.</p>\n\n<p>Some would argue that since we could store the model of the previous tasks, why don't we test the task directly with the stored model, instead of doing the less easier EWC thing? The thing is, EWC only uses the model of the previous tasks to train current and future tasks, which aggregate them into a single model. Once the training of the task is done, the storage for those parameters can be released. However, this make the future tasks not able to use EWC anymore, which is a disadvantage for EWC.</p>\n", "annotation": ": dict[str, torch.nn.modules.module.Module]"}, {"fullname": "clarena.cl_algorithms.ewc.EWC.parameter_change_reg_factor", "modulename": "clarena.cl_algorithms.ewc", "qualname": "EWC.parameter_change_reg_factor", "kind": "variable", "doc": "<p>Store parameter change regularisation factor.</p>\n"}, {"fullname": "clarena.cl_algorithms.ewc.EWC.parameter_change_reg_p_norm", "modulename": "clarena.cl_algorithms.ewc", "qualname": "EWC.parameter_change_reg_p_norm", "kind": "variable", "doc": "<p>Store norm of the distance used in parameter change regularisation.</p>\n"}, {"fullname": "clarena.cl_algorithms.ewc.EWC.parameter_change_reg", "modulename": "clarena.cl_algorithms.ewc", "qualname": "EWC.parameter_change_reg", "kind": "variable", "doc": "<p>Initialise and store the parameter change regulariser.</p>\n"}, {"fullname": "clarena.cl_algorithms.ewc.EWC.sanity_check", "modulename": "clarena.cl_algorithms.ewc", "qualname": "EWC.sanity_check", "kind": "function", "doc": "<p>Check the sanity of the arguments.</p>\n\n<p><strong>Raises:</strong></p>\n\n<ul>\n<li><strong>ValueError</strong>: If the regularisation factor is not positive.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.ewc.EWC.calculate_parameter_importance", "modulename": "clarena.cl_algorithms.ewc", "qualname": "EWC.calculate_parameter_importance", "kind": "function", "doc": "<p>Calculate the parameter importance for the learned task. This is only called after the training of a task, which is the last previous task $t-1$. The calculated importance is stored in <code>self.parameter_importance[self.task_id]</code> for constructing the regularisation loss in the future tasks.</p>\n\n<p>According to <a href=\"https://www.pnas.org/doi/10.1073/pnas.1611835114\">the EWC paper</a>, the importance tensor is a Laplace approximation to Fisher information matrix by taking the digonal, i.e. $F_i$, where $i$ is the index of a parameter. The calculation is not following that theory but the derived formula below:</p>\n\n<p>$$\\omega_i = F_i  =\\frac{1}{N_{t-1}} \\sum_{(\\mathbf{x}, y)\\in \\mathcal{D}^{(t-1)}_{\\text{train}}} \\left[\\frac{\\partial l(f^{(t-1)}\\left(\\mathbf{x}, \\theta), y\\right)}{\\partial \\theta_i}\\right]^2$$</p>\n\n<p>For a parameter $i$, its importance is the magnitude (square here) of gradient of the loss of model just trained over the training data just used. The $l$ is the classification loss. It shows the sensitivity of the loss to the parameter. The larger it is, the more it changed the performance (which is the loss) of the model, which indicates the importance of the parameter.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.ewc.EWC.training_step", "modulename": "clarena.cl_algorithms.ewc", "qualname": "EWC.training_step", "kind": "function", "doc": "<p>Training step for current task <code>self.task_id</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of training data.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and other metrics from this training step. Key (<code>str</code>) is the metrics name, value (<code>Tensor</code>) is the metrics. Must include the key 'loss' which is total loss in the case of automatic optimization, according to PyTorch Lightning docs.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.ewc.EWC.on_train_end", "modulename": "clarena.cl_algorithms.ewc", "qualname": "EWC.on_train_end", "kind": "function", "doc": "<p>Calculate the parameter importance and store the backbone model after the training of a task.</p>\n\n<p>The calculated importance and model are stored in <code>self.parameter_importance[self.task_id]</code> and <code>self.previous_task_backbones[self.task_id]</code> respectively for constructing the regularisation loss in the future tasks.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.finetuning", "modulename": "clarena.cl_algorithms.finetuning", "kind": "module", "doc": "<p>The submodule in <code>cl_algorithms</code> for Finetuning algorithm.</p>\n"}, {"fullname": "clarena.cl_algorithms.finetuning.Finetuning", "modulename": "clarena.cl_algorithms.finetuning", "qualname": "Finetuning", "kind": "class", "doc": "<p>Finetuning algorithm.</p>\n\n<p>It is the most naive way for task-incremental learning. It simply initialises the backbone from the last task when training new task.</p>\n", "bases": "clarena.cl_algorithms.base.CLAlgorithm"}, {"fullname": "clarena.cl_algorithms.finetuning.Finetuning.__init__", "modulename": "clarena.cl_algorithms.finetuning", "qualname": "Finetuning.__init__", "kind": "function", "doc": "<p>Initialise the Finetuning algorithm with the network. It has no additional hyperparameters.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone</strong> (<code>CLBackbone</code>): backbone network.</li>\n<li><strong>heads</strong> (<code>HeadsTIL</code> | <code>HeadsCIL</code>): output heads.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLBackbone</span>,</span><span class=\"param\">\t<span class=\"n\">heads</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_heads</span><span class=\"o\">.</span><span class=\"n\">heads_til</span><span class=\"o\">.</span><span class=\"n\">HeadsTIL</span> <span class=\"o\">|</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_heads</span><span class=\"o\">.</span><span class=\"n\">heads_cil</span><span class=\"o\">.</span><span class=\"n\">HeadsCIL</span></span>)</span>"}, {"fullname": "clarena.cl_algorithms.finetuning.Finetuning.forward", "modulename": "clarena.cl_algorithms.finetuning", "qualname": "Finetuning.forward", "kind": "function", "doc": "<p>The forward pass for data from task <code>task_id</code>. Note that it is nothing to do with <code>forward()</code> method in <code>nn.Module</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): The input tensor from data.</li>\n<li><strong>stage</strong> (<code>str</code>): the stage of the forward pass, should be one of the following:\n<ol>\n<li>'train': training stage.</li>\n<li>'validation': validation stage.</li>\n<li>'test': testing stage.</li>\n</ol></li>\n<li><strong>task_id</strong> (<code>int</code>): the task ID where the data are from. If stage is 'train' or <code>validation</code>, it is usually from the current task <code>self.task_id</code>. If stage is 'test', it could be from any seen task. In TIL, the task IDs of test data are provided thus this argument can be used. In CIL, they are not provided, so it is just a placeholder for API consistence but never used, and best practices are not to provide this argument and leave it as the default value. Finetuning algorithm works both for TIL and CIL.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>logits</strong> (<code>Tensor</code>): the output logits tensor.</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): the hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name, value (<code>Tensor</code>) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes. Although Finetuning algorithm does not need this, it is still provided for API consistence for other algorithms inherited this <code>forward()</code> method of <code>Finetuning</code> class.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.finetuning.Finetuning.training_step", "modulename": "clarena.cl_algorithms.finetuning", "qualname": "Finetuning.training_step", "kind": "function", "doc": "<p>Training step for current task <code>self.task_id</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of training data.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and other metrics from this training step. Key (<code>str</code>) is the metrics name, value (<code>Tensor</code>) is the metrics. Must include the key 'loss' which is total loss in the case of automatic optimization, according to PyTorch Lightning docs.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.finetuning.Finetuning.validation_step", "modulename": "clarena.cl_algorithms.finetuning", "qualname": "Finetuning.validation_step", "kind": "function", "doc": "<p>Validation step for current task <code>self.task_id</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of validation data.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and other metrics from this validation step. Key (<code>str</code>) is the metrics name, value (<code>Tensor</code>) is the metrics.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.finetuning.Finetuning.test_step", "modulename": "clarena.cl_algorithms.finetuning", "qualname": "Finetuning.test_step", "kind": "function", "doc": "<p>Test step for current task <code>self.task_id</code>, which tests for all seen tasks indexed by <code>dataloader_idx</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of test data.</li>\n<li><strong>dataloader_idx</strong> (<code>int</code>): the task ID of seen tasks to be tested. A default value of 0 is given otherwise the LightningModule will raise a <code>RuntimeError</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and other metrics from this test step. Key (<code>str</code>) is the metrics name, value (<code>Tensor</code>) is the metrics.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">dataloader_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.fix", "modulename": "clarena.cl_algorithms.fix", "kind": "module", "doc": "<p>The submodule in <code>cl_algorithms</code> for Fix algorithm.</p>\n"}, {"fullname": "clarena.cl_algorithms.fix.Fix", "modulename": "clarena.cl_algorithms.fix", "qualname": "Fix", "kind": "class", "doc": "<p>Fix algorithm.</p>\n\n<p>It is another naive way for task-incremental learning aside from Finetuning. It serves as kind of toy algorithm when discussing stability-plasticity dilemma in continual learning. It simply fixes the backbone forever after training first task.</p>\n\n<p>We implement Fix as a subclass of Finetuning algorithm, as Fix has the same <code>forward()</code>, <code>validation_step()</code> and <code>test_step()</code> method as <code>Finetuning</code> class.</p>\n", "bases": "clarena.cl_algorithms.finetuning.Finetuning"}, {"fullname": "clarena.cl_algorithms.fix.Fix.__init__", "modulename": "clarena.cl_algorithms.fix", "qualname": "Fix.__init__", "kind": "function", "doc": "<p>Initialise the Fix algorithm with the network. It has no additional hyperparameters.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone</strong> (<code>CLBackbone</code>): backbone network.</li>\n<li><strong>heads</strong> (<code>HeadsTIL</code> | <code>HeadsCIL</code>): output heads.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLBackbone</span>,</span><span class=\"param\">\t<span class=\"n\">heads</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_heads</span><span class=\"o\">.</span><span class=\"n\">heads_til</span><span class=\"o\">.</span><span class=\"n\">HeadsTIL</span> <span class=\"o\">|</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_heads</span><span class=\"o\">.</span><span class=\"n\">heads_cil</span><span class=\"o\">.</span><span class=\"n\">HeadsCIL</span></span>)</span>"}, {"fullname": "clarena.cl_algorithms.fix.Fix.training_step", "modulename": "clarena.cl_algorithms.fix", "qualname": "Fix.training_step", "kind": "function", "doc": "<p>Training step for current task <code>self.task_id</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of training data.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and other metrics from this training step. Key (<code>str</code>) is the metrics name, value (<code>Tensor</code>) is the metrics. Must include the key 'loss' which is total loss in the case of automatic optimization, according to PyTorch Lightning docs.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.hat", "modulename": "clarena.cl_algorithms.hat", "kind": "module", "doc": "<p>The submodule in <code>cl_algorithms</code> for <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task) algorithm</a>.</p>\n"}, {"fullname": "clarena.cl_algorithms.hat.HAT", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT", "kind": "class", "doc": "<p>HAT (Hard Attention to the Task) algorithm.</p>\n\n<p><a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task, 2018)</a> is an architecture-based continual learning approach that uses learnable hard attention masks to select the task-specific parameters.</p>\n", "bases": "clarena.cl_algorithms.base.CLAlgorithm"}, {"fullname": "clarena.cl_algorithms.hat.HAT.__init__", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.__init__", "kind": "function", "doc": "<p>Initialise the HAT algorithm with the network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone</strong> (<code>HATMaskBackbone</code>): must be a backbone network with HAT mask mechanism.</li>\n<li><strong>heads</strong> (<code>HeadsTIL</code> | <code>HeadsCIL</code>): output heads.</li>\n<li><strong>adjustment_mode</strong> (<code>str</code>): the strategy of adjustment i.e. the mode of gradient clipping, should be one of the following:\n<ol>\n<li>'hat': set the gradients of parameters linking to masked units to zero. This is the way that HAT does, which fixes the part of network for previous tasks completely. See equation (2) in chapter 2.3 \"Network Training\" in <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</li>\n<li>'hat_random': set the gradients of parameters linking to masked units to random 0-1 values. See the \"Baselines\" section in chapter 4.1 in <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>.</li>\n<li>'hat_const_alpha': set the gradients of parameters linking to masked units to a constant value of <code>alpha</code>. See the \"Baselines\" section in chapter 4.1 in <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>.</li>\n<li>'hat_const_1': set the gradients of parameters linking to masked units to a constant value of 1, which means no gradient constraint on any parameter at all. See the \"Baselines\" section in chapter 4.1 in <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>.</li>\n</ol></li>\n<li><strong>s_max</strong> (<code>float</code>): hyperparameter, the maximum scaling factor in the gate function. See chapter 2.4 \"Hard Attention Training\" in <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</li>\n<li><strong>clamp_threshold</strong> (<code>float</code>): the threshold for task embedding gradient compensation. See chapter 2.5 \"Embedding Gradient Compensation\" in <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</li>\n<li><strong>mask_sparsity_reg_factor</strong> (<code>float</code>): hyperparameter, the regularisation factor for mask sparsity.</li>\n<li><strong>mask_sparsity_reg_mode</strong> (<code>str</code>): the mode of mask sparsity regularisation, should be one of the following:\n<ol>\n<li>'original' (default): the original mask sparsity regularisation in HAT paper.</li>\n<li>'cross': the cross version mask sparsity regularisation.</li>\n</ol></li>\n<li><strong>task_embedding_init_mode</strong> (<code>str</code>): the initialisation mode for task embeddings, should be one of the following:\n<ol>\n<li>'N01' (default): standard normal distribution $N(0, 1)$.</li>\n<li>'U-11': uniform distribution $U(-1, 1)$.</li>\n<li>'U01': uniform distribution $U(0, 1)$.</li>\n<li>'U-10': uniform distribution $U(-1, 0)$.</li>\n<li>'last': inherit task embedding from last task.</li>\n</ol></li>\n<li><strong>alpha</strong> (<code>float</code> | <code>None</code>): the <code>alpha</code> in the 'HAT-const-alpha' mode. See the \"Baselines\" section in chapter 4.1 in <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>. It applies only when adjustment_mode is 'hat_const_alpha'.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">HATMaskBackbone</span>,</span><span class=\"param\">\t<span class=\"n\">heads</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_heads</span><span class=\"o\">.</span><span class=\"n\">heads_til</span><span class=\"o\">.</span><span class=\"n\">HeadsTIL</span> <span class=\"o\">|</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_heads</span><span class=\"o\">.</span><span class=\"n\">heads_cil</span><span class=\"o\">.</span><span class=\"n\">HeadsCIL</span>,</span><span class=\"param\">\t<span class=\"n\">adjustment_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">s_max</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">clamp_threshold</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">mask_sparsity_reg_factor</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">mask_sparsity_reg_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;original&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">task_embedding_init_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;N01&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">alpha</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_algorithms.hat.HAT.adjustment_mode", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.adjustment_mode", "kind": "variable", "doc": "<p>Store the adjustment mode for gradient clipping.</p>\n"}, {"fullname": "clarena.cl_algorithms.hat.HAT.s_max", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.s_max", "kind": "variable", "doc": "<p>Store s_max.</p>\n"}, {"fullname": "clarena.cl_algorithms.hat.HAT.clamp_threshold", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.clamp_threshold", "kind": "variable", "doc": "<p>Store the clamp threshold for task embedding gradient compensation.</p>\n"}, {"fullname": "clarena.cl_algorithms.hat.HAT.mask_sparsity_reg_factor", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.mask_sparsity_reg_factor", "kind": "variable", "doc": "<p>Store the mask sparsity regularisation factor.</p>\n"}, {"fullname": "clarena.cl_algorithms.hat.HAT.mask_sparsity_reg_mode", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.mask_sparsity_reg_mode", "kind": "variable", "doc": "<p>Store the mask sparsity regularisation mode.</p>\n"}, {"fullname": "clarena.cl_algorithms.hat.HAT.mark_sparsity_reg", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.mark_sparsity_reg", "kind": "variable", "doc": "<p>Initialise and store the mask sparsity regulariser.</p>\n"}, {"fullname": "clarena.cl_algorithms.hat.HAT.task_embedding_init_mode", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.task_embedding_init_mode", "kind": "variable", "doc": "<p>Store the task embedding initialisation mode.</p>\n"}, {"fullname": "clarena.cl_algorithms.hat.HAT.alpha", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.alpha", "kind": "variable", "doc": "<p>Store the alpha for <code>hat_const_alpha</code>.</p>\n"}, {"fullname": "clarena.cl_algorithms.hat.HAT.epsilon", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.epsilon", "kind": "variable", "doc": "<p>HAT doesn't use the epsilon for <code>hat_const_alpha</code>. We still set it here to be consistent with the <code>epsilon</code> in <code>clip_grad_by_adjustment()</code> method in <code>HATMaskBackbone</code>.</p>\n"}, {"fullname": "clarena.cl_algorithms.hat.HAT.masks", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.masks", "kind": "variable", "doc": "<p>Store the binary attention mask of each previous task gated from the task embedding. Keys are task IDs (string type) and values are the corresponding mask. Each mask is a dict where keys are layer names and values are the binary mask tensor for the layer. The mask tensor has size (number of units).</p>\n", "annotation": ": dict[str, dict[str, torch.Tensor]]"}, {"fullname": "clarena.cl_algorithms.hat.HAT.cumulative_mask_for_previous_tasks", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.cumulative_mask_for_previous_tasks", "kind": "variable", "doc": "<p>Store the cumulative binary attention mask $\\mathrm{M}^{<t}$ of previous tasks $1,\\cdots, t-1$, gated from the task embedding. Keys are task IDs and values are the corresponding cumulative mask. Each cumulative mask is a dict where keys are layer names and values are the binary mask tensor for the layer. The mask tensor has size (number of units).</p>\n", "annotation": ": dict[str, torch.Tensor]"}, {"fullname": "clarena.cl_algorithms.hat.HAT.automatic_optimization", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.automatic_optimization", "kind": "variable", "doc": "<p>If set to <code>False</code> you are responsible for calling <code>.backward()</code>, <code>.step()</code>, <code>.zero_grad()</code>.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.cl_algorithms.hat.HAT.sanity_check", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.sanity_check", "kind": "function", "doc": "<p>Check the sanity of the arguments.</p>\n\n<p><strong>Raises:</strong></p>\n\n<ul>\n<li><strong>ValueError</strong>: when backbone is not designed for HAT, or the <code>mask_sparsity_reg_mode</code> or <code>task_embedding_init_mode</code> is not one of the valid options. Also, if <code>alpha</code> is not given when <code>adjustment_mode</code> is 'hat_const_alpha'.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.hat.HAT.on_train_start", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.on_train_start", "kind": "function", "doc": "<p>Initialise the task embedding before training the next task and initialise the cumulative mask at the beginning of first task.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.hat.HAT.clip_grad_by_adjustment", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.clip_grad_by_adjustment", "kind": "function", "doc": "<p>Clip the gradients by the adjustment rate.</p>\n\n<p>Note that as the task embedding fully covers every layer in the backbone network, no parameters are left out of this system. This applies not only the parameters in between layers with task embedding, but also those before the first layer. We designed it seperately in the codes.</p>\n\n<p>Network capacity is measured along with this method. Network capacity is defined as the average adjustment rate over all parameters. See chapter 4.1 in <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>capacity</strong> (<code>Tensor</code>): the calculated network capacity.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.hat.HAT.compensate_task_embedding_gradients", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.compensate_task_embedding_gradients", "kind": "function", "doc": "<p>Compensate the gradients of task embeddings during training. See chapter 2.5 \"Embedding Gradient Compensation\" in <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch_idx</strong> (<code>int</code>): the current training batch index.</li>\n<li><strong>num_batches</strong> (<code>int</code>): the total number of training batches.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>, </span><span class=\"param\"><span class=\"n\">num_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.hat.HAT.forward", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.forward", "kind": "function", "doc": "<p>The forward pass for data from task <code>task_id</code>. Note that it is nothing to do with <code>forward()</code> method in <code>nn.Module</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): The input tensor from data.</li>\n<li><strong>stage</strong> (<code>str</code>): the stage of the forward pass, should be one of the following:\n<ol>\n<li>'train': training stage.</li>\n<li>'validation': validation stage.</li>\n<li>'test': testing stage.</li>\n</ol></li>\n<li><strong>batch_idx</strong> (<code>int</code> | <code>None</code>): the current batch index. Applies only to training stage. For other stages, it is default <code>None</code>.</li>\n<li><strong>num_batches</strong> (<code>int</code> | <code>None</code>): the total number of batches. Applies only to training stage. For other stages, it is default <code>None</code>.</li>\n<li><strong>task_id</strong> (<code>int</code>| <code>None</code>): the task ID where the data are from. If the stage is 'train' or 'validation', it should be the current task <code>self.task_id</code>. If stage is 'test', it could be from any seen task. In TIL, the task IDs of test data are provided thus this argument can be used. HAT algorithm works only for TIL.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>logits</strong> (<code>Tensor</code>): the output logits tensor.</li>\n<li><strong>mask</strong> (<code>dict[str, Tensor]</code>): the mask for the current task. Key (<code>str</code>) is layer name, value (<code>Tensor</code>) is the mask tensor. The mask tensor has size (number of units).</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): the hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name, value (<code>Tensor</code>) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes. Although HAT algorithm does not need this, it is still provided for API consistence for other HAT-based algorithms inherited this <code>forward()</code> method of <code>HAT</code> class.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.hat.HAT.training_step", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.training_step", "kind": "function", "doc": "<p>Training step for current task <code>self.task_id</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of training data.</li>\n<li><strong>batch_idx</strong> (<code>int</code>): the index of the batch. Used for calculating annealed scalar in HAT. See chapter 2.4 \"Hard Attention Training\" in <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and other metrics from this training step. Key (<code>str</code>) is the metrics name, value (<code>Tensor</code>) is the metrics. Must include the key 'loss' which is total loss in the case of automatic optimization, according to PyTorch Lightning docs. For HAT, it includes 'mask' and 'capacity' for logging.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>, </span><span class=\"param\"><span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.hat.HAT.on_train_end", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.on_train_end", "kind": "function", "doc": "<p>Store the mask and update cumulative mask after training the task.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.hat.HAT.validation_step", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.validation_step", "kind": "function", "doc": "<p>Validation step for current task <code>self.task_id</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of validation data.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and other metrics from this validation step. Key (<code>str</code>) is the metrics name, value (<code>Tensor</code>) is the metrics.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.hat.HAT.test_step", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.test_step", "kind": "function", "doc": "<p>Test step for current task <code>self.task_id</code>, which tests for all seen tasks indexed by <code>dataloader_idx</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of test data.</li>\n<li><strong>dataloader_idx</strong> (<code>int</code>): the task ID of seen tasks to be tested. A default value of 0 is given otherwise the LightningModule will raise a <code>RuntimeError</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and other metrics from this test step. Key (<code>str</code>) is the metrics name, value (<code>Tensor</code>) is the metrics.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">dataloader_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.lwf", "modulename": "clarena.cl_algorithms.lwf", "kind": "module", "doc": "<p>The submodule in <code>cl_algorithms</code> for <a href=\"https://ieeexplore.ieee.org/abstract/document/8107520\">LwF (Learning without Forgetting) algorithm</a>.</p>\n"}, {"fullname": "clarena.cl_algorithms.lwf.LwF", "modulename": "clarena.cl_algorithms.lwf", "qualname": "LwF", "kind": "class", "doc": "<p>LwF (Learning without Forgetting) algorithm.</p>\n\n<p><a href=\"https://ieeexplore.ieee.org/abstract/document/8107520\">LwF (Learning without Forgetting, 2017)</a> is a regularisation-based continual learning approach that constrains the feature output of the model to be similar to that of the previous tasks. From the perspective of knowledge distillation, it distills previous tasks models into the training process for new task in the regularisation term. It is a simple yet effective method for continual learning.</p>\n\n<p>We implement LwF as a subclass of Finetuning algorithm, as LwF has the same <code>forward()</code>, <code>validation_step()</code> and <code>test_step()</code> method as <code>Finetuning</code> class.</p>\n", "bases": "clarena.cl_algorithms.finetuning.Finetuning"}, {"fullname": "clarena.cl_algorithms.lwf.LwF.__init__", "modulename": "clarena.cl_algorithms.lwf", "qualname": "LwF.__init__", "kind": "function", "doc": "<p>Initialise the LwF algorithm with the network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone</strong> (<code>CLBackbone</code>): backbone network.</li>\n<li><strong>heads</strong> (<code>HeadsTIL</code> | <code>HeadsCIL</code>): output heads.</li>\n<li><strong>distillation_reg_factor</strong> (<code>float</code>): hyperparameter, the distillation regularisation factor. It controls the strength of preventing forgetting.</li>\n<li><strong>distillation_reg_temparture</strong> (<code>float</code>): hyperparameter, the temperature in the distillation regularisation. It controls the softness of the labels that the student model (here is the current model) learns from the teacher models (here are the previous models), thereby controlling the strength of the distillation. It controls the strength of preventing forgetting.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLBackbone</span>,</span><span class=\"param\">\t<span class=\"n\">heads</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_heads</span><span class=\"o\">.</span><span class=\"n\">heads_til</span><span class=\"o\">.</span><span class=\"n\">HeadsTIL</span> <span class=\"o\">|</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_heads</span><span class=\"o\">.</span><span class=\"n\">heads_cil</span><span class=\"o\">.</span><span class=\"n\">HeadsCIL</span>,</span><span class=\"param\">\t<span class=\"n\">distillation_reg_factor</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">distillation_reg_temparture</span><span class=\"p\">:</span> <span class=\"nb\">float</span></span>)</span>"}, {"fullname": "clarena.cl_algorithms.lwf.LwF.previous_task_backbones", "modulename": "clarena.cl_algorithms.lwf", "qualname": "LwF.previous_task_backbones", "kind": "variable", "doc": "<p>Store the backbone models of the previous tasks. Keys are task IDs (string type) and values are the corresponding models. Each model is a <code>nn.Module</code> backbone after the corresponding previous task was trained.</p>\n\n<p>Some would argue that since we could store the model of the previous tasks, why don't we test the task directly with the stored model, instead of doing the less easier LwF thing? The thing is, LwF only uses the model of the previous tasks to train current and future tasks, which aggregate them into a single model. Once the training of the task is done, the storage for those parameters can be released. However, this make the future tasks not able to use LwF anymore, which is a disadvantage for LwF.</p>\n", "annotation": ": dict[str, torch.nn.modules.module.Module]"}, {"fullname": "clarena.cl_algorithms.lwf.LwF.distillation_reg_factor", "modulename": "clarena.cl_algorithms.lwf", "qualname": "LwF.distillation_reg_factor", "kind": "variable", "doc": "<p>Store distillation regularisation factor.</p>\n"}, {"fullname": "clarena.cl_algorithms.lwf.LwF.distillation_reg_temperature", "modulename": "clarena.cl_algorithms.lwf", "qualname": "LwF.distillation_reg_temperature", "kind": "variable", "doc": "<p>Store distillation regularisation temperature.</p>\n"}, {"fullname": "clarena.cl_algorithms.lwf.LwF.distillation_reg", "modulename": "clarena.cl_algorithms.lwf", "qualname": "LwF.distillation_reg", "kind": "variable", "doc": "<p>Initialise and store the distillation regulariser.</p>\n"}, {"fullname": "clarena.cl_algorithms.lwf.LwF.sanity_check", "modulename": "clarena.cl_algorithms.lwf", "qualname": "LwF.sanity_check", "kind": "function", "doc": "<p>Check the sanity of the arguments.</p>\n\n<p><strong>Raises:</strong></p>\n\n<ul>\n<li><strong>ValueError</strong>: If the regularisation factor and distillation temperature is not positive.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.lwf.LwF.training_step", "modulename": "clarena.cl_algorithms.lwf", "qualname": "LwF.training_step", "kind": "function", "doc": "<p>Training step for current task <code>self.task_id</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of training data.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and other metrics from this training step. Key (<code>str</code>) is the metrics name, value (<code>Tensor</code>) is the metrics. Must include the key 'loss' which is total loss in the case of automatic optimization, according to PyTorch Lightning docs.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.lwf.LwF.on_train_end", "modulename": "clarena.cl_algorithms.lwf", "qualname": "LwF.on_train_end", "kind": "function", "doc": "<p>Store the backbone model after the training of a task.</p>\n\n<p>The model is stored in <code>self.previous_task_backbones</code> for constructing the regularisation loss in the future tasks.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.regularisers", "modulename": "clarena.cl_algorithms.regularisers", "kind": "module", "doc": "<h1 id=\"continual-learning-regularisers\">Continual Learning Regularisers</h1>\n\n<p>This submodule provides the <strong>regularisers</strong> which are added to the loss function of corresponding continual learning algorithms in CLArena. It can promote forgetting preventing which is the major mechanism in regularisation-based approaches, or for other purposes.</p>\n\n<p>Please note that this is an API documantation. Please refer to the main documentation pages for more information about the regularisers: </p>\n\n<ul>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/implement-your-cl-modules/cl-algorithm#sec-regularisers\"><strong>Implement your regularisers in CL algorithms</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/posts/continual-learning-beginners-guide#sec-regularisation-based-approaches\"><strong>A Beginners' Guide to Continual Learning (Regularisation-based Approaches)</strong></a></li>\n</ul>\n\n<p>The regularisers are implemented as subclasses of <code>nn.Module</code>.</p>\n"}, {"fullname": "clarena.cl_algorithms.regularisers.distillation", "modulename": "clarena.cl_algorithms.regularisers.distillation", "kind": "module", "doc": "<p>The submodule in <code>regularisers</code> for distillation regularisation.</p>\n"}, {"fullname": "clarena.cl_algorithms.regularisers.distillation.DistillationReg", "modulename": "clarena.cl_algorithms.regularisers.distillation", "qualname": "DistillationReg", "kind": "class", "doc": "<p>Distillation regulariser. This is the core of <a href=\"https://research.google/pubs/distilling-the-knowledge-in-a-neural-network/\">knowledge distillation</a> used as a regulariser in continual learning.</p>\n\n<p>$$R(\\theta^{\\text{student}}) = \\text{factor} * \\frac1N \\sum_{(\\mathbf{x}, y)\\in \\mathcal{D}} \\text{distance}\\left(f(\\mathbf{x};\\theta^{\\text{student}}),f(\\mathbf{x};\\theta^{\\text{teacher}})\\right)$$</p>\n\n<p>It promotes the target (student) model output logits $f(\\mathbf{x};\\theta^{\\text{student}})$ not changing too much from the reference (teacher) model output logits $f(\\mathbf{x};\\theta^{\\text{teacher}})$. The loss is averaged over the dataset $\\mathcal{D}$.</p>\n\n<p>It is used in:</p>\n\n<ul>\n<li><a href=\"https://ieeexplore.ieee.org/abstract/document/8107520\">LwF (Learning without Forgetting) algorithm</a>: as a distillation regulariser for the output logits by current task model to be closer to output logits by previous tasks models. It uses a modified cross entropy as the distance. See equation (2) (3) in the <a href=\"https://ieeexplore.ieee.org/abstract/document/8107520\">LwF paper</a>.</li>\n</ul>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "clarena.cl_algorithms.regularisers.distillation.DistillationReg.__init__", "modulename": "clarena.cl_algorithms.regularisers.distillation", "qualname": "DistillationReg.__init__", "kind": "function", "doc": "<p>Initialise the regulariser.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>factor</strong> (<code>float</code>): the regularisation factor.</li>\n<li><strong>temperature</strong> (<code>float</code>): the temperature of the distillation, should be a positive float.</li>\n<li><strong>distance</strong> (<code>str</code>): the type of distance function used in the distillation, should be one of the following:\n<ol>\n<li>\"lwf_cross_entropy\": the modified cross entropy loss from LwF. See equation (3) in the <a href=\"https://ieeexplore.ieee.org/abstract/document/8107520\">LwF paper</a>.</li>\n</ol></li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">factor</span><span class=\"p\">:</span> <span class=\"nb\">float</span>, </span><span class=\"param\"><span class=\"n\">temperature</span><span class=\"p\">:</span> <span class=\"nb\">float</span>, </span><span class=\"param\"><span class=\"n\">distance</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span>)</span>"}, {"fullname": "clarena.cl_algorithms.regularisers.distillation.DistillationReg.factor", "modulename": "clarena.cl_algorithms.regularisers.distillation", "qualname": "DistillationReg.factor", "kind": "variable", "doc": "<p>Store the regularisation factor for distillation.</p>\n"}, {"fullname": "clarena.cl_algorithms.regularisers.distillation.DistillationReg.temperature", "modulename": "clarena.cl_algorithms.regularisers.distillation", "qualname": "DistillationReg.temperature", "kind": "variable", "doc": "<p>Store the temperature of the distillation.</p>\n"}, {"fullname": "clarena.cl_algorithms.regularisers.distillation.DistillationReg.distance", "modulename": "clarena.cl_algorithms.regularisers.distillation", "qualname": "DistillationReg.distance", "kind": "variable", "doc": "<p>Store the type of distance function used in the distillation.</p>\n"}, {"fullname": "clarena.cl_algorithms.regularisers.distillation.DistillationReg.forward", "modulename": "clarena.cl_algorithms.regularisers.distillation", "qualname": "DistillationReg.forward", "kind": "function", "doc": "<p>Calculate the regularisation loss.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>student_logits</strong> (<code>Tensor</code>): the output logits of target (student) model to learn the knowledge from distillation. In LwF, it's the model of current training task.</li>\n<li><strong>teacher_logits</strong> (<code>Tensor</code>): the output logits of reference (teacher) model that knowledge is distilled. In LwF, it's the model of one of the previous tasks.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>reg</strong> (<code>Tensor</code>): the distillation regularisation value.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">student_logits</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"n\">teacher_logits</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.regularisers.hat_mask_sparsity", "modulename": "clarena.cl_algorithms.regularisers.hat_mask_sparsity", "kind": "module", "doc": "<p>The submodule in <code>regularisers</code> for HAT mask sparsity regularisation.</p>\n"}, {"fullname": "clarena.cl_algorithms.regularisers.hat_mask_sparsity.HATMaskSparsityReg", "modulename": "clarena.cl_algorithms.regularisers.hat_mask_sparsity", "qualname": "HATMaskSparsityReg", "kind": "class", "doc": "<p>Mask sparsity regulariser of HAT (Hard Attention to the Task).</p>\n\n<p>$$\nR\\left(\\textsf{M}^t,\\textsf{M}^{<t}\\right)=\\text{factor} * \\frac{\\sum_{l=1}^{L-1}\\sum_{i=1}^{N_l}m_{l,i}^t\\left(1-m_{l,i}^{<t}\\right)}{\\sum_{l=1}^{L-1}\\sum_{i=1}^{N_l}\\left(1-m_{l,i}^{<t}\\right)}\n$$</p>\n\n<p>It promotes the low capacity usage that is reflected by occupation of masks in the parameter space.</p>\n\n<p>See chapter 2.6 \"Promoting Low Capacity Usage\" in <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "clarena.cl_algorithms.regularisers.hat_mask_sparsity.HATMaskSparsityReg.__init__", "modulename": "clarena.cl_algorithms.regularisers.hat_mask_sparsity", "qualname": "HATMaskSparsityReg.__init__", "kind": "function", "doc": "<p>Initialise the regulariser.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>factor</strong> (<code>float</code>): the regularisation factor.</li>\n<li><strong>mode</strong> (<code>str</code>): the mode of mask sparsity regularisation, should be one of the following:\n<ol>\n<li>'original' (default): the original mask sparsity regularisation in HAT paper.</li>\n<li>'cross': the cross version mask sparsity regularisation.</li>\n</ol></li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">factor</span><span class=\"p\">:</span> <span class=\"nb\">float</span>, </span><span class=\"param\"><span class=\"n\">mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;original&#39;</span></span>)</span>"}, {"fullname": "clarena.cl_algorithms.regularisers.hat_mask_sparsity.HATMaskSparsityReg.factor", "modulename": "clarena.cl_algorithms.regularisers.hat_mask_sparsity", "qualname": "HATMaskSparsityReg.factor", "kind": "variable", "doc": "<p>Store the regularisation factor for mask sparsity.</p>\n"}, {"fullname": "clarena.cl_algorithms.regularisers.hat_mask_sparsity.HATMaskSparsityReg.mode", "modulename": "clarena.cl_algorithms.regularisers.hat_mask_sparsity", "qualname": "HATMaskSparsityReg.mode", "kind": "variable", "doc": "<p>Store the mode of mask sparsity regularisation.</p>\n"}, {"fullname": "clarena.cl_algorithms.regularisers.hat_mask_sparsity.HATMaskSparsityReg.forward", "modulename": "clarena.cl_algorithms.regularisers.hat_mask_sparsity", "qualname": "HATMaskSparsityReg.forward", "kind": "function", "doc": "<p>Calculate the mask sparsity regularisation loss.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>mask</strong> (<code>dict[str, Tensor]</code>): the mask for the current task.</li>\n<li><strong>previous_cumulative_mask</strong> (<code>dict[str, Tensor]</code>): the cumulative mask for the previous tasks.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>reg</strong> (<code>Tensor</code>): the mask sparsity regularisation value.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">mask</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">previous_cumulative_mask</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.regularisers.hat_mask_sparsity.HATMaskSparsityReg.original_reg", "modulename": "clarena.cl_algorithms.regularisers.hat_mask_sparsity", "qualname": "HATMaskSparsityReg.original_reg", "kind": "function", "doc": "<p>Calculate the original mask sparsity regularisation loss in HAT paper.</p>\n\n<p>See chapter 2.6 \"Promoting Low Capacity Usage\" in <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>mask</strong> (<code>dict[str, Tensor]</code>): the mask for the current task. The $\\mathrm{A}^t$ in the paper.</li>\n<li><strong>previous_cumulative_mask</strong> (<code>dict[str, Tensor]</code>): the cumulative mask for the previous tasks. The $\\mathrm{A}^{<t}$ in the paper.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>reg</strong> (<code>Tensor</code>): the original mask sparsity regularisation loss.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">mask</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">previous_cumulative_mask</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.regularisers.hat_mask_sparsity.HATMaskSparsityReg.cross_reg", "modulename": "clarena.cl_algorithms.regularisers.hat_mask_sparsity", "qualname": "HATMaskSparsityReg.cross_reg", "kind": "function", "doc": "<p>Calculate the cross mask sparsity regularisation loss. This is an attempting improvement by me to the original regularisation, which not only considers the sparsity in available units but also the density in the units occupied by previous tasks.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>mask</strong> (<code>dict[str, Tensor]</code>): the mask for the current task. The $\\mathrm{A}^t$ in the paper.</li>\n<li><strong>previous_cumulative_mask</strong> (<code>dict[str, Tensor]</code>): the cumulative mask for the previous tasks. The $\\mathrm{A}^{<t}$ in the paper.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>reg</strong> (<code>Tensor</code>): the cross mask sparsity regularisation loss.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">mask</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">previous_cumulative_mask</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.regularisers.parameter_change", "modulename": "clarena.cl_algorithms.regularisers.parameter_change", "kind": "module", "doc": "<p>The submodule in <code>regularisers</code> for parameter change regularisation.</p>\n"}, {"fullname": "clarena.cl_algorithms.regularisers.parameter_change.ParameterChangeReg", "modulename": "clarena.cl_algorithms.regularisers.parameter_change", "qualname": "ParameterChangeReg", "kind": "class", "doc": "<p>Parameter change regulariser.</p>\n\n<p>$$R(\\theta) = \\text{factor} * \\sum_i w_i \\|\\theta_i - \\theta^\\star_i\\|^p$$</p>\n\n<p>It promotes the target set of parameters $\\theta = {\\theta_i}_i$ not changing too much from another set of parameters $\\theta^\\star = {\\theta^\\star_i}_i$. The parameter distance here is $L^p$ distance. The regularisation can be parameter-wise weighted, i.e. $w_i$ in the formula.</p>\n\n<p>It is used in:</p>\n\n<ul>\n<li><a href=\"https://www.pnas.org/doi/10.1073/pnas.1611835114\">L2 Regularisation algorithm</a>: as a L2 regulariser for the current task parameters to prevent them from changing too much from the previous task parameters.</li>\n<li><a href=\"https://www.pnas.org/doi/10.1073/pnas.1611835114\">EWC (Elastic Weight Consolidation) algorithm</a>: as a weighted L2 regulariser for the current task parameters to prevent them from changing too much from the previous task parameters. The regularisation weights are parameter importance measure calculated from fisher information. See equation 3 in the <a href=\"https://www.pnas.org/doi/10.1073/pnas.1611835114\">EWC paper</a>.</li>\n</ul>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "clarena.cl_algorithms.regularisers.parameter_change.ParameterChangeReg.__init__", "modulename": "clarena.cl_algorithms.regularisers.parameter_change", "qualname": "ParameterChangeReg.__init__", "kind": "function", "doc": "<p>Initialise the regulariser.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>factor</strong> (<code>float</code>): the regularisation factor. Note that it is $\\frac{\\lambda}{2}$ rather than $\\lambda$ in the <a href=\"https://www.pnas.org/doi/10.1073/pnas.1611835114\">EWC paper</a>.</li>\n<li><strong>p_norm</strong> (<code>float</code>): the norm of the distance, should be a positive float.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">factor</span><span class=\"p\">:</span> <span class=\"nb\">float</span>, </span><span class=\"param\"><span class=\"n\">p_norm</span><span class=\"p\">:</span> <span class=\"nb\">float</span></span>)</span>"}, {"fullname": "clarena.cl_algorithms.regularisers.parameter_change.ParameterChangeReg.factor", "modulename": "clarena.cl_algorithms.regularisers.parameter_change", "qualname": "ParameterChangeReg.factor", "kind": "variable", "doc": "<p>Store the regularisation factor for parameter change.</p>\n"}, {"fullname": "clarena.cl_algorithms.regularisers.parameter_change.ParameterChangeReg.p_norm", "modulename": "clarena.cl_algorithms.regularisers.parameter_change", "qualname": "ParameterChangeReg.p_norm", "kind": "variable", "doc": "<p>Store the the norm of the distance of two set of parameters.</p>\n"}, {"fullname": "clarena.cl_algorithms.regularisers.parameter_change.ParameterChangeReg.forward", "modulename": "clarena.cl_algorithms.regularisers.parameter_change", "qualname": "ParameterChangeReg.forward", "kind": "function", "doc": "<p>Calculate the regularisation loss.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>target_model</strong> (<code>nn.Module</code>): the model of the target parameters. In EWC, it's the model of current training task.</li>\n<li><strong>ref_model</strong> (<code>nn.Module</code>): the reference model that you want target model parameters to prevent changing from. The reference model must have the same structure as the target model. In EWC, it's the model of one of the previous tasks.</li>\n<li><strong>weights</strong> (<code>dict[str, Tensor]</code>): the regularisation weight for each parameter. Keys are parameter names and values are the weight tensors. The weight tensors must match the shape of model parameters. In EWC, it's the importance measure of each parameter, calculated from fisher information thing.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>reg</strong> (<code>Tensor</code>): the parameter change regularisation value.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">target_model</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"n\">ref_model</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"n\">weights</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets", "modulename": "clarena.cl_datasets", "kind": "module", "doc": "<h1 id=\"continual-learning-datasets\">Continual Learning Datasets</h1>\n\n<p>This submodule provides the <strong>continual learning datasets</strong> that can be used in CLArena. </p>\n\n<p>Please note that this is an API documantation. Please refer to the main documentation pages for more information about the CL datasets and how to configure and implement them:</p>\n\n<ul>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/configure-your-experiment/cl-dataset\"><strong>Configure CL Dataset</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/implement-your-cl-modules/cl-dataset\"><strong>Implement Your CL Dataset Class</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/posts/continual-learning-beginners-guide#sec-CL-dataset\"><strong>A Beginners' Guide to Continual Learning (CL Dataset)</strong></a></li>\n</ul>\n\n<p>The datasets are implemented as subclasses of <code>CLDataset</code> classes, which are the base class for all continual learning datasets in CLArena.</p>\n\n<ul>\n<li><code>CLDataset</code>: The base class for continual learning datasets.</li>\n<li><code>CLPermutedDataset</code>: The base class for permuted continual learning datasets. A child class of <code>CLDataset</code>.</li>\n</ul>\n"}, {"fullname": "clarena.cl_datasets.CLDataset", "modulename": "clarena.cl_datasets", "qualname": "CLDataset", "kind": "class", "doc": "<p>The base class of continual learning datasets, inherited from <code>LightningDataModule</code>.</p>\n", "bases": "lightning.pytorch.core.datamodule.LightningDataModule"}, {"fullname": "clarena.cl_datasets.CLDataset.__init__", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.__init__", "kind": "function", "doc": "<p>Initialise the CL dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original data files for constructing the CL dataset physically live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some of the training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform. <code>ToTensor()</code>, normalise, permute and so on are not included.</li>\n<li><strong>custom_target_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom target transforms to apply to dataset labels. Can be a single transform, composed transforms or no transform. CL class mapping is not included.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">10</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">custom_target_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.CLDataset.root", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.root", "kind": "variable", "doc": "<p>Store the root directory of the original data files. Used when constructing the dataset.</p>\n", "annotation": ": str"}, {"fullname": "clarena.cl_datasets.CLDataset.num_tasks", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.num_tasks", "kind": "variable", "doc": "<p>Store the maximum number of tasks supported by the dataset.</p>\n", "annotation": ": int"}, {"fullname": "clarena.cl_datasets.CLDataset.validation_percentage", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.validation_percentage", "kind": "variable", "doc": "<p>Store the percentage to randomly split some of the training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.CLDataset.batch_size", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.batch_size", "kind": "variable", "doc": "<p>Store the batch size. Used when constructing train, val, test dataloader.</p>\n", "annotation": ": int"}, {"fullname": "clarena.cl_datasets.CLDataset.num_workers", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.num_workers", "kind": "variable", "doc": "<p>Store the number of workers. Used when constructing train, val, test dataloader.</p>\n", "annotation": ": int"}, {"fullname": "clarena.cl_datasets.CLDataset.custom_transforms", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.custom_transforms", "kind": "variable", "doc": "<p>Store the custom transforms other than the basics. Used when constructing the dataset.</p>\n", "annotation": ": Union[Callable, torchvision.transforms.transforms.Compose, NoneType]"}, {"fullname": "clarena.cl_datasets.CLDataset.custom_target_transforms", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.custom_target_transforms", "kind": "variable", "doc": "<p>Store the custom target transforms other than the CL class mapping. Used when constructing the dataset.</p>\n", "annotation": ": Union[Callable, torchvision.transforms.transforms.Compose, NoneType]"}, {"fullname": "clarena.cl_datasets.CLDataset.task_id", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.task_id", "kind": "variable", "doc": "<p>Task ID counter indicating which task is being processed. Self updated during the task loop.</p>\n", "annotation": ": int"}, {"fullname": "clarena.cl_datasets.CLDataset.cl_paradigm", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.cl_paradigm", "kind": "variable", "doc": "<p>Store the continual learning paradigm, either 'TIL' (Task-Incremental Learning) or 'CIL' (Class-Incremental Learning). Gotten from <code>set_cl_paradigm</code> and used to define the CL class map.</p>\n", "annotation": ": str"}, {"fullname": "clarena.cl_datasets.CLDataset.cl_class_map_t", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.cl_class_map_t", "kind": "variable", "doc": "<p>Store the CL class map for the current task <code>self.task_id</code>.</p>\n", "annotation": ": dict[str | int, int]"}, {"fullname": "clarena.cl_datasets.CLDataset.cl_class_mapping_t", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.cl_class_mapping_t", "kind": "variable", "doc": "<p>Store the CL class mapping transform for the current task <code>self.task_id</code>.</p>\n", "annotation": ": Callable"}, {"fullname": "clarena.cl_datasets.CLDataset.dataset_train", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.dataset_train", "kind": "variable", "doc": "<p>The training dataset object. Can be a PyTorch Dataset object or any other dataset object.</p>\n", "annotation": ": object"}, {"fullname": "clarena.cl_datasets.CLDataset.dataset_val", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.dataset_val", "kind": "variable", "doc": "<p>The validation dataset object. Can be a PyTorch Dataset object or any other dataset object.</p>\n", "annotation": ": object"}, {"fullname": "clarena.cl_datasets.CLDataset.dataset_test", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.dataset_test", "kind": "variable", "doc": "<p>The dictionary to store test dataset object. Keys are task IDs (string type) and values are the dataset objects. Can be PyTorch Dataset objects or any other dataset objects.</p>\n", "annotation": ": dict[str, object]"}, {"fullname": "clarena.cl_datasets.CLDataset.sanity_check", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.sanity_check", "kind": "function", "doc": "<p>Check the sanity of the arguments.</p>\n\n<p><strong>Raises:</strong></p>\n\n<ul>\n<li><strong>ValueError</strong>: when the <code>validation_percentage</code> is not in the range of 0-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLDataset.cl_class_map", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.cl_class_map", "kind": "function", "doc": "<p>The mapping of classes of task <code>task_id</code> to fit continual learning settings <code>self.cl_paradigm</code>. It must be implemented by subclasses.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): The task ID to query CL class map.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>cl_class_map</strong>(<code>dict[str | int, int]</code>): the CL class map of the task. Key is original class label, value is integer class label for continual learning.\n<ul>\n<li>If <code>self.cl_paradigm</code> is 'TIL', the mapped class labels of a task should be continuous integers from 0 to the number of classes.</li>\n<li>If <code>self.cl_paradigm</code> is 'CIL', the mapped class labels of a task should be continuous integers from the number of classes of previous tasks to the number of classes of the current task.</li>\n</ul></li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLDataset.prepare_data", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.prepare_data", "kind": "function", "doc": "<p>Use this to download and prepare data. It must be implemented by subclasses, regulated by <code>LightningDatamodule</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLDataset.setup", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.setup", "kind": "function", "doc": "<p>Set up the dataset for different stages.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>stage</strong> (<code>str</code>): the stage of the experiment. Should be one of the following:\n<ul>\n<li>'fit' or 'validation': training and validation dataset of current task <code>self.task_id</code> should be assigned to <code>self.dataset_train_t</code> and <code>self.dataset_val_t</code>.</li>\n<li>'test': a list of test dataset of all seen tasks (from task 0 to <code>self.task_id</code>) should be assigned to <code>self.dataset_test</code>.</li>\n</ul></li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLDataset.setup_task_id", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.setup_task_id", "kind": "function", "doc": "<p>Set up which task's dataset the CL experiment is on. This must be done before <code>setup()</code> method is called.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLDataset.set_cl_paradigm", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.set_cl_paradigm", "kind": "function", "doc": "<p>Set the continual learning paradigm to <code>self.cl_paradigm</code>. It is used to define the CL class map.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>cl_paradigm</strong> (<code>str</code>): the continual learning paradigmeither 'TIL' (Task-Incremental Learning) or 'CIL' (Class-Incremental Learning).</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">cl_paradigm</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLDataset.mean", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.mean", "kind": "function", "doc": "<p>The mean values for normalisation of task <code>task_id</code>. Used when constructing the dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>mean</strong> (<code>tuple[float]</code>): the mean values for normalisation.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLDataset.std", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.std", "kind": "function", "doc": "<p>The standard deviation values for normalisation of task <code>task_id</code>. Used when constructing the dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>std</strong> (<code>tuple[float]</code>): the standard deviation values for normalisation.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLDataset.train_and_val_transforms", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.train_and_val_transforms", "kind": "function", "doc": "<p>Transforms generator for train and validation dataset incorporating the custom transforms with basic transforms like <code>normalisation</code> and <code>ToTensor()</code>. It is a handy tool to use in subclasses when constructing the dataset.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>to_tensor</strong> (<code>bool</code>): whether to include <code>ToTensor()</code> transform.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_transforms</strong> (<code>transforms.Compose</code>): the composed training transforms.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLDataset.test_transforms", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.test_transforms", "kind": "function", "doc": "<p>Transforms generator for test dataset. Only basic transforms like <code>normalisation</code> and <code>ToTensor()</code> are included. It is a handy tool to use in subclasses when constructing the dataset.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>to_tensor</strong> (<code>bool</code>): whether to include <code>ToTensor()</code> transform.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_transforms</strong> (<code>transforms.Compose</code>): the composed training transforms.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLDataset.target_transforms", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.target_transforms", "kind": "function", "doc": "<p>The target transform for the dataset. It is a handy tool to use in subclasses when constructing the dataset.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>target</strong> (<code>Tensor</code>): the target tensor.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>target_transforms</strong> (<code>transforms.Compose</code>): the transformed target tensor.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLDataset.train_and_val_dataset", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>. It must be implemented by subclasses.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>Any</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Any</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLDataset.test_dataset", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>. It must be implemented by subclasses.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>Any</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Any</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLDataset.train_dataloader", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.train_dataloader", "kind": "function", "doc": "<p>DataLoader generator for stage train of task <code>self.task_id</code>. It is automatically called before training.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_dataloader</strong> (<code>Dataloader</code>): the train DataLoader of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLDataset.val_dataloader", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.val_dataloader", "kind": "function", "doc": "<p>DataLoader generator for stage validate. It is automatically called before validation.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>val_dataloader</strong> (<code>Dataloader</code>): the validation DataLoader of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLDataset.test_dataloader", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.test_dataloader", "kind": "function", "doc": "<p>DataLoader generator for stage test. It is automatically called before testing.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataloader</strong> (<code>dict[int, DataLoader]</code>): the test DataLoader dict of <code>self.task_id</code> and all tasks before (as the test is conducted on all seen tasks). Keys are task IDs (integer type) and values are the DataLoaders.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLPermutedDataset", "modulename": "clarena.cl_datasets", "qualname": "CLPermutedDataset", "kind": "class", "doc": "<p>The base class of continual learning datasets which are constructed as permutations from an original dataset, inherited from <code>CLDataset</code>.</p>\n", "bases": "clarena.cl_datasets.base.CLDataset"}, {"fullname": "clarena.cl_datasets.CLPermutedDataset.__init__", "modulename": "clarena.cl_datasets", "qualname": "CLPermutedDataset.__init__", "kind": "function", "doc": "<p>Initialise the CL dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original data files for constructing the CL dataset physically live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some of the training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform. <code>ToTensor()</code>, normalise, permute and so on are not included.</li>\n<li><strong>custom_target_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom target transforms to apply to dataset labels. Can be a single transform, composed transforms or no transform. CL class mapping is not included.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation, should be one of the following:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>list[int]</code> or <code>None</code>): the seeds for permutation operations used to construct tasks. Make sure it has the same number of seeds as <code>num_tasks</code>. Default is None, which creates a list of seeds from 1 to <code>num_tasks</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">10</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">custom_target_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.CLPermutedDataset.num_classes", "modulename": "clarena.cl_datasets", "qualname": "CLPermutedDataset.num_classes", "kind": "variable", "doc": "<p>The number of classes in the original dataset before permutation. It must be provided in subclasses.</p>\n", "annotation": ": int"}, {"fullname": "clarena.cl_datasets.CLPermutedDataset.img_size", "modulename": "clarena.cl_datasets", "qualname": "CLPermutedDataset.img_size", "kind": "variable", "doc": "<p>The size of images in the original dataset before permutation. Used when constructing permutation operations. It must be provided in subclasses.</p>\n", "annotation": ": torch.Size"}, {"fullname": "clarena.cl_datasets.CLPermutedDataset.mean_original", "modulename": "clarena.cl_datasets", "qualname": "CLPermutedDataset.mean_original", "kind": "variable", "doc": "<p>The mean values for normalisation. It must be provided in subclasses.</p>\n", "annotation": ": tuple[float]"}, {"fullname": "clarena.cl_datasets.CLPermutedDataset.std_original", "modulename": "clarena.cl_datasets", "qualname": "CLPermutedDataset.std_original", "kind": "variable", "doc": "<p>The standard deviation values for normalisation. It must be provided in subclasses.</p>\n", "annotation": ": tuple[float]"}, {"fullname": "clarena.cl_datasets.CLPermutedDataset.permutation_mode", "modulename": "clarena.cl_datasets", "qualname": "CLPermutedDataset.permutation_mode", "kind": "variable", "doc": "<p>Store the mode of permutation. Used when permutation operations used to construct tasks.</p>\n", "annotation": ": str"}, {"fullname": "clarena.cl_datasets.CLPermutedDataset.permutation_seeds", "modulename": "clarena.cl_datasets", "qualname": "CLPermutedDataset.permutation_seeds", "kind": "variable", "doc": "<p>Store the permutation seeds for all tasks. Use when permutation operations used to construct tasks.</p>\n", "annotation": ": list[int]"}, {"fullname": "clarena.cl_datasets.CLPermutedDataset.permutation_seed_t", "modulename": "clarena.cl_datasets", "qualname": "CLPermutedDataset.permutation_seed_t", "kind": "variable", "doc": "<p>Store the permutation seed for the current task <code>self.task_id</code>.</p>\n", "annotation": ": int"}, {"fullname": "clarena.cl_datasets.CLPermutedDataset.permute_t", "modulename": "clarena.cl_datasets", "qualname": "CLPermutedDataset.permute_t", "kind": "variable", "doc": "<p>Store the permutation transform for the current task <code>self.task_id</code>.</p>\n", "annotation": ": clarena.cl_datasets.base.Permute"}, {"fullname": "clarena.cl_datasets.CLPermutedDataset.sanity_check", "modulename": "clarena.cl_datasets", "qualname": "CLPermutedDataset.sanity_check", "kind": "function", "doc": "<p>Check the sanity of the arguments.</p>\n\n<p><strong>Raises:</strong></p>\n\n<ul>\n<li><strong>ValueError</strong>: when the <code>permutation_seeds</code> is not equal to <code>num_tasks</code>, or the <code>permutation_mode</code> is not one of the valid options.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLPermutedDataset.cl_class_map", "modulename": "clarena.cl_datasets", "qualname": "CLPermutedDataset.cl_class_map", "kind": "function", "doc": "<p>The mapping of classes of task <code>task_id</code> to fit continual learning settings <code>self.cl_paradigm</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): The task ID to query CL class map.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>cl_class_map</strong>(<code>dict[str | int, int]</code>): the CL class map of the task. Key is original class label, value is integer class label for continual learning.\n<ul>\n<li>If <code>self.cl_paradigm</code> is 'TIL', the mapped class labels of a task should be continuous integers from 0 to the number of classes.</li>\n<li>If <code>self.cl_paradigm</code> is 'CIL', the mapped class labels of a task should be continuous integers from the number of classes of previous tasks to the number of classes of the current task.</li>\n</ul></li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLPermutedDataset.setup_task_id", "modulename": "clarena.cl_datasets", "qualname": "CLPermutedDataset.setup_task_id", "kind": "function", "doc": "<p>Set up which task's dataset the CL experiment is on. This must be done before <code>setup()</code> method is called.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLPermutedDataset.mean", "modulename": "clarena.cl_datasets", "qualname": "CLPermutedDataset.mean", "kind": "function", "doc": "<p>The mean values for normalisation of task <code>task_id</code>. Used when constructing the dataset. In permuted CL dataset, the mean values are the same as the original dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>mean</strong> (<code>tuple[float]</code>): the mean values for normalisation.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLPermutedDataset.std", "modulename": "clarena.cl_datasets", "qualname": "CLPermutedDataset.std", "kind": "function", "doc": "<p>The standard deviation values for normalisation of task <code>task_id</code>. Used when constructing the dataset. In permuted CL dataset, the mean values are the same as the original dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>std</strong> (<code>tuple[float]</code>): the standard deviation values for normalisation.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLPermutedDataset.train_and_val_transforms", "modulename": "clarena.cl_datasets", "qualname": "CLPermutedDataset.train_and_val_transforms", "kind": "function", "doc": "<p>Transforms generator for train and validation dataset incorporating the custom transforms with basic transforms like <code>normalisation</code> and <code>ToTensor()</code>. In permuted CL datasets, permute transform also applies. It is a handy tool to use in subclasses when constructing the dataset.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>to_tensor</strong> (<code>bool</code>): whether to include <code>ToTensor()</code> transform.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_transforms</strong> (<code>transforms.Compose</code>): the composed training transforms.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLPermutedDataset.test_transforms", "modulename": "clarena.cl_datasets", "qualname": "CLPermutedDataset.test_transforms", "kind": "function", "doc": "<p>Transforms generator for test dataset. Only basic transforms like <code>normalisation</code> and <code>ToTensor()</code> are included. It is a handy tool to use in subclasses when constructing the dataset.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>to_tensor</strong> (<code>bool</code>): whether to include <code>ToTensor()</code> transform.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_transforms</strong> (<code>transforms.Compose</code>): the composed training transforms.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLSplitDataset", "modulename": "clarena.cl_datasets", "qualname": "CLSplitDataset", "kind": "class", "doc": "<p>The base class of continual learning datasets, which are constructed as permutations from an original dataset, inherited from <code>CLDataset</code>.</p>\n", "bases": "clarena.cl_datasets.base.CLDataset"}, {"fullname": "clarena.cl_datasets.CLSplitDataset.__init__", "modulename": "clarena.cl_datasets", "qualname": "CLSplitDataset.__init__", "kind": "function", "doc": "<p>Initialise the CL dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original data files for constructing the CL dataset physically live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset.</li>\n<li><strong>class_split</strong> (<code>list[list[int]]</code>): the class split for each task. Each element in the list is a list of class labels (integers starting from 0) to split for a task.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some of the training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform. <code>ToTensor()</code>, normalise, permute and so on are not included.</li>\n<li><strong>custom_target_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom target transforms to apply to dataset labels. Can be a single transform, composed transforms or no transform. CL class mapping is not included.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">class_split</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">10</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">custom_target_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.CLSplitDataset.num_classes", "modulename": "clarena.cl_datasets", "qualname": "CLSplitDataset.num_classes", "kind": "variable", "doc": "<p>The number of classes in the original dataset before permutation. It must be provided in subclasses.</p>\n", "annotation": ": int"}, {"fullname": "clarena.cl_datasets.CLSplitDataset.mean_original", "modulename": "clarena.cl_datasets", "qualname": "CLSplitDataset.mean_original", "kind": "variable", "doc": "<p>The mean values for normalisation. It must be provided in subclasses.</p>\n", "annotation": ": tuple[float]"}, {"fullname": "clarena.cl_datasets.CLSplitDataset.std_original", "modulename": "clarena.cl_datasets", "qualname": "CLSplitDataset.std_original", "kind": "variable", "doc": "<p>The standard deviation values for normalisation. It must be provided in subclasses.</p>\n", "annotation": ": tuple[float]"}, {"fullname": "clarena.cl_datasets.CLSplitDataset.class_split", "modulename": "clarena.cl_datasets", "qualname": "CLSplitDataset.class_split", "kind": "variable", "doc": "<p>Store the class split for each task. Used when constructing the split dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.CLSplitDataset.sanity_check", "modulename": "clarena.cl_datasets", "qualname": "CLSplitDataset.sanity_check", "kind": "function", "doc": "<p>Check the sanity of the arguments.</p>\n\n<p><strong>Raises:</strong></p>\n\n<ul>\n<li><strong>ValueError</strong>: when the length of <code>class_split</code> is not equal to <code>num_tasks</code>.</li>\n<li><strong>ValueError</strong>: when any of the lists in <code>class_split</code> has less than 2 elements. A classification task must have less than 2 classes.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLSplitDataset.cl_class_map", "modulename": "clarena.cl_datasets", "qualname": "CLSplitDataset.cl_class_map", "kind": "function", "doc": "<p>The mapping of classes of task <code>task_id</code> to fit continual learning settings <code>self.cl_paradigm</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): The task ID to query CL class map.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>cl_class_map</strong>(<code>dict[str | int, int]</code>): the CL class map of the task. Key is original class label, value is integer class label for continual learning.\n<ul>\n<li>If <code>self.cl_paradigm</code> is 'TIL', the mapped class labels of a task should be continuous integers from 0 to the number of classes.</li>\n<li>If <code>self.cl_paradigm</code> is 'CIL', the mapped class labels of a task should be continuous integers from the number of classes of previous tasks to the number of classes of the current task.</li>\n</ul></li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLSplitDataset.mean", "modulename": "clarena.cl_datasets", "qualname": "CLSplitDataset.mean", "kind": "function", "doc": "<p>The mean values for normalisation of task <code>task_id</code>. Used when constructing the dataset. In split CL dataset, the mean values are the same as the original dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>mean</strong> (<code>tuple[float]</code>): the mean values for normalisation.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLSplitDataset.std", "modulename": "clarena.cl_datasets", "qualname": "CLSplitDataset.std", "kind": "function", "doc": "<p>The standard deviation values for normalisation of task <code>task_id</code>. Used when constructing the dataset. In split CL dataset, the mean values are the same as the original dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>std</strong> (<code>tuple[float]</code>): he standard deviation values for normalisation.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLSplitDataset.get_subset_of_classes", "modulename": "clarena.cl_datasets", "qualname": "CLSplitDataset.get_subset_of_classes", "kind": "function", "doc": "<p>Provide a util method here to retrieve a subset from PyTorch Dataset of current classes of <code>self.task_id</code>. It could be useful when you constructing the split CL dataset.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>dataset</strong> (<code>Dataset</code>): the original dataset to retrieve subset from.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>subset</strong> (<code>Dataset</code>): subset of original dataset in classes.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">dataset</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLClassMapping", "modulename": "clarena.cl_datasets", "qualname": "CLClassMapping", "kind": "class", "doc": "<p>CL Class mapping to dataset labels. Used as a PyTorch target Transform.</p>\n"}, {"fullname": "clarena.cl_datasets.CLClassMapping.__init__", "modulename": "clarena.cl_datasets", "qualname": "CLClassMapping.__init__", "kind": "function", "doc": "<p>Initialise the CL class mapping transform object from the CL class map of a task.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>cl_class_map</strong> (<code>dict[str | int, int]</code>): the CL class map for a task.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">cl_class_map</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.CLClassMapping.cl_class_map", "modulename": "clarena.cl_datasets", "qualname": "CLClassMapping.cl_class_map", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "clarena.cl_datasets.Permute", "modulename": "clarena.cl_datasets", "qualname": "Permute", "kind": "class", "doc": "<p>Permutation operation to image. Used to construct permuted CL dataset.</p>\n\n<p>Used as a PyTorch Dataset Transform.</p>\n"}, {"fullname": "clarena.cl_datasets.Permute.__init__", "modulename": "clarena.cl_datasets", "qualname": "Permute.__init__", "kind": "function", "doc": "<p>Initialise the Permute transform object. The permutation order is constructed in the initialisation to save runtime.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>img_size</strong> (<code>torch.Size</code>): the size of the image to be permuted.</li>\n<li><strong>mode</strong> (<code>str</code>): the mode of permutation, shouble be one of the following:\n<ul>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ul></li>\n<li><strong>seed</strong> (<code>int</code> or <code>None</code>): seed for permutation operation. If None, the permutation will use a default seed from PyTorch generator.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">img_size</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Size</span>,</span><span class=\"param\">\t<span class=\"n\">mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">seed</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.Permute.mode", "modulename": "clarena.cl_datasets", "qualname": "Permute.mode", "kind": "variable", "doc": "<p>Store the mode of permutation.</p>\n"}, {"fullname": "clarena.cl_datasets.Permute.permute", "modulename": "clarena.cl_datasets", "qualname": "Permute.permute", "kind": "variable", "doc": "<p>The permutation order, a <code>Tensor</code> permuted from [1,2, ..., <code>num_pixels</code>] with the given seed. It is the core element of permutation operation.</p>\n", "annotation": ": torch.Tensor"}, {"fullname": "clarena.cl_datasets.permuted_mnist", "modulename": "clarena.cl_datasets.permuted_mnist", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted MNIST dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_mnist.PermutedMNIST", "modulename": "clarena.cl_datasets.permuted_mnist", "qualname": "PermutedMNIST", "kind": "class", "doc": "<p>Permuted MNIST dataset. The <a href=\"http://yann.lecun.com/exdb/mnist/\">original MNIST dataset</a> is a collection of handwritten digits. It consists of 70,000 28x28 B&amp;W images in 10 classes (correspond to 10 digits), with 7000 images per class. There are 60,000 training examples and 10,000 test examples.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_mnist.PermutedMNIST.__init__", "modulename": "clarena.cl_datasets.permuted_mnist", "qualname": "PermutedMNIST.__init__", "kind": "function", "doc": "<p>Initialise the Permuted MNIST dataset.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original MNIST data 'MNIST/raw/train-images-idx3-ubyte' and 'MNIST/raw/t10k-images-idx3-ubyte' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some of the training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform.\n<code>ToTensor()</code>, normalise, permute and so on are not included.</li>\n<li><strong>custom_target_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom target transforms to apply to dataset labels. Can be a single transform, composed transforms or no transform. CL class mapping is not included.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation, should be one of the following:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>list[int]</code> or <code>None</code>): the seeds for permutation operations used to construct tasks. Make sure it has the same number of seeds as <code>num_tasks</code>. Default is None, which creates a list of seeds from 1 to <code>num_tasks</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">8</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">custom_target_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_mnist.PermutedMNIST.num_classes", "modulename": "clarena.cl_datasets.permuted_mnist", "qualname": "PermutedMNIST.num_classes", "kind": "variable", "doc": "<p>The number of classes in MNIST.</p>\n", "annotation": ": int", "default_value": "10"}, {"fullname": "clarena.cl_datasets.permuted_mnist.PermutedMNIST.img_size", "modulename": "clarena.cl_datasets.permuted_mnist", "qualname": "PermutedMNIST.img_size", "kind": "variable", "doc": "<p>The size of MNIST images.</p>\n", "annotation": ": torch.Size", "default_value": "torch.Size([1, 28, 28])"}, {"fullname": "clarena.cl_datasets.permuted_mnist.PermutedMNIST.mean_original", "modulename": "clarena.cl_datasets.permuted_mnist", "qualname": "PermutedMNIST.mean_original", "kind": "variable", "doc": "<p>The mean values for normalisation.</p>\n", "annotation": ": tuple[float]", "default_value": "(0.1307,)"}, {"fullname": "clarena.cl_datasets.permuted_mnist.PermutedMNIST.std_original", "modulename": "clarena.cl_datasets.permuted_mnist", "qualname": "PermutedMNIST.std_original", "kind": "variable", "doc": "<p>The standard deviatfion values for normalisation.</p>\n", "annotation": ": tuple[float]", "default_value": "(0.3081,)"}, {"fullname": "clarena.cl_datasets.permuted_mnist.PermutedMNIST.prepare_data", "modulename": "clarena.cl_datasets.permuted_mnist", "qualname": "PermutedMNIST.prepare_data", "kind": "function", "doc": "<p>Download the original MNIST dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_mnist.PermutedMNIST.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_mnist", "qualname": "PermutedMNIST.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_mnist.PermutedMNIST.test_dataset", "modulename": "clarena.cl_datasets.permuted_mnist", "qualname": "PermutedMNIST.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.split_cifar100", "modulename": "clarena.cl_datasets.split_cifar100", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Split CIFAR-100 dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.split_cifar100.SplitCIFAR100", "modulename": "clarena.cl_datasets.split_cifar100", "qualname": "SplitCIFAR100", "kind": "class", "doc": "<p>Split CIFAR-100 dataset. The <a href=\"https://www.cs.toronto.edu/~kriz/cifar.html\">original CIFAR100 dataset</a> is a subset of the 80 million tiny images dataset. It consists of 60,000 32x32 colour images in 100 classes, with 600 images per class. There are 50,000 training examples and 10,000 test examples.</p>\n", "bases": "clarena.cl_datasets.base.CLSplitDataset"}, {"fullname": "clarena.cl_datasets.split_cifar100.SplitCIFAR100.__init__", "modulename": "clarena.cl_datasets.split_cifar100", "qualname": "SplitCIFAR100.__init__", "kind": "function", "doc": "<p>Initialise the Split CIFAR-100 dataset.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original CIFAR-100 data 'cifar-100-python/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset.</li>\n<li><strong>class_split</strong> (<code>list[list[int]]</code>): the class split for each task. Each element in the list is a list of class labels (integers starting from 0) to split for a task.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some of the training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform.\n<code>ToTensor()</code>, normalise, permute and so on are not included.</li>\n<li><strong>custom_target_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom target transforms to apply to dataset labels. Can be a single transform, composed transforms or no transform. CL class mapping is not included.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation, should be one of the following:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>list[int]</code> or <code>None</code>): the seeds for permutation operations used to construct tasks. Make sure it has the same number of seeds as <code>num_tasks</code>. Default is None, which creates a list of seeds from 1 to <code>num_tasks</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">class_split</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">8</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">custom_target_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.split_cifar100.SplitCIFAR100.num_classes", "modulename": "clarena.cl_datasets.split_cifar100", "qualname": "SplitCIFAR100.num_classes", "kind": "variable", "doc": "<p>The number of classes in CIFAR-100 dataset.</p>\n", "annotation": ": int", "default_value": "100"}, {"fullname": "clarena.cl_datasets.split_cifar100.SplitCIFAR100.mean_original", "modulename": "clarena.cl_datasets.split_cifar100", "qualname": "SplitCIFAR100.mean_original", "kind": "variable", "doc": "<p>The mean values for normalisation.</p>\n", "annotation": ": tuple[float]", "default_value": "(0.5074, 0.4867, 0.4411)"}, {"fullname": "clarena.cl_datasets.split_cifar100.SplitCIFAR100.std_original", "modulename": "clarena.cl_datasets.split_cifar100", "qualname": "SplitCIFAR100.std_original", "kind": "variable", "doc": "<p>The standard deviation values for normalisation.</p>\n", "annotation": ": tuple[float]", "default_value": "(0.2011, 0.1987, 0.2025)"}, {"fullname": "clarena.cl_datasets.split_cifar100.SplitCIFAR100.prepare_data", "modulename": "clarena.cl_datasets.split_cifar100", "qualname": "SplitCIFAR100.prepare_data", "kind": "function", "doc": "<p>Download the original CIFAR-100 dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.split_cifar100.SplitCIFAR100.train_and_val_dataset", "modulename": "clarena.cl_datasets.split_cifar100", "qualname": "SplitCIFAR100.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.split_cifar100.SplitCIFAR100.test_dataset", "modulename": "clarena.cl_datasets.split_cifar100", "qualname": "SplitCIFAR100.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_heads", "modulename": "clarena.cl_heads", "kind": "module", "doc": "<h1 id=\"continual-learning-heads\">Continual Learning Heads</h1>\n\n<p>This submodule provides the <strong>continual learning heads</strong> in CLArena. </p>\n\n<p>There are two types of heads in CLArena: <code>HeadsTIL</code> and <code>HeadsCIL</code>, corresponding to two CL paradigms respectively: Task-Incremental Learning (TIL) and Class-Incremental Learning (CIL). </p>\n\n<p>Please note that this is an API documantation. Please refer to the main documentation pages for more information about the heads.</p>\n\n<ul>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/configure-your-experiment/experiment-index-config\"><strong>Configure CL Paradigm in Experiment Index Config</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/posts/continual-learning-beginners-guide#sec-CL-classification\">**A Beginners' Guide to Continual Learning (Multi-head Classifier)</a></li>\n</ul>\n"}, {"fullname": "clarena.cl_heads.HeadsTIL", "modulename": "clarena.cl_heads", "qualname": "HeadsTIL", "kind": "class", "doc": "<p>The output heads for Task-Incremental Learning (TIL). Independent head assigned to each TIL task takes the output from backbone network and forwards it into logits for predicting classes of the task.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "clarena.cl_heads.HeadsTIL.__init__", "modulename": "clarena.cl_heads", "qualname": "HeadsTIL.__init__", "kind": "function", "doc": "<p>Initializes TIL heads object with no heads.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_dim</strong> (<code>int</code>): the input dimension of the heads. Must be equal to the <code>output_dim</code> of the connected backbone.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span>)</span>"}, {"fullname": "clarena.cl_heads.HeadsTIL.heads", "modulename": "clarena.cl_heads", "qualname": "HeadsTIL.heads", "kind": "variable", "doc": "<p>TIL output heads are stored independently in a <code>ModuleDict</code>. Keys are task IDs (string type) and values are the corresponding <code>nn.Linear</code> heads. We use <code>ModuleDict</code> rather than <code>dict</code> to make sure <code>LightningModule</code> can track these model parameters for the purpose of, such as automatically to device, recorded in model summaries.</p>\n\n<p>Note that the task IDs must be string type in order to let <code>LightningModule</code> identify this part of the model.</p>\n", "annotation": ": torch.nn.modules.container.ModuleDict"}, {"fullname": "clarena.cl_heads.HeadsTIL.input_dim", "modulename": "clarena.cl_heads", "qualname": "HeadsTIL.input_dim", "kind": "variable", "doc": "<p>Store the input dimension of the heads. Used when creating new heads.</p>\n", "annotation": ": int"}, {"fullname": "clarena.cl_heads.HeadsTIL.task_id", "modulename": "clarena.cl_heads", "qualname": "HeadsTIL.task_id", "kind": "variable", "doc": "<p>Task ID counter indicating which task is being processed. Self updated during the task loop.</p>\n", "annotation": ": int"}, {"fullname": "clarena.cl_heads.HeadsTIL.setup_task_id", "modulename": "clarena.cl_heads", "qualname": "HeadsTIL.setup_task_id", "kind": "function", "doc": "<p>Create the output head when task <code>task_id</code> arrives if there's no. This must be done before <code>forward()</code> is called.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID.</li>\n<li><strong>num_classes_t</strong> (<code>int</code>): the number of classes in the task.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span>, </span><span class=\"param\"><span class=\"n\">num_classes_t</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_heads.HeadsTIL.forward", "modulename": "clarena.cl_heads", "qualname": "HeadsTIL.forward", "kind": "function", "doc": "<p>The forward pass for data from task <code>task_id</code>. A head is selected according to the task_id and the feature is passed through the head.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>feature</strong> (<code>Tensor</code>): the feature tensor from the backbone network.</li>\n<li><strong>task_id</strong> (<code>int</code>): the task ID where the data are from, which is provided by task-incremental setting.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>logits</strong> (<code>Tensor</code>): the output logits tensor.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">feature</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_heads.HeadsCIL", "modulename": "clarena.cl_heads", "qualname": "HeadsCIL", "kind": "class", "doc": "<p>The output heads for Class-Incremental Learning (CIL). Head of all classes from CIL tasks takes the output from backbone network and forwards it into logits for predicting classes of all tasks.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "clarena.cl_heads.HeadsCIL.__init__", "modulename": "clarena.cl_heads", "qualname": "HeadsCIL.__init__", "kind": "function", "doc": "<p>Initializes a CIL heads object with no heads.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_dim</strong> (<code>int</code>): the input dimension of the heads. Must be equal to the <code>output_dim</code> of the connected backbone.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span>)</span>"}, {"fullname": "clarena.cl_heads.HeadsCIL.heads", "modulename": "clarena.cl_heads", "qualname": "HeadsCIL.heads", "kind": "variable", "doc": "<p>CIL output heads are stored in a <code>ModuleDict</code>. Keys are task IDs (string type) and values are the corresponding <code>nn.Linear</code> heads. We use <code>ModuleDict</code> rather than <code>dict</code> to make sure <code>LightningModule</code> can track these model parameters for the purpose of, such as automatically to device, recorded in model summaries.</p>\n", "annotation": ": torch.nn.modules.container.ModuleDict"}, {"fullname": "clarena.cl_heads.HeadsCIL.input_dim", "modulename": "clarena.cl_heads", "qualname": "HeadsCIL.input_dim", "kind": "variable", "doc": "<p>The input dimension of the heads. Used when creating new heads.</p>\n", "annotation": ": int"}, {"fullname": "clarena.cl_heads.HeadsCIL.task_id", "modulename": "clarena.cl_heads", "qualname": "HeadsCIL.task_id", "kind": "variable", "doc": "<p>Task ID counter indicating which task is being processed. Self updated during the task loop.</p>\n", "annotation": ": int"}, {"fullname": "clarena.cl_heads.HeadsCIL.setup_task_id", "modulename": "clarena.cl_heads", "qualname": "HeadsCIL.setup_task_id", "kind": "function", "doc": "<p>Create the output head when task <code>task_id</code> arrives if there's no. This must be done before <code>forward()</code> is called.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID.</li>\n<li><strong>num_classes_t</strong> (<code>int</code>): the number of classes in the task.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span>, </span><span class=\"param\"><span class=\"n\">num_classes_t</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_heads.HeadsCIL.forward", "modulename": "clarena.cl_heads", "qualname": "HeadsCIL.forward", "kind": "function", "doc": "<p>The forward pass for data. The information of which <code>task_id</code> the data are from is not provided. The head for all classes is selected and the feature is passed.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>feature</strong> (<code>Tensor</code>): the feature tensor from the backbone network.</li>\n<li><strong>task_id</strong> (<code>int</code> or <code>None</code>): the task ID where the data are from. In CIL, it is just a placeholder for API consistence with the TIL heads but never used. Best practices are not to provide this argument and leave it as the default value.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>logits</strong> (<code>Tensor</code>): the output logits tensor.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">feature</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.utils", "modulename": "clarena.utils", "kind": "module", "doc": "<h1 id=\"utilities\">Utilities</h1>\n\n<p>This submodule provides utilities that are used in CLArena, which includes:</p>\n\n<ul>\n<li><strong>Metrics</strong>: define metrics particularly for CL.</li>\n<li><strong>Save</strong>: for saving results to files.</li>\n<li><strong>Plot</strong>: for plotting figures.</li>\n</ul>\n"}, {"fullname": "clarena.utils.cfg", "modulename": "clarena.utils.cfg", "kind": "module", "doc": "<p>The submodule in <code>utils</code> with tools related to configs.</p>\n"}, {"fullname": "clarena.utils.cfg.preprocess_config", "modulename": "clarena.utils.cfg", "qualname": "preprocess_config", "kind": "function", "doc": "<p>Preprocess the configuration before constructing experiment, which may include:</p>\n\n<ol>\n<li>Convert the <code>DictConfig</code> to a Rich <code>Tree</code>.</li>\n<li>Print the Rich <code>Tree</code>.</li>\n<li>Save the Rich <code>Tree</code> to a file.</li>\n</ol>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>cfg</strong> (<code>DictConfig</code>): the config dict to preprocess.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.utils.cfg.cfg_to_tree", "modulename": "clarena.utils.cfg", "qualname": "cfg_to_tree", "kind": "function", "doc": "<p>Convert the configuration to a Rich <code>Tree</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>cfg</strong> (<code>DictConfig</code>): the target config dict to be converted.</li>\n<li><strong>config_tree_cfg</strong> (<code>DictConfig</code>): the configuration for conversion of config tree.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>tree</strong> (<code>Tree</code>): the Rich <code>Tree</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span>,</span><span class=\"param\">\t<span class=\"n\">config_tree_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"n\">rich</span><span class=\"o\">.</span><span class=\"n\">tree</span><span class=\"o\">.</span><span class=\"n\">Tree</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.utils.cfg.save_tree_to_file", "modulename": "clarena.utils.cfg", "qualname": "save_tree_to_file", "kind": "function", "doc": "<p>Save Rich <code>Tree</code> to a file.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>tree</strong> (<code>dict</code>): the Rich <code>Tree</code> to save.</li>\n<li><strong>save_path</strong> (<code>str</code>): the path to save the tree.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">tree</span><span class=\"p\">:</span> <span class=\"nb\">dict</span>, </span><span class=\"param\"><span class=\"n\">save_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.utils.metrics", "modulename": "clarena.utils.metrics", "kind": "module", "doc": "<p>The submodule in <code>utils</code> for plotting utils.</p>\n"}, {"fullname": "clarena.utils.metrics.MeanMetricBatch", "modulename": "clarena.utils.metrics", "qualname": "MeanMetricBatch", "kind": "class", "doc": "<p>A TorchMetrics metric to calculate the mean of metrics across data batches.</p>\n\n<p>This is used for accumulated metrics in deep learning. See <a href=\"https://pengxiang-wang.com/posts/continual-learning-metrics.html#nte-accumulate\">here</a> for more details.</p>\n", "bases": "torchmetrics.aggregation.BaseAggregator"}, {"fullname": "clarena.utils.metrics.MeanMetricBatch.__init__", "modulename": "clarena.utils.metrics", "qualname": "MeanMetricBatch.__init__", "kind": "function", "doc": "<p>Initialise the metric. Add state variables.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">nan_strategy</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;error&#39;</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span>)</span>"}, {"fullname": "clarena.utils.metrics.MeanMetricBatch.sum", "modulename": "clarena.utils.metrics", "qualname": "MeanMetricBatch.sum", "kind": "variable", "doc": "<p>State variable created by <code>super().__init__()</code> to store the sum of the metric values till this batch.</p>\n", "annotation": ": torch.Tensor"}, {"fullname": "clarena.utils.metrics.MeanMetricBatch.num", "modulename": "clarena.utils.metrics", "qualname": "MeanMetricBatch.num", "kind": "variable", "doc": "<p>State variable created by <code>add_state()</code> to store the number of the data till this batch.</p>\n", "annotation": ": torch.Tensor"}, {"fullname": "clarena.utils.metrics.MeanMetricBatch.update", "modulename": "clarena.utils.metrics", "qualname": "MeanMetricBatch.update", "kind": "function", "doc": "<p>Update and accumulate the sum of metric value and num of the data till this batch from the batch.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>val</strong> (<code>torch.Tensor</code>): the metric value of the batch to update the sum.</li>\n<li><strong>batch_size</strong> (<code>int</code>): the value to update the num, which is the batch size.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">value</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.utils.metrics.MeanMetricBatch.compute", "modulename": "clarena.utils.metrics", "qualname": "MeanMetricBatch.compute", "kind": "function", "doc": "<p>Compute this mean metric value till this batch.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>mean</strong> (<code>Tensor</code>): the calculated mean result.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.utils.metrics.HATNetworkCapacity", "modulename": "clarena.utils.metrics", "qualname": "HATNetworkCapacity", "kind": "class", "doc": "<p>A torchmetrics metric to calculate the network capacity of HAT (Hard Attention to the Task) algorithm.</p>\n\n<p>Network capacity is defined as the average adjustment rate over all parameters. See chapter 4.1 in <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>.</p>\n", "bases": "torchmetrics.aggregation.BaseAggregator"}, {"fullname": "clarena.utils.metrics.HATNetworkCapacity.__init__", "modulename": "clarena.utils.metrics", "qualname": "HATNetworkCapacity.__init__", "kind": "function", "doc": "<p>Initialise the HAT network capacity metric. Add state variables.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">nan_strategy</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;error&#39;</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span>)</span>"}, {"fullname": "clarena.utils.metrics.HATNetworkCapacity.sum_adjustment_rate", "modulename": "clarena.utils.metrics", "qualname": "HATNetworkCapacity.sum_adjustment_rate", "kind": "variable", "doc": "<p>State variable created by <code>add_state()</code> to store the sum of the adjustment rate values till this layer.</p>\n", "annotation": ": torch.Tensor"}, {"fullname": "clarena.utils.metrics.HATNetworkCapacity.num_params", "modulename": "clarena.utils.metrics", "qualname": "HATNetworkCapacity.num_params", "kind": "variable", "doc": "<p>State variable created by <code>add_state()</code> to store the number of the parameters till this layer.</p>\n", "annotation": ": torch.Tensor"}, {"fullname": "clarena.utils.metrics.HATNetworkCapacity.update", "modulename": "clarena.utils.metrics", "qualname": "HATNetworkCapacity.update", "kind": "function", "doc": "<p>Update and accumulate the sum of adjustment rate values till this layer from the layer.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>adjustment_rate_weight_layer</strong> (<code>Tensor</code>): the adjustment rate values of the weight matrix of the layer.</li>\n<li><strong>adjustment_rate_bias_layer</strong> (<code>Tensor</code>): the adjustment rate values of the bias vector of the layer.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">adjustment_rate_weight_layer</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">adjustment_rate_bias_layer</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.utils.metrics.HATNetworkCapacity.compute", "modulename": "clarena.utils.metrics", "qualname": "HATNetworkCapacity.compute", "kind": "function", "doc": "<p>Compute this HAT network capacity till this layer.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>network_capacity</strong> (<code>Tensor</code>): the calculated network capacity result.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.utils.plot", "modulename": "clarena.utils.plot", "kind": "module", "doc": "<p>The submodule in <code>utils</code> for plotting utils.</p>\n"}, {"fullname": "clarena.utils.plot.plot_test_ave_acc_curve_from_csv", "modulename": "clarena.utils.plot", "qualname": "plot_test_ave_acc_curve_from_csv", "kind": "function", "doc": "<p>Plot the test average accuracy curve over different training tasks from saved csv file and save the plot to the designated directory.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>csv_path</strong> (<code>str</code>): the path to the csv file where the <code>utils.update_test_acc_to_csv()</code> saved the test accuracy metric.</li>\n<li><strong>task_id</strong> (<code>int</code>): plot the test average accuracy metric from task 1 to <code>task_id</code>.</li>\n<li><strong>plot_path</strong> (<code>str</code>): the path to save plot. Better same as the output directory of the experiment. E.g. './outputs/expr_name/1970-01-01_00-00-00/ave_acc.png'.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span>, </span><span class=\"param\"><span class=\"n\">plot_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.utils.plot.plot_test_acc_matrix_from_csv", "modulename": "clarena.utils.plot", "qualname": "plot_test_acc_matrix_from_csv", "kind": "function", "doc": "<p>Plot the test accuracy matrix from saved csv file and save the plot to the designated directory.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>csv_path</strong> (<code>str</code>): the path to the csv file where the <code>utils.update_test_acc_to_csv()</code> saved the test accuracy metric.</li>\n<li><strong>task_id</strong> (<code>int</code>): plot the test accuracy metric from task 1 to <code>task_id</code>.</li>\n<li><strong>plot_path</strong> (<code>str</code>): the path to save plot. Better same as the output directory of the experiment. E.g. './outputs/expr_name/1970-01-01_00-00-00/acc_matrix.png'.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span>, </span><span class=\"param\"><span class=\"n\">plot_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.utils.plot.plot_test_ave_loss_cls_curve_from_csv", "modulename": "clarena.utils.plot", "qualname": "plot_test_ave_loss_cls_curve_from_csv", "kind": "function", "doc": "<p>Plot the test average classification loss curve over different training tasks from saved csv file and save the plot to the designated directory.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>csv_path</strong> (<code>str</code>): the path to the csv file where the <code>utils.update_loss_cls_to_csv()</code> saved the test classification loss metric.</li>\n<li><strong>task_id</strong> (<code>int</code>): plot the test average accuracy metric from task 1 to <code>task_id</code>.</li>\n<li><strong>plot_path</strong> (<code>str</code>): the path to save plot. Better same as the output directory of the experiment. E.g. './outputs/expr_name/1970-01-01_00-00-00/ave_loss_cls.png'.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span>, </span><span class=\"param\"><span class=\"n\">plot_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.utils.plot.plot_test_loss_cls_matrix_from_csv", "modulename": "clarena.utils.plot", "qualname": "plot_test_loss_cls_matrix_from_csv", "kind": "function", "doc": "<p>Plot the test classification loss matrix from saved csv file and save the plot to the designated directory.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>csv_path</strong> (<code>str</code>): the path to the csv file where the <code>utils.update_loss_cls_to_csv()</code> saved the test classification loss metric.</li>\n<li><strong>task_id</strong> (<code>int</code>): plot the test classification loss metric from task 1 to <code>task_id</code>.</li>\n<li><strong>plot_path</strong> (<code>str</code>): the path to save plot. Better same as the output directory of the experiment. E.g. './outputs/expr_name/1970-01-01_00-00-00/loss_cls_matrix.png'.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span>, </span><span class=\"param\"><span class=\"n\">plot_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.utils.plot.plot_hat_mask", "modulename": "clarena.utils.plot", "qualname": "plot_hat_mask", "kind": "function", "doc": "<p>Plot mask in <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task)</a>) algorithm. This includes the mask and cumulative mask.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>mask</strong> (<code>dict[str, Tensor]</code>): the hard attention (whose values are 0 or 1) mask. Key (<code>str</code>) is layer name, value (<code>Tensor</code>) is the mask tensor. The mask tensor has size (number of units).</li>\n<li><strong>plot_dir</strong> (<code>str</code>): the directory to save plot. Better same as the output directory of the experiment.</li>\n<li><strong>task_id</strong> (<code>int</code>): the task ID of the mask to be plotted. This is to form the plot name.</li>\n<li><strong>step</strong> (<code>int</code>): the training step (batch index) of the mask to be plotted. Apply to the training mask only. This is to form the plot name. Keep <code>None</code> for not showing the step in the plot name.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">mask</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">plot_dir</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">step</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.utils.save", "modulename": "clarena.utils.save", "kind": "module", "doc": "<p>The submodule in <code>utils</code> for saving data in the experiment as files.</p>\n"}, {"fullname": "clarena.utils.save.update_test_acc_to_csv", "modulename": "clarena.utils.save", "qualname": "update_test_acc_to_csv", "kind": "function", "doc": "<p>Update the test accuracy metrics of task 1 to <code>num_tasks</code> at the last line to an existing csv file. A new file will be created if not existing. Used in <code>CLMetricsCallback</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>test_acc_metric</strong> (<code>dict[int, MeanMetricBatch]</code>): classification accuracy of the test data of each seen task. Accumulated and calculated from the test batches.</li>\n<li><strong>csv_path</strong> (<code>str</code>): save the test metric to path. E.g. './outputs/expr_name/1970-01-01_00-00-00/acc.csv'.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">test_acc_metric</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">metrics</span><span class=\"o\">.</span><span class=\"n\">MeanMetricBatch</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.utils.save.update_test_loss_cls_to_csv", "modulename": "clarena.utils.save", "qualname": "update_test_loss_cls_to_csv", "kind": "function", "doc": "<p>Update the test classification loss metrics of task 1 to <code>num_tasks</code> at the last line to an existing csv file. A new file will be created if not existing. Used in <code>CLMetricsCallback</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>test_loss_cls_metric</strong> (<code>dict[int, MeanMetricBatch]</code>): classification loss of the test data of each seen task. Accumulated and calculated from the test batches.</li>\n<li><strong>csv_path</strong> (<code>str</code>): save the test metric to path. E.g. './outputs/expr_name/1970-01-01_00-00-00/loss_cls.csv'.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">test_loss_cls_metric</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">metrics</span><span class=\"o\">.</span><span class=\"n\">MeanMetricBatch</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.utils.transforms", "modulename": "clarena.utils.transforms", "kind": "module", "doc": "<p>The submodule in <code>utils</code> for transforming tensors.</p>\n"}, {"fullname": "clarena.utils.transforms.min_max_normalise", "modulename": "clarena.utils.transforms", "qualname": "min_max_normalise", "kind": "function", "doc": "<p>Normalise the tensor using min-max normalisation.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>tensor</strong> (<code>Tensor</code>): the input tensor to normalise.</li>\n<li><strong>dim</strong> (<code>int</code> | <code>None</code>): the dimension to normalise along. If <code>None</code>, normalise the whole tensor.</li>\n<li><strong>epsilon</strong> (<code>float</code>): the epsilon value to avoid division by zero.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>tensor</strong> (<code>Tensor</code>): the normalised tensor.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">tensor</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">epsilon</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1e-08</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}];

    // mirrored in build-search-index.js (part 1)
    // Also split on html tags. this is a cheap heuristic, but good enough.
    elasticlunr.tokenizer.setSeperator(/[\s\-.;&_'"=,()]+|<[^>]*>/);

    let searchIndex;
    if (docs._isPrebuiltIndex) {
        console.info("using precompiled search index");
        searchIndex = elasticlunr.Index.load(docs);
    } else {
        console.time("building search index");
        // mirrored in build-search-index.js (part 2)
        searchIndex = elasticlunr(function () {
            this.pipeline.remove(elasticlunr.stemmer);
            this.pipeline.remove(elasticlunr.stopWordFilter);
            this.addField("qualname");
            this.addField("fullname");
            this.addField("annotation");
            this.addField("default_value");
            this.addField("signature");
            this.addField("bases");
            this.addField("doc");
            this.setRef("fullname");
        });
        for (let doc of docs) {
            searchIndex.addDoc(doc);
        }
        console.timeEnd("building search index");
    }

    return (term) => searchIndex.search(term, {
        fields: {
            qualname: {boost: 4},
            fullname: {boost: 2},
            annotation: {boost: 2},
            default_value: {boost: 2},
            signature: {boost: 2},
            bases: {boost: 2},
            doc: {boost: 1},
        },
        expand: true
    });
})();