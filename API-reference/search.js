window.pdocSearch = (function(){
/** elasticlunr - http://weixsong.github.io * Copyright (C) 2017 Oliver Nightingale * Copyright (C) 2017 Wei Song * MIT Licensed */!function(){function e(e){if(null===e||"object"!=typeof e)return e;var t=e.constructor();for(var n in e)e.hasOwnProperty(n)&&(t[n]=e[n]);return t}var t=function(e){var n=new t.Index;return n.pipeline.add(t.trimmer,t.stopWordFilter,t.stemmer),e&&e.call(n,n),n};t.version="0.9.5",lunr=t,t.utils={},t.utils.warn=function(e){return function(t){e.console&&console.warn&&console.warn(t)}}(this),t.utils.toString=function(e){return void 0===e||null===e?"":e.toString()},t.EventEmitter=function(){this.events={}},t.EventEmitter.prototype.addListener=function(){var e=Array.prototype.slice.call(arguments),t=e.pop(),n=e;if("function"!=typeof t)throw new TypeError("last argument must be a function");n.forEach(function(e){this.hasHandler(e)||(this.events[e]=[]),this.events[e].push(t)},this)},t.EventEmitter.prototype.removeListener=function(e,t){if(this.hasHandler(e)){var n=this.events[e].indexOf(t);-1!==n&&(this.events[e].splice(n,1),0==this.events[e].length&&delete this.events[e])}},t.EventEmitter.prototype.emit=function(e){if(this.hasHandler(e)){var t=Array.prototype.slice.call(arguments,1);this.events[e].forEach(function(e){e.apply(void 0,t)},this)}},t.EventEmitter.prototype.hasHandler=function(e){return e in this.events},t.tokenizer=function(e){if(!arguments.length||null===e||void 0===e)return[];if(Array.isArray(e)){var n=e.filter(function(e){return null===e||void 0===e?!1:!0});n=n.map(function(e){return t.utils.toString(e).toLowerCase()});var i=[];return n.forEach(function(e){var n=e.split(t.tokenizer.seperator);i=i.concat(n)},this),i}return e.toString().trim().toLowerCase().split(t.tokenizer.seperator)},t.tokenizer.defaultSeperator=/[\s\-]+/,t.tokenizer.seperator=t.tokenizer.defaultSeperator,t.tokenizer.setSeperator=function(e){null!==e&&void 0!==e&&"object"==typeof e&&(t.tokenizer.seperator=e)},t.tokenizer.resetSeperator=function(){t.tokenizer.seperator=t.tokenizer.defaultSeperator},t.tokenizer.getSeperator=function(){return t.tokenizer.seperator},t.Pipeline=function(){this._queue=[]},t.Pipeline.registeredFunctions={},t.Pipeline.registerFunction=function(e,n){n in t.Pipeline.registeredFunctions&&t.utils.warn("Overwriting existing registered function: "+n),e.label=n,t.Pipeline.registeredFunctions[n]=e},t.Pipeline.getRegisteredFunction=function(e){return e in t.Pipeline.registeredFunctions!=!0?null:t.Pipeline.registeredFunctions[e]},t.Pipeline.warnIfFunctionNotRegistered=function(e){var n=e.label&&e.label in this.registeredFunctions;n||t.utils.warn("Function is not registered with pipeline. This may cause problems when serialising the index.\n",e)},t.Pipeline.load=function(e){var n=new t.Pipeline;return e.forEach(function(e){var i=t.Pipeline.getRegisteredFunction(e);if(!i)throw new Error("Cannot load un-registered function: "+e);n.add(i)}),n},t.Pipeline.prototype.add=function(){var e=Array.prototype.slice.call(arguments);e.forEach(function(e){t.Pipeline.warnIfFunctionNotRegistered(e),this._queue.push(e)},this)},t.Pipeline.prototype.after=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i+1,0,n)},t.Pipeline.prototype.before=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i,0,n)},t.Pipeline.prototype.remove=function(e){var t=this._queue.indexOf(e);-1!==t&&this._queue.splice(t,1)},t.Pipeline.prototype.run=function(e){for(var t=[],n=e.length,i=this._queue.length,o=0;n>o;o++){for(var r=e[o],s=0;i>s&&(r=this._queue[s](r,o,e),void 0!==r&&null!==r);s++);void 0!==r&&null!==r&&t.push(r)}return t},t.Pipeline.prototype.reset=function(){this._queue=[]},t.Pipeline.prototype.get=function(){return this._queue},t.Pipeline.prototype.toJSON=function(){return this._queue.map(function(e){return t.Pipeline.warnIfFunctionNotRegistered(e),e.label})},t.Index=function(){this._fields=[],this._ref="id",this.pipeline=new t.Pipeline,this.documentStore=new t.DocumentStore,this.index={},this.eventEmitter=new t.EventEmitter,this._idfCache={},this.on("add","remove","update",function(){this._idfCache={}}.bind(this))},t.Index.prototype.on=function(){var e=Array.prototype.slice.call(arguments);return this.eventEmitter.addListener.apply(this.eventEmitter,e)},t.Index.prototype.off=function(e,t){return this.eventEmitter.removeListener(e,t)},t.Index.load=function(e){e.version!==t.version&&t.utils.warn("version mismatch: current "+t.version+" importing "+e.version);var n=new this;n._fields=e.fields,n._ref=e.ref,n.documentStore=t.DocumentStore.load(e.documentStore),n.pipeline=t.Pipeline.load(e.pipeline),n.index={};for(var i in e.index)n.index[i]=t.InvertedIndex.load(e.index[i]);return n},t.Index.prototype.addField=function(e){return this._fields.push(e),this.index[e]=new t.InvertedIndex,this},t.Index.prototype.setRef=function(e){return this._ref=e,this},t.Index.prototype.saveDocument=function(e){return this.documentStore=new t.DocumentStore(e),this},t.Index.prototype.addDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.addDoc(i,e),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));this.documentStore.addFieldLength(i,n,o.length);var r={};o.forEach(function(e){e in r?r[e]+=1:r[e]=1},this);for(var s in r){var u=r[s];u=Math.sqrt(u),this.index[n].addToken(s,{ref:i,tf:u})}},this),n&&this.eventEmitter.emit("add",e,this)}},t.Index.prototype.removeDocByRef=function(e){if(e&&this.documentStore.isDocStored()!==!1&&this.documentStore.hasDoc(e)){var t=this.documentStore.getDoc(e);this.removeDoc(t,!1)}},t.Index.prototype.removeDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.hasDoc(i)&&(this.documentStore.removeDoc(i),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));o.forEach(function(e){this.index[n].removeToken(e,i)},this)},this),n&&this.eventEmitter.emit("remove",e,this))}},t.Index.prototype.updateDoc=function(e,t){var t=void 0===t?!0:t;this.removeDocByRef(e[this._ref],!1),this.addDoc(e,!1),t&&this.eventEmitter.emit("update",e,this)},t.Index.prototype.idf=function(e,t){var n="@"+t+"/"+e;if(Object.prototype.hasOwnProperty.call(this._idfCache,n))return this._idfCache[n];var i=this.index[t].getDocFreq(e),o=1+Math.log(this.documentStore.length/(i+1));return this._idfCache[n]=o,o},t.Index.prototype.getFields=function(){return this._fields.slice()},t.Index.prototype.search=function(e,n){if(!e)return[];e="string"==typeof e?{any:e}:JSON.parse(JSON.stringify(e));var i=null;null!=n&&(i=JSON.stringify(n));for(var o=new t.Configuration(i,this.getFields()).get(),r={},s=Object.keys(e),u=0;u<s.length;u++){var a=s[u];r[a]=this.pipeline.run(t.tokenizer(e[a]))}var l={};for(var c in o){var d=r[c]||r.any;if(d){var f=this.fieldSearch(d,c,o),h=o[c].boost;for(var p in f)f[p]=f[p]*h;for(var p in f)p in l?l[p]+=f[p]:l[p]=f[p]}}var v,g=[];for(var p in l)v={ref:p,score:l[p]},this.documentStore.hasDoc(p)&&(v.doc=this.documentStore.getDoc(p)),g.push(v);return g.sort(function(e,t){return t.score-e.score}),g},t.Index.prototype.fieldSearch=function(e,t,n){var i=n[t].bool,o=n[t].expand,r=n[t].boost,s=null,u={};return 0!==r?(e.forEach(function(e){var n=[e];1==o&&(n=this.index[t].expandToken(e));var r={};n.forEach(function(n){var o=this.index[t].getDocs(n),a=this.idf(n,t);if(s&&"AND"==i){var l={};for(var c in s)c in o&&(l[c]=o[c]);o=l}n==e&&this.fieldSearchStats(u,n,o);for(var c in o){var d=this.index[t].getTermFrequency(n,c),f=this.documentStore.getFieldLength(c,t),h=1;0!=f&&(h=1/Math.sqrt(f));var p=1;n!=e&&(p=.15*(1-(n.length-e.length)/n.length));var v=d*a*h*p;c in r?r[c]+=v:r[c]=v}},this),s=this.mergeScores(s,r,i)},this),s=this.coordNorm(s,u,e.length)):void 0},t.Index.prototype.mergeScores=function(e,t,n){if(!e)return t;if("AND"==n){var i={};for(var o in t)o in e&&(i[o]=e[o]+t[o]);return i}for(var o in t)o in e?e[o]+=t[o]:e[o]=t[o];return e},t.Index.prototype.fieldSearchStats=function(e,t,n){for(var i in n)i in e?e[i].push(t):e[i]=[t]},t.Index.prototype.coordNorm=function(e,t,n){for(var i in e)if(i in t){var o=t[i].length;e[i]=e[i]*o/n}return e},t.Index.prototype.toJSON=function(){var e={};return this._fields.forEach(function(t){e[t]=this.index[t].toJSON()},this),{version:t.version,fields:this._fields,ref:this._ref,documentStore:this.documentStore.toJSON(),index:e,pipeline:this.pipeline.toJSON()}},t.Index.prototype.use=function(e){var t=Array.prototype.slice.call(arguments,1);t.unshift(this),e.apply(this,t)},t.DocumentStore=function(e){this._save=null===e||void 0===e?!0:e,this.docs={},this.docInfo={},this.length=0},t.DocumentStore.load=function(e){var t=new this;return t.length=e.length,t.docs=e.docs,t.docInfo=e.docInfo,t._save=e.save,t},t.DocumentStore.prototype.isDocStored=function(){return this._save},t.DocumentStore.prototype.addDoc=function(t,n){this.hasDoc(t)||this.length++,this.docs[t]=this._save===!0?e(n):null},t.DocumentStore.prototype.getDoc=function(e){return this.hasDoc(e)===!1?null:this.docs[e]},t.DocumentStore.prototype.hasDoc=function(e){return e in this.docs},t.DocumentStore.prototype.removeDoc=function(e){this.hasDoc(e)&&(delete this.docs[e],delete this.docInfo[e],this.length--)},t.DocumentStore.prototype.addFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&(this.docInfo[e]||(this.docInfo[e]={}),this.docInfo[e][t]=n)},t.DocumentStore.prototype.updateFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&this.addFieldLength(e,t,n)},t.DocumentStore.prototype.getFieldLength=function(e,t){return null===e||void 0===e?0:e in this.docs&&t in this.docInfo[e]?this.docInfo[e][t]:0},t.DocumentStore.prototype.toJSON=function(){return{docs:this.docs,docInfo:this.docInfo,length:this.length,save:this._save}},t.stemmer=function(){var e={ational:"ate",tional:"tion",enci:"ence",anci:"ance",izer:"ize",bli:"ble",alli:"al",entli:"ent",eli:"e",ousli:"ous",ization:"ize",ation:"ate",ator:"ate",alism:"al",iveness:"ive",fulness:"ful",ousness:"ous",aliti:"al",iviti:"ive",biliti:"ble",logi:"log"},t={icate:"ic",ative:"",alize:"al",iciti:"ic",ical:"ic",ful:"",ness:""},n="[^aeiou]",i="[aeiouy]",o=n+"[^aeiouy]*",r=i+"[aeiou]*",s="^("+o+")?"+r+o,u="^("+o+")?"+r+o+"("+r+")?$",a="^("+o+")?"+r+o+r+o,l="^("+o+")?"+i,c=new RegExp(s),d=new RegExp(a),f=new RegExp(u),h=new RegExp(l),p=/^(.+?)(ss|i)es$/,v=/^(.+?)([^s])s$/,g=/^(.+?)eed$/,m=/^(.+?)(ed|ing)$/,y=/.$/,S=/(at|bl|iz)$/,x=new RegExp("([^aeiouylsz])\\1$"),w=new RegExp("^"+o+i+"[^aeiouwxy]$"),I=/^(.+?[^aeiou])y$/,b=/^(.+?)(ational|tional|enci|anci|izer|bli|alli|entli|eli|ousli|ization|ation|ator|alism|iveness|fulness|ousness|aliti|iviti|biliti|logi)$/,E=/^(.+?)(icate|ative|alize|iciti|ical|ful|ness)$/,D=/^(.+?)(al|ance|ence|er|ic|able|ible|ant|ement|ment|ent|ou|ism|ate|iti|ous|ive|ize)$/,F=/^(.+?)(s|t)(ion)$/,_=/^(.+?)e$/,P=/ll$/,k=new RegExp("^"+o+i+"[^aeiouwxy]$"),z=function(n){var i,o,r,s,u,a,l;if(n.length<3)return n;if(r=n.substr(0,1),"y"==r&&(n=r.toUpperCase()+n.substr(1)),s=p,u=v,s.test(n)?n=n.replace(s,"$1$2"):u.test(n)&&(n=n.replace(u,"$1$2")),s=g,u=m,s.test(n)){var z=s.exec(n);s=c,s.test(z[1])&&(s=y,n=n.replace(s,""))}else if(u.test(n)){var z=u.exec(n);i=z[1],u=h,u.test(i)&&(n=i,u=S,a=x,l=w,u.test(n)?n+="e":a.test(n)?(s=y,n=n.replace(s,"")):l.test(n)&&(n+="e"))}if(s=I,s.test(n)){var z=s.exec(n);i=z[1],n=i+"i"}if(s=b,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+e[o])}if(s=E,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+t[o])}if(s=D,u=F,s.test(n)){var z=s.exec(n);i=z[1],s=d,s.test(i)&&(n=i)}else if(u.test(n)){var z=u.exec(n);i=z[1]+z[2],u=d,u.test(i)&&(n=i)}if(s=_,s.test(n)){var z=s.exec(n);i=z[1],s=d,u=f,a=k,(s.test(i)||u.test(i)&&!a.test(i))&&(n=i)}return s=P,u=d,s.test(n)&&u.test(n)&&(s=y,n=n.replace(s,"")),"y"==r&&(n=r.toLowerCase()+n.substr(1)),n};return z}(),t.Pipeline.registerFunction(t.stemmer,"stemmer"),t.stopWordFilter=function(e){return e&&t.stopWordFilter.stopWords[e]!==!0?e:void 0},t.clearStopWords=function(){t.stopWordFilter.stopWords={}},t.addStopWords=function(e){null!=e&&Array.isArray(e)!==!1&&e.forEach(function(e){t.stopWordFilter.stopWords[e]=!0},this)},t.resetStopWords=function(){t.stopWordFilter.stopWords=t.defaultStopWords},t.defaultStopWords={"":!0,a:!0,able:!0,about:!0,across:!0,after:!0,all:!0,almost:!0,also:!0,am:!0,among:!0,an:!0,and:!0,any:!0,are:!0,as:!0,at:!0,be:!0,because:!0,been:!0,but:!0,by:!0,can:!0,cannot:!0,could:!0,dear:!0,did:!0,"do":!0,does:!0,either:!0,"else":!0,ever:!0,every:!0,"for":!0,from:!0,get:!0,got:!0,had:!0,has:!0,have:!0,he:!0,her:!0,hers:!0,him:!0,his:!0,how:!0,however:!0,i:!0,"if":!0,"in":!0,into:!0,is:!0,it:!0,its:!0,just:!0,least:!0,let:!0,like:!0,likely:!0,may:!0,me:!0,might:!0,most:!0,must:!0,my:!0,neither:!0,no:!0,nor:!0,not:!0,of:!0,off:!0,often:!0,on:!0,only:!0,or:!0,other:!0,our:!0,own:!0,rather:!0,said:!0,say:!0,says:!0,she:!0,should:!0,since:!0,so:!0,some:!0,than:!0,that:!0,the:!0,their:!0,them:!0,then:!0,there:!0,these:!0,they:!0,"this":!0,tis:!0,to:!0,too:!0,twas:!0,us:!0,wants:!0,was:!0,we:!0,were:!0,what:!0,when:!0,where:!0,which:!0,"while":!0,who:!0,whom:!0,why:!0,will:!0,"with":!0,would:!0,yet:!0,you:!0,your:!0},t.stopWordFilter.stopWords=t.defaultStopWords,t.Pipeline.registerFunction(t.stopWordFilter,"stopWordFilter"),t.trimmer=function(e){if(null===e||void 0===e)throw new Error("token should not be undefined");return e.replace(/^\W+/,"").replace(/\W+$/,"")},t.Pipeline.registerFunction(t.trimmer,"trimmer"),t.InvertedIndex=function(){this.root={docs:{},df:0}},t.InvertedIndex.load=function(e){var t=new this;return t.root=e.root,t},t.InvertedIndex.prototype.addToken=function(e,t,n){for(var n=n||this.root,i=0;i<=e.length-1;){var o=e[i];o in n||(n[o]={docs:{},df:0}),i+=1,n=n[o]}var r=t.ref;n.docs[r]?n.docs[r]={tf:t.tf}:(n.docs[r]={tf:t.tf},n.df+=1)},t.InvertedIndex.prototype.hasToken=function(e){if(!e)return!1;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return!1;t=t[e[n]]}return!0},t.InvertedIndex.prototype.getNode=function(e){if(!e)return null;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return null;t=t[e[n]]}return t},t.InvertedIndex.prototype.getDocs=function(e){var t=this.getNode(e);return null==t?{}:t.docs},t.InvertedIndex.prototype.getTermFrequency=function(e,t){var n=this.getNode(e);return null==n?0:t in n.docs?n.docs[t].tf:0},t.InvertedIndex.prototype.getDocFreq=function(e){var t=this.getNode(e);return null==t?0:t.df},t.InvertedIndex.prototype.removeToken=function(e,t){if(e){var n=this.getNode(e);null!=n&&t in n.docs&&(delete n.docs[t],n.df-=1)}},t.InvertedIndex.prototype.expandToken=function(e,t,n){if(null==e||""==e)return[];var t=t||[];if(void 0==n&&(n=this.getNode(e),null==n))return t;n.df>0&&t.push(e);for(var i in n)"docs"!==i&&"df"!==i&&this.expandToken(e+i,t,n[i]);return t},t.InvertedIndex.prototype.toJSON=function(){return{root:this.root}},t.Configuration=function(e,n){var e=e||"";if(void 0==n||null==n)throw new Error("fields should not be null");this.config={};var i;try{i=JSON.parse(e),this.buildUserConfig(i,n)}catch(o){t.utils.warn("user configuration parse failed, will use default configuration"),this.buildDefaultConfig(n)}},t.Configuration.prototype.buildDefaultConfig=function(e){this.reset(),e.forEach(function(e){this.config[e]={boost:1,bool:"OR",expand:!1}},this)},t.Configuration.prototype.buildUserConfig=function(e,n){var i="OR",o=!1;if(this.reset(),"bool"in e&&(i=e.bool||i),"expand"in e&&(o=e.expand||o),"fields"in e)for(var r in e.fields)if(n.indexOf(r)>-1){var s=e.fields[r],u=o;void 0!=s.expand&&(u=s.expand),this.config[r]={boost:s.boost||0===s.boost?s.boost:1,bool:s.bool||i,expand:u}}else t.utils.warn("field name in user configuration not found in index instance fields");else this.addAllFields2UserConfig(i,o,n)},t.Configuration.prototype.addAllFields2UserConfig=function(e,t,n){n.forEach(function(n){this.config[n]={boost:1,bool:e,expand:t}},this)},t.Configuration.prototype.get=function(){return this.config},t.Configuration.prototype.reset=function(){this.config={}},lunr.SortedSet=function(){this.length=0,this.elements=[]},lunr.SortedSet.load=function(e){var t=new this;return t.elements=e,t.length=e.length,t},lunr.SortedSet.prototype.add=function(){var e,t;for(e=0;e<arguments.length;e++)t=arguments[e],~this.indexOf(t)||this.elements.splice(this.locationFor(t),0,t);this.length=this.elements.length},lunr.SortedSet.prototype.toArray=function(){return this.elements.slice()},lunr.SortedSet.prototype.map=function(e,t){return this.elements.map(e,t)},lunr.SortedSet.prototype.forEach=function(e,t){return this.elements.forEach(e,t)},lunr.SortedSet.prototype.indexOf=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;){if(r===e)return o;e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o]}return r===e?o:-1},lunr.SortedSet.prototype.locationFor=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;)e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o];return r>e?o:e>r?o+1:void 0},lunr.SortedSet.prototype.intersect=function(e){for(var t=new lunr.SortedSet,n=0,i=0,o=this.length,r=e.length,s=this.elements,u=e.elements;;){if(n>o-1||i>r-1)break;s[n]!==u[i]?s[n]<u[i]?n++:s[n]>u[i]&&i++:(t.add(s[n]),n++,i++)}return t},lunr.SortedSet.prototype.clone=function(){var e=new lunr.SortedSet;return e.elements=this.toArray(),e.length=e.elements.length,e},lunr.SortedSet.prototype.union=function(e){var t,n,i;this.length>=e.length?(t=this,n=e):(t=e,n=this),i=t.clone();for(var o=0,r=n.toArray();o<r.length;o++)i.add(r[o]);return i},lunr.SortedSet.prototype.toJSON=function(){return this.toArray()},function(e,t){"function"==typeof define&&define.amd?define(t):"object"==typeof exports?module.exports=t():e.elasticlunr=t()}(this,function(){return t})}();
    /** pdoc search index */const docs = [{"fullname": "clarena", "modulename": "clarena", "kind": "module", "doc": "<h1 id=\"welcome-to-clarena\">Welcome to CLArena</h1>\n\n<p><strong>CLArena (Continual Learning Arena)</strong> is a open-source Python package for Continual Learning (CL) research. In this package, we provide a integrated environment and various APIs to conduct CL experiments for research purposes, as well as implemented CL algorithms and datasets that you can give it a spin immediately. We also developed an environment for Continual Unlearning (CUL), where you can conduct CUL experiments and use various unlearning algorithms.</p>\n\n<p>Please note that this is an API documantation providing detailed information about the available classes, functions, and modules in CLArena. Please refer to the main documentation and my beginners' guide to continual learning for more intuitive tutorials, examples, and guides on how to use CLArena:</p>\n\n<ul>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena\"><strong>Main Documentation</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/posts/continual-learning-beginners-guide\"><strong>A Beginners' Guide to Continual Learning</strong></a></li>\n</ul>\n\n<p>We provide various components of continual learning and unlearning system in the submodules:</p>\n\n<ul>\n<li><code>clarena.experiments</code>: Continual learning and unlearning experiment objects, as well as other reference experiments.</li>\n<li><code>clarena.cl_datasets</code>: Continual learning datasets.</li>\n<li><code>clarena.backbones</code>: Neural network architectures used as backbones for CL algorithms.</li>\n<li><code>clarena.heads</code>: Multi-head classifiers for outputs. Task-Incremental Learning (TIL) head, Class-Incremental Learning (CIL) head, and Multi-Task Learning (MTL) head are included.</li>\n<li><code>clarena.cl_algorithms</code>: Implementation of various continual learning algorithms.</li>\n<li><code>clarena.cul_algorithms</code>: Implementation of various unlearning algorithms on top of continual learning.</li>\n<li><code>clarena.callbacks</code>: Extra actions added to the continual learning process.</li>\n<li><code>clarena.utils</code>: Utility functions for continual learning experiments.</li>\n</ul>\n"}, {"fullname": "clarena.backbones", "modulename": "clarena.backbones", "kind": "module", "doc": "<h1 id=\"backbone-networks\">Backbone Networks</h1>\n\n<p>This submodule provides the <strong>backbone neural network architectures</strong> for various machine learning paradigms in CLArena.</p>\n\n<p>Here are the base classes for backbone networks, which inherit from PyTorch <code>nn.Module</code>:</p>\n\n<ul>\n<li><code>Backbone</code>: The base class for backbones.</li>\n<li><code>CLBackbone</code>: The base class for continual learning backbones.</li>\n<li><code>HATMaskBackbone</code>: The base class for backbones used in <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task)</a> CL algorithm.</li>\n<li><code>WSNMaskBackbone</code>: The base class for backbones used in <a href=\"https://proceedings.mlr.press/v162/kang22b/kang22b.pdf\">WSN (Winning Subnetworks)</a> CL algorithm.</li>\n</ul>\n\n<p>Please note that this is an API documentation. Please refer to the main documentation pages for more information about how to configure and implement backbone networks:</p>\n\n<ul>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/continual-learning/configure-main-experiment/backbone-network\"><strong>Configure Backbone Network (CL Main)</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/multi-task-learning-arena/configure-experiment/backbone-network\"><strong>Configure Backbone Network (MTL)</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/single-task-learning-arena/configure-experiment/backbone-network\"><strong>Configure Backbone Network (STL)</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/custom-implementation/backbone-network\"><strong>Implement Custom Backbone Network</strong></a></li>\n</ul>\n"}, {"fullname": "clarena.backbones.Backbone", "modulename": "clarena.backbones", "qualname": "Backbone", "kind": "class", "doc": "<p>The base class for continual learning backbone networks; inherits from <code>nn.Module</code>.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "clarena.backbones.Backbone.__init__", "modulename": "clarena.backbones", "qualname": "Backbone.__init__", "kind": "function", "doc": "<p>Initialize the backbone network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>output_dim</strong> (<code>int</code> | <code>None</code>): The output dimension that connects to output heads. The <code>input_dim</code> of output heads is expected to be the same as this <code>output_dim</code>. In some cases, this class is used as a block in the backbone network that doesn't have an output dimension. In this case, it can be <code>None</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "clarena.backbones.Backbone.output_dim", "modulename": "clarena.backbones", "qualname": "Backbone.output_dim", "kind": "variable", "doc": "<p>Store the output dimension of the backbone network.</p>\n", "annotation": ": int"}, {"fullname": "clarena.backbones.Backbone.weighted_layer_names", "modulename": "clarena.backbones", "qualname": "Backbone.weighted_layer_names", "kind": "variable", "doc": "<p>Maintain a list of weighted layer names. A weighted layer has weights connecting to other weighted layers. They are the main part of neural networks. <strong>It must be provided in subclasses.</strong></p>\n\n<p>The names follow the <code>nn.Module</code> internal naming mechanism. For example, if a layer is assigned to <code>self.conv1</code>, the name becomes <code>conv1</code>. If <code>nn.Sequential</code> is used, the name becomes the index of the layer in the sequence, such as <code>0</code>, <code>1</code>, etc. If a hierarchical structure is used (for example, a <code>nn.Module</code> is assigned to <code>self.block</code> which has <code>self.conv1</code>), the name becomes <code>block/conv1</code>. Note that it should be <code>block.conv1</code> according to <code>nn.Module</code>'s internal mechanism, but we use '/' instead of '.' to avoid errors when using '.' as keys in a <code>ModuleDict</code>.</p>\n", "annotation": ": list[str]"}, {"fullname": "clarena.backbones.Backbone.get_layer_by_name", "modulename": "clarena.backbones", "qualname": "Backbone.get_layer_by_name", "kind": "function", "doc": "<p>Get the layer by its name.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>layer_name</strong> (<code>str</code> | <code>None</code>): The layer name with '.' replaced by '/', like <code>block/conv1</code>, rather than <code>block.conv1</code>. If <code>None</code>, return <code>None</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>layer</strong> (<code>nn.Module</code> | <code>None</code>): The layer.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.Backbone.preceding_layer_name", "modulename": "clarena.backbones", "qualname": "Backbone.preceding_layer_name", "kind": "function", "doc": "<p>Get the name of the preceding layer of the given layer from the stored <code>self.masked_layer_order</code>. If the given layer is the first layer, return <code>None</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>layer_name</strong> (<code>str</code>): The name of the layer.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>preceding_layer_name</strong> (<code>str</code>): The name of the preceding layer.</li>\n</ul>\n\n<p><strong>Raises:</strong></p>\n\n<ul>\n<li><strong>ValueError</strong>: If <code>layer_name</code> is not in the weighted layer order.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.Backbone.next_layer_name", "modulename": "clarena.backbones", "qualname": "Backbone.next_layer_name", "kind": "function", "doc": "<p>Get the name of the next layer of the given layer from the stored <code>self.masked_layer_order</code>. If the given layer is the last layer of the BACKBONE, return <code>None</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>layer_name</strong> (<code>str</code>): The name of the layer.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>next_layer_name</strong> (<code>str</code>): The name of the next layer.</li>\n</ul>\n\n<p><strong>Raises:</strong></p>\n\n<ul>\n<li><strong>ValueError</strong>: If <code>layer_name</code> is not in the weighted layer order.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.Backbone.forward", "modulename": "clarena.backbones", "qualname": "Backbone.forward", "kind": "function", "doc": "<p>The forward pass. <strong>It must be implemented by subclasses.</strong></p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): The input tensor from data.</li>\n<li><strong>stage</strong> (<code>str</code>): The stage of the forward pass; one of:\n<ol>\n<li>'train': training stage.</li>\n<li>'validation': validation stage.</li>\n<li>'test': testing stage.</li>\n</ol></li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>output_feature</strong> (<code>Tensor</code>): The output feature tensor to be passed into heads. This is the main target of backpropagation.</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): The hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name, value (<code>Tensor</code>) is the hidden feature tensor. This is used for certain algorithms that need to use the hidden features for various purposes.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.CLBackbone", "modulename": "clarena.backbones", "qualname": "CLBackbone", "kind": "class", "doc": "<p>The base class of continual learning backbone networks, inherited from <code>Backbone</code>.</p>\n", "bases": "clarena.backbones.base.Backbone"}, {"fullname": "clarena.backbones.CLBackbone.__init__", "modulename": "clarena.backbones", "qualname": "CLBackbone.__init__", "kind": "function", "doc": "<p>Initialize the CL backbone network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>output_dim</strong> (<code>int</code> | <code>None</code>): The output dimension that connects to CL output heads. The <code>input_dim</code> of output heads is expected to be the same as this <code>output_dim</code>. In some cases, this class is used as a block in the backbone network that doesn't have an output dimension. In this case, it can be <code>None</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "clarena.backbones.CLBackbone.task_id", "modulename": "clarena.backbones", "qualname": "CLBackbone.task_id", "kind": "variable", "doc": "<p>Task ID counter indicating which task is being processed. Self-updated during the task loop. Starting from 1.</p>\n", "annotation": ": int"}, {"fullname": "clarena.backbones.CLBackbone.processed_task_ids", "modulename": "clarena.backbones", "qualname": "CLBackbone.processed_task_ids", "kind": "variable", "doc": "<p>Task IDs that have been processed in the experiment.</p>\n", "annotation": ": list[int]"}, {"fullname": "clarena.backbones.CLBackbone.setup_task_id", "modulename": "clarena.backbones", "qualname": "CLBackbone.setup_task_id", "kind": "function", "doc": "<p>Set up task <code>task_id</code>. This must be done before the <code>forward()</code> method is called.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): The target task ID.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.CLBackbone.forward", "modulename": "clarena.backbones", "qualname": "CLBackbone.forward", "kind": "function", "doc": "<p>The forward pass for data from task <code>task_id</code>. In some backbones, the forward pass might be different for different tasks. <strong>It must be implemented by subclasses.</strong></p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): The input tensor from data.</li>\n<li><strong>stage</strong> (<code>str</code>): The stage of the forward pass; one of:\n<ol>\n<li>'train': training stage.</li>\n<li>'validation': validation stage.</li>\n<li>'test': testing stage.</li>\n</ol></li>\n<li><strong>task_id</strong> (<code>int</code> | <code>None</code>): The task ID where the data are from. If the stage is 'train' or 'validation', it is usually the current task <code>self.task_id</code>. If the stage is 'test', it could be from any seen task. In TIL, the task IDs of test data are provided; thus this argument can be used. In CIL, they are not provided, so it is just a placeholder for API consistency and is not used. Best practice is not to provide this argument and leave it as the default value.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>output_feature</strong> (<code>Tensor</code>): The output feature tensor to be passed into heads. This is the main target of backpropagation.</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): The hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name, value (<code>Tensor</code>) is the hidden feature tensor. This is used for continual learning algorithms that need hidden features for various purposes.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.HATMaskBackbone", "modulename": "clarena.backbones", "qualname": "HATMaskBackbone", "kind": "class", "doc": "<p>The backbone network for HAT-based algorithms with learnable hard attention masks.</p>\n\n<p>HAT-based algorithms:</p>\n\n<ul>\n<li><a href=\"http://proceedings.mlr.press/v80/serra18a\"><strong>HAT (Hard Attention to the Task, 2018)</strong></a> is an architecture-based continual learning approach that uses learnable hard attention masks to select task-specific parameters.</li>\n<li><a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\"><strong>Adaptive HAT (Adaptive Hard Attention to the Task, 2024)</strong></a> is an architecture-based continual learning approach that improves HAT by introducing adaptive soft gradient clipping based on parameter importance and network sparsity.</li>\n<li><strong>FG-AdaHAT</strong> is an architecture-based continual learning approach that improves HAT by introducing fine-grained neuron-wise importance measures guiding the adaptive adjustment mechanism in AdaHAT.</li>\n</ul>\n", "bases": "clarena.backbones.base.CLBackbone"}, {"fullname": "clarena.backbones.HATMaskBackbone.__init__", "modulename": "clarena.backbones", "qualname": "HATMaskBackbone.__init__", "kind": "function", "doc": "<p>Initialize the HAT mask backbone network with task embeddings and masks.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>output_dim</strong> (<code>int</code>): The output dimension that connects to CL output heads. The <code>input_dim</code> of output heads is expected to be the same as this <code>output_dim</code>. In some cases, this class is used as a block in the backbone network that doesn't have an output dimension. In this case, it can be <code>None</code>.</li>\n<li><strong>gate</strong> (<code>str</code>): The type of gate function turning the real value task embeddings into attention masks; one of:\n<ul>\n<li><code>sigmoid</code>: the sigmoid function.</li>\n</ul></li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">gate</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "clarena.backbones.HATMaskBackbone.gate", "modulename": "clarena.backbones", "qualname": "HATMaskBackbone.gate", "kind": "variable", "doc": "<p>Store the type of gate function.</p>\n", "annotation": ": str"}, {"fullname": "clarena.backbones.HATMaskBackbone.task_embedding_t", "modulename": "clarena.backbones", "qualname": "HATMaskBackbone.task_embedding_t", "kind": "variable", "doc": "<p>Store the task embedding for the current task. Keys are layer names and values are the task embedding <code>nn.Embedding</code> for the layer. Each task embedding has size (1, number of units).</p>\n\n<p>We use <code>ModuleDict</code> rather than <code>dict</code> to ensure <code>LightningModule</code> properly registers these model parameters for purposes such as automatic device transfer and model summaries.</p>\n\n<p>We use <code>nn.Embedding</code> rather than <code>nn.Parameter</code> to store the task embedding for each layer, which is a type of <code>nn.Module</code> and can be accepted by <code>nn.ModuleDict</code>. (<code>nn.Parameter</code> cannot be accepted by <code>nn.ModuleDict</code>.)</p>\n\n<p><strong>This must be defined to cover each weighted layer (as listed in <code>self.weighted_layer_names</code>) in the backbone network.</strong> Otherwise, the uncovered parts will keep updating for all tasks and become a source of catastrophic forgetting.</p>\n", "annotation": ": torch.nn.modules.container.ModuleDict"}, {"fullname": "clarena.backbones.HATMaskBackbone.masks", "modulename": "clarena.backbones", "qualname": "HATMaskBackbone.masks", "kind": "variable", "doc": "<p>Store the binary attention mask of each previous task gated from the task embedding. Keys are task IDs and values are the corresponding mask. Each mask is a dict where keys are layer names and values are the binary mask tensor for the layer. The mask tensor has size (number of units, ).</p>\n", "annotation": ": dict[str, dict[str, torch.Tensor]]"}, {"fullname": "clarena.backbones.HATMaskBackbone.initialize_task_embedding", "modulename": "clarena.backbones", "qualname": "HATMaskBackbone.initialize_task_embedding", "kind": "function", "doc": "<p>Initialize the task embedding for the current task.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>mode</strong> (<code>str</code>): The initialization mode for task embeddings; one of:\n<ol>\n<li>'N01' (default): standard normal distribution $N(0, 1)$.</li>\n<li>'U-11': uniform distribution $U(-1, 1)$.</li>\n<li>'U01': uniform distribution $U(0, 1)$.</li>\n<li>'U-10': uniform distribution $U(-1, 0)$.</li>\n<li>'last': inherit task embeddings from the last task.</li>\n</ol></li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.HATMaskBackbone.sanity_check", "modulename": "clarena.backbones", "qualname": "HATMaskBackbone.sanity_check", "kind": "function", "doc": "<p>Check the sanity of the arguments.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.HATMaskBackbone.get_mask", "modulename": "clarena.backbones", "qualname": "HATMaskBackbone.get_mask", "kind": "function", "doc": "<p>Get the hard attention mask used in the <code>forward()</code> method for different stages.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>stage</strong> (<code>str</code>): The stage when applying the conversion; one of:\n<ol>\n<li>'train': training stage. Get the mask from the current task embedding through the gate function, scaled by an annealed scalar. See \u00a72.4 \"Hard Attention Training\" in the HAT paper.</li>\n<li>'validation': validation stage. Get the mask from the current task embedding through the gate function, scaled by <code>s_max</code>, where large scaling makes masks nearly binary. (Note that in this stage, the binary mask hasn't been stored yet, as training is not over.)</li>\n<li>'test': testing stage. Apply the test mask directly from the stored masks using <code>test_task_id</code>.</li>\n</ol></li>\n<li><strong>s_max</strong> (<code>float</code>): The maximum scaling factor in the gate function. Doesn't apply to the testing stage. See \u00a72.4 in the HAT paper.</li>\n<li><strong>batch_idx</strong> (<code>int</code> | <code>None</code>): The current batch index. Applies only to the training stage. For other stages, it is <code>None</code>.</li>\n<li><strong>num_batches</strong> (<code>int</code> | <code>None</code>): The total number of batches. Applies only to the training stage. For other stages, it is <code>None</code>.</li>\n<li><strong>test_task_id</strong> (<code>int</code> | <code>None</code>): The test task ID. Applies only to the testing stage. For other stages, it is <code>None</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>mask</strong> (<code>dict[str, Tensor]</code>): The hard attention (with values 0 or 1) mask. Key (<code>str</code>) is the layer name; value (<code>Tensor</code>) is the mask tensor. The mask tensor has size (number of units,).</li>\n</ul>\n\n<p><strong>Raises:</strong></p>\n\n<ul>\n<li><strong>ValueError</strong>: If <code>batch_idx</code> and <code>batch_num</code> are not provided in the 'train' stage; if <code>s_max</code> is not provided in the 'validation' stage; if <code>task_id</code> is not provided in the 'test' stage.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">s_max</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">test_task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.HATMaskBackbone.te_to_binary_mask", "modulename": "clarena.backbones", "qualname": "HATMaskBackbone.te_to_binary_mask", "kind": "function", "doc": "<p>Convert the current task embedding to a binary mask.</p>\n\n<p>This method is used before the testing stage to convert the task embedding into a binary mask for each layer. The binary mask is used to select parameters for the current task.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>mask_t</strong> (<code>dict[str, Tensor]</code>): The binary mask for the current task. Key (<code>str</code>) is the layer name; value (<code>Tensor</code>) is the mask tensor. The mask tensor has size (number of units,).</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.HATMaskBackbone.store_mask", "modulename": "clarena.backbones", "qualname": "HATMaskBackbone.store_mask", "kind": "function", "doc": "<p>Store the mask for the current task.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.HATMaskBackbone.get_layer_measure_parameter_wise", "modulename": "clarena.backbones", "qualname": "HATMaskBackbone.get_layer_measure_parameter_wise", "kind": "function", "doc": "<p>Get the parameter-wise measure on the parameters right before the given layer.</p>\n\n<p>It is calculated from the given unit-wise measure. It aggregates two feature-sized vectors (corresponding to the given layer and the preceding layer) into a weight-wise matrix (corresponding to the weights in between) and a bias-wise vector (corresponding to the bias of the given layer), using the given aggregation method. For example, given two feature-sized measures $m_{l,i}$ and $m_{l-1,j}$ and 'min' aggregation, the parameter-wise measure is $\\min \\left(a_{l,i}, a_{l-1,j}\\right)$, a matrix with respect to $i, j$.</p>\n\n<p>Note that if the given layer is the first layer with no preceding layer, we will get the parameter-wise measure directly broadcast from the unit-wise measure of the given layer.</p>\n\n<p>This method is used to calculate parameter-wise measures in various HAT-based algorithms:</p>\n\n<ul>\n<li><strong>HAT</strong>: the parameter-wise measure is the binary mask for previous tasks from the unit-wise cumulative mask of previous tasks <code>self.cumulative_mask_for_previous_tasks</code>, which is $\\min \\left(a_{l,i}^{<t}, a_{l-1,j}^{<t}\\right)$ in Eq. (2) in the HAT paper.</li>\n<li><strong>AdaHAT</strong>: the parameter-wise measure is the parameter importance for previous tasks from the unit-wise summative mask of previous tasks <code>self.summative_mask_for_previous_tasks</code>, which is $\\min \\left(m_{l,i}^{<t,\\text{sum}}, m_{l-1,j}^{<t,\\text{sum}}\\right)$ in Eq. (9) in the AdaHAT paper.</li>\n<li><strong>CBPHAT</strong>: the parameter-wise measure is the parameter importance for previous tasks from the unit-wise importance of previous tasks <code>self.unit_importance_for_previous_tasks</code> based on contribution utility, which is $\\min \\left(I_{l,i}^{(t-1)}, I_{l-1,j}^{(t-1)}\\right)$ in the adjustment rate formula in the paper draft.</li>\n</ul>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>unit_wise_measure</strong> (<code>dict[str, Tensor]</code>): The unit-wise measure. Key is layer name; value is the unit-wise measure tensor. The tensor has size (number of units,).</li>\n<li><strong>layer_name</strong> (<code>str</code>): The name of the given layer.</li>\n<li><strong>aggregation_mode</strong> (<code>str</code>): The aggregation mode turning two feature-wise measures into a weight-wise matrix; one of:\n<ul>\n<li>'min': takes the minimum of the two connected unit measures.</li>\n<li>'max': takes the maximum of the two connected unit measures.</li>\n<li>'mean': takes the mean of the two connected unit measures.\n<strong>Returns:</strong></li>\n</ul></li>\n<li><strong>weight_measure</strong> (<code>Tensor</code>): The weight measure matrix, the same size as the corresponding weights.</li>\n<li><strong>bias_measure</strong> (<code>Tensor</code>): The bias measure vector, the same size as the corresponding bias.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">unit_wise_measure</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">aggregation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.HATMaskBackbone.forward", "modulename": "clarena.backbones", "qualname": "HATMaskBackbone.forward", "kind": "function", "doc": "<p>The forward pass for data from task <code>task_id</code>. Task-specific masks for <code>task_id</code> are applied to the units in each layer.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): The input tensor from data.</li>\n<li><strong>stage</strong> (<code>str</code>): The stage of the forward pass; one of:\n<ol>\n<li>'train': training stage.</li>\n<li>'validation': validation stage.</li>\n<li>'test': testing stage.</li>\n</ol></li>\n<li><strong>s_max</strong> (<code>float</code>): The maximum scaling factor in the gate function. See \u00a72.4 \"Hard Attention Training\" in the HAT paper.</li>\n<li><strong>batch_idx</strong> (<code>int</code> | <code>None</code>): The current batch index. Applies only to the training stage. For other stages, it is <code>None</code>.</li>\n<li><strong>num_batches</strong> (<code>int</code> | <code>None</code>): The total number of batches. Applies only to the training stage. For other stages, it is <code>None</code>.</li>\n<li><strong>test_task_id</strong> (<code>int</code> | <code>None</code>): The test task ID. Applies only to the testing stage. For other stages, it is <code>None</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>output_feature</strong> (<code>Tensor</code>): The output feature tensor to be passed into heads. This is the main target of backpropagation.</li>\n<li><strong>mask</strong> (<code>dict[str, Tensor]</code>): The mask for the current task. Key (<code>str</code>) is layer name; value (<code>Tensor</code>) is the mask tensor. The mask tensor has size (number of units,).</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): The hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name; value (<code>Tensor</code>) is the hidden feature tensor. This is used for continual learning algorithms that need hidden features. Although the HAT algorithm does not need this, it is still provided for API consistency for other HAT-based algorithms that inherit this <code>forward()</code> method of the <code>HAT</code> class.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">s_max</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">test_task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">],</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.WSNMaskBackbone", "modulename": "clarena.backbones", "qualname": "WSNMaskBackbone", "kind": "class", "doc": "<p>The backbone network for the WSN algorithm with learnable parameter masks.</p>\n\n<p><a href=\"https://proceedings.mlr.press/v162/kang22b/kang22b.pdf\">WSN (Winning Subnetworks, 2022)</a> is an architecture-based continual learning algorithm. It trains learnable parameter-wise scores and selects the most scored $c\\%$ of the network parameters to be used for each task.</p>\n", "bases": "clarena.backbones.base.CLBackbone"}, {"fullname": "clarena.backbones.WSNMaskBackbone.__init__", "modulename": "clarena.backbones", "qualname": "WSNMaskBackbone.__init__", "kind": "function", "doc": "<p>Initialize the WSN mask backbone network with task embeddings and masks.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>output_dim</strong> (<code>int</code>): The output dimension that connects to CL output heads. The <code>input_dim</code> of output heads is expected to be the same as this <code>output_dim</code>. In some cases, this class is used as a block in the backbone network that doesn't have an output dimension. In this case, it can be <code>None</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "clarena.backbones.WSNMaskBackbone.gate_fn", "modulename": "clarena.backbones", "qualname": "WSNMaskBackbone.gate_fn", "kind": "variable", "doc": "<p>The gate function turning the real-value parameter score into binary parameter masks. It is a custom autograd function that applies percentile parameter masking by score.</p>\n", "annotation": ": torch.autograd.function.Function"}, {"fullname": "clarena.backbones.WSNMaskBackbone.weight_score_t", "modulename": "clarena.backbones", "qualname": "WSNMaskBackbone.weight_score_t", "kind": "variable", "doc": "<p>Store the weight score for the current task. Keys are the layer names and values are the task embedding <code>nn.Embedding</code> for the layer. Each task embedding has the same size (output features, input features) as the weight.</p>\n\n<p>We use <code>ModuleDict</code> rather than <code>dict</code> to ensure <code>LightningModule</code> properly registers these model parameters for purposes such as automatic device transfer and model summaries.</p>\n\n<p>We use <code>nn.Embedding</code> rather than <code>nn.Parameter</code> to store the task embedding for each layer, which is a type of <code>nn.Module</code> and can be accepted by <code>nn.ModuleDict</code>. (<code>nn.Parameter</code> cannot be accepted by <code>nn.ModuleDict</code>.)</p>\n\n<p><strong>This must be defined to cover each weighted layer (as listed in <code>self.weighted_layer_names</code>) in the backbone network.</strong> Otherwise, the uncovered parts will keep updating for all tasks and become a source of catastrophic forgetting.</p>\n", "annotation": ": torch.nn.modules.container.ModuleDict"}, {"fullname": "clarena.backbones.WSNMaskBackbone.bias_score_t", "modulename": "clarena.backbones", "qualname": "WSNMaskBackbone.bias_score_t", "kind": "variable", "doc": "<p>Store the bias score for the current task. Keys are the layer names and values are the task embedding <code>nn.Embedding</code> for the layer. Each task embedding has the same size (1, output features) as the bias. If the layer doesn't have a bias, it is <code>None</code>.</p>\n\n<p>We use <code>ModuleDict</code> rather than <code>dict</code> to ensure <code>LightningModule</code> properly registers these model parameters for purposes such as automatic device transfer and model summaries.</p>\n\n<p>We use <code>nn.Embedding</code> rather than <code>nn.Parameter</code> to store the task embedding for each layer, which is a type of <code>nn.Module</code> and can be accepted by <code>nn.ModuleDict</code>. (<code>nn.Parameter</code> cannot be accepted by <code>nn.ModuleDict</code>.)</p>\n\n<p><strong>This must be defined to cover each weighted layer (as listed in <code>self.weighted_layer_names</code>) in the backbone network.</strong> Otherwise, the uncovered parts will keep updating for all tasks and become a source of catastrophic forgetting.</p>\n", "annotation": ": torch.nn.modules.container.ModuleDict"}, {"fullname": "clarena.backbones.WSNMaskBackbone.sanity_check", "modulename": "clarena.backbones", "qualname": "WSNMaskBackbone.sanity_check", "kind": "function", "doc": "<p>Check the sanity of the arguments.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.WSNMaskBackbone.initialize_parameter_score", "modulename": "clarena.backbones", "qualname": "WSNMaskBackbone.initialize_parameter_score", "kind": "function", "doc": "<p>Initialize the parameter score for the current task.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>mode</strong> (<code>str</code>): The initialization mode for parameter scores; one of:\n<ol>\n<li>'default': the default initialization mode in the original WSN code.</li>\n<li>'N01': standard normal distribution $N(0, 1)$.</li>\n<li>'U01': uniform distribution $U(0, 1)$.</li>\n</ol></li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.WSNMaskBackbone.get_mask", "modulename": "clarena.backbones", "qualname": "WSNMaskBackbone.get_mask", "kind": "function", "doc": "<p>Get the binary parameter mask used in the <code>forward()</code> method for different stages.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>stage</strong> (<code>str</code>): The stage when applying the conversion; one of:\n<ol>\n<li>'train': training stage. Get the mask from the parameter score of the current task through the gate function that masks the top $c\\%$ largest scored parameters. See \u00a73.1 \"Winning Subnetworks\" in the WSN paper.</li>\n<li>'validation': validation stage. Same as 'train'. (Note that in this stage, the binary mask hasn't been stored yet, as training is not over.)</li>\n<li>'test': testing stage. Apply the test mask directly from the argument <code>test_mask</code>.</li>\n</ol></li>\n<li><strong>test_mask</strong> (<code>tuple[dict[str, Tensor], dict[str, Tensor]]</code> | <code>None</code>): The binary weight and bias masks used for testing. Applies only to the testing stage. For other stages, it is <code>None</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>weight_mask</strong> (<code>dict[str, Tensor]</code>): The binary mask on weights. Key (<code>str</code>) is the layer name; value (<code>Tensor</code>) is the mask tensor. The mask tensor has the same size (output features, input features) as the weight.</li>\n<li><strong>bias_mask</strong> (<code>dict[str, Tensor]</code>): The binary mask on biases. Key (<code>str</code>) is the layer name; value (<code>Tensor</code>) is the mask tensor. The mask tensor has the same size (output features,) as the bias. If the layer doesn't have a bias, it is <code>None</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">mask_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">test_mask</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">],</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.WSNMaskBackbone.forward", "modulename": "clarena.backbones", "qualname": "WSNMaskBackbone.forward", "kind": "function", "doc": "<p>The forward pass for data from task <code>task_id</code>. Task-specific mask for <code>task_id</code> are applied to the units in each layer.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): The input tensor from data.</li>\n<li><strong>stage</strong> (<code>str</code>): The stage of the forward pass; one of:\n<ol>\n<li>'train': training stage.</li>\n<li>'validation': validation stage.</li>\n<li>'test': testing stage.</li>\n</ol></li>\n<li><strong>mask_percentage</strong> (<code>float</code>): The percentage of parameters to be masked. The value should be between 0 and 1.</li>\n<li><strong>test_mask</strong> (<code>tuple[dict[str, Tensor], dict[str, Tensor]]</code> | <code>None</code>): The binary weight and bias mask used for test. Applies only to the testing stage. For other stages, it is <code>None</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>output_feature</strong> (<code>Tensor</code>): The output feature tensor to be passed into heads. This is the main target of backpropagation.</li>\n<li><strong>weight_mask</strong> (<code>dict[str, Tensor]</code>): The weight mask for the current task. Key (<code>str</code>) is layer name; value (<code>Tensor</code>) is the mask tensor. The mask tensor has same (output features, input features) as the weight.</li>\n<li><strong>bias_mask</strong> (<code>dict[str, Tensor]</code>): The bias mask for the current task. Key (<code>str</code>) is layer name; value (<code>Tensor</code>) is the mask tensor. The mask tensor has same (output features, ) as the bias. If the layer doesn't have a bias, it is <code>None</code>.</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): The hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name; value (<code>Tensor</code>) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">mask_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">test_mask</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">],</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">],</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">],</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.NISPAMaskBackbone", "modulename": "clarena.backbones", "qualname": "NISPAMaskBackbone", "kind": "class", "doc": "<p>The backbone network for the NISPA algorithm with neuron masks.</p>\n\n<p><a href=\"https://proceedings.mlr.press/v162/gurbuz22a/gurbuz22a.pdf\">NISPA (Neuro-Inspired Stability-Plasticity Adaptation)</a> is an architecture-based continual learning algorithm.</p>\n", "bases": "clarena.backbones.base.CLBackbone"}, {"fullname": "clarena.backbones.NISPAMaskBackbone.__init__", "modulename": "clarena.backbones", "qualname": "NISPAMaskBackbone.__init__", "kind": "function", "doc": "<p>Initialize the NISPA mask backbone network with masks.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>output_dim</strong> (<code>int</code>): The output dimension that connects to CL output heads. The <code>input_dim</code> of output heads is expected to be the same as this <code>output_dim</code>. In some cases, this class is used as a block in the backbone network that doesn't have an output dimension. In this case, it can be <code>None</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "clarena.backbones.NISPAMaskBackbone.weight_mask_t", "modulename": "clarena.backbones", "qualname": "NISPAMaskBackbone.weight_mask_t", "kind": "variable", "doc": "<p>Store the weight mask for each layer. Key (<code>str</code>) is layer name, value (<code>Tensor</code>) is the mask tensor. The mask tensor has the same size (output features, input features) as weight.</p>\n", "annotation": ": dict[str, torch.Tensor]"}, {"fullname": "clarena.backbones.NISPAMaskBackbone.frozen_weight_mask_t", "modulename": "clarena.backbones", "qualname": "NISPAMaskBackbone.frozen_weight_mask_t", "kind": "variable", "doc": "<p>Store the frozen weight mask for each layer. Key (<code>str</code>) is layer name, value (<code>Tensor</code>) is the mask tensor. The mask tensor has the same size (output features, input features) as weight.</p>\n", "annotation": ": dict[str, torch.Tensor]"}, {"fullname": "clarena.backbones.NISPAMaskBackbone.bias_mask_t", "modulename": "clarena.backbones", "qualname": "NISPAMaskBackbone.bias_mask_t", "kind": "variable", "doc": "<p>Store the bias mask for each layer. Key (<code>str</code>) is layer name, value (<code>Tensor</code>) is the mask tensor. The mask tensor has the same size (output features, ) as bias. If the layer doesn't have bias, it is <code>None</code>.</p>\n", "annotation": ": dict[str, torch.Tensor]"}, {"fullname": "clarena.backbones.NISPAMaskBackbone.frozen_bias_mask_t", "modulename": "clarena.backbones", "qualname": "NISPAMaskBackbone.frozen_bias_mask_t", "kind": "variable", "doc": "<p>Store the frozen bias mask for each layer. Key (<code>str</code>) is layer name, value (<code>Tensor</code>) is the mask tensor. The mask tensor has the same size (output features, ) as bias. If the layer doesn't have bias, it is <code>None</code>.</p>\n", "annotation": ": dict[str, torch.Tensor]"}, {"fullname": "clarena.backbones.NISPAMaskBackbone.sanity_check", "modulename": "clarena.backbones", "qualname": "NISPAMaskBackbone.sanity_check", "kind": "function", "doc": "<p>Check the sanity of the arguments.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.NISPAMaskBackbone.initialize_parameter_mask", "modulename": "clarena.backbones", "qualname": "NISPAMaskBackbone.initialize_parameter_mask", "kind": "function", "doc": "<p>Initialize the parameter masks as zeros.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.NISPAMaskBackbone.forward", "modulename": "clarena.backbones", "qualname": "NISPAMaskBackbone.forward", "kind": "function", "doc": "<p>The forward pass for data from task <code>task_id</code>. The parameter mask is applied to the parameters in each layer.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): The input tensor from data.</li>\n<li><strong>stage</strong> (<code>str</code>): The stage of the forward pass; one of:\n<ol>\n<li>'train': training stage.</li>\n<li>'validation': validation stage.</li>\n<li>'test': testing stage.</li>\n</ol></li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>output_feature</strong> (<code>Tensor</code>): The output feature tensor to be passed into heads. This is the main target of backpropagation.</li>\n<li><strong>weight_mask</strong> (<code>dict[str, Tensor]</code>): The weight mask. Key (<code>str</code>) is layer name; value (<code>Tensor</code>) is the mask tensor. The mask tensor has the same size (output features, input features) as the weight.</li>\n<li><strong>bias_mask</strong> (<code>dict[str, Tensor]</code>): The bias mask. Key (<code>str</code>) is layer name; value (<code>Tensor</code>) is the mask tensor. The mask tensor has the same size (output features, ) as the bias. If the layer doesn't have a bias, it is <code>None</code>.</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): The hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name; value (<code>Tensor</code>) is the hidden feature tensor. This is used for continual learning algorithms that need hidden features for various purposes.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">],</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">],</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.mlp", "modulename": "clarena.backbones.mlp", "kind": "module", "doc": "<p>The submodule in <code>backbones</code> for the MLP backbone network. It includes multiple versions of MLP, including the basic MLP, the continual learning MLP, the <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT</a> masked MLP, and the <a href=\"https://proceedings.mlr.press/v162/kang22b/kang22b.pdf\">WSN</a> masked MLP.</p>\n"}, {"fullname": "clarena.backbones.mlp.MLP", "modulename": "clarena.backbones.mlp", "qualname": "MLP", "kind": "class", "doc": "<p>Multi-layer perceptron (MLP), a.k.a. fully connected network.</p>\n\n<p>MLP is a dense network architecture with several fully connected layers, each followed by an activation function. The last layer connects to the output heads.</p>\n", "bases": "clarena.backbones.base.Backbone"}, {"fullname": "clarena.backbones.mlp.MLP.__init__", "modulename": "clarena.backbones.mlp", "qualname": "MLP.__init__", "kind": "function", "doc": "<p>Construct and initialize the MLP backbone network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_dim</strong> (<code>int</code>): The input dimension. Any data need to be flattened before entering the MLP. Note that it is not required in convolutional networks.</li>\n<li><strong>hidden_dims</strong> (<code>list[int]</code>): List of hidden layer dimensions. It can be an empty list, which means a single-layer MLP, and it can be as many layers as you want. Note that it doesn't include the last dimension, which we take as the output dimension.</li>\n<li><strong>output_dim</strong> (<code>int</code>): The output dimension that connects to output heads.</li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code> | <code>None</code>): Activation function of each layer (if not <code>None</code>). If <code>None</code>, this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>batch_normalization</strong> (<code>bool</code>): Whether to use batch normalization after the fully connected layers. Default <code>False</code>.</li>\n<li><strong>bias</strong> (<code>bool</code>): Whether to use bias in the linear layer. Default <code>True</code>.</li>\n<li><strong>dropout</strong> (<code>float</code> | <code>None</code>): The probability for the dropout layer. If <code>None</code>, this layer won't be used. Default <code>None</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">hidden_dims</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">batch_normalization</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">dropout</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "clarena.backbones.mlp.MLP.input_dim", "modulename": "clarena.backbones.mlp", "qualname": "MLP.input_dim", "kind": "variable", "doc": "<p>Store the input dimension of the MLP backbone network. Used when constructing the first fully-connected layer.</p>\n", "annotation": ": int"}, {"fullname": "clarena.backbones.mlp.MLP.hidden_dims", "modulename": "clarena.backbones.mlp", "qualname": "MLP.hidden_dims", "kind": "variable", "doc": "<p>Store the hidden dimensions of the MLP backbone network. Used when constructing the fully-connected layers.</p>\n", "annotation": ": list[int]"}, {"fullname": "clarena.backbones.mlp.MLP.output_dim", "modulename": "clarena.backbones.mlp", "qualname": "MLP.output_dim", "kind": "variable", "doc": "<p>Store the output dimension of the MLP backbone network. Used when constructing the last fully-connected layer.</p>\n", "annotation": ": int"}, {"fullname": "clarena.backbones.mlp.MLP.num_fc_layers", "modulename": "clarena.backbones.mlp", "qualname": "MLP.num_fc_layers", "kind": "variable", "doc": "<p>Store the number of fully-connected layers in the MLP backbone network, which helps form the loops in constructing layers and forward pass.</p>\n", "annotation": ": int"}, {"fullname": "clarena.backbones.mlp.MLP.batch_normalization", "modulename": "clarena.backbones.mlp", "qualname": "MLP.batch_normalization", "kind": "variable", "doc": "<p>Store whether to use batch normalization after the fully-connected layers.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.backbones.mlp.MLP.activation", "modulename": "clarena.backbones.mlp", "qualname": "MLP.activation", "kind": "variable", "doc": "<p>Store whether to use activation function after the fully-connected layers.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.backbones.mlp.MLP.dropout", "modulename": "clarena.backbones.mlp", "qualname": "MLP.dropout", "kind": "variable", "doc": "<p>Store whether to use dropout after the fully-connected layers.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.backbones.mlp.MLP.fc", "modulename": "clarena.backbones.mlp", "qualname": "MLP.fc", "kind": "variable", "doc": "<p>The list of fully connected (<code>nn.Linear</code>) layers.</p>\n", "annotation": ": torch.nn.modules.container.ModuleList"}, {"fullname": "clarena.backbones.mlp.MLP.forward", "modulename": "clarena.backbones.mlp", "qualname": "MLP.forward", "kind": "function", "doc": "<p>The forward pass for data. It is the same for all tasks.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): The input tensor from data.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>output_feature</strong> (<code>Tensor</code>): The output feature tensor to be passed into heads. This is the main target of backpropagation.</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): The hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name; value (<code>Tensor</code>) is the hidden feature tensor. This is used for certain algorithms that need to use hidden features for various purposes.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.mlp.CLMLP", "modulename": "clarena.backbones.mlp", "qualname": "CLMLP", "kind": "class", "doc": "<p>Multi-layer perceptron (MLP), a.k.a. fully connected network. Used as a continual learning backbone.</p>\n\n<p>MLP is a dense network architecture with several fully connected layers, each followed by an activation function. The last layer connects to the CL output heads.</p>\n", "bases": "clarena.backbones.base.CLBackbone, MLP"}, {"fullname": "clarena.backbones.mlp.CLMLP.__init__", "modulename": "clarena.backbones.mlp", "qualname": "CLMLP.__init__", "kind": "function", "doc": "<p>Construct and initialize the CLMLP backbone network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_dim</strong> (<code>int</code>): the input dimension. Any data need to be flattened before going in MLP. Note that it is not required in convolutional networks.</li>\n<li><strong>hidden_dims</strong> (<code>list[int]</code>): list of hidden layer dimensions. It can be empty list which means single-layer MLP, and it can be as many layers as you want. Note that it doesn't include the last dimension which we take as output dimension.</li>\n<li><strong>output_dim</strong> (<code>int</code>): the output dimension which connects to CL output heads.</li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code> | <code>None</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>batch_normalization</strong> (<code>bool</code>): whether to use batch normalization after the fully-connected layers. Default <code>False</code>.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the linear layer. Default <code>True</code>.</li>\n<li><strong>dropout</strong> (<code>float</code> | <code>None</code>): the probability for the dropout layer, if <code>None</code> this layer won't be used. Default <code>None</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">hidden_dims</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">batch_normalization</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">dropout</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "clarena.backbones.mlp.CLMLP.forward", "modulename": "clarena.backbones.mlp", "qualname": "CLMLP.forward", "kind": "function", "doc": "<p>The forward pass for data. It is the same for all tasks.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): The input tensor from data.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>output_feature</strong> (<code>Tensor</code>): The output feature tensor to be passed into heads. This is the main target of backpropagation.</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): The hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name; value (<code>Tensor</code>) is the hidden feature tensor. This is used for continual learning algorithms that need hidden features for various purposes.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.mlp.HATMaskMLP", "modulename": "clarena.backbones.mlp", "qualname": "HATMaskMLP", "kind": "class", "doc": "<p>HAT-masked multi-layer perceptron (MLP).</p>\n\n<p><a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task, 2018)</a> is an architecture-based continual learning approach that uses learnable hard attention masks to select task-specific parameters.</p>\n\n<p>MLP is a dense network architecture with several fully connected layers, each followed by an activation function. The last layer connects to the CL output heads.</p>\n\n<p>The mask is applied to units (neurons) in each fully connected layer. The mask is generated from the unit-wise task embedding and the gate function.</p>\n", "bases": "clarena.backbones.base.HATMaskBackbone, MLP"}, {"fullname": "clarena.backbones.mlp.HATMaskMLP.__init__", "modulename": "clarena.backbones.mlp", "qualname": "HATMaskMLP.__init__", "kind": "function", "doc": "<p>Construct and initialize the HAT-masked MLP backbone network with task embeddings. Note that batch normalization is incompatible with the HAT mechanism.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_dim</strong> (<code>int</code>): The input dimension. Any data need to be flattened before entering the MLP.</li>\n<li><strong>hidden_dims</strong> (<code>list[int]</code>): List of hidden layer dimensions. It can be an empty list, which means a single-layer MLP, and it can be as many layers as you want. Note that it doesn't include the last dimension, which we take as the output dimension.</li>\n<li><strong>output_dim</strong> (<code>int</code>): The output dimension that connects to CL output heads.</li>\n<li><strong>gate</strong> (<code>str</code>): The type of gate function turning real-valued task embeddings into attention masks; one of:\n<ul>\n<li><code>sigmoid</code>: the sigmoid function.</li>\n</ul></li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code> | <code>None</code>): Activation function of each layer (if not <code>None</code>). If <code>None</code>, this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>batch_normalization</strong> (<code>str</code> | <code>None</code>): How to use batch normalization after the fully connected layers; one of:\n<ul>\n<li><code>None</code>: no batch normalization layers.</li>\n<li><code>shared</code>: use a single batch normalization layer for all tasks. Note that this can cause catastrophic forgetting.</li>\n<li><code>independent</code>: use independent batch normalization layers for each task.</li>\n</ul></li>\n<li><strong>bias</strong> (<code>bool</code>): Whether to use bias in the linear layer. Default <code>True</code>.</li>\n<li><strong>dropout</strong> (<code>float</code> | <code>None</code>): The probability for the dropout layer. If <code>None</code>, this layer won't be used. Default <code>None</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">hidden_dims</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">gate</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">batch_normalization</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">dropout</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.backbones.mlp.HATMaskMLP.batch_normalization", "modulename": "clarena.backbones.mlp", "qualname": "HATMaskMLP.batch_normalization", "kind": "variable", "doc": "<p>Store the way to use batch normalization after the fully-connected layers. This overrides the <code>batch_normalization</code> argument in <code>MLP</code> class.</p>\n", "annotation": ": str | None"}, {"fullname": "clarena.backbones.mlp.HATMaskMLP.setup_task_id", "modulename": "clarena.backbones.mlp", "qualname": "HATMaskMLP.setup_task_id", "kind": "function", "doc": "<p>Set up task <code>task_id</code>. This must be done before the <code>forward()</code> method is called.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): The target task ID.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.mlp.HATMaskMLP.get_bn", "modulename": "clarena.backbones.mlp", "qualname": "HATMaskMLP.get_bn", "kind": "function", "doc": "<p>Get the batch normalization layer used in the <code>forward()</code> method for different stages.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>stage</strong> (<code>str</code>): The stage of the forward pass; one of:\n<ol>\n<li>'train': training stage.</li>\n<li>'validation': validation stage.</li>\n<li>'test': testing stage.</li>\n</ol></li>\n<li><strong>test_task_id</strong> (<code>int</code> | <code>None</code>): The test task ID. Applies only to the testing stage. For other stages, it is <code>None</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>fc_bn</strong> (<code>nn.Module</code> | <code>None</code>): The batch normalization objects.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">test_task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.mlp.HATMaskMLP.initialize_independent_bn", "modulename": "clarena.backbones.mlp", "qualname": "HATMaskMLP.initialize_independent_bn", "kind": "function", "doc": "<p>Initialize the independent batch normalization layer for the current task. This is called when a new task is created. Applies only when <code>self.batch_normalization</code> is \"independent\".</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.mlp.HATMaskMLP.store_bn", "modulename": "clarena.backbones.mlp", "qualname": "HATMaskMLP.store_bn", "kind": "function", "doc": "<p>Store the batch normalization layer for the current task <code>self.task_id</code>. Applies only when <code>self.batch_normalization</code> is \"independent\".</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.mlp.HATMaskMLP.forward", "modulename": "clarena.backbones.mlp", "qualname": "HATMaskMLP.forward", "kind": "function", "doc": "<p>The forward pass for data from task <code>task_id</code>. Task-specific masks for <code>task_id</code> are applied to units (neurons) in each fully connected layer.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): The input tensor from data.</li>\n<li><strong>stage</strong> (<code>str</code>): The stage of the forward pass; one of:\n<ol>\n<li>'train': training stage.</li>\n<li>'validation': validation stage.</li>\n<li>'test': testing stage.</li>\n</ol></li>\n<li><strong>s_max</strong> (<code>float</code> | <code>None</code>): The maximum scaling factor in the gate function. Doesn't apply to the testing stage. See \u00a72.4 \"Hard Attention Training\" in the HAT paper.</li>\n<li><strong>batch_idx</strong> (<code>int</code> | <code>None</code>): The current batch index. Applies only to the training stage. For other stages, it is <code>None</code>.</li>\n<li><strong>num_batches</strong> (<code>int</code> | <code>None</code>): The total number of batches. Applies only to the training stage. For other stages, it is <code>None</code>.</li>\n<li><strong>test_task_id</strong> (<code>int</code> | <code>None</code>): The test task ID. Applies only to the testing stage. For other stages, it is <code>None</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>output_feature</strong> (<code>Tensor</code>): The output feature tensor to be passed into heads. This is the main target of backpropagation.</li>\n<li><strong>mask</strong> (<code>dict[str, Tensor]</code>): The mask for the current task. Key (<code>str</code>) is the layer name; value (<code>Tensor</code>) is the mask tensor. The mask tensor has size (number of units,).</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): The hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name; value (<code>Tensor</code>) is the hidden feature tensor. This is used for continual learning algorithms that need hidden features. Although the HAT algorithm does not need this, it is still provided for API consistency for other HAT-based algorithms that inherit this <code>forward()</code> method of the <code>HAT</code> class.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">s_max</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">test_task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">],</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.resnet", "modulename": "clarena.backbones.resnet", "kind": "module", "doc": "<p>The submodule in <code>backbones</code> for ResNet backbone network. It includes multiple versions of ResNet, including the basic ResNet, the continual learning ResNet, and the <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT</a> masked ResNet.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBlockSmall", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockSmall", "kind": "class", "doc": "<p>The smaller building block for ResNet-18/34.</p>\n\n<p>It consists of 2 weight convolutional layers, each followed by an activation function. See Table 1 or Figure 5 (left) in the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a>.</p>\n", "bases": "clarena.backbones.base.Backbone"}, {"fullname": "clarena.backbones.resnet.ResNetBlockSmall.__init__", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockSmall.__init__", "kind": "function", "doc": "<p>Construct and initialize the smaller building block.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outer_layer_name</strong> (<code>str</code>): pass the name of the multi-building-block layer that contains this building block to construct the full name of each weighted convolutional layer.</li>\n<li><strong>block_idx</strong> (<code>int</code>): the index of the building blocks in the multi-building-block layer to construct the full name of each weighted convolutional layer.</li>\n<li><strong>preceding_output_channels</strong> (<code>int</code>): the number of channels of preceding output of this particular building block.</li>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block.</li>\n<li><strong>overall_stride</strong> (<code>int</code>): the overall stride of this building block. This stride is performed at 2nd (last) convolutional layer where the 1st convolutional layer remain stride of 1.</li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>batch_normalization</strong> (<code>bool</code>): whether to use batch normalization after the weight convolutional layers. Default <code>True</code>, same as what the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a> does.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>, because batch normalization are doing the similar thing with bias.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">outer_layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">block_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">preceding_output_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">overall_stride</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">batch_normalization</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.ResNetBlockSmall.input_channels", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockSmall.input_channels", "kind": "variable", "doc": "<p>Store the number of input channels of this building block. This is used to construct the identity downsample function if needed.</p>\n", "annotation": ": int"}, {"fullname": "clarena.backbones.resnet.ResNetBlockSmall.batch_normalization", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockSmall.batch_normalization", "kind": "variable", "doc": "<p>Store whether to use batch normalization after the fully-connected layers.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.backbones.resnet.ResNetBlockSmall.activation", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockSmall.activation", "kind": "variable", "doc": "<p>Store whether to use activation function after the fully-connected layers.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.backbones.resnet.ResNetBlockSmall.full_1st_layer_name", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockSmall.full_1st_layer_name", "kind": "variable", "doc": "<p>Format and store full name of the 1st weighted convolutional layer.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBlockSmall.full_2nd_layer_name", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockSmall.full_2nd_layer_name", "kind": "variable", "doc": "<p>Format and store full name of the 2nd weighted convolutional layer.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBlockSmall.conv1", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockSmall.conv1", "kind": "variable", "doc": "<p>The 1st weight convolutional layer of the smaller building block.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBlockSmall.conv2", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockSmall.conv2", "kind": "variable", "doc": "<p>The 2nd weight convolutional layer of the smaller building block.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBlockSmall.identity_downsample", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockSmall.identity_downsample", "kind": "variable", "doc": "<p>The convolutional layer for downsampling identity in the shortcut connection if the dimension of identity from input doesn't match the output's. This case only happens when the number of input channels doesn't equal to the number of preceding output channels or a layer with stride &gt; 1 exists.</p>\n", "annotation": ": torch.nn.modules.module.Module"}, {"fullname": "clarena.backbones.resnet.ResNetBlockSmall.forward", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockSmall.forward", "kind": "function", "doc": "<p>The forward pass for data.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): the input feature maps.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>output_feature</strong> (<code>Tensor</code>): the output feature maps.</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): the hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name, value (<code>Tensor</code>) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.resnet.ResNetBlockLarge", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockLarge", "kind": "class", "doc": "<p>The larger building block for ResNet-50/101/152. It is referred to \"bottleneck\" building block in the paper.</p>\n\n<p>It consists of 3 weight convolutional layers, each followed by an activation function. See Table 1 or Figure 5 (right) in the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a>.</p>\n", "bases": "clarena.backbones.base.Backbone"}, {"fullname": "clarena.backbones.resnet.ResNetBlockLarge.__init__", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockLarge.__init__", "kind": "function", "doc": "<p>Construct and initialize the larger building block.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outer_layer_name</strong> (<code>str</code>): pass the name of the multi-building-block layer that contains this building block to construct the full name of each weighted convolutional layer.</li>\n<li><strong>block_idx</strong> (<code>int</code>): the index of the building blocks in the multi-building-block layer to construct the full name of each weighted convolutional layer.</li>\n<li><strong>preceding_output_channels</strong> (<code>int</code>): the number of channels of preceding output of this particular building block.</li>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block.</li>\n<li><strong>overall_stride</strong> (<code>int</code>): the overall stride of this building block. This stride is performed at 2nd (middle) convolutional layer where 1st and 3rd convolutional layers remain stride of 1.</li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>batch_normalization</strong> (<code>bool</code>): whether to use batch normalization after the weight convolutional layers. Default <code>True</code>, same as what the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a> does.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>, because batch normalization are doing the similar thing with bias.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">outer_layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">block_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">preceding_output_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">overall_stride</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">batch_normalization</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.ResNetBlockLarge.batch_normalization", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockLarge.batch_normalization", "kind": "variable", "doc": "<p>Store whether to use batch normalization after the fully-connected layers.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.backbones.resnet.ResNetBlockLarge.activation", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockLarge.activation", "kind": "variable", "doc": "<p>Store whether to use activation function after the fully-connected layers.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.backbones.resnet.ResNetBlockLarge.full_1st_layer_name", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockLarge.full_1st_layer_name", "kind": "variable", "doc": "<p>Format and store full name of the 1st weighted convolutional layer.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBlockLarge.full_2nd_layer_name", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockLarge.full_2nd_layer_name", "kind": "variable", "doc": "<p>Format and store full name of the 2nd weighted convolutional layer.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBlockLarge.full_3rd_layer_name", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockLarge.full_3rd_layer_name", "kind": "variable", "doc": "<p>Format and store full name of the 3rd weighted convolutional layer.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBlockLarge.conv1", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockLarge.conv1", "kind": "variable", "doc": "<p>The 1st weight convolutional layer of the larger building block.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBlockLarge.conv2", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockLarge.conv2", "kind": "variable", "doc": "<p>The 2nd weight convolutional layer of the larger building block.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBlockLarge.conv3", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockLarge.conv3", "kind": "variable", "doc": "<p>The 3rd weight convolutional layer of the larger building block.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBlockLarge.identity_downsample", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockLarge.identity_downsample", "kind": "variable", "doc": "<p>The convolutional layer for downsampling identity in the shortcut connection if the dimension of identity from input doesn't match the output's. This case only happens when the number of input channels doesn't equal to the number of preceding output channels or a layer with stride &gt; 1 exists.</p>\n", "annotation": ": torch.nn.modules.module.Module"}, {"fullname": "clarena.backbones.resnet.ResNetBlockLarge.forward", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockLarge.forward", "kind": "function", "doc": "<p>The forward pass for data.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): the input feature maps.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>output_feature</strong> (<code>Tensor</code>): the output feature maps.</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): the hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name, value (<code>Tensor</code>) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.resnet.ResNetBase", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBase", "kind": "class", "doc": "<p>The base class of <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">residual network (ResNet)</a>.</p>\n\n<p>ResNet is a convolutional network architecture, which has 1st convolutional parameter layer and a maxpooling layer, connecting to 4 convolutional layers which contains multiple convolutional parameter layer. Each layer of the 4 are constructed from basic building blocks which are either small (<code>ResNetBlockSmall</code>) or large (<code>ResNetBlockLarge</code>). Each building block contains several convolutional parameter layers. The building blocks are connected by a skip connection which is a direct connection from the input of the block to the output of the block, and this is why it's called residual (find \"shortcut connections\" in the paper for more details). After the 5th convolutional layer, there are average pooling layer and a fully connected layer which connects to the CL output heads.</p>\n", "bases": "clarena.backbones.base.Backbone"}, {"fullname": "clarena.backbones.resnet.ResNetBase.__init__", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBase.__init__", "kind": "function", "doc": "<p>Construct and initialize the ResNet backbone network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input. Image data are kept channels when going in ResNet. Note that convolutional networks require number of input channels instead of dimension.</li>\n<li><strong>building_block_type</strong> (<code>ResNetBlockSmall</code> | <code>ResNetBlockLarge</code>): the type of building block used in the ResNet.</li>\n<li><strong>building_block_nums</strong> (<code>tuple[int, int, int, int]</code>): the number of building blocks in the 2-5 convolutional layer correspondingly.</li>\n<li><strong>building_block_preceding_output_channels</strong> (<code>tuple[int, int, int, int]</code>): the number of channels of preceding output of each building block in the 2-5 convolutional layer correspondingly.</li>\n<li><strong>building_block_input_channels</strong> (<code>tuple[int, int, int, int]</code>): the number of channels of input of each building block in the 2-5 convolutional layer correspondingly.</li>\n<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>batch_normalization</strong> (<code>bool</code>): whether to use batch normalization after the weight convolutional layers. Default <code>True</code>, same as what the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a> does.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>, because batch normalization are doing the similar thing with bias.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">building_block_type</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">resnet</span><span class=\"o\">.</span><span class=\"n\">ResNetBlockSmall</span> <span class=\"o\">|</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">resnet</span><span class=\"o\">.</span><span class=\"n\">ResNetBlockLarge</span>,</span><span class=\"param\">\t<span class=\"n\">building_block_nums</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">building_block_preceding_output_channels</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">building_block_input_channels</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">batch_normalization</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.ResNetBase.batch_normalization", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBase.batch_normalization", "kind": "variable", "doc": "<p>Store whether to use batch normalization after the fully-connected layers.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.backbones.resnet.ResNetBase.activation", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBase.activation", "kind": "variable", "doc": "<p>Store whether to use activation function after the fully-connected layers.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.backbones.resnet.ResNetBase.conv1", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBase.conv1", "kind": "variable", "doc": "<p>The 1st weight convolutional layer of the entire ResNet. It  is always with fixed kernel size 7x7, stride 2, and padding 3.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBase.maxpool", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBase.maxpool", "kind": "variable", "doc": "<p>The max pooling layer which is laid in between 1st and 2nd convolutional layers with kernel size 3x3, stride 2.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBase.conv2x", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBase.conv2x", "kind": "variable", "doc": "<p>The 2nd convolutional layer of the ResNet, which contains multiple blocks.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBase.conv3x", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBase.conv3x", "kind": "variable", "doc": "<p>The 3rd convolutional layer of the ResNet, which contains multiple blocks.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBase.conv4x", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBase.conv4x", "kind": "variable", "doc": "<p>The 4th convolutional layer of the ResNet, which contains multiple blocks.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBase.conv5x", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBase.conv5x", "kind": "variable", "doc": "<p>The 5th convolutional layer of the ResNet, which contains multiple blocks.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBase.avepool", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBase.avepool", "kind": "variable", "doc": "<p>The average pooling layer which is laid after the convolutional layers and before feature maps are flattened.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBase.forward", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBase.forward", "kind": "function", "doc": "<p>The forward pass for data. It is the same for all tasks.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): the input tensor from data.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>output_feature</strong> (<code>Tensor</code>): the output feature tensor to be passed into heads. This is the main target of backpropagation.</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): the hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name, value (<code>Tensor</code>) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.resnet.ResNet18", "modulename": "clarena.backbones.resnet", "qualname": "ResNet18", "kind": "class", "doc": "<p>ResNet-18 backbone network.</p>\n\n<p>This is a smaller architecture proposed in the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a>. It consists of 18 weight convolutional layers in total. See Table 1 in the paper for details.</p>\n", "bases": "ResNetBase"}, {"fullname": "clarena.backbones.resnet.ResNet18.__init__", "modulename": "clarena.backbones.resnet", "qualname": "ResNet18.__init__", "kind": "function", "doc": "<p>Construct and initialize the ResNet-18 backbone network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</li>\n<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>batch_normalization</strong> (<code>bool</code>): whether to use batch normalization after the weight convolutional layers. Default <code>True</code>, same as what the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a> does.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>, because batch normalization are doing the similar thing with bias.</li>\n<li><strong>pretrained_weights</strong> (<code>str</code>): the name of pretrained weights to be loaded. See <a href=\"https://pytorch.org/vision/main/models.html\">TorchVision docs</a>. If <code>None</code>, no pretrained weights are loaded. Default <code>None</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">batch_normalization</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">pretrained_weights</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.ResNet34", "modulename": "clarena.backbones.resnet", "qualname": "ResNet34", "kind": "class", "doc": "<p>ResNet-34 backbone network.</p>\n\n<p>This is a smaller architecture proposed in the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a>. It consists of 34 weight convolutional layers in total. See Table 1 in the paper for details.</p>\n", "bases": "ResNetBase"}, {"fullname": "clarena.backbones.resnet.ResNet34.__init__", "modulename": "clarena.backbones.resnet", "qualname": "ResNet34.__init__", "kind": "function", "doc": "<p>Construct and initialize the ResNet-34 backbone network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</li>\n<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>batch_normalization</strong> (<code>bool</code>): whether to use batch normalization after the weight convolutional layers. Default <code>True</code>, same as what the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a> does.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>, because batch normalization are doing the similar thing with bias.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">batch_normalization</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.ResNet50", "modulename": "clarena.backbones.resnet", "qualname": "ResNet50", "kind": "class", "doc": "<p>ResNet-50 backbone network.</p>\n\n<p>This is a larger architecture proposed in the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a>. It consists of 50 weight convolutional layers in total. See Table 1 in the paper for details.</p>\n", "bases": "ResNetBase"}, {"fullname": "clarena.backbones.resnet.ResNet50.__init__", "modulename": "clarena.backbones.resnet", "qualname": "ResNet50.__init__", "kind": "function", "doc": "<p>Construct and initialize the ResNet-50 backbone network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</li>\n<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>batch_normalization</strong> (<code>bool</code>): whether to use batch normalization after the weight convolutional layers. Default <code>True</code>, same as what the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a> does.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>, because batch normalization are doing the similar thing with bias.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">batch_normalization</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.ResNet101", "modulename": "clarena.backbones.resnet", "qualname": "ResNet101", "kind": "class", "doc": "<p>ResNet-101 backbone network.</p>\n\n<p>This is a larger architecture proposed in the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a>. It consists of 101 weight convolutional layers in total. See Table 1 in the paper for details.</p>\n", "bases": "ResNetBase"}, {"fullname": "clarena.backbones.resnet.ResNet101.__init__", "modulename": "clarena.backbones.resnet", "qualname": "ResNet101.__init__", "kind": "function", "doc": "<p>Construct and initialize the ResNet-101 backbone network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</li>\n<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>batch_normalization</strong> (<code>bool</code>): whether to use batch normalization after the weight convolutional layers. Default <code>True</code>, same as what the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a> does.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>, because batch normalization are doing the similar thing with bias.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">batch_normalization</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.ResNet152", "modulename": "clarena.backbones.resnet", "qualname": "ResNet152", "kind": "class", "doc": "<p>ResNet-152 backbone network.</p>\n\n<p>This is the largest architecture proposed in the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a>. It consists of 152 weight convolutional layers in total. See Table 1 in the paper for details.</p>\n", "bases": "ResNetBase"}, {"fullname": "clarena.backbones.resnet.ResNet152.__init__", "modulename": "clarena.backbones.resnet", "qualname": "ResNet152.__init__", "kind": "function", "doc": "<p>Construct and initialize the ResNet-152 backbone network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</li>\n<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>batch_normalization</strong> (<code>bool</code>): whether to use batch normalization after the weight convolutional layers. Default <code>True</code>, same as what the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a> does.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>, because batch normalization are doing the similar thing with bias.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">batch_normalization</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.HATMaskResNetBlockSmall", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNetBlockSmall", "kind": "class", "doc": "<p>The smaller building block for HAT masked ResNet-18/34.</p>\n\n<p>It consists of 2 weight convolutional layers, each followed by an activation function. See Table 1 or Figure 5 (left) in the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a>.</p>\n\n<p>Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the unit-wise task embedding and gate function.</p>\n", "bases": "clarena.backbones.base.HATMaskBackbone, ResNetBlockSmall"}, {"fullname": "clarena.backbones.resnet.HATMaskResNetBlockSmall.__init__", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNetBlockSmall.__init__", "kind": "function", "doc": "<p>Construct and initialize the smaller building block with task embedding.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outer_layer_name</strong> (<code>str</code>): pass the name of the multi-building-block layer that contains this building block to construct the full name of each weighted convolutional layer.</li>\n<li><strong>block_idx</strong> (<code>int</code>): the index of the building blocks in the multi-building-block layer to construct the full name of each weighted convolutional layer.</li>\n<li><strong>preceding_output_channels</strong> (<code>int</code>): the number of channels of preceding output of this particular building block.</li>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block.</li>\n<li><strong>overall_stride</strong> (<code>int</code>): the overall stride of this building block. This stride is performed at 2nd (last) convolutional layer where the 1st convolutional layer remain stride of 1.</li>\n<li><strong>gate</strong> (<code>str</code>): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:\n<ul>\n<li><code>sigmoid</code>: the sigmoid function.</li>\n</ul></li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>.</li>\n<li><strong>batch_normalization</strong> (<code>str</code> | <code>None</code>): the way to use batch normalization after the fully-connected layers, should be one of the following:\n<ul>\n<li><code>None</code>: no batch normalization layers.</li>\n<li><code>shared</code>: use a single batch normalization layer for all tasks. Note that this can cause catastrophic forgetting.</li>\n<li><code>independent</code>: use independent batch normalization layers for each task.</li>\n</ul></li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>, because batch normalization are doing the similar thing with bias.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">outer_layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">block_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">preceding_output_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">overall_stride</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">gate</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">batch_normalization</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.HATMaskResNetBlockSmall.setup_task_id", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNetBlockSmall.setup_task_id", "kind": "function", "doc": "<p>Setup about task <code>task_id</code>. This must be done before <code>forward()</code> method is called.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.resnet.HATMaskResNetBlockSmall.forward", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNetBlockSmall.forward", "kind": "function", "doc": "<p>The forward pass for data from task <code>task_id</code>. Task-specific mask for <code>task_id</code> are applied to the units which are channels in each weighted convolutional layer.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): The input tensor from data.</li>\n<li><strong>stage</strong> (<code>str</code>): the stage of the forward pass, should be one of the following:\n<ol>\n<li>'train': training stage.</li>\n<li>'validation': validation stage.</li>\n<li>'test': testing stage.</li>\n</ol></li>\n<li><strong>s_max</strong> (<code>float</code> | <code>None</code>): the maximum scaling factor in the gate function. Doesn't apply to testing stage. See chapter 2.4 \"Hard Attention Training\" in <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</li>\n<li><strong>batch_idx</strong> (<code>int</code> | <code>None</code>): the current batch index. Applies only to training stage. For other stages, it is default <code>None</code>.</li>\n<li><strong>num_batches</strong> (<code>int</code> | <code>None</code>): the total number of batches. Applies only to training stage. For other stages, it is default <code>None</code>.</li>\n<li><strong>test_task_id</strong> (<code>dict[str, Tensor]</code> | <code>None</code>): the test task ID. Applies only to testing stage. For other stages, it is default <code>None</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>output_feature</strong> (<code>Tensor</code>): the output feature maps.</li>\n<li><strong>mask</strong> (<code>dict[str, Tensor]</code>): the mask for the current task. Key (<code>str</code>) is layer name, value (<code>Tensor</code>) is the mask tensor. The mask tensor has size (number of units, ).</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): the hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name, value (<code>Tensor</code>) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes. Although HAT algorithm does not need this, it is still provided for API consistence for other HAT-based algorithms inherited this <code>forward()</code> method of <code>HAT</code> class.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">s_max</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">test_task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">],</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.resnet.HATMaskResNetBlockLarge", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNetBlockLarge", "kind": "class", "doc": "<p>The larger building block for ResNet-50/101/152. It is referred to \"bottleneck\" building block in the ResNet paper.</p>\n\n<p>It consists of 3 weight convolutional layers, each followed by an activation function. See Table 1 or Figure 5 (right) in the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a>.</p>\n\n<p>Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the unit-wise task embedding and gate function.</p>\n", "bases": "clarena.backbones.base.HATMaskBackbone, ResNetBlockLarge"}, {"fullname": "clarena.backbones.resnet.HATMaskResNetBlockLarge.__init__", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNetBlockLarge.__init__", "kind": "function", "doc": "<p>Construct and initialize the larger building block with task embedding.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outer_layer_name</strong> (<code>str</code>): pass the name of the multi-building-block layer that contains this building block to construct the full name of each weighted convolutional layer.</li>\n<li><strong>block_idx</strong> (<code>int</code>): the index of the building blocks in the multi-building-block layer to construct the full name of each weighted convolutional layer.</li>\n<li><strong>preceding_output_channels</strong> (<code>int</code>): the number of channels of preceding output of this particular building block.</li>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block.</li>\n<li><strong>overall_stride</strong> (<code>int</code>): the overall stride of this building block. This stride is performed at 2nd (middle) convolutional layer where 1st and 3rd convolutional layers remain stride of 1.</li>\n<li><strong>gate</strong> (<code>str</code>): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:\n<ul>\n<li><code>sigmoid</code>: the sigmoid function.</li>\n</ul></li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">outer_layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">block_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">preceding_output_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">overall_stride</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">gate</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.HATMaskResNetBlockLarge.forward", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNetBlockLarge.forward", "kind": "function", "doc": "<p>The forward pass for data from task <code>task_id</code>. Task-specific mask for <code>task_id</code> are applied to the units which are channels in each weighted convolutional layer.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): The input tensor from data.</li>\n<li><strong>stage</strong> (<code>str</code>): the stage of the forward pass, should be one of the following:\n<ol>\n<li>'train': training stage.</li>\n<li>'validation': validation stage.</li>\n<li>'test': testing stage.</li>\n</ol></li>\n<li><strong>s_max</strong> (<code>float</code> | <code>None</code>): the maximum scaling factor in the gate function. Doesn't apply to testing stage. See chapter 2.4 \"Hard Attention Training\" in <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</li>\n<li><strong>batch_idx</strong> (<code>int</code> | <code>None</code>): the current batch index. Applies only to training stage. For other stages, it is default <code>None</code>.</li>\n<li><strong>num_batches</strong> (<code>int</code> | <code>None</code>): the total number of batches. Applies only to training stage. For other stages, it is default <code>None</code>.</li>\n<li><strong>test_task_id</strong> (<code>dict[str, Tensor]</code> | <code>None</code>): the test task ID. Applies only to testing stage. For other stages, it is default <code>None</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>output_feature</strong> (<code>Tensor</code>): the output feature maps.</li>\n<li><strong>mask</strong> (<code>dict[str, Tensor]</code>): the mask for the current task. Key (<code>str</code>) is layer name, value (<code>Tensor</code>) is the mask tensor. The mask tensor has size (number of units, ).</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): the hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name, value (<code>Tensor</code>) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes. Although HAT algorithm does not need this, it is still provided for API consistence for other HAT-based algorithms inherited this <code>forward()</code> method of <code>HAT</code> class.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">s_max</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">test_task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">],</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.resnet.HATMaskResNetBase", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNetBase", "kind": "class", "doc": "<p>The base class of HAT masked <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">residual network (ResNet)</a>.</p>\n\n<p><a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task, 2018)</a> is an architecture-based continual learning approach that uses learnable hard attention masks to select the task-specific parameters.</p>\n\n<p>ResNet is a convolutional network architecture, which has 1st convolutional parameter layer and a maxpooling layer, connecting to 4 convolutional layers which contains multiple convolutional parameter layer. Each layer of the 4 are constructed from basic building blocks which are either small (<code>ResNetBlockSmall</code>) or large (<code>ResNetBlockLarge</code>). Each building block contains several convolutional parameter layers. The building blocks are connected by a skip connection which is a direct connection from the input of the block to the output of the block, and this is why it's called residual (find \"shortcut connections\" in the paper for more details). After the 5th convolutional layer, there are average pooling layer and a fully connected layer which connects to the CL output heads.</p>\n\n<p>Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the unit-wise task embedding and gate function.</p>\n", "bases": "clarena.backbones.base.HATMaskBackbone, ResNetBase"}, {"fullname": "clarena.backbones.resnet.HATMaskResNetBase.__init__", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNetBase.__init__", "kind": "function", "doc": "<p>Construct and initialize the HAT masked ResNet backbone network with task embedding. Note that batch normalization is incompatible with HAT mechanism.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input. Image data are kept channels when going in ResNet. Note that convolutional networks require number of input channels instead of dimension.</li>\n<li><strong>building_block_type</strong> (<code>HATMaskResNetBlockSmall</code> | <code>HATMaskResNetBlockLarge</code>): the type of building block used in the ResNet.</li>\n<li><strong>building_block_nums</strong> (<code>tuple[int, int, int, int]</code>): the number of building blocks in the 2-5 convolutional layer correspondingly.</li>\n<li><strong>building_block_preceding_output_channels</strong> (<code>tuple[int, int, int, int]</code>): the number of channels of preceding output of each building block in the 2-5 convolutional layer correspondingly.</li>\n<li><strong>building_block_input_channels</strong> (<code>tuple[int, int, int, int]</code>): the number of channels of input of each building block in the 2-5 convolutional layer correspondingly.</li>\n<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>\n<li><strong>gate</strong> (<code>str</code>): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:\n<ul>\n<li><code>sigmoid</code>: the sigmoid function.</li>\n</ul></li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>, because batch normalization are doing the similar thing with bias.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">building_block_type</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">resnet</span><span class=\"o\">.</span><span class=\"n\">HATMaskResNetBlockSmall</span> <span class=\"o\">|</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">resnet</span><span class=\"o\">.</span><span class=\"n\">HATMaskResNetBlockLarge</span>,</span><span class=\"param\">\t<span class=\"n\">building_block_nums</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">building_block_preceding_output_channels</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">building_block_input_channels</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">gate</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.HATMaskResNetBase.update_multiple_blocks_task_embedding", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNetBase.update_multiple_blocks_task_embedding", "kind": "function", "doc": "<p>Collect the task embeddings in the multiple building blocks (2-5 convolutional layers) and sync to the weighted layer names list in the outer network.</p>\n\n<p>This should only be called explicitly after the <code>__init__()</code> method, just because task embedding as <code>nn.Module</code> instance was wiped out at the beginning of it.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.resnet.HATMaskResNetBase.forward", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNetBase.forward", "kind": "function", "doc": "<p>The forward pass for data from task <code>task_id</code>. Task-specific mask for <code>task_id</code> are applied to the units which are channels in each weighted convolutional layer.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): the input tensor from data.</li>\n<li><strong>stage</strong> (<code>str</code>): the stage of the forward pass, should be one of the following:\n<ol>\n<li>'train': training stage.</li>\n<li>'validation': validation stage.</li>\n<li>'test': testing stage.</li>\n</ol></li>\n<li><strong>s_max</strong> (<code>float</code> | <code>None</code>): the maximum scaling factor in the gate function. Doesn't apply to testing stage. See chapter 2.4 \"Hard Attention Training\" in <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</li>\n<li><strong>batch_idx</strong> (<code>int</code> | <code>None</code>): the current batch index. Applies only to training stage. For other stages, it is default <code>None</code>.</li>\n<li><strong>num_batches</strong> (<code>int</code> | <code>None</code>): the total number of batches. Applies only to training stage. For other stages, it is default <code>None</code>.</li>\n<li><strong>test_task_id</strong> (<code>dict[str, Tensor]</code> | <code>None</code>): the test task ID. Applies only to testing stage. For other stages, it is default <code>None</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>output_feature</strong> (<code>Tensor</code>): the output feature tensor to be passed to the heads.</li>\n<li><strong>mask</strong> (<code>dict[str, Tensor]</code>): the mask for the current task. Key (<code>str</code>) is layer name, value (<code>Tensor</code>) is the mask tensor. The mask tensor has size (number of units, ).</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): the hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name, value (<code>Tensor</code>) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes. Although HAT algorithm does not need this, it is still provided for API consistence for other HAT-based algorithms inherited this <code>forward()</code> method of <code>HAT</code> class.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">s_max</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">test_task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">],</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.resnet.HATMaskResNet18", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNet18", "kind": "class", "doc": "<p>HAT masked ResNet-18 backbone network.</p>\n\n<p><a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task, 2018)</a> is an architecture-based continual learning approach that uses learnable hard attention masks to select the task-specific parameters.</p>\n\n<p>ResNet-18 is a smaller architecture proposed in the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a>. It consists of 18 weight convolutional layers in total. See Table 1 in the paper for details.</p>\n\n<p>Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the unit-wise task embedding and gate function.</p>\n", "bases": "HATMaskResNetBase"}, {"fullname": "clarena.backbones.resnet.HATMaskResNet18.__init__", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNet18.__init__", "kind": "function", "doc": "<p>Construct and initialize the ResNet-18 backbone network with task embedding. Note that batch normalization is incompatible with HAT mechanism.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</li>\n<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>\n<li><strong>gate</strong> (<code>str</code>): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:\n<ul>\n<li><code>sigmoid</code>: the sigmoid function.</li>\n</ul></li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>.</li>\n<li><strong>pretrained_weights</strong> (<code>str</code>): the name of pretrained weights to be loaded. See <a href=\"https://pytorch.org/vision/main/models.html\">TorchVision docs</a>. If <code>None</code>, no pretrained weights are loaded. Default <code>None</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">gate</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">pretrained_weights</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.HATMaskResNet34", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNet34", "kind": "class", "doc": "<p>HAT masked ResNet-34 backbone network.</p>\n\n<p><a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task, 2018)</a> is an architecture-based continual learning approach that uses learnable hard attention masks to select the task-specific parameters.</p>\n\n<p>ResNet-34 is a smaller architecture proposed in the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a>. It consists of 34 weight convolutional layers in total. See Table 1 in the paper for details.</p>\n\n<p>Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the unit-wise task embedding and gate function.</p>\n", "bases": "HATMaskResNetBase"}, {"fullname": "clarena.backbones.resnet.HATMaskResNet34.__init__", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNet34.__init__", "kind": "function", "doc": "<p>Construct and initialize the ResNet-34 backbone network with task embedding. Note that batch normalization is incompatible with HAT mechanism.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</li>\n<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>\n<li><strong>gate</strong> (<code>str</code>): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:\n<ul>\n<li><code>sigmoid</code>: the sigmoid function.</li>\n</ul></li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">gate</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.HATMaskResNet50", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNet50", "kind": "class", "doc": "<p>HAT masked ResNet-50 backbone network.</p>\n\n<p><a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task, 2018)</a> is an architecture-based continual learning approach that uses learnable hard attention masks to select the task-specific parameters.</p>\n\n<p>ResNet-50 is a larger architecture proposed in the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a>. It consists of 50 weight convolutional layers in total. See Table 1 in the paper for details.</p>\n\n<p>Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the unit-wise task embedding and gate function.</p>\n", "bases": "HATMaskResNetBase"}, {"fullname": "clarena.backbones.resnet.HATMaskResNet50.__init__", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNet50.__init__", "kind": "function", "doc": "<p>Construct and initialize the ResNet-50 backbone network with task embedding. Note that batch normalization is incompatible with HAT mechanism.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</li>\n<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>\n<li><strong>gate</strong> (<code>str</code>): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:\n<ul>\n<li><code>sigmoid</code>: the sigmoid function.</li>\n</ul></li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">gate</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.HATMaskResNet101", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNet101", "kind": "class", "doc": "<p>HAT masked ResNet-101 backbone network.</p>\n\n<p><a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task, 2018)</a> is an architecture-based continual learning approach that uses learnable hard attention masks to select the task-specific parameters.</p>\n\n<p>ResNet-101 is a larger architecture proposed in the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a>. It consists of 101 weight convolutional layers in total. See Table 1 in the paper for details.</p>\n\n<p>Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the unit-wise task embedding and gate function.</p>\n", "bases": "HATMaskResNetBase"}, {"fullname": "clarena.backbones.resnet.HATMaskResNet101.__init__", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNet101.__init__", "kind": "function", "doc": "<p>Construct and initialize the ResNet-101 backbone network with task embedding. Note that batch normalization is incompatible with HAT mechanism.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</li>\n<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>\n<li><strong>gate</strong> (<code>str</code>): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:\n<ul>\n<li><code>sigmoid</code>: the sigmoid function.</li>\n</ul></li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">gate</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.HATMaskResNet152", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNet152", "kind": "class", "doc": "<p>HAT masked ResNet-152 backbone network.</p>\n\n<p><a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task, 2018)</a> is an architecture-based continual learning approach that uses learnable hard attention masks to select the task-specific parameters.</p>\n\n<p>ResNet-152 is the largest architecture proposed in the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a>. It consists of 152 weight convolutional layers in total. See Table 1 in the paper for details.</p>\n\n<p>Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the unit-wise task embedding and gate function.</p>\n", "bases": "HATMaskResNetBase"}, {"fullname": "clarena.backbones.resnet.HATMaskResNet152.__init__", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNet152.__init__", "kind": "function", "doc": "<p>Construct and initialize the ResNet-152 backbone network with task embedding. Note that batch normalization is incompatible with HAT mechanism.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</li>\n<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>\n<li><strong>gate</strong> (<code>str</code>): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:\n<ul>\n<li><code>sigmoid</code>: the sigmoid function.</li>\n</ul></li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">gate</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.callbacks", "modulename": "clarena.callbacks", "kind": "module", "doc": "<h1 id=\"callbacks\">Callbacks</h1>\n\n<p>This submodule provides <strong>callbacks</strong> (other than metric callbacks) that can be used in CLArena.</p>\n\n<p>The callbacks inherit from <code>lightning.Callback</code>.</p>\n\n<p>Please note that this is an API documantation. Please refer to the main documentation pages for more information about how to configure and implement callbacks:</p>\n\n<ul>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/continual-learning/configure-main-experiment/callbacks\"><strong>Configure Callbacks (CL)</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/continual-unlearning/configure-main-experiment/callbacks\"><strong>Configure Callbacks (CUL)</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/multi-task-learning/configure-main-experiment/callbacks\"><strong>Configure Callbacks (MTL)</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/single-task-learning/configure-main-experiment/callbacks\"><strong>Configure Callbacks (STL)</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/implement-your-cl-modules/callback\"><strong>Implement Your Callbacks</strong></a></li>\n</ul>\n"}, {"fullname": "clarena.callbacks.cl_rich_progress_bar", "modulename": "clarena.callbacks.cl_rich_progress_bar", "kind": "module", "doc": "<p>The submodule in <code>callbacks</code> for <code>CLRichProgressBar</code>.</p>\n"}, {"fullname": "clarena.callbacks.cl_rich_progress_bar.CLRichProgressBar", "modulename": "clarena.callbacks.cl_rich_progress_bar", "qualname": "CLRichProgressBar", "kind": "class", "doc": "<p>Customised <code>RichProgressBar</code> for continual learning.</p>\n", "bases": "lightning.pytorch.callbacks.progress.rich_progress.RichProgressBar"}, {"fullname": "clarena.callbacks.cl_rich_progress_bar.CLRichProgressBar.get_metrics", "modulename": "clarena.callbacks.cl_rich_progress_bar", "qualname": "CLRichProgressBar.get_metrics", "kind": "function", "doc": "<p>Filter out the version number from the metrics displayed in the progress bar.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"o\">*</span><span class=\"n\">args</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.pylogger", "modulename": "clarena.callbacks.pylogger", "kind": "module", "doc": "<p>The submodule in <code>callbacks</code> for Pylogger callbacks.</p>\n"}, {"fullname": "clarena.callbacks.pylogger.CLPylogger", "modulename": "clarena.callbacks.pylogger", "qualname": "CLPylogger", "kind": "class", "doc": "<p>Provides additional logging messages for during continual learning progress.</p>\n\n<p>Put logging messages here if you don't want to mess up the <code>CLAlgorithm</code> (<code>LightningModule</code>) with a huge amount of logging codes.</p>\n", "bases": "lightning.pytorch.callbacks.callback.Callback"}, {"fullname": "clarena.callbacks.pylogger.CLPylogger.on_fit_start", "modulename": "clarena.callbacks.pylogger", "qualname": "CLPylogger.on_fit_start", "kind": "function", "doc": "<p>Log messages for the start of training task.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.pylogger.CLPylogger.on_train_end", "modulename": "clarena.callbacks.pylogger", "qualname": "CLPylogger.on_train_end", "kind": "function", "doc": "<p>Log messages for the end of training task.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.pylogger.CLPylogger.on_test_start", "modulename": "clarena.callbacks.pylogger", "qualname": "CLPylogger.on_test_start", "kind": "function", "doc": "<p>Log messages for the start of testing task.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.pylogger.CLPylogger.on_test_end", "modulename": "clarena.callbacks.pylogger", "qualname": "CLPylogger.on_test_end", "kind": "function", "doc": "<p>Log messages for the end of testing task.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.pylogger.CULPylogger", "modulename": "clarena.callbacks.pylogger", "qualname": "CULPylogger", "kind": "class", "doc": "<p>Provides additional logging messages for during continual learning progress.</p>\n\n<p>Put logging messages here if you don't want to mess up the <code>CLAlgorithm</code> (<code>LightningModule</code>) with a huge amount of logging codes.</p>\n", "bases": "lightning.pytorch.callbacks.callback.Callback"}, {"fullname": "clarena.callbacks.pylogger.CULPylogger.on_fit_start", "modulename": "clarena.callbacks.pylogger", "qualname": "CULPylogger.on_fit_start", "kind": "function", "doc": "<p>Log messages for the start of training task.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.pylogger.CULPylogger.on_train_end", "modulename": "clarena.callbacks.pylogger", "qualname": "CULPylogger.on_train_end", "kind": "function", "doc": "<p>Log messages for the end of training task.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.pylogger.CULPylogger.on_test_start", "modulename": "clarena.callbacks.pylogger", "qualname": "CULPylogger.on_test_start", "kind": "function", "doc": "<p>Log messages for the start of testing task.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.pylogger.CULPylogger.on_test_end", "modulename": "clarena.callbacks.pylogger", "qualname": "CULPylogger.on_test_end", "kind": "function", "doc": "<p>Log messages for the end of testing task.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.pylogger.MTLPylogger", "modulename": "clarena.callbacks.pylogger", "qualname": "MTLPylogger", "kind": "class", "doc": "<p>Pylogger Callback provides additional logging for during multi-task learning progress.</p>\n\n<p>Put logging messages here if you don't want to mess up the <code>MTLAlgorithm</code> (<code>LightningModule</code>) with a huge amount of logging codes.</p>\n", "bases": "lightning.pytorch.callbacks.callback.Callback"}, {"fullname": "clarena.callbacks.pylogger.MTLPylogger.on_fit_start", "modulename": "clarena.callbacks.pylogger", "qualname": "MTLPylogger.on_fit_start", "kind": "function", "doc": "<p>Log messages for the start of training.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">mtl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">MTLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.pylogger.MTLPylogger.on_train_end", "modulename": "clarena.callbacks.pylogger", "qualname": "MTLPylogger.on_train_end", "kind": "function", "doc": "<p>Log messages for the end of training.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">mtl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">MTLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.pylogger.MTLPylogger.on_test_start", "modulename": "clarena.callbacks.pylogger", "qualname": "MTLPylogger.on_test_start", "kind": "function", "doc": "<p>Log messages for the start of testing.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">mtl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">MTLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.pylogger.MTLPylogger.on_test_end", "modulename": "clarena.callbacks.pylogger", "qualname": "MTLPylogger.on_test_end", "kind": "function", "doc": "<p>Log messages for the end of testing.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">mtl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">MTLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.pylogger.STLPylogger", "modulename": "clarena.callbacks.pylogger", "qualname": "STLPylogger", "kind": "class", "doc": "<p>Pylogger Callback provides additional logging for during single-task learning progress.</p>\n\n<p>Put logging messages here if you don't want to mess up the <code>STLAlgorithm</code> (<code>LightningModule</code>) with a huge amount of logging codes.</p>\n", "bases": "lightning.pytorch.callbacks.callback.Callback"}, {"fullname": "clarena.callbacks.pylogger.STLPylogger.on_fit_start", "modulename": "clarena.callbacks.pylogger", "qualname": "STLPylogger.on_fit_start", "kind": "function", "doc": "<p>Log messages for the start of training.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">stl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">STLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.pylogger.STLPylogger.on_train_end", "modulename": "clarena.callbacks.pylogger", "qualname": "STLPylogger.on_train_end", "kind": "function", "doc": "<p>Log messages for the end of training.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">stl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">STLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.pylogger.STLPylogger.on_test_start", "modulename": "clarena.callbacks.pylogger", "qualname": "STLPylogger.on_test_start", "kind": "function", "doc": "<p>Log messages for the start of testing.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">stl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">STLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.pylogger.STLPylogger.on_test_end", "modulename": "clarena.callbacks.pylogger", "qualname": "STLPylogger.on_test_end", "kind": "function", "doc": "<p>Log messages for the end of testing.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">stl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">STLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.save_first_batch_images", "modulename": "clarena.callbacks.save_first_batch_images", "kind": "module", "doc": "<p>The submodule in <code>callbacks</code> for callback of saving first batch images.</p>\n"}, {"fullname": "clarena.callbacks.save_first_batch_images.SaveFirstBatchImages", "modulename": "clarena.callbacks.save_first_batch_images", "qualname": "SaveFirstBatchImages", "kind": "class", "doc": "<p>Saves images and labels of the first batch of training data into files. In continual learning / unlearning, applies to all tasks.</p>\n", "bases": "lightning.pytorch.callbacks.callback.Callback"}, {"fullname": "clarena.callbacks.save_first_batch_images.SaveFirstBatchImages.__init__", "modulename": "clarena.callbacks.save_first_batch_images", "qualname": "SaveFirstBatchImages.__init__", "kind": "function", "doc": "<p>Initialize the Save First Batch Images Callback.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>save_dir</strong> (<code>str</code>): the directory to save images and labels as files. Better inside the output directory.</li>\n<li><strong>img_prefix</strong> (<code>str</code>): the prefix for image files.</li>\n<li><strong>labels_filename</strong> (<code>str</code>): the filename for the labels file as texts.</li>\n<li><strong>task_ids_filename</strong> (<code>str</code> | <code>None</code>): the filename for the task IDs file as texts. Only used in MTL algorithms. If <code>None</code>, no task IDs file is saved.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">save_dir</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">img_prefix</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;sample&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">labels_filename</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;labels.txt&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">task_ids_filename</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;tasks.txt&#39;</span></span>)</span>"}, {"fullname": "clarena.callbacks.save_first_batch_images.SaveFirstBatchImages.save_dir", "modulename": "clarena.callbacks.save_first_batch_images", "qualname": "SaveFirstBatchImages.save_dir", "kind": "variable", "doc": "<p>Store the directory to save images and labels as files.</p>\n", "annotation": ": str"}, {"fullname": "clarena.callbacks.save_first_batch_images.SaveFirstBatchImages.img_prefix", "modulename": "clarena.callbacks.save_first_batch_images", "qualname": "SaveFirstBatchImages.img_prefix", "kind": "variable", "doc": "<p>Store the prefix for image files.</p>\n", "annotation": ": str"}, {"fullname": "clarena.callbacks.save_first_batch_images.SaveFirstBatchImages.labels_filename", "modulename": "clarena.callbacks.save_first_batch_images", "qualname": "SaveFirstBatchImages.labels_filename", "kind": "variable", "doc": "<p>Store the filename for the labels file.</p>\n", "annotation": ": str"}, {"fullname": "clarena.callbacks.save_first_batch_images.SaveFirstBatchImages.task_ids_filename", "modulename": "clarena.callbacks.save_first_batch_images", "qualname": "SaveFirstBatchImages.task_ids_filename", "kind": "variable", "doc": "<p>Store the filename for the task IDs file. Only used in MTL algorithms. If <code>None</code>, no task IDs file is saved.</p>\n", "annotation": ": str | None"}, {"fullname": "clarena.callbacks.save_first_batch_images.SaveFirstBatchImages.called", "modulename": "clarena.callbacks.save_first_batch_images", "qualname": "SaveFirstBatchImages.called", "kind": "variable", "doc": "<p>Flag to avoid calling the callback multiple times.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.callbacks.save_first_batch_images.SaveFirstBatchImages.on_train_start", "modulename": "clarena.callbacks.save_first_batch_images", "qualname": "SaveFirstBatchImages.on_train_start", "kind": "function", "doc": "<p>Save images and labels into files in the first batch of training data at the beginning of the training of the task.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span> <span class=\"o\">|</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">mtl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">MTLAlgorithm</span> <span class=\"o\">|</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">stl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">STLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.save_models", "modulename": "clarena.callbacks.save_models", "kind": "module", "doc": "<p>The submodule in <code>callbacks</code> for callback of saving models.</p>\n"}, {"fullname": "clarena.callbacks.save_models.SaveModels", "modulename": "clarena.callbacks.save_models", "qualname": "SaveModels", "kind": "class", "doc": "<p>Saves the model at the end of training. In continual learning / unlearning, applies to all tasks.</p>\n", "bases": "lightning.pytorch.callbacks.callback.Callback"}, {"fullname": "clarena.callbacks.save_models.SaveModels.__init__", "modulename": "clarena.callbacks.save_models", "qualname": "SaveModels.__init__", "kind": "function", "doc": "<p>Initialize the SaveModel callback.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>save_path</strong> (<code>str</code>): the path to save the model.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">save_dir</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span>)</span>"}, {"fullname": "clarena.callbacks.save_models.SaveModels.save_dir", "modulename": "clarena.callbacks.save_models", "qualname": "SaveModels.save_dir", "kind": "variable", "doc": "<p>Store the path to save the model.</p>\n"}, {"fullname": "clarena.callbacks.save_models.SaveModels.on_fit_end", "modulename": "clarena.callbacks.save_models", "qualname": "SaveModels.on_fit_end", "kind": "function", "doc": "<p>Save the model at the end of each training task.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span> <span class=\"o\">|</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">mtl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">MTLAlgorithm</span> <span class=\"o\">|</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">stl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">STLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms", "modulename": "clarena.cl_algorithms", "kind": "module", "doc": "<h1 id=\"continual-learning-algorithms\">Continual Learning Algorithms</h1>\n\n<p>This submodule provides the <strong>continual learning algorithms</strong> in CLArena.</p>\n\n<p>The algorithms are implemented as subclasses of <code>CLAlgorithm</code>.</p>\n\n<p>Please note that this is an API documentation. Please refer to the main documentation pages for more information about and how to configure and implement CL algorithms:</p>\n\n<ul>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/continual-learning/configure-main-experiment/cl-algorithm\"><strong>Configure CL Algorithm (CL Main)</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/custom-implementation/cl-algorithm\"><strong>Implement Custom CL Algorithm</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/posts/continual-learning-beginners-guide#sec-methodology\"><strong>A Beginners' Guide to Continual Learning (Methodology Overview)</strong></a></li>\n</ul>\n"}, {"fullname": "clarena.cl_algorithms.CLAlgorithm", "modulename": "clarena.cl_algorithms", "qualname": "CLAlgorithm", "kind": "class", "doc": "<p>The base class of continual learning algorithms, inherits from <code>LightningModule</code>.</p>\n", "bases": "lightning.pytorch.core.module.LightningModule"}, {"fullname": "clarena.cl_algorithms.CLAlgorithm.__init__", "modulename": "clarena.cl_algorithms", "qualname": "CLAlgorithm.__init__", "kind": "function", "doc": "<p>Initialize the CL algorithm with the network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone</strong> (<code>CLBackbone</code>): backbone network.</li>\n<li><strong>heads</strong> (<code>HeadsTIL</code> | <code>HeadsCIL</code>): output heads.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLBackbone</span>,</span><span class=\"param\">\t<span class=\"n\">heads</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_til</span><span class=\"o\">.</span><span class=\"n\">HeadsTIL</span> <span class=\"o\">|</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_cil</span><span class=\"o\">.</span><span class=\"n\">HeadsCIL</span></span>)</span>"}, {"fullname": "clarena.cl_algorithms.CLAlgorithm.backbone", "modulename": "clarena.cl_algorithms", "qualname": "CLAlgorithm.backbone", "kind": "variable", "doc": "<p>The backbone network.</p>\n", "annotation": ": clarena.backbones.base.CLBackbone"}, {"fullname": "clarena.cl_algorithms.CLAlgorithm.heads", "modulename": "clarena.cl_algorithms", "qualname": "CLAlgorithm.heads", "kind": "variable", "doc": "<p>The output heads.</p>\n", "annotation": ": clarena.heads.heads_til.HeadsTIL | clarena.heads.heads_cil.HeadsCIL"}, {"fullname": "clarena.cl_algorithms.CLAlgorithm.optimizer_t", "modulename": "clarena.cl_algorithms", "qualname": "CLAlgorithm.optimizer_t", "kind": "variable", "doc": "<p>Optimizer (partially initialized) for backpropagation of task <code>self.task_id</code>. Will be equipped with parameters in <code>configure_optimizers()</code>.</p>\n", "annotation": ": torch.optim.optimizer.Optimizer"}, {"fullname": "clarena.cl_algorithms.CLAlgorithm.lr_scheduler_t", "modulename": "clarena.cl_algorithms", "qualname": "CLAlgorithm.lr_scheduler_t", "kind": "variable", "doc": "<p>Learning rate scheduler for the optimizer. If <code>None</code>, no scheduler is used.</p>\n", "annotation": ": torch.optim.lr_scheduler.LRScheduler | None"}, {"fullname": "clarena.cl_algorithms.CLAlgorithm.criterion", "modulename": "clarena.cl_algorithms", "qualname": "CLAlgorithm.criterion", "kind": "variable", "doc": "<p>Loss function between the output logits and the target labels. Default is cross-entropy loss.</p>\n"}, {"fullname": "clarena.cl_algorithms.CLAlgorithm.if_forward_func_return_logits_only", "modulename": "clarena.cl_algorithms", "qualname": "CLAlgorithm.if_forward_func_return_logits_only", "kind": "variable", "doc": "<p>Whether the <code>forward()</code> method returns logits only. If <code>False</code>, it returns a dictionary containing logits and other information. Default is <code>False</code>.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.cl_algorithms.CLAlgorithm.task_id", "modulename": "clarena.cl_algorithms", "qualname": "CLAlgorithm.task_id", "kind": "variable", "doc": "<p>Task ID counter indicating which task is being processed. Self updated during the task loop. Valid from 1 to <code>cl_dataset.num_tasks</code>.</p>\n", "annotation": ": int"}, {"fullname": "clarena.cl_algorithms.CLAlgorithm.processed_task_ids", "modulename": "clarena.cl_algorithms", "qualname": "CLAlgorithm.processed_task_ids", "kind": "variable", "doc": "<p>Task IDs that have been processed in the experiment.</p>\n", "annotation": ": list[int]"}, {"fullname": "clarena.cl_algorithms.CLAlgorithm.sanity_check", "modulename": "clarena.cl_algorithms", "qualname": "CLAlgorithm.sanity_check", "kind": "function", "doc": "<p>Check the sanity of the arguments.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.CLAlgorithm.setup_task_id", "modulename": "clarena.cl_algorithms", "qualname": "CLAlgorithm.setup_task_id", "kind": "function", "doc": "<p>Set up which task the CL experiment is on. This must be done before <code>forward()</code> method is called.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID.</li>\n<li><strong>num_classes_t</strong> (<code>int</code>): the number of classes in the task.</li>\n<li><strong>optimizer</strong> (<code>Optimizer</code>): the optimizer object (partially initialized) for the task <code>self.task_id</code>.</li>\n<li><strong>lr_scheduler</strong> (<code>LRScheduler</code> | <code>None</code>): the learning rate scheduler for the optimizer. If <code>None</code>, no scheduler is used.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">num_classes_t</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">optimizer</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">Optimizer</span>,</span><span class=\"param\">\t<span class=\"n\">lr_scheduler</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">lr_scheduler</span><span class=\"o\">.</span><span class=\"n\">LRScheduler</span> <span class=\"o\">|</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.CLAlgorithm.get_test_task_id_from_dataloader_idx", "modulename": "clarena.cl_algorithms", "qualname": "CLAlgorithm.get_test_task_id_from_dataloader_idx", "kind": "function", "doc": "<p>Get the test task ID from the dataloader index.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>dataloader_idx</strong> (<code>int</code>): the dataloader index.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_task_id</strong> (<code>str</code>): the test task ID.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">dataloader_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">int</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.CLAlgorithm.set_forward_func_return_logits_only", "modulename": "clarena.cl_algorithms", "qualname": "CLAlgorithm.set_forward_func_return_logits_only", "kind": "function", "doc": "<p>Set whether the <code>forward()</code> method returns logits only.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>forward_func_return_logits_only</strong> (<code>bool</code>): whether the <code>forward()</code> method returns logits only. If <code>False</code>, it returns a dictionary containing logits and other information.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">forward_func_return_logits_only</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.CLAlgorithm.preceding_layer", "modulename": "clarena.cl_algorithms", "qualname": "CLAlgorithm.preceding_layer", "kind": "function", "doc": "<p>Get the preceding layer of the given layer. If the given layer is the first layer, return <code>None</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>layer_name</strong> (<code>str</code>): the name of the layer.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>preceding_layer</strong> (<code>nn.Module</code> | <code>None</code>): the preceding layer.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.CLAlgorithm.next_layer", "modulename": "clarena.cl_algorithms", "qualname": "CLAlgorithm.next_layer", "kind": "function", "doc": "<p>Get the next layer of the given layer. If the given layer is the last layer, return <code>None</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>layer_name</strong> (<code>str</code>): the name of the layer.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>preceding_layer</strong> (<code>nn.Module</code> | <code>None</code>): the next layer.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.CLAlgorithm.forward", "modulename": "clarena.cl_algorithms", "qualname": "CLAlgorithm.forward", "kind": "function", "doc": "<p>The forward pass for data from task <code>task_id</code>. Note that it is nothing to do with <code>forward()</code> method in <code>nn.Module</code>. This definition provides a template that many CL algorithm including the vanilla Finetuning algorithm use. It works both for TIL and CIL.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): the input tensor from data.</li>\n<li><strong>stage</strong> (<code>str</code>): the stage of the forward pass, should be one of the following:\n<ol>\n<li>'train': training stage.</li>\n<li>'validation': validation stage.</li>\n<li>'test': testing stage.</li>\n</ol></li>\n<li><strong>task_id</strong> (<code>int</code>): the task ID where the data are from. If stage is 'train' or <code>validation</code>, it is usually from the current task <code>self.task_id</code>. If stage is 'test', it could be from any seen task. In TIL, the task IDs of test data are provided thus this argument can be used. In CIL, they are not provided, so it is just a placeholder for API consistence but never used, and best practices are not to provide this argument and leave it as the default value.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>logits</strong> (<code>Tensor</code>): the output logits tensor.</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): the hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name, value (<code>Tensor</code>) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.CLAlgorithm.configure_optimizers", "modulename": "clarena.cl_algorithms", "qualname": "CLAlgorithm.configure_optimizers", "kind": "function", "doc": "<p>Configure optimizer hooks by Lightning. See <a href=\"https://lightning.ai/docs/pytorch/stable/common/lightning_module.html#configure-optimizers\">Lightning docs</a> for more details.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">Optimizer</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.UnlearnableCLAlgorithm", "modulename": "clarena.cl_algorithms", "qualname": "UnlearnableCLAlgorithm", "kind": "class", "doc": "<p>The base class of unlearnable continual learning algorithms, inherits from <code>CLAlgorithm</code>.</p>\n", "bases": "clarena.cl_algorithms.base.CLAlgorithm"}, {"fullname": "clarena.cl_algorithms.UnlearnableCLAlgorithm.__init__", "modulename": "clarena.cl_algorithms", "qualname": "UnlearnableCLAlgorithm.__init__", "kind": "function", "doc": "<p>Initialize the CL algorithm with the network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone</strong> (<code>CLBackbone</code>): backbone network.</li>\n<li><strong>heads</strong> (<code>HeadsTIL</code> | <code>HeadsCIL</code>): output heads.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLBackbone</span>,</span><span class=\"param\">\t<span class=\"n\">heads</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_til</span><span class=\"o\">.</span><span class=\"n\">HeadsTIL</span> <span class=\"o\">|</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_cil</span><span class=\"o\">.</span><span class=\"n\">HeadsCIL</span></span>)</span>"}, {"fullname": "clarena.cl_algorithms.UnlearnableCLAlgorithm.unlearning_task_ids", "modulename": "clarena.cl_algorithms", "qualname": "UnlearnableCLAlgorithm.unlearning_task_ids", "kind": "variable", "doc": "<p>The list of task IDs to be unlearned after <code>self.task_id</code>.</p>\n", "annotation": ": list[int]"}, {"fullname": "clarena.cl_algorithms.UnlearnableCLAlgorithm.unlearned_task_ids", "modulename": "clarena.cl_algorithms", "qualname": "UnlearnableCLAlgorithm.unlearned_task_ids", "kind": "variable", "doc": "<p>The list of task IDs that have been unlearned in the experiment.</p>\n", "annotation": ": set[int]"}, {"fullname": "clarena.cl_algorithms.UnlearnableCLAlgorithm.sanity_check", "modulename": "clarena.cl_algorithms", "qualname": "UnlearnableCLAlgorithm.sanity_check", "kind": "function", "doc": "<p>Check the sanity of the arguments.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.UnlearnableCLAlgorithm.aggregated_backbone_output", "modulename": "clarena.cl_algorithms", "qualname": "UnlearnableCLAlgorithm.aggregated_backbone_output", "kind": "function", "doc": "<p>Get the aggregated backbone output for the input data. All parts of backbones should be aggregated together.</p>\n\n<p>This output feature is used for measuring unlearning metrics, such as Distribution Distance (DD). An aggregated output involving every part of the backbone is needed to ensure the fairness of the metric.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): the input tensor from data.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>output</strong> (<code>Tensor</code>): the aggregated backbone output tensor.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.adahat", "modulename": "clarena.cl_algorithms.adahat", "kind": "module", "doc": "<p>The submodule in <code>cl_algorithms</code> for <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT (Adaptive Hard Attention to the Task)</a> algorithm.</p>\n"}, {"fullname": "clarena.cl_algorithms.adahat.AdaHAT", "modulename": "clarena.cl_algorithms.adahat", "qualname": "AdaHAT", "kind": "class", "doc": "<p><a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT (Adaptive Hard Attention to the Task)</a> algorithm.</p>\n\n<p>An architecture-based continual learning approach that improves <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task)</a> by introducing adaptive soft gradient clipping based on parameter importance and network sparsity.</p>\n\n<p>We implement AdaHAT as a subclass of HAT, as it shares the same <code>forward()</code>, <code>compensate_task_embedding_gradients()</code>, <code>training_step()</code>, <code>on_train_end()</code>, <code>validation_step()</code>, and <code>test_step()</code> methods as the <code>HAT</code> class.</p>\n", "bases": "clarena.cl_algorithms.hat.HAT"}, {"fullname": "clarena.cl_algorithms.adahat.AdaHAT.__init__", "modulename": "clarena.cl_algorithms.adahat", "qualname": "AdaHAT.__init__", "kind": "function", "doc": "<p>Initialize the AdaHAT algorithm with the network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone</strong> (<code>HATMaskBackbone</code>): must be a backbone network with the HAT mask mechanism.</li>\n<li><strong>heads</strong> (<code>HeadsTIL</code>): output heads. AdaHAT supports only TIL (Task-Incremental Learning).</li>\n<li><strong>adjustment_mode</strong> (<code>str</code>): the strategy of adjustment (i.e., the mode of gradient clipping), must be one of:\n<ol>\n<li>'adahat': set gradients of parameters linking to masked units to a soft adjustment rate in the original AdaHAT approach (allows slight updates on previous-task parameters). See Eqs. (8) and (9) in Sec. 3.1 of the <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>.</li>\n<li>'adahat_no_sum': as above but without parameter-importance (i.e., no summative mask). See Sec. 4.3 (ablation study) in the <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>.</li>\n<li>'adahat_no_reg': as above but without network sparsity (i.e., no mask sparsity regularization term). See Sec. 4.3 (ablation study) in the <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>.</li>\n</ol></li>\n<li><strong>adjustment_intensity</strong> (<code>float</code>): hyperparameter, controls the overall intensity of gradient adjustment (the $\\alpha$ in Eq. (9) of the <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>).</li>\n<li><strong>s_max</strong> (<code>float</code>): hyperparameter, the maximum scaling factor in the gate function. See Sec. 2.4 \"Hard Attention Training\" in the <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</li>\n<li><strong>clamp_threshold</strong> (<code>float</code>): the threshold for task embedding gradient compensation. See Sec. 2.5 \"Embedding Gradient Compensation\" in the <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</li>\n<li><strong>mask_sparsity_reg_factor</strong> (<code>float</code>): hyperparameter, the regularization factor for mask sparsity.</li>\n<li><strong>mask_sparsity_reg_mode</strong> (<code>str</code>): the mode of mask sparsity regularization, must be one of:\n<ol>\n<li>'original' (default): the original mask sparsity regularization in the <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</li>\n<li>'cross': the cross version of mask sparsity regularization.</li>\n</ol></li>\n<li><strong>task_embedding_init_mode</strong> (<code>str</code>): the initialization mode for task embeddings, must be one of:\n<ol>\n<li>'N01' (default): standard normal distribution $N(0, 1)$.</li>\n<li>'U-11': uniform distribution $U(-1, 1)$.</li>\n<li>'U01': uniform distribution $U(0, 1)$.</li>\n<li>'U-10': uniform distribution $U(-1, 0)$.</li>\n<li>'last': inherit the task embedding from the last task.</li>\n</ol></li>\n<li><strong>epsilon</strong> (<code>float</code>): the value added to network sparsity to avoid division by zero (appearing in Eq. (9) of the <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>).</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">HATMaskBackbone</span>,</span><span class=\"param\">\t<span class=\"n\">heads</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_til</span><span class=\"o\">.</span><span class=\"n\">HeadsTIL</span>,</span><span class=\"param\">\t<span class=\"n\">adjustment_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">adjustment_intensity</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">s_max</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">clamp_threshold</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">mask_sparsity_reg_factor</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">mask_sparsity_reg_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;original&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">task_embedding_init_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;N01&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">epsilon</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span></span>)</span>"}, {"fullname": "clarena.cl_algorithms.adahat.AdaHAT.adjustment_intensity", "modulename": "clarena.cl_algorithms.adahat", "qualname": "AdaHAT.adjustment_intensity", "kind": "variable", "doc": "<p>The adjustment intensity in Eq. (9) of the <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_algorithms.adahat.AdaHAT.epsilon", "modulename": "clarena.cl_algorithms.adahat", "qualname": "AdaHAT.epsilon", "kind": "variable", "doc": "<p>The small value to avoid division by zero (appearing in Eq. (9) of the <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>).</p>\n", "annotation": ": float | None"}, {"fullname": "clarena.cl_algorithms.adahat.AdaHAT.summative_mask_for_previous_tasks", "modulename": "clarena.cl_algorithms.adahat", "qualname": "AdaHAT.summative_mask_for_previous_tasks", "kind": "variable", "doc": "<p>The summative binary attention mask $\\mathrm{M}^{<t,\\text{sum}}$ of previous tasks $1,\\cdots, t-1$, gated from the task embedding. It is a dict where keys are layer names and values are the binary mask tensors for the layers. The mask tensor has size (number of units, ).</p>\n", "annotation": ": dict[str, torch.Tensor]"}, {"fullname": "clarena.cl_algorithms.adahat.AdaHAT.automatic_optimization", "modulename": "clarena.cl_algorithms.adahat", "qualname": "AdaHAT.automatic_optimization", "kind": "variable", "doc": "<p>If set to <code>False</code> you are responsible for calling <code>.backward()</code>, <code>.step()</code>, <code>.zero_grad()</code>.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.cl_algorithms.adahat.AdaHAT.sanity_check", "modulename": "clarena.cl_algorithms.adahat", "qualname": "AdaHAT.sanity_check", "kind": "function", "doc": "<p>Check the sanity of the arguments.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.adahat.AdaHAT.on_train_start", "modulename": "clarena.cl_algorithms.adahat", "qualname": "AdaHAT.on_train_start", "kind": "function", "doc": "<p>Additionally initialize the summative mask at the beginning of the first task.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.adahat.AdaHAT.clip_grad_by_adjustment", "modulename": "clarena.cl_algorithms.adahat", "qualname": "AdaHAT.clip_grad_by_adjustment", "kind": "function", "doc": "<p>Clip the gradients by the adjustment rate. See Eq. (8) in the <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>.</p>\n\n<p>Note that because the task embedding fully covers every layer in the backbone network, no parameters are left out of this system. This applies not only to parameters between layers with task embeddings, but also to those before the first layer. We design it separately in the code.</p>\n\n<p>Network capacity is measured alongside this method. Network capacity is defined as the average adjustment rate over all parameters. See Sec. 4.1 in the <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>network_sparsity</strong> (<code>dict[str, Tensor]</code> | <code>None</code>): the network sparsity (i.e., the mask sparsity loss of each layer) for the current task. Keys are layer names and values are the network sparsity values. It is used to calculate the adjustment rate for gradients. Applies only to mode <code>adahat</code> and <code>adahat_no_sum</code>, not <code>adahat_no_reg</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>adjustment_rate_weight</strong> (<code>dict[str, Tensor]</code>): the adjustment rate for weights. Keys (<code>str</code>) are layer names and values (<code>Tensor</code>) are the adjustment rate tensors.</li>\n<li><strong>adjustment_rate_bias</strong> (<code>dict[str, Tensor]</code>): the adjustment rate for biases. Keys (<code>str</code>) are layer names and values (<code>Tensor</code>) are the adjustment rate tensors.</li>\n<li><strong>capacity</strong> (<code>Tensor</code>): the calculated network capacity.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">network_sparsity</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">],</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">],</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.adahat.AdaHAT.on_train_end", "modulename": "clarena.cl_algorithms.adahat", "qualname": "AdaHAT.on_train_end", "kind": "function", "doc": "<p>Additionally update the summative mask after training the task.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.cbp", "modulename": "clarena.cl_algorithms.cbp", "kind": "module", "doc": "<p>The submodule in <code>cl_algorithms</code> for <a href=\"https://www.nature.com/articles/s41586-024-07711-7\">CBP (Continual Backpropagation)</a> algorithm.</p>\n"}, {"fullname": "clarena.cl_algorithms.cbp.CBP", "modulename": "clarena.cl_algorithms.cbp", "qualname": "CBP", "kind": "class", "doc": "<p><a href=\"https://www.nature.com/articles/s41586-024-07711-7\">CBP (Continual Backpropagation)</a> algorithm.</p>\n\n<p>A continual learning approach that reinitializes a small number of units during training, using an utility measures to determine which units to reinitialize. It aims to address loss of plasticity problem for learning new tasks, yet not very well solve the catastrophic forgetting problem in continual learning.</p>\n\n<p>We implement CBP as a subclass of Finetuning algorithm, as CBP has the same <code>forward()</code>, <code>training_step()</code>, <code>validation_step()</code> and <code>test_step()</code> method as <code>Finetuning</code> class.</p>\n", "bases": "clarena.cl_algorithms.finetuning.Finetuning"}, {"fullname": "clarena.cl_algorithms.cbp.CBP.__init__", "modulename": "clarena.cl_algorithms.cbp", "qualname": "CBP.__init__", "kind": "function", "doc": "<p>Initialize the Finetuning algorithm with the network. It has no additional hyperparameters.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone</strong> (<code>CLBackbone</code>): backbone network.</li>\n<li><strong>heads</strong> (<code>HeadsTIL</code> | <code>HeadsCIL</code>): output heads.</li>\n<li><strong>replacement_rate</strong> (<code>float</code>): the replacement rate of units. It is the precentage of units to be reinitialized during training.</li>\n<li><strong>maturity_threshold</strong> (<code>int</code>): the maturity threshold of units. It is the number of training steps before a unit can be reinitialized.</li>\n<li><strong>utility_decay_rate</strong> (<code>float</code>): the utility decay rate of units. It is the rate at which the utility of a unit decays over time.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLBackbone</span>,</span><span class=\"param\">\t<span class=\"n\">heads</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_til</span><span class=\"o\">.</span><span class=\"n\">HeadsTIL</span> <span class=\"o\">|</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_cil</span><span class=\"o\">.</span><span class=\"n\">HeadsCIL</span>,</span><span class=\"param\">\t<span class=\"n\">replacement_rate</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">maturity_threshold</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">utility_decay_rate</span><span class=\"p\">:</span> <span class=\"nb\">float</span></span>)</span>"}, {"fullname": "clarena.cl_algorithms.cbp.CBP.replacement_rate", "modulename": "clarena.cl_algorithms.cbp", "qualname": "CBP.replacement_rate", "kind": "variable", "doc": "<p>Store the replacement rate of units.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_algorithms.cbp.CBP.maturity_threshold", "modulename": "clarena.cl_algorithms.cbp", "qualname": "CBP.maturity_threshold", "kind": "variable", "doc": "<p>Store the maturity threshold of units.</p>\n", "annotation": ": int"}, {"fullname": "clarena.cl_algorithms.cbp.CBP.utility_decay_rate", "modulename": "clarena.cl_algorithms.cbp", "qualname": "CBP.utility_decay_rate", "kind": "variable", "doc": "<p>Store the utility decay rate of units.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_algorithms.cbp.CBP.contribution_utility", "modulename": "clarena.cl_algorithms.cbp", "qualname": "CBP.contribution_utility", "kind": "variable", "doc": "<p>Store the contribution utility of units. See equation (1) in the <a href=\"https://www.nature.com/articles/s41586-024-07711-7\">continual backpropagation paper</a>. Keys are layer names and values are the utility tensor for the layer. The utility tensor is the same size as the feature tensor with size (number of units, ).</p>\n", "annotation": ": dict[str, torch.Tensor]"}, {"fullname": "clarena.cl_algorithms.cbp.CBP.num_replacements", "modulename": "clarena.cl_algorithms.cbp", "qualname": "CBP.num_replacements", "kind": "variable", "doc": "<p>Store the number of replacements of units in each layer. Keys are layer names and values are the number of replacements for the layer.</p>\n", "annotation": ": dict[str, int]"}, {"fullname": "clarena.cl_algorithms.cbp.CBP.age", "modulename": "clarena.cl_algorithms.cbp", "qualname": "CBP.age", "kind": "variable", "doc": "<p>Store the age of units. Keys are layer names and values are the age tensor for the layer. The age tensor is the same size as the feature tensor with size (1, number of units).</p>\n", "annotation": ": dict[str, torch.Tensor]"}, {"fullname": "clarena.cl_algorithms.cbp.CBP.on_train_start", "modulename": "clarena.cl_algorithms.cbp", "qualname": "CBP.on_train_start", "kind": "function", "doc": "<p>Initialize the utility, number of replacements and age for each layer as zeros.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.cbp.CBP.on_train_batch_end", "modulename": "clarena.cl_algorithms.cbp", "qualname": "CBP.on_train_batch_end", "kind": "function", "doc": "<p>Update the contribution utility and age of units after each training step, and conduct reinitialization of units based on utility measures. This is the core of the CBP algorithm.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the training step, which is the returns of the <code>training_step()</code> method in the <code>CLAlgorithm</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the training data batch.</li>\n<li><strong>batch_idx</strong> (<code>int</code>): the index of the current batch. This is for the file name of mask figures.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>, </span><span class=\"param\"><span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.ewc", "modulename": "clarena.cl_algorithms.ewc", "kind": "module", "doc": "<p>The submodule in <code>cl_algorithms</code> for <a href=\"https://www.pnas.org/doi/10.1073/pnas.1611835114\">EWC (Elastic Weight Consolidation) algorithm</a>.</p>\n"}, {"fullname": "clarena.cl_algorithms.ewc.EWC", "modulename": "clarena.cl_algorithms.ewc", "qualname": "EWC", "kind": "class", "doc": "<p><a href=\"https://www.pnas.org/doi/10.1073/pnas.1611835114\">EWC (Elastic Weight Consolidation)</a> algorithm.</p>\n\n<p>A regularization-based approach that calculates the fisher information as parameter importance for the previous tasks and penalizes the current task loss with the importance of the parameters.</p>\n\n<p>We implement EWC as a subclass of Finetuning algorithm, as EWC has the same <code>forward()</code>, <code>validation_step()</code> and <code>test_step()</code> method as <code>Finetuning</code> class.</p>\n", "bases": "clarena.cl_algorithms.finetuning.Finetuning"}, {"fullname": "clarena.cl_algorithms.ewc.EWC.__init__", "modulename": "clarena.cl_algorithms.ewc", "qualname": "EWC.__init__", "kind": "function", "doc": "<p>Initialize the HAT algorithm with the network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone</strong> (<code>CLBackbone</code>): backbone network.</li>\n<li><strong>heads</strong> (<code>HeadsTIL</code> | <code>HeadsCIL</code>): output heads.</li>\n<li><strong>parameter_change_reg_factor</strong> (<code>float</code>): the parameter change regularization factor. It controls the strength of preventing forgetting.</li>\n<li><strong>when_calculate_fisher_information</strong> (<code>str</code>): when to calculate the fisher information. It should be one of the following:\n<ol>\n<li>'train_end': calculate the fisher information at the end of training of the task.</li>\n<li>'train': accumulate the fisher information in the training step of the task.</li>\n</ol></li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLBackbone</span>,</span><span class=\"param\">\t<span class=\"n\">heads</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_til</span><span class=\"o\">.</span><span class=\"n\">HeadsTIL</span> <span class=\"o\">|</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_cil</span><span class=\"o\">.</span><span class=\"n\">HeadsCIL</span>,</span><span class=\"param\">\t<span class=\"n\">parameter_change_reg_factor</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">when_calculate_fisher_information</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span>)</span>"}, {"fullname": "clarena.cl_algorithms.ewc.EWC.parameter_importance", "modulename": "clarena.cl_algorithms.ewc", "qualname": "EWC.parameter_importance", "kind": "variable", "doc": "<p>Store the parameter importance of each previous task. Keys are task IDs and values are the corresponding importance. Each importance entity is a dict where keys are parameter names (named by <code>named_parameters()</code> of the <code>nn.Module</code>) and values are the importance tensor for the layer. It has the same shape as the parameters of the layer.</p>\n", "annotation": ": dict[str, dict[str, torch.Tensor]]"}, {"fullname": "clarena.cl_algorithms.ewc.EWC.previous_task_backbones", "modulename": "clarena.cl_algorithms.ewc", "qualname": "EWC.previous_task_backbones", "kind": "variable", "doc": "<p>Store the backbone models of the previous tasks. Keys are task IDs and values are the corresponding models. Each model is a <code>nn.Module</code> backbone after the corresponding previous task was trained.</p>\n\n<p>Some would argue that since we could store the model of the previous tasks, why don't we test the task directly with the stored model, instead of doing the less easier EWC thing? The thing is, EWC only uses the model of the previous tasks to train current and future tasks, which aggregate them into a single model. Once the training of the task is done, the storage for those parameters can be released. However, this make the future tasks not able to use EWC anymore, which is a disadvantage for EWC.</p>\n", "annotation": ": dict[str, torch.nn.modules.module.Module]"}, {"fullname": "clarena.cl_algorithms.ewc.EWC.parameter_change_reg_factor", "modulename": "clarena.cl_algorithms.ewc", "qualname": "EWC.parameter_change_reg_factor", "kind": "variable", "doc": "<p>Store parameter change regularization factor.</p>\n"}, {"fullname": "clarena.cl_algorithms.ewc.EWC.parameter_change_reg", "modulename": "clarena.cl_algorithms.ewc", "qualname": "EWC.parameter_change_reg", "kind": "variable", "doc": "<p>Initialize and store the parameter change regulariser.</p>\n"}, {"fullname": "clarena.cl_algorithms.ewc.EWC.when_calculate_fisher_information", "modulename": "clarena.cl_algorithms.ewc", "qualname": "EWC.when_calculate_fisher_information", "kind": "variable", "doc": "<p>Store when to calculate the fisher information.</p>\n", "annotation": ": str"}, {"fullname": "clarena.cl_algorithms.ewc.EWC.num_data", "modulename": "clarena.cl_algorithms.ewc", "qualname": "EWC.num_data", "kind": "variable", "doc": "<p>Store the number of data used to calculate the fisher information. It is used to average the fisher information over the data.</p>\n", "annotation": ": int"}, {"fullname": "clarena.cl_algorithms.ewc.EWC.automatic_optimization", "modulename": "clarena.cl_algorithms.ewc", "qualname": "EWC.automatic_optimization", "kind": "variable", "doc": "<p>If set to <code>False</code> you are responsible for calling <code>.backward()</code>, <code>.step()</code>, <code>.zero_grad()</code>.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.cl_algorithms.ewc.EWC.sanity_check", "modulename": "clarena.cl_algorithms.ewc", "qualname": "EWC.sanity_check", "kind": "function", "doc": "<p>Check the sanity of the arguments.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.ewc.EWC.on_train_start", "modulename": "clarena.cl_algorithms.ewc", "qualname": "EWC.on_train_start", "kind": "function", "doc": "<p>Initialize the parameter importance and num of data counter.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.ewc.EWC.training_step", "modulename": "clarena.cl_algorithms.ewc", "qualname": "EWC.training_step", "kind": "function", "doc": "<p>Training step for current task <code>self.task_id</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of training data.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and other metrics from this training step. Keys (<code>str</code>) are the metrics names, and values (<code>Tensor</code>) are the metrics. Must include the key 'loss' which is total loss in the case of automatic optimization, according to PyTorch Lightning docs.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.ewc.EWC.on_train_end", "modulename": "clarena.cl_algorithms.ewc", "qualname": "EWC.on_train_end", "kind": "function", "doc": "<p>Calculate the fisher information as parameter importance and store the backbone model after the training of a task.</p>\n\n<p>The calculated importance and model are stored in <code>self.parameter_importance[self.task_id]</code> and <code>self.previous_task_backbones[self.task_id]</code> respectively for constructing the regularization loss in the future tasks.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.ewc.EWC.accumulate_fisher_information_on_train_end", "modulename": "clarena.cl_algorithms.ewc", "qualname": "EWC.accumulate_fisher_information_on_train_end", "kind": "function", "doc": "<p>Accumulate the fisher information as the parameter importance for the learned task <code>self.task_id</code> at the end of its training. This is only called after the training of a task, which is the last previous task $t-1$. The accumulate importance is stored in <code>self.parameter_importance[self.task_id]</code> for constructing the regularization loss in the future tasks.</p>\n\n<p>According to <a href=\"https://www.pnas.org/doi/10.1073/pnas.1611835114\">the EWC paper</a>, the importance tensor is a Laplace approximation to Fisher information matrix by taking the digonal, i.e. $F_i$, where $i$ is the index of a parameter. The calculation is not following that theory but the derived formula below:</p>\n\n<p>$$\\omega_i = F_i  =\\frac{1}{N_{t-1}} \\sum_{(\\mathbf{x}, y)\\in \\mathcal{D}^{(t-1)}_{\\text{train}}} \\left[\\frac{\\partial l(f^{(t-1)}\\left(\\mathbf{x}, \\theta), y\\right)}{\\partial \\theta_i}\\right]^2$$</p>\n\n<p>For a parameter $i$, its fisher information is the magnitude (square here) of gradient of the loss of model just trained over the training data just used. The $l$ is the classification loss. It shows the sensitivity of the loss to the parameter. The larger it is, the more it changed the performance (which is the loss) of the model, which indicates the importance of the parameter.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>fisher_information_t</strong> (<code>dict[str, Tensor]</code>): the fisher information for the learned task. Keys are parameter names (named by <code>named_parameters()</code> of the <code>nn.Module</code>) and values are the importance tensor for the layer. It has the same shape as the parameters of the layer.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.fgadahat", "modulename": "clarena.cl_algorithms.fgadahat", "kind": "module", "doc": "<p>The submodule in <code>cl_algorithms</code> for FG-AdaHAT algorithm.</p>\n"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT", "kind": "class", "doc": "<p>FG-AdaHAT (Fine-Grained Adaptive Hard Attention to the Task) algorithm.</p>\n\n<p>An architecture-based continual learning approach that improves <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT (Adaptive Hard Attention to the Task)</a> by introducing fine-grained neuron-wise importance measures guiding the adaptive adjustment mechanism in AdaHAT.</p>\n\n<p>We implement FG-AdaHAT as a subclass of AdaHAT, as it reuses AdaHAT's summative mask and other components.</p>\n", "bases": "clarena.cl_algorithms.adahat.AdaHAT"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.__init__", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.__init__", "kind": "function", "doc": "<p>Initialize the FG-AdaHAT algorithm with the network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone</strong> (<code>HATMaskBackbone</code>): must be a backbone network with the HAT mask mechanism.</li>\n<li><strong>heads</strong> (<code>HeadsTIL</code>): output heads. FG-AdaHAT supports only TIL (Task-Incremental Learning).</li>\n<li><strong>adjustment_intensity</strong> (<code>float</code>): hyperparameter, controls the overall intensity of gradient adjustment (the $\\alpha$ in the paper).</li>\n<li><strong>importance_type</strong> (<code>str</code>): the type of neuron-wise importance, must be one of:\n<ol>\n<li>'input_weight_abs_sum': sum of absolute input weights;</li>\n<li>'output_weight_abs_sum': sum of absolute output weights;</li>\n<li>'input_weight_gradient_abs_sum': sum of absolute gradients of the input weights (Input Gradients (IG) in the paper);</li>\n<li>'output_weight_gradient_abs_sum': sum of absolute gradients of the output weights (Output Gradients (OG) in the paper);</li>\n<li>'activation_abs': absolute activation;</li>\n<li>'input_weight_abs_sum_x_activation_abs': sum of absolute input weights multiplied by absolute activation (Input Contribution Utility (ICU) in the paper);</li>\n<li>'output_weight_abs_sum_x_activation_abs': sum of absolute output weights multiplied by absolute activation (Contribution Utility (CU) in the paper);</li>\n<li>'gradient_x_activation_abs': absolute gradient (the saliency) multiplied by activation;</li>\n<li>'input_weight_gradient_square_sum': sum of squared gradients of the input weights;</li>\n<li>'output_weight_gradient_square_sum': sum of squared gradients of the output weights;</li>\n<li>'input_weight_gradient_square_sum_x_activation_abs': sum of squared gradients of the input weights multiplied by absolute activation (Activation Fisher Information (AFI) in the paper);</li>\n<li>'output_weight_gradient_square_sum_x_activation_abs': sum of squared gradients of the output weights multiplied by absolute activation;</li>\n<li>'conductance_abs': absolute layer conductance;</li>\n<li>'internal_influence_abs': absolute internal influence (Internal Influence (II) in the paper);</li>\n<li>'gradcam_abs': absolute Grad-CAM;</li>\n<li>'deeplift_abs': absolute DeepLIFT (DeepLIFT (DL) in the paper);</li>\n<li>'deepliftshap_abs': absolute DeepLIFT-SHAP;</li>\n<li>'gradientshap_abs': absolute Gradient-SHAP (Gradient SHAP (GS) in the paper);</li>\n<li>'integrated_gradients_abs': absolute Integrated Gradients;</li>\n<li>'feature_ablation_abs': absolute Feature Ablation (Feature Ablation (FA) in the paper);</li>\n<li>'lrp_abs': absolute Layer-wise Relevance Propagation (LRP);</li>\n<li>'cbp_adaptation': the adaptation function in <a href=\"https://www.nature.com/articles/s41586-024-07711-7\">Continual Backpropagation (CBP)</a>;</li>\n<li>'cbp_adaptive_contribution': the adaptive contribution function in <a href=\"https://www.nature.com/articles/s41586-024-07711-7\">Continual Backpropagation (CBP)</a>;</li>\n</ol></li>\n<li><strong>importance_summing_strategy</strong> (<code>str</code>): the strategy to sum neuron-wise importance for previous tasks, must be one of:\n<ol>\n<li>'add_latest': add the latest neuron-wise importance to the summative importance;</li>\n<li>'add_all': add all previous neuron-wise importance (including the latest) to the summative importance;</li>\n<li>'add_average': add the average of all previous neuron-wise importance (including the latest) to the summative importance;</li>\n<li>'linear_decrease': weigh the previous neuron-wise importance by a linear factor that decreases with the task ID;</li>\n<li>'quadratic_decrease': weigh the previous neuron-wise importance that decreases quadratically with the task ID;</li>\n<li>'cubic_decrease': weigh the previous neuron-wise importance that decreases cubically with the task ID;</li>\n<li>'exponential_decrease': weigh the previous neuron-wise importance by an exponential factor that decreases with the task ID;</li>\n<li>'log_decrease': weigh the previous neuron-wise importance by a logarithmic factor that decreases with the task ID;</li>\n<li>'factorial_decrease': weigh the previous neuron-wise importance that decreases factorially with the task ID;</li>\n</ol></li>\n<li><strong>importance_scheduler_type</strong> (<code>str</code>): the scheduler for importance, i.e., the factor $c^t$ multiplied to parameter importance. Must be one of:\n<ol>\n<li>'linear_sparsity_reg': $c^t = (t+b_L) \\cdot [R(M^t, M^{<t}) + b_R]$, where $R(M^t, M^{<t})$ is the mask sparsity regularization betwwen the current task and previous tasks, $b_L$ is the base linear factor (see argument <code>base_linear</code>), and $b_R$ is the base mask sparsity regularization factor (see argument <code>base_mask_sparsity_reg</code>);</li>\n<li>'sparsity_reg': $c^t = [R(M^t, M^{<t}) + b_R]$;</li>\n<li>'summative_mask_sparsity_reg': $c^t_{l,ij} = \\left(\\min \\left(m^{<t, \\text{sum}}_{l,i}, m^{<t, \\text{sum}}_{l-1,j}\\right)+b_L\\right) \\cdot [R(M^t, M^{<t}) + b_R]$.</li>\n</ol></li>\n<li><strong>neuron_to_weight_importance_aggregation_mode</strong> (<code>str</code>): aggregation mode from neuron-wise to weight-wise importance ($\\text{Agg}(\\cdot)$ in the paper), must be one of:\n<ol>\n<li>'min': take the minimum of neuron-wise importance for each weight;</li>\n<li>'max': take the maximum of neuron-wise importance for each weight;</li>\n<li>'mean': take the mean of neuron-wise importance for each weight.</li>\n</ol></li>\n<li><strong>s_max</strong> (<code>float</code>): hyperparameter, the maximum scaling factor in the gate function. See Sec. 2.4 \"Hard Attention Training\" in the <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</li>\n<li><strong>clamp_threshold</strong> (<code>float</code>): the threshold for task embedding gradient compensation. See Sec. 2.5 \"Embedding Gradient Compensation\" in the <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</li>\n<li><strong>mask_sparsity_reg_factor</strong> (<code>float</code>): hyperparameter, the regularization factor for mask sparsity.</li>\n<li><strong>mask_sparsity_reg_mode</strong> (<code>str</code>): the mode of mask sparsity regularization, must be one of:\n<ol>\n<li>'original' (default): the original mask sparsity regularization in the <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</li>\n<li>'cross': the cross version of mask sparsity regularization.</li>\n</ol></li>\n<li><strong>base_importance</strong> (<code>float</code>): base value added to importance ($b_I$ in the paper). Default: 0.01.</li>\n<li><strong>base_mask_sparsity_reg</strong> (<code>float</code>): base value added to mask sparsity regularization factor in the importance scheduler ($b_R$ in the paper). Default: 0.1.</li>\n<li><strong>base_linear</strong> (<code>float</code>): base value added to the linear factor in the importance scheduler ($b_L$ in the paper). Default: 10.</li>\n<li><strong>filter_by_cumulative_mask</strong> (<code>bool</code>): whether to multiply the cumulative mask to the importance when calculating adjustment rate. Default: False.</li>\n<li><strong>filter_unmasked_importance</strong> (<code>bool</code>): whether to filter unmasked importance values (set to 0) at the end of task training. Default: False.</li>\n<li><strong>step_multiply_training_mask</strong> (<code>bool</code>): whether to multiply the training mask to the importance at each training step. Default: True.</li>\n<li><strong>task_embedding_init_mode</strong> (<code>str</code>): the initialization mode for task embeddings, must be one of:\n<ol>\n<li>'N01' (default): standard normal distribution $N(0, 1)$.</li>\n<li>'U-11': uniform distribution $U(-1, 1)$.</li>\n<li>'U01': uniform distribution $U(0, 1)$.</li>\n<li>'U-10': uniform distribution $U(-1, 0)$.</li>\n<li>'last': inherit the task embedding from the last task.</li>\n</ol></li>\n<li><strong>importance_summing_strategy_linear_step</strong> (<code>float</code> | <code>None</code>): linear step for the importance summing strategy (used when <code>importance_summing_strategy</code> is 'linear_decrease'). Must be &gt; 0.</li>\n<li><strong>importance_summing_strategy_exponential_rate</strong> (<code>float</code> | <code>None</code>): exponential rate for the importance summing strategy (used when <code>importance_summing_strategy</code> is 'exponential_decrease'). Must be &gt; 1.</li>\n<li><strong>importance_summing_strategy_log_base</strong> (<code>float</code> | <code>None</code>): base for the logarithm in the importance summing strategy (used when <code>importance_summing_strategy</code> is 'log_decrease'). Must be &gt; 1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">HATMaskBackbone</span>,</span><span class=\"param\">\t<span class=\"n\">heads</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_til</span><span class=\"o\">.</span><span class=\"n\">HeadsTIL</span>,</span><span class=\"param\">\t<span class=\"n\">adjustment_intensity</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">importance_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">importance_summing_strategy</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">importance_scheduler_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">neuron_to_weight_importance_aggregation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">s_max</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">clamp_threshold</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">mask_sparsity_reg_factor</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">mask_sparsity_reg_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;original&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">base_importance</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.01</span>,</span><span class=\"param\">\t<span class=\"n\">base_mask_sparsity_reg</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span>,</span><span class=\"param\">\t<span class=\"n\">base_linear</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mi\">10</span>,</span><span class=\"param\">\t<span class=\"n\">filter_by_cumulative_mask</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">filter_unmasked_importance</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">step_multiply_training_mask</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">task_embedding_init_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;N01&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">importance_summing_strategy_linear_step</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">importance_summing_strategy_exponential_rate</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">importance_summing_strategy_log_base</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.importance_type", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.importance_type", "kind": "variable", "doc": "<p>The type of the neuron-wise importance added to AdaHAT importance.</p>\n", "annotation": ": str | None"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.importance_scheduler_type", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.importance_scheduler_type", "kind": "variable", "doc": "<p>Store the type of the importance scheduler.</p>\n", "annotation": ": str"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.neuron_to_weight_importance_aggregation_mode", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.neuron_to_weight_importance_aggregation_mode", "kind": "variable", "doc": "<p>The mode of aggregation from neuron-wise to weight-wise importance.</p>\n", "annotation": ": str"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.filter_by_cumulative_mask", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.filter_by_cumulative_mask", "kind": "variable", "doc": "<p>The flag to filter importance by the cumulative mask when calculating the adjustment rate.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.filter_unmasked_importance", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.filter_unmasked_importance", "kind": "variable", "doc": "<p>The flag to filter unmasked importance values (set them to 0) at the end of task training.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.step_multiply_training_mask", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.step_multiply_training_mask", "kind": "variable", "doc": "<p>The flag to multiply the training mask to the importance at each training step.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.importance_summing_strategy", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.importance_summing_strategy", "kind": "variable", "doc": "<p>The strategy to sum the neuron-wise importance for previous tasks.</p>\n", "annotation": ": str"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.base_importance", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.base_importance", "kind": "variable", "doc": "<p>The base value added to the importance to avoid zero.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.base_mask_sparsity_reg", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.base_mask_sparsity_reg", "kind": "variable", "doc": "<p>The base value added to the mask sparsity regularization to avoid zero.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.base_linear", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.base_linear", "kind": "variable", "doc": "<p>The base value added to the linear layer to avoid zero.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.importances", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.importances", "kind": "variable", "doc": "<p>The min-max scaled ($[0, 1]$) neuron-wise importance of units. It is $I^{\\tau}_{l}$ in the paper. Keys are task IDs and values are the corresponding importance tensors. Each importance tensor is a dict where keys are layer names and values are the importance tensor for the layer. The utility tensor is the same size as the feature tensor with size (number of units, ).</p>\n", "annotation": ": dict[int, dict[str, torch.Tensor]]"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.summative_importance_for_previous_tasks", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.summative_importance_for_previous_tasks", "kind": "variable", "doc": "<p>The summative neuron-wise importance values of units for previous tasks before the current task <code>self.task_id</code>. See $I^{<t}_{l}$ in the paper. Keys are layer names and values are the summative importance tensor for the layer. The summative importance tensor has the same size as the feature tensor with size (number of units, ).</p>\n", "annotation": ": dict[str, torch.Tensor]"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.num_steps_t", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.num_steps_t", "kind": "variable", "doc": "<p>Store the number of training steps for the current task <code>self.task_id</code>.</p>\n", "annotation": ": int"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.automatic_optimization", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.automatic_optimization", "kind": "variable", "doc": "<p>If set to <code>False</code> you are responsible for calling <code>.backward()</code>, <code>.step()</code>, <code>.zero_grad()</code>.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.sanity_check", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.sanity_check", "kind": "function", "doc": "<p>Check the sanity of the arguments.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.on_train_start", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.on_train_start", "kind": "function", "doc": "<p>Initialize neuron importance accumulation variable for each layer as zeros, in addition to AdaHAT's summative mask initialization.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.clip_grad_by_adjustment", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.clip_grad_by_adjustment", "kind": "function", "doc": "<p>Clip the gradients by the adjustment rate. See Eq. (1) in the paper.</p>\n\n<p>Note that because the task embedding fully covers every layer in the backbone network, no parameters are left out of this system. This applies not only to parameters between layers with task embeddings, but also to those before the first layer. We design it separately in the code.</p>\n\n<p>Network capacity is measured alongside this method. Network capacity is defined as the average adjustment rate over all parameters. See Sec. 4.1 in the <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>network_sparsity</strong> (<code>dict[str, Tensor]</code>): the network sparsity (i.e., mask sparsity loss of each layer) for the current task. Keys are layer names and values are the network sparsity values. It is used to calculate the adjustment rate for gradients. In FG-AdaHAT, it is used to construct the importance scheduler.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>adjustment_rate_weight</strong> (<code>dict[str, Tensor]</code>): the adjustment rate for weights. Keys (<code>str</code>) are layer names and values (<code>Tensor</code>) are the adjustment rate tensors.</li>\n<li><strong>adjustment_rate_bias</strong> (<code>dict[str, Tensor]</code>): the adjustment rate for biases. Keys (<code>str</code>) are layer names and values (<code>Tensor</code>) are the adjustment rate tensors.</li>\n<li><strong>capacity</strong> (<code>Tensor</code>): the calculated network capacity.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">network_sparsity</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">],</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">],</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.on_train_batch_end", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.on_train_batch_end", "kind": "function", "doc": "<p>Calculate the step-wise importance, update the accumulated importance and number of steps counter after each training step.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): outputs of the training step (returns of <code>training_step()</code> in <code>CLAlgorithm</code>).</li>\n<li><strong>batch</strong> (<code>Any</code>): training data batch.</li>\n<li><strong>batch_idx</strong> (<code>int</code>): index of the current batch (for mask figure file name).</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>, </span><span class=\"param\"><span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.on_train_end", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.on_train_end", "kind": "function", "doc": "<p>Additionally calculate neuron-wise importance for previous tasks at the end of training each task.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.get_importance_step_layer_weight_abs_sum", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.get_importance_step_layer_weight_abs_sum", "kind": "function", "doc": "<p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\\tau}_l(\\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the sum of absolute values of layer input or output weights.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>\n<li><strong>if_output_weight</strong> (<code>bool</code>): whether to use the output weights or input weights.</li>\n<li><strong>reciprocal</strong> (<code>bool</code>): whether to take reciprocal.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">if_output_weight</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>,</span><span class=\"param\">\t<span class=\"n\">reciprocal</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum", "kind": "function", "doc": "<p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\\tau}_l(\\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the sum of absolute values of gradients of the layer input or output weights.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>\n<li><strong>if_output_weight</strong> (<code>bool</code>): whether to use the output weights or input weights.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">if_output_weight</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.get_importance_step_layer_activation_abs", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.get_importance_step_layer_activation_abs", "kind": "function", "doc": "<p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\\tau}_l(\\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute value of activation of the layer. This is our own implementation of <a href=\"https://captum.ai/api/layer.html#layer-activation\">Layer Activation</a> in Captum.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>activation</strong> (<code>Tensor</code>): the activation tensor of the layer. It has the same size of (number of units, ).</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">activation</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs", "kind": "function", "doc": "<p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\\tau}_l(\\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the sum of absolute values of layer input / output weights multiplied by absolute values of activation. The input weights version is equal to the contribution utility in <a href=\"https://www.nature.com/articles/s41586-024-07711-7\">CBP</a>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>\n<li><strong>activation</strong> (<code>Tensor</code>): the activation tensor of the layer. It has the same size of (number of units, ).</li>\n<li><strong>if_output_weight</strong> (<code>bool</code>): whether to use the output weights or input weights.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">activation</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">if_output_weight</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs", "kind": "function", "doc": "<p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\\tau}_l(\\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of the gradient of layer activation multiplied by the activation. We implement this using <a href=\"https://captum.ai/api/layer.html#layer-gradient-x-activation\">Layer Gradient X Activation</a> in Captum.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>\n<li><strong>input</strong> (<code>Tensor</code> | <code>tuple[Tensor, ...]</code>): the input batch of the training step.</li>\n<li><strong>target</strong> (<code>Tensor</code> | <code>None</code>): the target batch of the training step.</li>\n<li><strong>batch_idx</strong> (<code>int</code>): the index of the current batch. This is an argument of the forward function during training.</li>\n<li><strong>num_batches</strong> (<code>int</code>): the number of batches in the training step. This is an argument of the forward function during training.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">target</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">num_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum", "kind": "function", "doc": "<p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\\tau}_l(\\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the sum of layer weight gradient squares. The weight gradient square is equal to fisher information in <a href=\"https://www.pnas.org/doi/10.1073/pnas.1611835114\">EWC</a>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>\n<li><strong>activation</strong> (<code>Tensor</code>): the activation tensor of the layer. It has the same size of (number of units, ).</li>\n<li><strong>if_output_weight</strong> (<code>bool</code>): whether to use the output weights or input weights.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">activation</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">if_output_weight</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs", "kind": "function", "doc": "<p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\\tau}_l(\\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the sum of layer weight gradient squares multiplied by absolute values of activation. The weight gradient square is equal to fisher information in <a href=\"https://www.pnas.org/doi/10.1073/pnas.1611835114\">EWC</a>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>\n<li><strong>activation</strong> (<code>Tensor</code>): the activation tensor of the layer. It has the same size of (number of units, ).</li>\n<li><strong>if_output_weight</strong> (<code>bool</code>): whether to use the output weights or input weights.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">activation</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">if_output_weight</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.get_importance_step_layer_conductance_abs", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.get_importance_step_layer_conductance_abs", "kind": "function", "doc": "<p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\\tau}_l(\\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of <a href=\"https://openreview.net/forum?id=SylKoo0cKm\">conductance</a>. We implement this using <a href=\"https://captum.ai/api/layer.html#layer-conductance\">Layer Conductance</a> in Captum.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>\n<li><strong>input</strong> (<code>Tensor</code> | <code>tuple[Tensor, ...]</code>): the input batch of the training step.</li>\n<li><strong>baselines</strong> (<code>None</code> | <code>int</code> | <code>float</code> | <code>Tensor</code> | <code>tuple[int | float | Tensor, ...]</code>): starting point from which integral is computed in this method. Please refer to the <a href=\"https://captum.ai/api/layer.html#captum.attr.LayerConductance.attribute\">Captum documentation</a> for more details.</li>\n<li><strong>target</strong> (<code>Tensor</code> | <code>None</code>): the target batch of the training step.</li>\n<li><strong>batch_idx</strong> (<code>int</code>): the index of the current batch. This is an argument of the forward function during training.</li>\n<li><strong>num_batches</strong> (<code>int</code>): the number of batches in the training step. This is an argument of the forward function during training.- <strong>mask</strong> (<code>Tensor</code>): the mask tensor of the layer. It has the same size as the feature tensor with size (number of units, ).</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">baselines</span><span class=\"p\">:</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">target</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">num_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.get_importance_step_layer_internal_influence_abs", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.get_importance_step_layer_internal_influence_abs", "kind": "function", "doc": "<p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\\tau}_l(\\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of <a href=\"https://openreview.net/forum?id=SJPpHzW0-\">internal influence</a>. We implement this using <a href=\"https://captum.ai/api/layer.html#internal-influence\">Internal Influence</a> in Captum.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>\n<li><strong>input</strong> (<code>Tensor</code> | <code>tuple[Tensor, ...]</code>): the input batch of the training step.</li>\n<li><strong>baselines</strong> (<code>None</code> | <code>int</code> | <code>float</code> | <code>Tensor</code> | <code>tuple[int | float | Tensor, ...]</code>): starting point from which integral is computed in this method. Please refer to the <a href=\"https://captum.ai/api/layer.html#captum.attr.InternalInfluence.attribute\">Captum documentation</a> for more details.</li>\n<li><strong>target</strong> (<code>Tensor</code> | <code>None</code>): the target batch of the training step.</li>\n<li><strong>batch_idx</strong> (<code>int</code>): the index of the current batch. This is an argument of the forward function during training.</li>\n<li><strong>num_batches</strong> (<code>int</code>): the number of batches in the training step. This is an argument of the forward function during training.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">baselines</span><span class=\"p\">:</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">target</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">num_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.get_importance_step_layer_gradcam_abs", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.get_importance_step_layer_gradcam_abs", "kind": "function", "doc": "<p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\\tau}_l(\\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of <a href=\"https://openreview.net/forum?id=SJPpHzW0-\">Grad-CAM</a>. We implement this using <a href=\"https://captum.ai/api/layer.html#gradcam\">Layer Grad-CAM</a> in Captum.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>\n<li><strong>input</strong> (<code>Tensor</code> | <code>tuple[Tensor, ...]</code>): the input batch of the training step.</li>\n<li><strong>target</strong> (<code>Tensor</code> | <code>None</code>): the target batch of the training step.</li>\n<li><strong>batch_idx</strong> (<code>int</code>): the index of the current batch. This is an argument of the forward function during training.</li>\n<li><strong>num_batches</strong> (<code>int</code>): the number of batches in the training step. This is an argument of the forward function during training.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">target</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">num_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.get_importance_step_layer_deeplift_abs", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.get_importance_step_layer_deeplift_abs", "kind": "function", "doc": "<p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\\tau}_l(\\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of <a href=\"https://proceedings.mlr.press/v70/shrikumar17a/shrikumar17a.pdf\">DeepLift</a>. We implement this using <a href=\"https://captum.ai/api/layer.html#layer-deeplift\">Layer DeepLift</a> in Captum.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>\n<li><strong>input</strong> (<code>Tensor</code> | <code>tuple[Tensor, ...]</code>): the input batch of the training step.</li>\n<li><strong>baselines</strong> (<code>None</code> | <code>int</code> | <code>float</code> | <code>Tensor</code> | <code>tuple[int | float | Tensor, ...]</code>): baselines define reference samples that are compared with the inputs. Please refer to the <a href=\"https://captum.ai/api/layer.html#captum.attr.LayerDeepLift.attribute\">Captum documentation</a> for more details.</li>\n<li><strong>target</strong> (<code>Tensor</code> | <code>None</code>): the target batch of the training step.</li>\n<li><strong>batch_idx</strong> (<code>int</code>): the index of the current batch. This is an argument of the forward function during training.</li>\n<li><strong>num_batches</strong> (<code>int</code>): the number of batches in the training step. This is an argument of the forward function during training.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">baselines</span><span class=\"p\">:</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">target</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">num_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.get_importance_step_layer_deepliftshap_abs", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.get_importance_step_layer_deepliftshap_abs", "kind": "function", "doc": "<p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\\tau}_l(\\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of <a href=\"https://proceedings.neurips.cc/paper_files/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf\">DeepLift SHAP</a>. We implement this using <a href=\"https://captum.ai/api/layer.html#layer-deepliftshap\">Layer DeepLiftShap</a> in Captum.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>\n<li><strong>input</strong> (<code>Tensor</code> | <code>tuple[Tensor, ...]</code>): the input batch of the training step.</li>\n<li><strong>baselines</strong> (<code>None</code> | <code>int</code> | <code>float</code> | <code>Tensor</code> | <code>tuple[int | float | Tensor, ...]</code>): baselines define reference samples that are compared with the inputs. Please refer to the <a href=\"https://captum.ai/api/layer.html#captum.attr.LayerDeepLiftShap.attribute\">Captum documentation</a> for more details.</li>\n<li><strong>target</strong> (<code>Tensor</code> | <code>None</code>): the target batch of the training step.</li>\n<li><strong>batch_idx</strong> (<code>int</code>): the index of the current batch. This is an argument of the forward function during training.</li>\n<li><strong>num_batches</strong> (<code>int</code>): the number of batches in the training step. This is an argument of the forward function during training.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">baselines</span><span class=\"p\">:</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">target</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">num_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.get_importance_step_layer_gradientshap_abs", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.get_importance_step_layer_gradientshap_abs", "kind": "function", "doc": "<p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\\tau}_l(\\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of gradient SHAP. We implement this using <a href=\"https://captum.ai/api/layer.html#layer-gradientshap\">Layer GradientShap</a> in Captum.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>\n<li><strong>input</strong> (<code>Tensor</code> | <code>tuple[Tensor, ...]</code>): the input batch of the training step.</li>\n<li><strong>baselines</strong> (<code>None</code> | <code>int</code> | <code>float</code> | <code>Tensor</code> | <code>tuple[int | float | Tensor, ...]</code>): starting point from which expectation is computed. Please refer to the <a href=\"https://captum.ai/api/layer.html#captum.attr.LayerGradientShap.attribute\">Captum documentation</a> for more details. If <code>None</code>, the baselines are set to zero.</li>\n<li><strong>target</strong> (<code>Tensor</code> | <code>None</code>): the target batch of the training step.</li>\n<li><strong>batch_idx</strong> (<code>int</code>): the index of the current batch. This is an argument of the forward function during training.</li>\n<li><strong>num_batches</strong> (<code>int</code>): the number of batches in the training step. This is an argument of the forward function during training.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">baselines</span><span class=\"p\">:</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">target</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">num_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.get_importance_step_layer_integrated_gradients_abs", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.get_importance_step_layer_integrated_gradients_abs", "kind": "function", "doc": "<p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\\tau}_l(\\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of <a href=\"https://proceedings.mlr.press/v70/sundararajan17a/sundararajan17a.pdf\">integrated gradients</a>. We implement this using <a href=\"https://captum.ai/api/layer.html#layer-integrated-gradients\">Layer Integrated Gradients</a> in Captum.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>\n<li><strong>input</strong> (<code>Tensor</code> | <code>tuple[Tensor, ...]</code>): the input batch of the training step.</li>\n<li><strong>baselines</strong> (<code>None</code> | <code>int</code> | <code>float</code> | <code>Tensor</code> | <code>tuple[int | float | Tensor, ...]</code>): starting point from which integral is computed. Please refer to the <a href=\"https://captum.ai/api/layer.html#captum.attr.LayerIntegratedGradients.attribute\">Captum documentation</a> for more details.</li>\n<li><strong>target</strong> (<code>Tensor</code> | <code>None</code>): the target batch of the training step.</li>\n<li><strong>batch_idx</strong> (<code>int</code>): the index of the current batch. This is an argument of the forward function during training.</li>\n<li><strong>num_batches</strong> (<code>int</code>): the number of batches in the training step. This is an argument of the forward function during training.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">baselines</span><span class=\"p\">:</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">target</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">num_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.get_importance_step_layer_feature_ablation_abs", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.get_importance_step_layer_feature_ablation_abs", "kind": "function", "doc": "<p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\\tau}_l(\\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of <a href=\"https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53\">feature ablation</a> attribution. We implement this using <a href=\"https://captum.ai/api/layer.html#layer-feature-ablation\">Layer Feature Ablation</a> in Captum.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>\n<li><strong>input</strong> (<code>Tensor</code> | <code>tuple[Tensor, ...]</code>): the input batch of the training step.</li>\n<li><strong>layer_baselines</strong> (<code>None</code> | <code>int</code> | <code>float</code> | <code>Tensor</code> | <code>tuple[int | float | Tensor, ...]</code>): reference values which replace each layer input / output value when ablated. Please refer to the <a href=\"https://captum.ai/api/layer.html#captum.attr.LayerFeatureAblation.attribute\">Captum documentation</a> for more details.</li>\n<li><strong>target</strong> (<code>Tensor</code> | <code>None</code>): the target batch of the training step.</li>\n<li><strong>batch_idx</strong> (<code>int</code>): the index of the current batch. This is an argument of the forward function during training.</li>\n<li><strong>num_batches</strong> (<code>int</code>): the number of batches in the training step. This is an argument of the forward function during training.</li>\n<li><strong>if_captum</strong> (<code>bool</code>): whether to use Captum or not. If <code>True</code>, we use Captum to calculate the feature ablation. If <code>False</code>, we use our implementation. Default is <code>False</code>, because our implementation is much faster.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">layer_baselines</span><span class=\"p\">:</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">target</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">num_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">if_captum</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.get_importance_step_layer_lrp_abs", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.get_importance_step_layer_lrp_abs", "kind": "function", "doc": "<p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\\tau}_l(\\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of <a href=\"https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140\">LRP</a>. We implement this using <a href=\"https://captum.ai/api/layer.html#layer-lrp\">Layer LRP</a> in Captum.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>\n<li><strong>input</strong> (<code>Tensor</code> | <code>tuple[Tensor, ...]</code>): the input batch of the training step.</li>\n<li><strong>target</strong> (<code>Tensor</code> | <code>None</code>): the target batch of the training step.</li>\n<li><strong>batch_idx</strong> (<code>int</code>): the index of the current batch. This is an argument of the forward function during training.</li>\n<li><strong>num_batches</strong> (<code>int</code>): the number of batches in the training step. This is an argument of the forward function during training.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">target</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">num_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution", "kind": "function", "doc": "<p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\\tau}_l(\\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the sum of absolute values of layer output weights multiplied by absolute values of activation, then divided by the reciprocal of sum of absolute values of layer input weights. It is equal to the adaptive contribution utility in <a href=\"https://www.nature.com/articles/s41586-024-07711-7\">CBP</a>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>\n<li><strong>activation</strong> (<code>Tensor</code>): the activation tensor of the layer. It has the same size of (number of units, ).</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">activation</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.finetuning", "modulename": "clarena.cl_algorithms.finetuning", "kind": "module", "doc": "<p>The submodule in <code>cl_algorithms</code> for Finetuning algorithm.</p>\n"}, {"fullname": "clarena.cl_algorithms.finetuning.Finetuning", "modulename": "clarena.cl_algorithms.finetuning", "qualname": "Finetuning", "kind": "class", "doc": "<p>Finetuning algorithm.</p>\n\n<p>The most naive way for task-incremental learning. It simply initializes the backbone from the last task when training new task.</p>\n", "bases": "clarena.cl_algorithms.base.CLAlgorithm"}, {"fullname": "clarena.cl_algorithms.finetuning.Finetuning.__init__", "modulename": "clarena.cl_algorithms.finetuning", "qualname": "Finetuning.__init__", "kind": "function", "doc": "<p>Initialize the Finetuning algorithm with the network. It has no additional hyperparameters.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone</strong> (<code>CLBackbone</code>): backbone network.</li>\n<li><strong>heads</strong> (<code>HeadsTIL</code> | <code>HeadsCIL</code>): output heads.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLBackbone</span>,</span><span class=\"param\">\t<span class=\"n\">heads</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_til</span><span class=\"o\">.</span><span class=\"n\">HeadsTIL</span> <span class=\"o\">|</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_cil</span><span class=\"o\">.</span><span class=\"n\">HeadsCIL</span></span>)</span>"}, {"fullname": "clarena.cl_algorithms.finetuning.Finetuning.training_step", "modulename": "clarena.cl_algorithms.finetuning", "qualname": "Finetuning.training_step", "kind": "function", "doc": "<p>Training step for current task <code>self.task_id</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of training data.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and other metrics from this training step. Keys (<code>str</code>) are the metrics names, and values (<code>Tensor</code>) are the metrics. Must include the key 'loss' which is total loss in the case of automatic optimization, according to PyTorch Lightning docs.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.finetuning.Finetuning.validation_step", "modulename": "clarena.cl_algorithms.finetuning", "qualname": "Finetuning.validation_step", "kind": "function", "doc": "<p>Validation step for current task <code>self.task_id</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of validation data.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and other metrics from this validation step. Key (<code>str</code>) are the metrics names, value (<code>Tensor</code>) are the metrics.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.finetuning.Finetuning.test_step", "modulename": "clarena.cl_algorithms.finetuning", "qualname": "Finetuning.test_step", "kind": "function", "doc": "<p>Test step for current task <code>self.task_id</code>, which tests all seen tasks indexed by <code>dataloader_idx</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of test data.</li>\n<li><strong>dataloader_idx</strong> (<code>int</code>): the task ID of seen tasks to be tested. A default value of 0 is given otherwise the LightningModule will raise a <code>RuntimeError</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and other metrics from this test step. Key (<code>str</code>) are the metrics name, value (<code>Tensor</code>) are the metrics.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">dataloader_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.fix", "modulename": "clarena.cl_algorithms.fix", "kind": "module", "doc": "<p>The submodule in <code>cl_algorithms</code> for Fix algorithm.</p>\n"}, {"fullname": "clarena.cl_algorithms.fix.Fix", "modulename": "clarena.cl_algorithms.fix", "qualname": "Fix", "kind": "class", "doc": "<p>Fix algorithm.</p>\n\n<p>Another naive way for task-incremental learning aside from Finetuning. It simply fixes the backbone forever after training first task. It serves as kind of toy algorithm when discussing stability-plasticity dilemma in continual learning.</p>\n\n<p>We implement <code>Fix</code> as a subclass of <code>Finetuning</code>, as it shares <code>forward()</code>, <code>validation_step()</code>, and <code>test_step()</code> with <code>Finetuning</code>.</p>\n", "bases": "clarena.cl_algorithms.finetuning.Finetuning"}, {"fullname": "clarena.cl_algorithms.fix.Fix.__init__", "modulename": "clarena.cl_algorithms.fix", "qualname": "Fix.__init__", "kind": "function", "doc": "<p>Initialize the Fix algorithm with the network. It has no additional hyperparameters.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone</strong> (<code>CLBackbone</code>): backbone network.</li>\n<li><strong>heads</strong> (<code>HeadsTIL</code> | <code>HeadsCIL</code>): output heads.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLBackbone</span>,</span><span class=\"param\">\t<span class=\"n\">heads</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_til</span><span class=\"o\">.</span><span class=\"n\">HeadsTIL</span> <span class=\"o\">|</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_cil</span><span class=\"o\">.</span><span class=\"n\">HeadsCIL</span></span>)</span>"}, {"fullname": "clarena.cl_algorithms.fix.Fix.training_step", "modulename": "clarena.cl_algorithms.fix", "qualname": "Fix.training_step", "kind": "function", "doc": "<p>Training step for current task <code>self.task_id</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of training data.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and other metrics from this training step. Keys (<code>str</code>) are the metrics names, and values (<code>Tensor</code>) are the metrics. Must include the key 'loss' which is total loss in the case of automatic optimization, according to PyTorch Lightning docs.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.hat", "modulename": "clarena.cl_algorithms.hat", "kind": "module", "doc": "<p>The submodule in <code>cl_algorithms</code> for <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task) algorithm</a>.</p>\n"}, {"fullname": "clarena.cl_algorithms.hat.HAT", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT", "kind": "class", "doc": "<p><a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task)</a> algorithm.</p>\n\n<p>An architecture-based continual learning approach that uses learnable hard attention masks to select task-specific parameters.</p>\n", "bases": "clarena.cl_algorithms.base.CLAlgorithm"}, {"fullname": "clarena.cl_algorithms.hat.HAT.__init__", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.__init__", "kind": "function", "doc": "<p>Initialize the HAT algorithm with the network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone</strong> (<code>HATMaskBackbone</code>): must be a backbone network with the HAT mask mechanism.</li>\n<li><strong>heads</strong> (<code>HeadsTIL</code>): output heads. HAT only supports TIL (Task-Incremental Learning).</li>\n<li><strong>adjustment_mode</strong> (<code>str</code>): the strategy of adjustment (i.e., the mode of gradient clipping), must be one of:\n<ol>\n<li>'hat': set gradients of parameters linking to masked units to zero. This is how HAT fixes the part of the network for previous tasks completely. See Eq. (2) in Sec. 2.3 \"Network Training\" in the <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</li>\n<li>'hat_random': set gradients of parameters linking to masked units to random 0\u20131 values. See \"Baselines\" in Sec. 4.1 in the <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>.</li>\n<li>'hat_const_alpha': set gradients of parameters linking to masked units to a constant value <code>alpha</code>. See \"Baselines\" in Sec. 4.1 in the <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>.</li>\n<li>'hat_const_1': set gradients of parameters linking to masked units to a constant value of 1 (i.e., no gradient constraint). See \"Baselines\" in Sec. 4.1 in the <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>.</li>\n</ol></li>\n<li><strong>s_max</strong> (<code>float</code>): hyperparameter, the maximum scaling factor in the gate function. See Sec. 2.4 \"Hard Attention Training\" in the <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</li>\n<li><strong>clamp_threshold</strong> (<code>float</code>): the threshold for task embedding gradient compensation. See Sec. 2.5 \"Embedding Gradient Compensation\" in the <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</li>\n<li><strong>mask_sparsity_reg_factor</strong> (<code>float</code>): hyperparameter, the regularization factor for mask sparsity.</li>\n<li><strong>mask_sparsity_reg_mode</strong> (<code>str</code>): the mode of mask sparsity regularization, must be one of:\n<ol>\n<li>'original' (default): the original mask sparsity regularization in the HAT paper.</li>\n<li>'cross': the cross version of mask sparsity regularization.</li>\n</ol></li>\n<li><strong>task_embedding_init_mode</strong> (<code>str</code>): the initialization mode for task embeddings, must be one of:\n<ol>\n<li>'N01' (default): standard normal distribution $N(0, 1)$.</li>\n<li>'U-11': uniform distribution $U(-1, 1)$.</li>\n<li>'U01': uniform distribution $U(0, 1)$.</li>\n<li>'U-10': uniform distribution $U(-1, 0)$.</li>\n<li>'last': inherit the task embedding from the last task.</li>\n</ol></li>\n<li><strong>alpha</strong> (<code>float</code> | <code>None</code>): the <code>alpha</code> in the 'HAT-const-alpha' mode. Applies only when <code>adjustment_mode</code> is 'hat_const_alpha'.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">HATMaskBackbone</span>,</span><span class=\"param\">\t<span class=\"n\">heads</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_til</span><span class=\"o\">.</span><span class=\"n\">HeadsTIL</span>,</span><span class=\"param\">\t<span class=\"n\">adjustment_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">s_max</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">clamp_threshold</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">mask_sparsity_reg_factor</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">mask_sparsity_reg_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;original&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">task_embedding_init_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;N01&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">alpha</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_algorithms.hat.HAT.adjustment_mode", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.adjustment_mode", "kind": "variable", "doc": "<p>The adjustment mode for gradient clipping.</p>\n", "annotation": ": str"}, {"fullname": "clarena.cl_algorithms.hat.HAT.s_max", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.s_max", "kind": "variable", "doc": "<p>The hyperparameter s_max.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_algorithms.hat.HAT.clamp_threshold", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.clamp_threshold", "kind": "variable", "doc": "<p>The clamp threshold for task embedding gradient compensation.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_algorithms.hat.HAT.mask_sparsity_reg_factor", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.mask_sparsity_reg_factor", "kind": "variable", "doc": "<p>The mask sparsity regularization factor.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_algorithms.hat.HAT.mask_sparsity_reg_mode", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.mask_sparsity_reg_mode", "kind": "variable", "doc": "<p>The mask sparsity regularization mode.</p>\n", "annotation": ": str"}, {"fullname": "clarena.cl_algorithms.hat.HAT.mark_sparsity_reg", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.mark_sparsity_reg", "kind": "variable", "doc": "<p>The mask sparsity regularizer.</p>\n", "annotation": ": clarena.cl_algorithms.regularizers.hat_mask_sparsity.HATMaskSparsityReg"}, {"fullname": "clarena.cl_algorithms.hat.HAT.task_embedding_init_mode", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.task_embedding_init_mode", "kind": "variable", "doc": "<p>Store the task embedding initialization mode.</p>\n", "annotation": ": str"}, {"fullname": "clarena.cl_algorithms.hat.HAT.alpha", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.alpha", "kind": "variable", "doc": "<p>The hyperparameter alpha for <code>hat_const_alpha</code>.</p>\n", "annotation": ": float | None"}, {"fullname": "clarena.cl_algorithms.hat.HAT.cumulative_mask_for_previous_tasks", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.cumulative_mask_for_previous_tasks", "kind": "variable", "doc": "<p>The cumulative binary attention mask $\\mathrm{M}^{<t}$ of previous tasks $1,\\cdots, t-1$, gated from the task embedding ($t$ is <code>self.task_id</code>). It is a dict where keys are layer names and values are the binary mask tensors for the layers. The mask tensor has size (number of units, ).</p>\n", "annotation": ": dict[str, torch.Tensor]"}, {"fullname": "clarena.cl_algorithms.hat.HAT.automatic_optimization", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.automatic_optimization", "kind": "variable", "doc": "<p>If set to <code>False</code> you are responsible for calling <code>.backward()</code>, <code>.step()</code>, <code>.zero_grad()</code>.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.cl_algorithms.hat.HAT.sanity_check", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.sanity_check", "kind": "function", "doc": "<p>Check the sanity of the arguments.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.hat.HAT.on_train_start", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.on_train_start", "kind": "function", "doc": "<p>Initialize the task embedding before training the next task and initialize the cumulative mask at the beginning of the first task.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.hat.HAT.clip_grad_by_adjustment", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.clip_grad_by_adjustment", "kind": "function", "doc": "<p>Clip the gradients by the adjustment rate. See Eq. (2) in Sec. 2.3 \"Network Training\" in the <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</p>\n\n<p>Note that because the task embedding fully covers every layer in the backbone network, no parameters are left out of this system.\nThis applies not only to parameters between layers with task embeddings, but also to those before the first layer. We design it separately in the code.</p>\n\n<p>Network capacity is measured alongside this method. Network capacity is defined as the average adjustment rate over all parameters.\nSee Sec. 4.1 in the <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>adjustment_rate_weight</strong> (<code>dict[str, Tensor]</code>): the adjustment rate for weights. Keys (<code>str</code>) are layer names and values (<code>Tensor</code>) are the adjustment rate tensors.</li>\n<li><strong>adjustment_rate_bias</strong> (<code>dict[str, Tensor]</code>): the adjustment rate for biases. Keys (<code>str</code>) are layer name and values (<code>Tensor</code>) are the adjustment rate tensors.</li>\n<li><strong>capacity</strong> (<code>Tensor</code>): the calculated network capacity.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">],</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">],</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.hat.HAT.compensate_task_embedding_gradients", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.compensate_task_embedding_gradients", "kind": "function", "doc": "<p>Compensate the gradients of task embeddings during training. See Sec. 2.5 \"Embedding Gradient Compensation\" in the <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch_idx</strong> (<code>int</code>): the current training batch index.</li>\n<li><strong>num_batches</strong> (<code>int</code>): the total number of training batches.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>, </span><span class=\"param\"><span class=\"n\">num_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.hat.HAT.forward", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.forward", "kind": "function", "doc": "<p>The forward pass for data from task <code>task_id</code>. Note that it is nothing to do with <code>forward()</code> method in <code>nn.Module</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): The input tensor from data.</li>\n<li><strong>stage</strong> (<code>str</code>): the stage of the forward pass, should be one of the following:\n<ol>\n<li>'train': training stage.</li>\n<li>'validation': validation stage.</li>\n<li>'test': testing stage.</li>\n</ol></li>\n<li><strong>task_id</strong> (<code>int</code>| <code>None</code>): the task ID where the data are from. If the stage is 'train' or 'validation', it should be the current task <code>self.task_id</code>. If stage is 'test', it could be from any seen task. In TIL, the task IDs of test data are provided thus this argument can be used. HAT algorithm works only for TIL.</li>\n<li><strong>batch_idx</strong> (<code>int</code> | <code>None</code>): the current batch index. Applies only to training stage. For other stages, it is default <code>None</code>.</li>\n<li><strong>num_batches</strong> (<code>int</code> | <code>None</code>): the total number of batches. Applies only to training stage. For other stages, it is default <code>None</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>logits</strong> (<code>Tensor</code>): the output logits tensor.</li>\n<li><strong>mask</strong> (<code>dict[str, Tensor]</code>): the mask for the current task. Key (<code>str</code>) is layer name, value (<code>Tensor</code>) is the mask tensor. The mask tensor has size (number of units, ).</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): the hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name, value (<code>Tensor</code>) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes. Although HAT algorithm does not need this, it is still provided for API consistence for other HAT-based algorithms inherited this <code>forward()</code> method of <code>HAT</code> class.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.hat.HAT.training_step", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.training_step", "kind": "function", "doc": "<p>Training step for current task <code>self.task_id</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of training data.</li>\n<li><strong>batch_idx</strong> (<code>int</code>): the index of the batch. Used for calculating annealed scalar in HAT. See Sec. 2.4 \"Hard Attention Training\" in <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary containing loss and other metrics from this training step. Keys (<code>str</code>) are metric names, and values (<code>Tensor</code>) are the metrics. Must include the key 'loss' (total loss) in the case of automatic optimization, according to PyTorch Lightning. For HAT, it includes 'mask' and 'capacity' for logging.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>, </span><span class=\"param\"><span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.hat.HAT.on_train_end", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.on_train_end", "kind": "function", "doc": "<p>Store the mask and update the cumulative mask after training the task.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.hat.HAT.validation_step", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.validation_step", "kind": "function", "doc": "<p>Validation step for current task <code>self.task_id</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of validation data.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and other metrics from this validation step. Keys (<code>str</code>) are the metrics names, and values (<code>Tensor</code>) are the metrics.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.hat.HAT.test_step", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.test_step", "kind": "function", "doc": "<p>Test step for current task <code>self.task_id</code>, which tests for all seen tasks indexed by <code>dataloader_idx</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of test data.</li>\n<li><strong>dataloader_idx</strong> (<code>int</code>): the task ID of seen tasks to be tested. A default value of 0 is given otherwise the LightningModule will raise a <code>RuntimeError</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and other metrics from this test step. Keys (<code>str</code>) are the metrics names, and values (<code>Tensor</code>) are the metrics.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">dataloader_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.independent", "modulename": "clarena.cl_algorithms.independent", "kind": "module", "doc": "<p>The submodule in <code>cl_algorithms</code> for Independent learning algorithm.</p>\n"}, {"fullname": "clarena.cl_algorithms.independent.Independent", "modulename": "clarena.cl_algorithms.independent", "qualname": "Independent", "kind": "class", "doc": "<p>Independent learning algorithm.</p>\n\n<p>Another naive way for task-incremental learning aside from Finetuning. It assigns a new independent model for each task. This is a simple way to avoid catastrophic forgetting at the extreme cost of memory. It achieves the theoretical upper bound of performance in continual learning.</p>\n\n<p>We implement Independent as a subclass of Finetuning algorithm, as Independent has the same <code>forward()</code>, <code>training_step()</code>, <code>validation_step()</code> and <code>test_step()</code> method as <code>Finetuning</code> class.</p>\n", "bases": "clarena.cl_algorithms.finetuning.Finetuning"}, {"fullname": "clarena.cl_algorithms.independent.Independent.__init__", "modulename": "clarena.cl_algorithms.independent", "qualname": "Independent.__init__", "kind": "function", "doc": "<p>Initialize the Independent algorithm with the network. It has no additional hyperparameters.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone</strong> (<code>CLBackbone</code>): backbone network.</li>\n<li><strong>heads</strong> (<code>HeadsTIL</code> | <code>HeadsCIL</code>): output heads.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLBackbone</span>,</span><span class=\"param\">\t<span class=\"n\">heads</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_til</span><span class=\"o\">.</span><span class=\"n\">HeadsTIL</span> <span class=\"o\">|</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_cil</span><span class=\"o\">.</span><span class=\"n\">HeadsCIL</span></span>)</span>"}, {"fullname": "clarena.cl_algorithms.independent.Independent.original_backbone_state_dict", "modulename": "clarena.cl_algorithms.independent", "qualname": "Independent.original_backbone_state_dict", "kind": "variable", "doc": "<p>The original backbone network state dict is stored as the source of creating new independent backbone.</p>\n", "annotation": ": dict"}, {"fullname": "clarena.cl_algorithms.independent.Independent.backbones", "modulename": "clarena.cl_algorithms.independent", "qualname": "Independent.backbones", "kind": "variable", "doc": "<p>The list of independent backbones for each task. Keys are task IDs and values are the corresponding backbone.</p>\n", "annotation": ": dict[int, clarena.backbones.base.CLBackbone]"}, {"fullname": "clarena.cl_algorithms.independent.Independent.on_fit_start", "modulename": "clarena.cl_algorithms.independent", "qualname": "Independent.on_fit_start", "kind": "function", "doc": "<p>Initialize an independent backbone for <code>self.task_id</code>, duplicated from the original backbone.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.independent.Independent.on_train_end", "modulename": "clarena.cl_algorithms.independent", "qualname": "Independent.on_train_end", "kind": "function", "doc": "<p>Store the trained independent backbone for <code>self.task_id</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.independent.Independent.test_step", "modulename": "clarena.cl_algorithms.independent", "qualname": "Independent.test_step", "kind": "function", "doc": "<p>Test step for current task <code>self.task_id</code>, which tests for all seen tasks indexed by <code>dataloader_idx</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of test data.</li>\n<li><strong>dataloader_idx</strong> (<code>int</code>): the task ID of seen tasks to be tested. A default value of 0 is given otherwise the LightningModule will raise a <code>RuntimeError</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and other metrics from this test step. Keys (<code>str</code>) are the metrics names, and values (<code>Tensor</code>) are the metrics.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">dataloader_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.lwf", "modulename": "clarena.cl_algorithms.lwf", "kind": "module", "doc": "<p>The submodule in <code>cl_algorithms</code> for <a href=\"https://ieeexplore.ieee.org/abstract/document/8107520\">LwF (Learning without Forgetting) algorithm</a>.</p>\n"}, {"fullname": "clarena.cl_algorithms.lwf.LwF", "modulename": "clarena.cl_algorithms.lwf", "qualname": "LwF", "kind": "class", "doc": "<p><a href=\"https://ieeexplore.ieee.org/abstract/document/8107520\">LwF (Learning without Forgetting)</a> algorithm.</p>\n\n<p>A regularization-based continual learning approach that constrains the feature output of the model to be similar to that of the previous tasks. From the perspective of knowledge distillation, it distills previous tasks models into the training process for new task in the regularization term. It is a simple yet effective method for continual learning.</p>\n\n<p>We implement LwF as a subclass of Finetuning algorithm, as LwF has the same <code>forward()</code>, <code>validation_step()</code> and <code>test_step()</code> method as <code>Finetuning</code> class.</p>\n", "bases": "clarena.cl_algorithms.finetuning.Finetuning"}, {"fullname": "clarena.cl_algorithms.lwf.LwF.__init__", "modulename": "clarena.cl_algorithms.lwf", "qualname": "LwF.__init__", "kind": "function", "doc": "<p>Initialize the LwF algorithm with the network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone</strong> (<code>CLBackbone</code>): backbone network.</li>\n<li><strong>heads</strong> (<code>HeadsTIL</code> | <code>HeadsCIL</code>): output heads.</li>\n<li><strong>distillation_reg_factor</strong> (<code>float</code>): hyperparameter, the distillation regularization factor. It controls the strength of preventing forgetting.</li>\n<li><strong>distillation_reg_temperature</strong> (<code>float</code>): hyperparameter, the temperature in the distillation regularization. It controls the softness of the labels that the student model (here is the current model) learns from the teacher models (here are the previous models), thereby controlling the strength of the distillation. It controls the strength of preventing forgetting.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLBackbone</span>,</span><span class=\"param\">\t<span class=\"n\">heads</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_til</span><span class=\"o\">.</span><span class=\"n\">HeadsTIL</span> <span class=\"o\">|</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_cil</span><span class=\"o\">.</span><span class=\"n\">HeadsCIL</span>,</span><span class=\"param\">\t<span class=\"n\">distillation_reg_factor</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">distillation_reg_temperature</span><span class=\"p\">:</span> <span class=\"nb\">float</span></span>)</span>"}, {"fullname": "clarena.cl_algorithms.lwf.LwF.previous_task_backbones", "modulename": "clarena.cl_algorithms.lwf", "qualname": "LwF.previous_task_backbones", "kind": "variable", "doc": "<p>Store the backbone models of the previous tasks. Keys are task IDs (int) and values are the corresponding models. Each model is a <code>CLBackbone</code> after the corresponding previous task was trained.</p>\n\n<p>Some would argue that since we could store the model of the previous tasks, why don't we test the task directly with the stored model, instead of doing the less easier LwF thing? The thing is, LwF only uses the model of the previous tasks to train current and future tasks, which aggregate them into a single model. Once the training of the task is done, the storage for those parameters can be released. However, this make the future tasks not able to use LwF anymore, which is a disadvantage for LwF.</p>\n", "annotation": ": dict[int, clarena.backbones.base.CLBackbone]"}, {"fullname": "clarena.cl_algorithms.lwf.LwF.distillation_reg_factor", "modulename": "clarena.cl_algorithms.lwf", "qualname": "LwF.distillation_reg_factor", "kind": "variable", "doc": "<p>Store distillation regularization factor.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_algorithms.lwf.LwF.distillation_reg_temperature", "modulename": "clarena.cl_algorithms.lwf", "qualname": "LwF.distillation_reg_temperature", "kind": "variable", "doc": "<p>Store distillation regularization temperature.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_algorithms.lwf.LwF.distillation_reg", "modulename": "clarena.cl_algorithms.lwf", "qualname": "LwF.distillation_reg", "kind": "variable", "doc": "<p>Initialize and store the distillation regulariser.</p>\n"}, {"fullname": "clarena.cl_algorithms.lwf.LwF.sanity_check", "modulename": "clarena.cl_algorithms.lwf", "qualname": "LwF.sanity_check", "kind": "function", "doc": "<p>Check the sanity of the arguments.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.lwf.LwF.training_step", "modulename": "clarena.cl_algorithms.lwf", "qualname": "LwF.training_step", "kind": "function", "doc": "<p>Training step for current task <code>self.task_id</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of training data.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and other metrics from this training step. Keys (<code>str</code>) are the metrics names, and values (<code>Tensor</code>) are the metrics. Must include the key 'loss' which is total loss in the case of automatic optimization, according to PyTorch Lightning docs.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.lwf.LwF.on_train_end", "modulename": "clarena.cl_algorithms.lwf", "qualname": "LwF.on_train_end", "kind": "function", "doc": "<p>Store the backbone model after the training of a task.</p>\n\n<p>The model is stored in <code>self.previous_task_backbones</code> for constructing the regularization loss in the future tasks.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.nispa", "modulename": "clarena.cl_algorithms.nispa", "kind": "module", "doc": "<p>The submodule in <code>cl_algorithms</code> for NISPA (Neuro-Inspired Stability-Plasticity Adaptation) algorithm.</p>\n"}, {"fullname": "clarena.cl_algorithms.nispa.NISPA", "modulename": "clarena.cl_algorithms.nispa", "qualname": "NISPA", "kind": "class", "doc": "<p><a href=\"https://proceedings.mlr.press/v162/gurbuz22a.html\">NISPA (Neuro-Inspired Stability-Plasticity Adaptation)</a> algorithm.</p>\n\n<p>An architecture-based approach that selects neurons and weights through manual rules.</p>\n", "bases": "clarena.cl_algorithms.base.CLAlgorithm"}, {"fullname": "clarena.cl_algorithms.nispa.NISPA.__init__", "modulename": "clarena.cl_algorithms.nispa", "qualname": "NISPA.__init__", "kind": "function", "doc": "<p>Initialize the CL algorithm with the network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone</strong> (<code>CLBackbone</code>): backbone network.</li>\n<li><strong>heads</strong> (<code>HeadsTIL</code> | <code>HeadsCIL</code>): output heads.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">NISPAMaskBackbone</span>,</span><span class=\"param\">\t<span class=\"n\">heads</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_til</span><span class=\"o\">.</span><span class=\"n\">HeadsTIL</span>,</span><span class=\"param\">\t<span class=\"n\">num_epochs_per_phase</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">accuracy_fall_threshold</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">k</span><span class=\"p\">:</span> <span class=\"nb\">float</span></span>)</span>"}, {"fullname": "clarena.cl_algorithms.nispa.NISPA.num_epochs_per_phase", "modulename": "clarena.cl_algorithms.nispa", "qualname": "NISPA.num_epochs_per_phase", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "clarena.cl_algorithms.nispa.NISPA.accuracy_fall_threshold", "modulename": "clarena.cl_algorithms.nispa", "qualname": "NISPA.accuracy_fall_threshold", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "clarena.cl_algorithms.nispa.NISPA.k", "modulename": "clarena.cl_algorithms.nispa", "qualname": "NISPA.k", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "clarena.cl_algorithms.nispa.NISPA.candidate_stable_unit_mask_t", "modulename": "clarena.cl_algorithms.nispa", "qualname": "NISPA.candidate_stable_unit_mask_t", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Dict[str, torch.Tensor]"}, {"fullname": "clarena.cl_algorithms.nispa.NISPA.stable_unit_mask_t", "modulename": "clarena.cl_algorithms.nispa", "qualname": "NISPA.stable_unit_mask_t", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Dict[str, torch.Tensor]"}, {"fullname": "clarena.cl_algorithms.nispa.NISPA.plastic_unit_mask_t", "modulename": "clarena.cl_algorithms.nispa", "qualname": "NISPA.plastic_unit_mask_t", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Dict[str, torch.Tensor]"}, {"fullname": "clarena.cl_algorithms.nispa.NISPA.best_phase_acc", "modulename": "clarena.cl_algorithms.nispa", "qualname": "NISPA.best_phase_acc", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_algorithms.nispa.NISPA.phase_idx", "modulename": "clarena.cl_algorithms.nispa", "qualname": "NISPA.phase_idx", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "clarena.cl_algorithms.nispa.NISPA.on_train_start", "modulename": "clarena.cl_algorithms.nispa", "qualname": "NISPA.on_train_start", "kind": "function", "doc": "<p>Initialize all masks at the very beginning of Task 1.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.nispa.NISPA.clip_grad_by_frozen_mask", "modulename": "clarena.cl_algorithms.nispa", "qualname": "NISPA.clip_grad_by_frozen_mask", "kind": "function", "doc": "<p>Zero\u2010out grads on frozen connections.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.nispa.NISPA.forward", "modulename": "clarena.cl_algorithms.nispa", "qualname": "NISPA.forward", "kind": "function", "doc": "<p>Wrapper around the backbone\u2019s forward + the heads.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">],</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">],</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.nispa.NISPA.training_step", "modulename": "clarena.cl_algorithms.nispa", "qualname": "NISPA.training_step", "kind": "function", "doc": "<p>Here you compute and return the training loss and some additional metrics for e.g. the progress bar or\nlogger.</p>\n\n<p>Args:\n    batch: The output of your data iterable, normally a <code>~torch.utils.data.DataLoader</code>.\n    batch_idx: The index of this batch.\n    dataloader_idx: The index of the dataloader that produced this batch.\n        (only if multiple dataloaders used)</p>\n\n<p>Return:\n    - <code>~torch.Tensor</code> - The loss tensor\n    - <code>dict</code> - A dictionary which can include any keys, but must include the key <code>'loss'</code> in the case of\n      automatic optimization.\n    - <code>None</code> - In automatic optimization, this will skip to the next batch (but is not supported for\n      multi-GPU, TPU, or DeepSpeed). For manual optimization, this has no special meaning, as returning\n      the loss is not required.</p>\n\n<p>In this step you'd normally do the forward pass and calculate the loss for a batch.\nYou can also do fancier things like multiple forward passes or something model specific.</p>\n\n<p>Example::</p>\n\n<pre><code>def training_step(self, batch, batch_idx):\n    x, y, z = batch\n    out = self.encoder(x)\n    loss = self.loss(out, x)\n    return loss\n</code></pre>\n\n<p>To use multiple optimizers, you can switch to 'manual optimization' and control their stepping:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"k\">def</span><span class=\"w\"> </span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n    <span class=\"nb\">super</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">()</span>\n    <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">automatic_optimization</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>\n\n\n<span class=\"c1\"># Multiple optimizers (e.g.: GANs)</span>\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">training_step</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">batch</span><span class=\"p\">,</span> <span class=\"n\">batch_idx</span><span class=\"p\">):</span>\n    <span class=\"n\">opt1</span><span class=\"p\">,</span> <span class=\"n\">opt2</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">optimizers</span><span class=\"p\">()</span>\n\n    <span class=\"c1\"># do training_step with encoder</span>\n    <span class=\"o\">...</span>\n    <span class=\"n\">opt1</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n    <span class=\"c1\"># do training_step with decoder</span>\n    <span class=\"o\">...</span>\n    <span class=\"n\">opt2</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n</code></pre>\n</div>\n\n<p>Note:\n    When <code>accumulate_grad_batches</code> &gt; 1, the loss returned here will be automatically\n    normalized by <code>accumulate_grad_batches</code> internally.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.nispa.NISPA.on_validation_epoch_end", "modulename": "clarena.cl_algorithms.nispa", "qualname": "NISPA.on_validation_epoch_end", "kind": "function", "doc": "<p>Every num_epochs_per_phase epochs, do a phase\u2010end check, selection, and rewire.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.nispa.NISPA.select_candidate_stable_units", "modulename": "clarena.cl_algorithms.nispa", "qualname": "NISPA.select_candidate_stable_units", "kind": "function", "doc": "<p>Pick the top\u2010activated units until we reach <code>activation_fraction</code> of total.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">activation_fraction</span><span class=\"p\">:</span> <span class=\"nb\">float</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.nispa.NISPA.drop_connections_plastic_to_stable", "modulename": "clarena.cl_algorithms.nispa", "qualname": "NISPA.drop_connections_plastic_to_stable", "kind": "function", "doc": "<p>Drop connections <em>from</em> plastic units <em>into</em> stable units:\ni.e. weight[stable_out, plastic_in] \u2192 0, bias[stable_out] \u2192 0.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.nispa.NISPA.grow_new_connections", "modulename": "clarena.cl_algorithms.nispa", "qualname": "NISPA.grow_new_connections", "kind": "function", "doc": "<p>Regrow new connections <em>among plastic units</em> to keep the network at a fixed density.\nFor simplicity, we randomly reassign the dropped number of connections.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">dropped</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.nispa.NISPA.on_train_end", "modulename": "clarena.cl_algorithms.nispa", "qualname": "NISPA.on_train_end", "kind": "function", "doc": "<p>Called at the very end of a task:\nfreeze <em>all</em> connections that were used (mask = 1) as part of this task.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.nispa.NISPA.validation_step", "modulename": "clarena.cl_algorithms.nispa", "qualname": "NISPA.validation_step", "kind": "function", "doc": "<p>Operates on a single batch of data from the validation set. In this step you'd might generate examples or\ncalculate anything of interest like accuracy.</p>\n\n<p>Args:\n    batch: The output of your data iterable, normally a <code>~torch.utils.data.DataLoader</code>.\n    batch_idx: The index of this batch.\n    dataloader_idx: The index of the dataloader that produced this batch.\n        (only if multiple dataloaders used)</p>\n\n<p>Return:\n    - <code>~torch.Tensor</code> - The loss tensor\n    - <code>dict</code> - A dictionary. Can include any keys, but must include the key <code>'loss'</code>.\n    - <code>None</code> - Skip to the next batch.</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"c1\"># if you have one val dataloader:</span>\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">validation_step</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">batch</span><span class=\"p\">,</span> <span class=\"n\">batch_idx</span><span class=\"p\">):</span> <span class=\"o\">...</span>\n\n\n<span class=\"c1\"># if you have multiple val dataloaders:</span>\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">validation_step</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">batch</span><span class=\"p\">,</span> <span class=\"n\">batch_idx</span><span class=\"p\">,</span> <span class=\"n\">dataloader_idx</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">):</span> <span class=\"o\">...</span>\n</code></pre>\n</div>\n\n<p>Examples::</p>\n\n<pre><code># CASE 1: A single validation dataset\ndef validation_step(self, batch, batch_idx):\n    x, y = batch\n\n    # implement your own\n    out = self(x)\n    loss = self.loss(out, y)\n\n    # log 6 example images\n    # or generated text... or whatever\n    sample_imgs = x[:6]\n    grid = torchvision.utils.make_grid(sample_imgs)\n    self.logger.experiment.add_image('example_images', grid, 0)\n\n    # calculate acc\n    labels_hat = torch.argmax(out, dim=1)\n    val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)\n\n    # log the outputs!\n    self.log_dict({'val_loss': loss, 'val_acc': val_acc})\n</code></pre>\n\n<p>If you pass in multiple val dataloaders, <code>validation_step()</code> will have an additional argument. We recommend\nsetting the default value of 0 so that you can quickly switch between single and multiple dataloaders.</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"c1\"># CASE 2: multiple validation dataloaders</span>\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">validation_step</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">batch</span><span class=\"p\">,</span> <span class=\"n\">batch_idx</span><span class=\"p\">,</span> <span class=\"n\">dataloader_idx</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">):</span>\n    <span class=\"c1\"># dataloader_idx tells you which dataset this is.</span>\n    <span class=\"o\">...</span>\n</code></pre>\n</div>\n\n<p>Note:\n    If you don't need to validate you don't need to implement this method.</p>\n\n<p>Note:\n    When the <code>validation_step()</code> is called, the model has been put in eval mode\n    and PyTorch gradients have been disabled. At the end of validation,\n    the model goes back to training mode and gradients are enabled.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.nispa.NISPA.test_step", "modulename": "clarena.cl_algorithms.nispa", "qualname": "NISPA.test_step", "kind": "function", "doc": "<p>Operates on a single batch of data from the test set. In this step you'd normally generate examples or\ncalculate anything of interest such as accuracy.</p>\n\n<p>Args:\n    batch: The output of your data iterable, normally a <code>~torch.utils.data.DataLoader</code>.\n    batch_idx: The index of this batch.\n    dataloader_idx: The index of the dataloader that produced this batch.\n        (only if multiple dataloaders used)</p>\n\n<p>Return:\n    - <code>~torch.Tensor</code> - The loss tensor\n    - <code>dict</code> - A dictionary. Can include any keys, but must include the key <code>'loss'</code>.\n    - <code>None</code> - Skip to the next batch.</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"c1\"># if you have one test dataloader:</span>\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">test_step</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">batch</span><span class=\"p\">,</span> <span class=\"n\">batch_idx</span><span class=\"p\">):</span> <span class=\"o\">...</span>\n\n\n<span class=\"c1\"># if you have multiple test dataloaders:</span>\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">test_step</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">batch</span><span class=\"p\">,</span> <span class=\"n\">batch_idx</span><span class=\"p\">,</span> <span class=\"n\">dataloader_idx</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">):</span> <span class=\"o\">...</span>\n</code></pre>\n</div>\n\n<p>Examples::</p>\n\n<pre><code># CASE 1: A single test dataset\ndef test_step(self, batch, batch_idx):\n    x, y = batch\n\n    # implement your own\n    out = self(x)\n    loss = self.loss(out, y)\n\n    # log 6 example images\n    # or generated text... or whatever\n    sample_imgs = x[:6]\n    grid = torchvision.utils.make_grid(sample_imgs)\n    self.logger.experiment.add_image('example_images', grid, 0)\n\n    # calculate acc\n    labels_hat = torch.argmax(out, dim=1)\n    test_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)\n\n    # log the outputs!\n    self.log_dict({'test_loss': loss, 'test_acc': test_acc})\n</code></pre>\n\n<p>If you pass in multiple test dataloaders, <code>test_step()</code> will have an additional argument. We recommend\nsetting the default value of 0 so that you can quickly switch between single and multiple dataloaders.</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"c1\"># CASE 2: multiple test dataloaders</span>\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">test_step</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">batch</span><span class=\"p\">,</span> <span class=\"n\">batch_idx</span><span class=\"p\">,</span> <span class=\"n\">dataloader_idx</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">):</span>\n    <span class=\"c1\"># dataloader_idx tells you which dataset this is.</span>\n    <span class=\"o\">...</span>\n</code></pre>\n</div>\n\n<p>Note:\n    If you don't need to test you don't need to implement this method.</p>\n\n<p>Note:\n    When the <code>test_step()</code> is called, the model has been put in eval mode and\n    PyTorch gradients have been disabled. At the end of the test epoch, the model goes back\n    to training mode and gradients are enabled.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">dataloader_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.random", "modulename": "clarena.cl_algorithms.random", "kind": "module", "doc": "<p>The submodule in <code>cl_algorithms</code> for Random algorithm.</p>\n"}, {"fullname": "clarena.cl_algorithms.random.Random", "modulename": "clarena.cl_algorithms.random", "qualname": "Random", "kind": "class", "doc": "<p>Random stratified model.</p>\n\n<p>Pass the training step and simply use the randomly initialized model to predict the test data. This serves as a reference model to compute forgetting rate. See chapter 4 in <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task) paper</a>.</p>\n\n<p>We implement Random as a subclass of Finetuning algorithm, as Random has the same <code>forward()</code>, <code>validation_step()</code> and <code>test_step()</code> method as <code>Finetuning</code> class.</p>\n", "bases": "clarena.cl_algorithms.finetuning.Finetuning"}, {"fullname": "clarena.cl_algorithms.random.Random.__init__", "modulename": "clarena.cl_algorithms.random", "qualname": "Random.__init__", "kind": "function", "doc": "<p>Initialize the Random algorithm with the network. It has no additional hyperparameters.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone</strong> (<code>CLBackbone</code>): backbone network.</li>\n<li><strong>heads</strong> (<code>HeadsTIL</code> | <code>HeadsCIL</code>): output heads.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLBackbone</span>,</span><span class=\"param\">\t<span class=\"n\">heads</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_til</span><span class=\"o\">.</span><span class=\"n\">HeadsTIL</span> <span class=\"o\">|</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_cil</span><span class=\"o\">.</span><span class=\"n\">HeadsCIL</span></span>)</span>"}, {"fullname": "clarena.cl_algorithms.random.Random.automatic_optimization", "modulename": "clarena.cl_algorithms.random", "qualname": "Random.automatic_optimization", "kind": "variable", "doc": "<p>If set to <code>False</code> you are responsible for calling <code>.backward()</code>, <code>.step()</code>, <code>.zero_grad()</code>.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.cl_algorithms.random.Random.training_step", "modulename": "clarena.cl_algorithms.random", "qualname": "Random.training_step", "kind": "function", "doc": "<p>Pass the training step for current task <code>self.task_id</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of training data.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and other metrics from this training step. Keys (<code>str</code>) are the metrics names, and values (<code>Tensor</code>) are the metrics. Must include the key 'loss' which is total loss in the case of automatic optimization, according to PyTorch Lightning docs.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.regularizers", "modulename": "clarena.cl_algorithms.regularizers", "kind": "module", "doc": "<h1 id=\"continual-learning-regularizers\">Continual Learning Regularizers</h1>\n\n<p>This submodule provides the <strong>regularizers</strong> which are added to the loss function of corresponding continual learning algorithms. It can promote forgetting preventing which is the major mechanism in regularization-based approaches, or for other purposes.</p>\n\n<p>The regularizers inherit from <code>nn.Module</code>.</p>\n\n<p>Please note that this is an API documantation. Please refer to the main documentation pages for more information about the regularizers:</p>\n\n<ul>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/implement-your-cl-modules/cl-algorithm#sec-regularizers\"><strong>Implement custom regularizers in CL algorithms</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/posts/continual-learning-beginners-guide#sec-regularisation-based-approaches\"><strong>A Beginners' Guide to Continual Learning (Regularization-based Approaches)</strong></a></li>\n</ul>\n"}, {"fullname": "clarena.cl_algorithms.regularizers.distillation", "modulename": "clarena.cl_algorithms.regularizers.distillation", "kind": "module", "doc": "<p>The submodule in <code>regularizers</code> for distillation regularization.</p>\n"}, {"fullname": "clarena.cl_algorithms.regularizers.distillation.DistillationReg", "modulename": "clarena.cl_algorithms.regularizers.distillation", "qualname": "DistillationReg", "kind": "class", "doc": "<p>Distillation regularizer. This is the core of <a href=\"https://research.google/pubs/distilling-the-knowledge-in-a-neural-network/\">knowledge distillation</a> used as a regularizer in continual learning.</p>\n\n<p>$$R(\\theta^{\\text{student}}) = \\text{factor} * \\frac1N \\sum_{(\\mathbf{x}, y)\\in \\mathcal{D}} \\text{distance}\\left(f(\\mathbf{x};\\theta^{\\text{student}}),f(\\mathbf{x};\\theta^{\\text{teacher}})\\right)$$</p>\n\n<p>It promotes the target (student) model output logits $f(\\mathbf{x};\\theta^{\\text{student}})$ not changing too much from the reference (teacher) model output logits $f(\\mathbf{x};\\theta^{\\text{teacher}})$. The loss is averaged over the dataset $\\mathcal{D}$.</p>\n\n<p>It is used in:</p>\n\n<ul>\n<li><a href=\"https://ieeexplore.ieee.org/abstract/document/8107520\">LwF (Learning without Forgetting) algorithm</a>: as a distillation regularizer for the output logits by current task model to be closer to output logits by previous tasks models. It uses a modified cross entropy as the distance. See equation (2) (3) in the <a href=\"https://ieeexplore.ieee.org/abstract/document/8107520\">LwF paper</a>.</li>\n</ul>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "clarena.cl_algorithms.regularizers.distillation.DistillationReg.__init__", "modulename": "clarena.cl_algorithms.regularizers.distillation", "qualname": "DistillationReg.__init__", "kind": "function", "doc": "<p>Initialize the regulariser.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>factor</strong> (<code>float</code>): the regularization factor.</li>\n<li><strong>temperature</strong> (<code>float</code>): the temperature of the distillation, should be a positive float.</li>\n<li><strong>distance</strong> (<code>str</code>): the type of distance function used in the distillation, should be one of the following:\n<ol>\n<li>\"lwf_cross_entropy\": the modified cross entropy loss from LwF. See equation (3) in the <a href=\"https://ieeexplore.ieee.org/abstract/document/8107520\">LwF paper</a>.</li>\n</ol></li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">factor</span><span class=\"p\">:</span> <span class=\"nb\">float</span>, </span><span class=\"param\"><span class=\"n\">temperature</span><span class=\"p\">:</span> <span class=\"nb\">float</span>, </span><span class=\"param\"><span class=\"n\">distance</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span>)</span>"}, {"fullname": "clarena.cl_algorithms.regularizers.distillation.DistillationReg.factor", "modulename": "clarena.cl_algorithms.regularizers.distillation", "qualname": "DistillationReg.factor", "kind": "variable", "doc": "<p>Store the regularisation factor for distillation.</p>\n"}, {"fullname": "clarena.cl_algorithms.regularizers.distillation.DistillationReg.temperature", "modulename": "clarena.cl_algorithms.regularizers.distillation", "qualname": "DistillationReg.temperature", "kind": "variable", "doc": "<p>Store the temperature of the distillation.</p>\n"}, {"fullname": "clarena.cl_algorithms.regularizers.distillation.DistillationReg.distance", "modulename": "clarena.cl_algorithms.regularizers.distillation", "qualname": "DistillationReg.distance", "kind": "variable", "doc": "<p>Store the type of distance function used in the distillation.</p>\n"}, {"fullname": "clarena.cl_algorithms.regularizers.distillation.DistillationReg.forward", "modulename": "clarena.cl_algorithms.regularizers.distillation", "qualname": "DistillationReg.forward", "kind": "function", "doc": "<p>Calculate the regularisation loss.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>student_logits</strong> (<code>Tensor</code>): the output logits of target (student) model to learn the knowledge from distillation. In LwF, it's the model of current training task.</li>\n<li><strong>teacher_logits</strong> (<code>Tensor</code>): the output logits of reference (teacher) model that knowledge is distilled. In LwF, it's the model of one of the previous tasks.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>reg</strong> (<code>Tensor</code>): the distillation regularisation value.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">student_logits</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"n\">teacher_logits</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.regularizers.hat_mask_sparsity", "modulename": "clarena.cl_algorithms.regularizers.hat_mask_sparsity", "kind": "module", "doc": "<p>The submodule in <code>regularizers</code> for <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task)</a> mask sparsity regularization.</p>\n"}, {"fullname": "clarena.cl_algorithms.regularizers.hat_mask_sparsity.HATMaskSparsityReg", "modulename": "clarena.cl_algorithms.regularizers.hat_mask_sparsity", "qualname": "HATMaskSparsityReg", "kind": "class", "doc": "<p>Mask sparsity regularizer of <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task)</a>.</p>\n\n<p>$$\nR\\left(\\textsf{M}^t,\\textsf{M}^{<t}\\right)=\\text{factor} * \\frac{\\sum_{l=1}^{L-1}\\sum_{i=1}^{N_l}m_{l,i}^t\\left(1-m_{l,i}^{<t}\\right)}{\\sum_{l=1}^{L-1}\\sum_{i=1}^{N_l}\\left(1-m_{l,i}^{<t}\\right)}\n$$</p>\n\n<p>It promotes the low capacity usage that is reflected by occupation of masks in the parameter space.</p>\n\n<p>See chapter 2.6 \"Promoting Low Capacity Usage\" in <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "clarena.cl_algorithms.regularizers.hat_mask_sparsity.HATMaskSparsityReg.__init__", "modulename": "clarena.cl_algorithms.regularizers.hat_mask_sparsity", "qualname": "HATMaskSparsityReg.__init__", "kind": "function", "doc": "<p>Initialize the regularizer.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>factor</strong> (<code>float</code>): the regularization factor.</li>\n<li><strong>mode</strong> (<code>str</code>): the mode of mask sparsity regularization, should be one of the following:\n<ol>\n<li>'original' (default): the original mask sparsity regularization in HAT paper.</li>\n<li>'cross': the cross version mask sparsity regularization.</li>\n</ol></li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">factor</span><span class=\"p\">:</span> <span class=\"nb\">float</span>, </span><span class=\"param\"><span class=\"n\">mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;original&#39;</span></span>)</span>"}, {"fullname": "clarena.cl_algorithms.regularizers.hat_mask_sparsity.HATMaskSparsityReg.factor", "modulename": "clarena.cl_algorithms.regularizers.hat_mask_sparsity", "qualname": "HATMaskSparsityReg.factor", "kind": "variable", "doc": "<p>Store the regularization factor for mask sparsity.</p>\n"}, {"fullname": "clarena.cl_algorithms.regularizers.hat_mask_sparsity.HATMaskSparsityReg.mode", "modulename": "clarena.cl_algorithms.regularizers.hat_mask_sparsity", "qualname": "HATMaskSparsityReg.mode", "kind": "variable", "doc": "<p>Store the mode of mask sparsity regularization.</p>\n"}, {"fullname": "clarena.cl_algorithms.regularizers.hat_mask_sparsity.HATMaskSparsityReg.forward", "modulename": "clarena.cl_algorithms.regularizers.hat_mask_sparsity", "qualname": "HATMaskSparsityReg.forward", "kind": "function", "doc": "<p>Calculate the mask sparsity regularization loss.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>mask</strong> (<code>dict[str, Tensor]</code>): the mask for the current task.</li>\n<li><strong>previous_cumulative_mask</strong> (<code>dict[str, Tensor]</code>): the cumulative mask for the previous tasks.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>reg</strong> (<code>Tensor</code>): the mask sparsity regularization value.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">mask</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">previous_cumulative_mask</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.regularizers.hat_mask_sparsity.HATMaskSparsityReg.original_reg", "modulename": "clarena.cl_algorithms.regularizers.hat_mask_sparsity", "qualname": "HATMaskSparsityReg.original_reg", "kind": "function", "doc": "<p>Calculate the original mask sparsity regularization loss in HAT paper.</p>\n\n<p>See chapter 2.6 \"Promoting Low Capacity Usage\" in <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>mask</strong> (<code>dict[str, Tensor]</code>): the mask for the current task. The $\\mathrm{A}^t$ in the paper.</li>\n<li><strong>previous_cumulative_mask</strong> (<code>dict[str, Tensor]</code>): the cumulative mask for the previous tasks. The $\\mathrm{A}^{<t}$ in the paper.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>reg</strong> (<code>Tensor</code>): the original mask sparsity regularization loss.</li>\n<li><strong>network_sparsity</strong> (<code>dict[str, Tensor]</code>): the network sparsity for each layer. Keys are layer names and values are the network sparsity value.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">mask</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">previous_cumulative_mask</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.regularizers.hat_mask_sparsity.HATMaskSparsityReg.cross_reg", "modulename": "clarena.cl_algorithms.regularizers.hat_mask_sparsity", "qualname": "HATMaskSparsityReg.cross_reg", "kind": "function", "doc": "<p>Calculate the cross mask sparsity regularization loss. This is an attempting improvement by me to the original regularization, which not only considers the sparsity in available units but also the density in the units occupied by previous tasks.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>mask</strong> (<code>dict[str, Tensor]</code>): the mask for the current task. The $\\mathrm{A}^t$ in the paper.</li>\n<li><strong>previous_cumulative_mask</strong> (<code>dict[str, Tensor]</code>): the cumulative mask for the previous tasks. The $\\mathrm{A}^{<t}$ in the paper.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>reg</strong> (<code>Tensor</code>): the cross mask sparsity regularization loss.</li>\n<li><strong>network_sparsity</strong> (<code>dict[str, Tensor]</code>): the network sparsity for each layer. Keys are layer names and values are the network sparsity value.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">mask</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">previous_cumulative_mask</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.regularizers.parameter_change", "modulename": "clarena.cl_algorithms.regularizers.parameter_change", "kind": "module", "doc": "<p>The submodule in <code>regularizers</code> for parameter change regularization.</p>\n"}, {"fullname": "clarena.cl_algorithms.regularizers.parameter_change.ParameterChangeReg", "modulename": "clarena.cl_algorithms.regularizers.parameter_change", "qualname": "ParameterChangeReg", "kind": "class", "doc": "<p>Parameter change regularizer.</p>\n\n<p>$$R(\\theta) = \\text{factor} * \\sum_i w_i \\|\\theta_i - \\theta^\\star_i\\|^2$$</p>\n\n<p>It promotes the target set of parameters $\\theta = {\\theta_i}_i$ not changing too much from another set of parameters $\\theta^\\star = {\\theta^\\star_i}_i$. The parameter distance here is $L^2$ distance. The regularization can be parameter-wise weighted, i.e. $w_i$ in the formula.</p>\n\n<p>It is used in:</p>\n\n<ul>\n<li><a href=\"https://www.pnas.org/doi/10.1073/pnas.1611835114\">L2 Regularisation algorithm</a>: as a L2 regularizer for the current task parameters to prevent them from changing too much from the previous task parameters.</li>\n<li><a href=\"https://www.pnas.org/doi/10.1073/pnas.1611835114\">EWC (Elastic Weight Consolidation) algorithm</a>: as a weighted L2 regularizer for the current task parameters to prevent them from changing too much from the previous task parameters. The regularization weights are parameter importance measure calculated from fisher information. See equation 3 in the <a href=\"https://www.pnas.org/doi/10.1073/pnas.1611835114\">EWC paper</a>.</li>\n</ul>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "clarena.cl_algorithms.regularizers.parameter_change.ParameterChangeReg.__init__", "modulename": "clarena.cl_algorithms.regularizers.parameter_change", "qualname": "ParameterChangeReg.__init__", "kind": "function", "doc": "<p>Initialize the regularizer.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>factor</strong> (<code>float</code>): the regularization factor. Note that it is $\\frac{\\lambda}{2}$ rather than $\\lambda$ in the <a href=\"https://www.pnas.org/doi/10.1073/pnas.1611835114\">EWC paper</a>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">factor</span><span class=\"p\">:</span> <span class=\"nb\">float</span></span>)</span>"}, {"fullname": "clarena.cl_algorithms.regularizers.parameter_change.ParameterChangeReg.factor", "modulename": "clarena.cl_algorithms.regularizers.parameter_change", "qualname": "ParameterChangeReg.factor", "kind": "variable", "doc": "<p>Store the regularization factor for parameter change.</p>\n"}, {"fullname": "clarena.cl_algorithms.regularizers.parameter_change.ParameterChangeReg.forward", "modulename": "clarena.cl_algorithms.regularizers.parameter_change", "qualname": "ParameterChangeReg.forward", "kind": "function", "doc": "<p>Calculate the regularization loss.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>target_model</strong> (nn.Module): the model of the target parameters. In EWC, it's the model of current training task.</li>\n<li><strong>ref_model</strong> (nn.Module): the reference model that you want target model parameters to prevent changing from. The reference model must have the same structure as the target model. In EWC, it's the model of one of the previous tasks.</li>\n<li><strong>weights</strong> (dict[str, Tensor]): the regularization weight for each parameter. Keys are parameter names and values are the weight tensors. The weight tensors must match the shape of model parameters. In EWC, it's the importance measure of each parameter, calculated from fisher information thing.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>reg</strong> (Tensor): the parameter change regularization value.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">target_model</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"n\">ref_model</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"n\">weights</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.wsn", "modulename": "clarena.cl_algorithms.wsn", "kind": "module", "doc": "<p>The submodule in <code>cl_algorithms</code> for <a href=\"https://proceedings.mlr.press/v162/kang22b/kang22b.pdf\">WSN (Winning Subnetworks)</a> algorithm.</p>\n"}, {"fullname": "clarena.cl_algorithms.wsn.WSN", "modulename": "clarena.cl_algorithms.wsn", "qualname": "WSN", "kind": "class", "doc": "<p><a href=\"https://proceedings.mlr.press/v162/kang22b/kang22b.pdf\">WSN (Winning Subnetworks)</a> algorithm.</p>\n\n<p>An architecture-based continual learning approach that trains learnable parameter-wise scores and selects the most scored c% of network parameters per task.</p>\n", "bases": "clarena.cl_algorithms.base.CLAlgorithm"}, {"fullname": "clarena.cl_algorithms.wsn.WSN.__init__", "modulename": "clarena.cl_algorithms.wsn", "qualname": "WSN.__init__", "kind": "function", "doc": "<p>Initialize the WSN algorithm with the network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone</strong> (<code>WSNMaskBackbone</code>): must be a backbone network with the WSN mask mechanism.</li>\n<li><strong>heads</strong> (<code>HeadsTIL</code>): output heads. WSN only supports TIL (Task-Incremental Learning).</li>\n<li><strong>mask_percentage</strong> (<code>float</code>): the percentage $c\\%$ of parameters to be used for each task. See Sec. 3 and Eq. (4) in the <a href=\"https://proceedings.mlr.press/v162/kang22b/kang22b.pdf\">WSN paper</a>.</li>\n<li><strong>parameter_score_init_mode</strong> (<code>str</code>): the initialization mode for parameter scores, must be one of:\n<ol>\n<li>'default': the default initialization in the original WSN code.</li>\n<li>'N01': standard normal distribution $N(0, 1)$.</li>\n<li>'U01': uniform distribution $U(0, 1)$.</li>\n</ol></li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">WSNMaskBackbone</span>,</span><span class=\"param\">\t<span class=\"n\">heads</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_til</span><span class=\"o\">.</span><span class=\"n\">HeadsTIL</span>,</span><span class=\"param\">\t<span class=\"n\">mask_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">parameter_score_init_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;default&#39;</span></span>)</span>"}, {"fullname": "clarena.cl_algorithms.wsn.WSN.mask_percentage", "modulename": "clarena.cl_algorithms.wsn", "qualname": "WSN.mask_percentage", "kind": "variable", "doc": "<p>The percentage of parameters to be used for each task.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_algorithms.wsn.WSN.parameter_score_init_mode", "modulename": "clarena.cl_algorithms.wsn", "qualname": "WSN.parameter_score_init_mode", "kind": "variable", "doc": "<p>The parameter score initialization mode.</p>\n", "annotation": ": str"}, {"fullname": "clarena.cl_algorithms.wsn.WSN.weight_masks", "modulename": "clarena.cl_algorithms.wsn", "qualname": "WSN.weight_masks", "kind": "variable", "doc": "<p>The binary weight mask of each previous task percentile-gated from the weight score. Keys are task IDs and values are the corresponding mask. Each mask is a dict where keys are layer names and values are the binary mask tensor for the layer. The mask tensor has the same size (output features, input features) as weight.</p>\n", "annotation": ": dict[int, dict[str, torch.Tensor]]"}, {"fullname": "clarena.cl_algorithms.wsn.WSN.bias_masks", "modulename": "clarena.cl_algorithms.wsn", "qualname": "WSN.bias_masks", "kind": "variable", "doc": "<p>The binary bias mask of each previous task percentile-gated from the bias score. Keys are task IDs and values are the corresponding mask. Each mask is a dict where keys are layer names and values are the binary mask tensor for the layer. The mask tensor has the same size (output features, ) as bias. If the layer doesn't have bias, it is <code>None</code>.</p>\n", "annotation": ": dict[int, dict[str, torch.Tensor]]"}, {"fullname": "clarena.cl_algorithms.wsn.WSN.cumulative_weight_mask_for_previous_tasks", "modulename": "clarena.cl_algorithms.wsn", "qualname": "WSN.cumulative_weight_mask_for_previous_tasks", "kind": "variable", "doc": "<p>The cumulative binary weight mask $\\mathbf{M}_{t-1}$ of previous tasks $1, \\cdots, t-1$, percentile-gated from the weight score. It is a dict where keys are layer names and values are the binary mask tensors for the layers. The mask tensor has the same size (output features, input features) as weight.</p>\n", "annotation": ": dict[str, torch.Tensor]"}, {"fullname": "clarena.cl_algorithms.wsn.WSN.cumulative_bias_mask_for_previous_tasks", "modulename": "clarena.cl_algorithms.wsn", "qualname": "WSN.cumulative_bias_mask_for_previous_tasks", "kind": "variable", "doc": "<p>The cumulative binary bias mask $\\mathbf{M}_{t-1}$ of previous tasks $1, \\cdots, t-1$, percentile-gated from the bias score. It is a dict where keys are layer names and values are the binary mask tensor for the layer. The mask tensor has the same size (output features, ) as bias. If the layer doesn't have bias, it is <code>None</code>.</p>\n", "annotation": ": dict[str, dict[str, torch.Tensor]]"}, {"fullname": "clarena.cl_algorithms.wsn.WSN.automatic_optimization", "modulename": "clarena.cl_algorithms.wsn", "qualname": "WSN.automatic_optimization", "kind": "variable", "doc": "<p>If set to <code>False</code> you are responsible for calling <code>.backward()</code>, <code>.step()</code>, <code>.zero_grad()</code>.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.cl_algorithms.wsn.WSN.sanity_check", "modulename": "clarena.cl_algorithms.wsn", "qualname": "WSN.sanity_check", "kind": "function", "doc": "<p>Check the sanity of the arguments.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.wsn.WSN.on_train_start", "modulename": "clarena.cl_algorithms.wsn", "qualname": "WSN.on_train_start", "kind": "function", "doc": "<p>Initialize the parameter scores before training the next task and initialize the cumulative masks at the beginning of the first task.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.wsn.WSN.clip_grad_by_mask", "modulename": "clarena.cl_algorithms.wsn", "qualname": "WSN.clip_grad_by_mask", "kind": "function", "doc": "<p>Clip the gradients by the cumulative masks. The gradients are multiplied by (1 - cumulative_previous_mask) to keep previously masked parameters fixed. See Eq. (4) in the <a href=\"https://proceedings.mlr.press/v162/kang22b/kang22b.pdf\">WSN paper</a>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.wsn.WSN.forward", "modulename": "clarena.cl_algorithms.wsn", "qualname": "WSN.forward", "kind": "function", "doc": "<p>The forward pass for data from task <code>task_id</code>. Note that it is nothing to do with <code>forward()</code> method in <code>nn.Module</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): the input tensor from data.</li>\n<li><strong>stage</strong> (<code>str</code>): the stage of the forward pass, should be one of:\n<ol>\n<li>'train': training stage.</li>\n<li>'validation': validation stage.</li>\n<li>'test': testing stage.</li>\n</ol></li>\n<li><strong>task_id</strong> (<code>int</code> | <code>None</code>): the task ID where the data are from. If the stage is 'train' or 'validation', it should be the current task <code>self.task_id</code>. If the stage is 'test', it could be from any seen task (TIL uses the provided task IDs for testing).</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>logits</strong> (<code>Tensor</code>): the output logits tensor.</li>\n<li><strong>weight_mask</strong> (<code>dict[str, Tensor]</code>): the weight mask for the current task. Key (<code>str</code>) is layer name, value (<code>Tensor</code>) is the mask tensor. The mask tensor has same (output features, input features) as weight.</li>\n<li><strong>bias_mask</strong> (<code>dict[str, Tensor]</code>): the bias mask for the current task. Key (<code>str</code>) is layer name, value (<code>Tensor</code>) is the mask tensor. The mask tensor has same (output features, ) as bias. If the layer doesn't have bias, it is <code>None</code>.</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): the hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name, value (<code>Tensor</code>) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.wsn.WSN.training_step", "modulename": "clarena.cl_algorithms.wsn", "qualname": "WSN.training_step", "kind": "function", "doc": "<p>Training step for current task <code>self.task_id</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of training data.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary containing loss and other metrics from this training step. Keys (<code>str</code>) are the metrics names, and values (<code>Tensor</code>) are the metrics. Must include the key 'loss' which is total loss in the case of automatic optimization, according to PyTorch Lightning docs. For WSN, it includes 'weight_mask' and 'bias_mask' for logging.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.wsn.WSN.on_train_end", "modulename": "clarena.cl_algorithms.wsn", "qualname": "WSN.on_train_end", "kind": "function", "doc": "<p>Store the weight and bias masks and update the cumulative masks after training the task.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.wsn.WSN.validation_step", "modulename": "clarena.cl_algorithms.wsn", "qualname": "WSN.validation_step", "kind": "function", "doc": "<p>Validation step for current task <code>self.task_id</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of validation data.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and other metrics from this validation step. Keys (<code>str</code>) are the metrics names, and values (<code>Tensor</code>) are the metrics.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.wsn.WSN.test_step", "modulename": "clarena.cl_algorithms.wsn", "qualname": "WSN.test_step", "kind": "function", "doc": "<p>Test step for current task <code>self.task_id</code>, which tests for all seen tasks indexed by <code>dataloader_idx</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of test data.</li>\n<li><strong>dataloader_idx</strong> (<code>int</code>): the task ID of seen tasks to be tested. A default value of 0 is given otherwise the LightningModule will raise a <code>RuntimeError</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and other metrics from this test step. Keys (<code>str</code>) are the metrics names, and values (<code>Tensor</code>) are the metrics.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">dataloader_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets", "modulename": "clarena.cl_datasets", "kind": "module", "doc": "<h1 id=\"continual-learning-datasets\">Continual Learning Datasets</h1>\n\n<p>This submodule provides the <strong>continual learning datasets</strong> that can be used in CLArena.</p>\n\n<p>Here are the base classes for continual learning datasets, which inherit from Lightning <code>LightningDataModule</code>:</p>\n\n<ul>\n<li><code>CLDataset</code>: The base class for continual learning datasets.</li>\n<li><code>CLPermutedDataset</code>: The base class for permuted continual learning datasets. A child class of <code>CLDataset</code>.</li>\n<li><code>CLSplitDataset</code>: The base class for split continual learning datasets. A child class of <code>CLDataset</code>.</li>\n<li><code>CLCombinedDataset</code>: The base class for combined continual learning datasets. A child class of <code>CLDataset</code>.</li>\n</ul>\n\n<p>Please note that this is an API documantation. Please refer to the main documentation pages for more information about how to configure and implement continual learning datasets:</p>\n\n<ul>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/continual-learning/configure-main-experiment/cl-dataset\"><strong>Configure CL Dataset</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/custom-implementation/cl-dataset\"><strong>Implement Custom CL Dataset</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/posts/continual-learning-beginners-guide#sec-CL-dataset\"><strong>A Beginners' Guide to Continual Learning (CL Dataset)</strong></a></li>\n</ul>\n"}, {"fullname": "clarena.cl_datasets.CLDataset", "modulename": "clarena.cl_datasets", "qualname": "CLDataset", "kind": "class", "doc": "<p>The base class of continual learning datasets, inherited from <code>LightningDataModule</code>.</p>\n", "bases": "lightning.pytorch.core.datamodule.LightningDataModule"}, {"fullname": "clarena.cl_datasets.CLDataset.__init__", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.__init__", "kind": "function", "doc": "<p>Initialize the CL dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code> | <code>dict[int, str]</code>): the root directory where the original data files for constructing the CL dataset physically live.\nIf it is a dict, the keys are task IDs and the values are the root directories for each task. If it is a string, it is the same root directory for all tasks.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.CLDataset.root", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.root", "kind": "variable", "doc": "<p>The dict of root directories of the original data files for each task.</p>\n", "annotation": ": dict[int, str]"}, {"fullname": "clarena.cl_datasets.CLDataset.num_tasks", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.num_tasks", "kind": "variable", "doc": "<p>The maximum number of tasks supported by the dataset.</p>\n", "annotation": ": int"}, {"fullname": "clarena.cl_datasets.CLDataset.cl_paradigm", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.cl_paradigm", "kind": "variable", "doc": "<p>The continual learning paradigm.</p>\n", "annotation": ": str"}, {"fullname": "clarena.cl_datasets.CLDataset.batch_size", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.batch_size", "kind": "variable", "doc": "<p>The dict of batch sizes for each task.</p>\n", "annotation": ": dict[int, int]"}, {"fullname": "clarena.cl_datasets.CLDataset.num_workers", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.num_workers", "kind": "variable", "doc": "<p>The dict of numbers of workers for each task.</p>\n", "annotation": ": dict[int, int]"}, {"fullname": "clarena.cl_datasets.CLDataset.custom_transforms", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.custom_transforms", "kind": "variable", "doc": "<p>The dict of custom transforms for each task.</p>\n", "annotation": ": dict[int, typing.Union[typing.Callable, torchvision.transforms.transforms.Compose, NoneType]]"}, {"fullname": "clarena.cl_datasets.CLDataset.repeat_channels", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.repeat_channels", "kind": "variable", "doc": "<p>The dict of number of channels to repeat for each task.</p>\n", "annotation": ": dict[int, int | None]"}, {"fullname": "clarena.cl_datasets.CLDataset.to_tensor", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.to_tensor", "kind": "variable", "doc": "<p>The dict of to_tensor flag for each task.</p>\n", "annotation": ": dict[int, bool]"}, {"fullname": "clarena.cl_datasets.CLDataset.resize", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.resize", "kind": "variable", "doc": "<p>The dict of sizes to resize to for each task.</p>\n", "annotation": ": dict[int, tuple[int, int] | None]"}, {"fullname": "clarena.cl_datasets.CLDataset.root_t", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.root_t", "kind": "variable", "doc": "<p>The root directory of the original data files for the current task <code>self.task_id</code>.</p>\n", "annotation": ": str"}, {"fullname": "clarena.cl_datasets.CLDataset.batch_size_t", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.batch_size_t", "kind": "variable", "doc": "<p>The batch size for the current task <code>self.task_id</code>.</p>\n", "annotation": ": int"}, {"fullname": "clarena.cl_datasets.CLDataset.num_workers_t", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.num_workers_t", "kind": "variable", "doc": "<p>The number of workers for the current task <code>self.task_id</code>.</p>\n", "annotation": ": int"}, {"fullname": "clarena.cl_datasets.CLDataset.custom_transforms_t", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.custom_transforms_t", "kind": "variable", "doc": "<p>The custom transforms for the current task <code>self.task_id</code>.</p>\n", "annotation": ": Union[Callable, torchvision.transforms.transforms.Compose, NoneType]"}, {"fullname": "clarena.cl_datasets.CLDataset.repeat_channels_t", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.repeat_channels_t", "kind": "variable", "doc": "<p>The number of channels to repeat for the current task <code>self.task_id</code>.</p>\n", "annotation": ": int | None"}, {"fullname": "clarena.cl_datasets.CLDataset.to_tensor_t", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.to_tensor_t", "kind": "variable", "doc": "<p>The to_tensor flag for the current task <code>self.task_id</code>.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.cl_datasets.CLDataset.resize_t", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.resize_t", "kind": "variable", "doc": "<p>The size to resize for the current task <code>self.task_id</code>.</p>\n", "annotation": ": tuple[int, int] | None"}, {"fullname": "clarena.cl_datasets.CLDataset.mean_t", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.mean_t", "kind": "variable", "doc": "<p>The mean values for normalization for the current task <code>self.task_id</code>.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.CLDataset.std_t", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.std_t", "kind": "variable", "doc": "<p>The standard deviation values for normalization for the current task <code>self.task_id</code>.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.CLDataset.dataset_train_t", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.dataset_train_t", "kind": "variable", "doc": "<p>The training dataset object. Can be a PyTorch Dataset object or any other dataset object.</p>\n", "annotation": ": Any"}, {"fullname": "clarena.cl_datasets.CLDataset.dataset_val_t", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.dataset_val_t", "kind": "variable", "doc": "<p>The validation dataset object. Can be a PyTorch Dataset object or any other dataset object.</p>\n", "annotation": ": Any"}, {"fullname": "clarena.cl_datasets.CLDataset.dataset_test", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.dataset_test", "kind": "variable", "doc": "<p>The dictionary to store test dataset object of each task. Keys are task IDs and values are the dataset objects. Can be PyTorch Dataset objects or any other dataset objects.</p>\n", "annotation": ": dict[int, typing.Any]"}, {"fullname": "clarena.cl_datasets.CLDataset.task_id", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.task_id", "kind": "variable", "doc": "<p>Task ID counter indicating which task is being processed. Self updated during the task loop. Valid from 1 to <code>cl_dataset.num_tasks</code>.</p>\n", "annotation": ": int"}, {"fullname": "clarena.cl_datasets.CLDataset.processed_task_ids", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.processed_task_ids", "kind": "variable", "doc": "<p>Task IDs that have been processed in the experiment.</p>\n", "annotation": ": list[int]"}, {"fullname": "clarena.cl_datasets.CLDataset.sanity_check", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.sanity_check", "kind": "function", "doc": "<p>Check the sanity of the arguments.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLDataset.get_cl_class_map", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.get_cl_class_map", "kind": "function", "doc": "<p>Get the mapping of classes of task <code>task_id</code> to fit continual learning settings <code>self.cl_paradigm</code>. It must be implemented by subclasses.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the task ID to query the CL class map.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>cl_class_map</strong> (<code>dict[str | int, int]</code>): the CL class map of the task. Key is the original class label, value is the integer class label for continual learning.\n<ul>\n<li>If <code>self.cl_paradigm</code> is 'TIL', the mapped class labels of each task should be continuous integers from 0 to the number of classes.</li>\n<li>If <code>self.cl_paradigm</code> is 'CIL', the mapped class labels of each task should be continuous integers from the number of classes of previous tasks to the number of classes of the current task.</li>\n</ul></li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLDataset.prepare_data", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.prepare_data", "kind": "function", "doc": "<p>Use this to download and prepare data. It must be implemented by subclasses, as required by <code>LightningDataModule</code>. This method is called at the beginning of each task.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLDataset.setup", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.setup", "kind": "function", "doc": "<p>Set up the dataset for different stages. This method is called at the beginning of each task.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>stage</strong> (<code>str</code>): the stage of the experiment. Should be one of the following:\n<ul>\n<li>'fit': training and validation datasets of the current task <code>self.task_id</code> should be assigned to <code>self.dataset_train_t</code> and <code>self.dataset_val_t</code>.</li>\n<li>'test': a dict of test datasets of all seen tasks should be assigned to <code>self.dataset_test</code>.</li>\n</ul></li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLDataset.setup_task_id", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.setup_task_id", "kind": "function", "doc": "<p>Set up which task's dataset the CL experiment is on. This must be done before <code>setup()</code> method is called.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLDataset.set_cl_paradigm", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.set_cl_paradigm", "kind": "function", "doc": "<p>Set the continual learning paradigm to <code>self.cl_paradigm</code>. It is used to define the CL class map.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>cl_paradigm</strong> (<code>str</code>): the continual learning paradigm, either 'TIL' (Task-Incremental Learning) or 'CIL' (Class-Incremental Learning).</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">cl_paradigm</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLDataset.train_and_val_transforms", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.train_and_val_transforms", "kind": "function", "doc": "<p>Transforms generator for train and validation datasets, incorporating the custom transforms with basic transforms like normalization and <code>ToTensor()</code>. It is a handy tool to use in subclasses when constructing the dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_transforms</strong> (<code>transforms.Compose</code>): the composed train/val transforms.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLDataset.test_transforms", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.test_transforms", "kind": "function", "doc": "<p>Transforms generator for the test dataset. Only basic transforms like normalization and <code>ToTensor()</code> are included. It is a handy tool to use in subclasses when constructing the dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_transforms</strong> (<code>transforms.Compose</code>): the composed test transforms.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLDataset.train_and_val_dataset", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation datasets of task <code>self.task_id</code>. It must be implemented by subclasses.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>Any</code>): the train and validation datasets of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Any</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLDataset.test_dataset", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>. It must be implemented by subclasses.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Any</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Any</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLDataset.train_dataloader", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.train_dataloader", "kind": "function", "doc": "<p>DataLoader generator for the train stage of task <code>self.task_id</code>. It is automatically called before training the task.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_dataloader</strong> (<code>DataLoader</code>): the train DataLoader of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLDataset.val_dataloader", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.val_dataloader", "kind": "function", "doc": "<p>DataLoader generator for the validation stage. It is automatically called before the task's validation.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>val_dataloader</strong> (<code>DataLoader</code>): the validation DataLoader of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLDataset.test_dataloader", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.test_dataloader", "kind": "function", "doc": "<p>DataLoader generator for the test stage. It is automatically called before testing the task.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataloader</strong> (<code>dict[int, DataLoader]</code>): the test DataLoader dict of <code>self.task_id</code> and all tasks before (as the test is conducted on all seen tasks). Keys are task IDs and values are the DataLoaders.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLPermutedDataset", "modulename": "clarena.cl_datasets", "qualname": "CLPermutedDataset", "kind": "class", "doc": "<p>The base class of continual learning datasets constructed as permutations of an original dataset, inherited from <code>CLDataset</code>.</p>\n", "bases": "clarena.cl_datasets.base.CLDataset"}, {"fullname": "clarena.cl_datasets.CLPermutedDataset.__init__", "modulename": "clarena.cl_datasets", "qualname": "CLPermutedDataset.__init__", "kind": "function", "doc": "<p>Initialize the CL Permuted dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original dataset live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation, should be one of the following:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.CLPermutedDataset.original_dataset_python_class", "modulename": "clarena.cl_datasets", "qualname": "CLPermutedDataset.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class. It must be provided in subclasses.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]"}, {"fullname": "clarena.cl_datasets.CLPermutedDataset.original_dataset_constants", "modulename": "clarena.cl_datasets", "qualname": "CLPermutedDataset.original_dataset_constants", "kind": "variable", "doc": "<p>The original dataset constants class.</p>\n", "annotation": ": type[clarena.stl_datasets.raw.constants.DatasetConstants]"}, {"fullname": "clarena.cl_datasets.CLPermutedDataset.permutation_mode", "modulename": "clarena.cl_datasets", "qualname": "CLPermutedDataset.permutation_mode", "kind": "variable", "doc": "<p>The mode of permutation.</p>\n", "annotation": ": str"}, {"fullname": "clarena.cl_datasets.CLPermutedDataset.permutation_seeds", "modulename": "clarena.cl_datasets", "qualname": "CLPermutedDataset.permutation_seeds", "kind": "variable", "doc": "<p>The dict of permutation seeds for each task.</p>\n", "annotation": ": dict[int, int]"}, {"fullname": "clarena.cl_datasets.CLPermutedDataset.permutation_seed_t", "modulename": "clarena.cl_datasets", "qualname": "CLPermutedDataset.permutation_seed_t", "kind": "variable", "doc": "<p>The permutation seed for the current task <code>self.task_id</code>.</p>\n", "annotation": ": int"}, {"fullname": "clarena.cl_datasets.CLPermutedDataset.permute_transform_t", "modulename": "clarena.cl_datasets", "qualname": "CLPermutedDataset.permute_transform_t", "kind": "variable", "doc": "<p>The permutation transform for the current task <code>self.task_id</code>.</p>\n", "annotation": ": clarena.utils.transforms.Permute"}, {"fullname": "clarena.cl_datasets.CLPermutedDataset.sanity_check", "modulename": "clarena.cl_datasets", "qualname": "CLPermutedDataset.sanity_check", "kind": "function", "doc": "<p>Check the sanity of the arguments.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLPermutedDataset.get_cl_class_map", "modulename": "clarena.cl_datasets", "qualname": "CLPermutedDataset.get_cl_class_map", "kind": "function", "doc": "<p>Get the mapping of classes of task <code>task_id</code> to fit continual learning settings <code>self.cl_paradigm</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the task ID to query the CL class map.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>cl_class_map</strong> (<code>dict[str | int, int]</code>): the CL class map of the task. Key is the original class label, value is the integer class label for continual learning.\n<ul>\n<li>If <code>self.cl_paradigm</code> is 'TIL', the mapped class labels of a task should be continuous integers from 0 to the number of classes.</li>\n<li>If <code>self.cl_paradigm</code> is 'CIL', the mapped class labels of a task should be continuous integers from the number of classes of previous tasks to the number of classes of the current task.</li>\n</ul></li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLPermutedDataset.setup_task_id", "modulename": "clarena.cl_datasets", "qualname": "CLPermutedDataset.setup_task_id", "kind": "function", "doc": "<p>Set up which task's dataset the CL experiment is on. This must be done before <code>setup()</code> method is called.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLPermutedDataset.train_and_val_transforms", "modulename": "clarena.cl_datasets", "qualname": "CLPermutedDataset.train_and_val_transforms", "kind": "function", "doc": "<p>Transforms generator for train and validation datasets, incorporating the custom transforms with basic transforms like normalization and <code>ToTensor()</code>. In permuted CL datasets, a permute transform also applies.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_transforms</strong> (<code>transforms.Compose</code>): the composed train/val transforms.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLPermutedDataset.test_transforms", "modulename": "clarena.cl_datasets", "qualname": "CLPermutedDataset.test_transforms", "kind": "function", "doc": "<p>Transforms generator for the test dataset. Only basic transforms like normalization and <code>ToTensor()</code> are included. In permuted CL datasets, a permute transform also applies.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_transforms</strong> (<code>transforms.Compose</code>): the composed test transforms.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLSplitDataset", "modulename": "clarena.cl_datasets", "qualname": "CLSplitDataset", "kind": "class", "doc": "<p>The base class of continual learning datasets constructed as splits of an original dataset, inherited from <code>CLDataset</code>.</p>\n", "bases": "clarena.cl_datasets.base.CLDataset"}, {"fullname": "clarena.cl_datasets.CLSplitDataset.__init__", "modulename": "clarena.cl_datasets", "qualname": "CLSplitDataset.__init__", "kind": "function", "doc": "<p>Initialize the CL Split dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original dataset live.</li>\n<li><strong>class_split</strong> (<code>dict[int, list[int]]</code>): the dict of classes for each task. The keys are task IDs ane the values are lists of class labels (integers starting from 0) to split for each task.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">class_split</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.CLSplitDataset.original_dataset_python_class", "modulename": "clarena.cl_datasets", "qualname": "CLSplitDataset.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class. It must be provided in subclasses.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]"}, {"fullname": "clarena.cl_datasets.CLSplitDataset.original_dataset_constants", "modulename": "clarena.cl_datasets", "qualname": "CLSplitDataset.original_dataset_constants", "kind": "variable", "doc": "<p>The original dataset constants class.</p>\n", "annotation": ": type[clarena.stl_datasets.raw.constants.DatasetConstants]"}, {"fullname": "clarena.cl_datasets.CLSplitDataset.class_split", "modulename": "clarena.cl_datasets", "qualname": "CLSplitDataset.class_split", "kind": "variable", "doc": "<p>Store the dict of class splits for each task.</p>\n", "annotation": ": dict[int, list[int]]"}, {"fullname": "clarena.cl_datasets.CLSplitDataset.sanity_check", "modulename": "clarena.cl_datasets", "qualname": "CLSplitDataset.sanity_check", "kind": "function", "doc": "<p>Check the sanity of the arguments.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLSplitDataset.get_cl_class_map", "modulename": "clarena.cl_datasets", "qualname": "CLSplitDataset.get_cl_class_map", "kind": "function", "doc": "<p>Get the mapping of classes of task <code>task_id</code> to fit continual learning settings <code>self.cl_paradigm</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the task ID to query the CL class map.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>cl_class_map</strong> (<code>dict[str | int, int]</code>): the CL class map of the task. Key is the original class label, value is the integer class label for continual learning.\n<ul>\n<li>If <code>self.cl_paradigm</code> is 'TIL', the mapped class labels of a task should be continuous integers from 0 to the number of classes.</li>\n<li>If <code>self.cl_paradigm</code> is 'CIL', the mapped class labels of a task should be continuous integers from the number of classes of previous tasks to the number of classes of the current task.</li>\n</ul></li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLSplitDataset.setup_task_id", "modulename": "clarena.cl_datasets", "qualname": "CLSplitDataset.setup_task_id", "kind": "function", "doc": "<p>Set up which task's dataset the CL experiment is on. This must be done before <code>setup()</code> method is called.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLSplitDataset.get_subset_of_classes", "modulename": "clarena.cl_datasets", "qualname": "CLSplitDataset.get_subset_of_classes", "kind": "function", "doc": "<p>Get a subset of classes from the dataset for the current task <code>self.task_id</code>. It is used when constructing the split. It must be implemented by subclasses.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>dataset</strong> (<code>Dataset</code>): the dataset to retrieve the subset from.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>subset</strong> (<code>Dataset</code>): the subset of classes from the dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">dataset</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLCombinedDataset", "modulename": "clarena.cl_datasets", "qualname": "CLCombinedDataset", "kind": "class", "doc": "<p>The base class of continual learning datasets constructed as combinations of several original datasets (one dataset per task), inherited from <code>CLDataset</code>.</p>\n", "bases": "clarena.cl_datasets.base.CLDataset"}, {"fullname": "clarena.cl_datasets.CLCombinedDataset.__init__", "modulename": "clarena.cl_datasets", "qualname": "CLCombinedDataset.__init__", "kind": "function", "doc": "<p>Initialize the CL Combined dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>datasets</strong> (<code>dict[int, str]</code>): the dict of dataset class paths for each task. The keys are task IDs and the values are the dataset class paths (as strings) to use for each task.</li>\n<li><strong>root</strong> (<code>str</code> | <code>dict[int, str]</code>): the root directory where the original data files for constructing the CL dataset physically live.\nIf it is a dict, the keys are task IDs and the values are the root directories for each task. If it is a string, it is the same root directory for all tasks.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">datasets</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.CLCombinedDataset.original_dataset_python_classes", "modulename": "clarena.cl_datasets", "qualname": "CLCombinedDataset.original_dataset_python_classes", "kind": "variable", "doc": "<p>The dict of dataset classes for each task.</p>\n", "annotation": ": dict[int, torch.utils.data.dataset.Dataset]"}, {"fullname": "clarena.cl_datasets.CLCombinedDataset.original_dataset_python_class_t", "modulename": "clarena.cl_datasets", "qualname": "CLCombinedDataset.original_dataset_python_class_t", "kind": "variable", "doc": "<p>The dataset class for the current task <code>self.task_id</code>.</p>\n", "annotation": ": torch.utils.data.dataset.Dataset"}, {"fullname": "clarena.cl_datasets.CLCombinedDataset.original_dataset_constants_t", "modulename": "clarena.cl_datasets", "qualname": "CLCombinedDataset.original_dataset_constants_t", "kind": "variable", "doc": "<p>The original dataset constants class for the current task <code>self.task_id</code>.</p>\n", "annotation": ": type[clarena.stl_datasets.raw.constants.DatasetConstants]"}, {"fullname": "clarena.cl_datasets.CLCombinedDataset.get_cl_class_map", "modulename": "clarena.cl_datasets", "qualname": "CLCombinedDataset.get_cl_class_map", "kind": "function", "doc": "<p>Get the mapping of classes of task <code>task_id</code> to fit continual learning settings <code>self.cl_paradigm</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the task ID to query the CL class map.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>cl_class_map</strong> (<code>dict[str | int, int]</code>): the CL class map of the task. Key is the original class label, value is the integer class label for continual learning.\n<ul>\n<li>If <code>self.cl_paradigm</code> is 'TIL', the mapped class labels of a task should be continuous integers from 0 to the number of classes.</li>\n<li>If <code>self.cl_paradigm</code> is 'CIL', the mapped class labels of a task should be continuous integers from the number of classes of previous tasks to the number of classes of the current task.</li>\n</ul></li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLCombinedDataset.setup_task_id", "modulename": "clarena.cl_datasets", "qualname": "CLCombinedDataset.setup_task_id", "kind": "function", "doc": "<p>Set up which task's dataset the CL experiment is on. This must be done before <code>setup()</code> method is called.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_semeion", "modulename": "clarena.cl_datasets", "qualname": "permuted_semeion", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "clarena.cl_datasets.combined", "modulename": "clarena.cl_datasets.combined", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for combined datasets.</p>\n"}, {"fullname": "clarena.cl_datasets.combined.Combined", "modulename": "clarena.cl_datasets.combined", "qualname": "Combined", "kind": "class", "doc": "<p>Combined CL dataset from available datasets.</p>\n", "bases": "clarena.cl_datasets.base.CLCombinedDataset"}, {"fullname": "clarena.cl_datasets.combined.Combined.__init__", "modulename": "clarena.cl_datasets.combined", "qualname": "Combined.__init__", "kind": "function", "doc": "<p>Initialize the Combined Torchvision dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>datasets</strong> (<code>list[str]</code>): the list of dataset class paths for each task. Each element in the list must be a string referring to a valid PyTorch Dataset class. It needs to be one in <code>self.AVAILABLE_DATASETS</code>.</li>\n<li><strong>root</strong> (<code>list[str]</code>): the list of root directory where the original data files for constructing the CL dataset physically live.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some of the training data into validation data (only if validation set is not provided in the dataset).</li>\n<li><strong>test_percentage</strong> (<code>float</code>): the percentage to randomly split some of the entire data into test data (only if test set is not provided in the dataset).</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>list[int]</code>): The batch size in train, val, test dataloader. If <code>list[str]</code>, it should be a list of integers, each integer is the batch size for each task.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>list[int]</code>): the number of workers for dataloaders. If <code>list[str]</code>, it should be a list of integers, each integer is the num of workers for each task.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or list of them): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform. <code>ToTensor()</code>, normalize, permute and so on are not included. If it is a list, each item is the custom transforms for each task.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | list of them): the number of channels to repeat for each task. Default is None, which means no repeat. If not None, it should be an integer. If it is a list, each item is the number of channels to repeat for each task.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>list[bool]</code>): whether to include <code>ToTensor()</code> transform. Default is True.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or list of them): the size to resize the images to. Default is None, which means no resize. If not None, it should be a tuple of two integers. If it is a list, each item is the size to resize for each task.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">datasets</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">test_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.combined.Combined.AVAILABLE_DATASETS", "modulename": "clarena.cl_datasets.combined", "qualname": "Combined.AVAILABLE_DATASETS", "kind": "variable", "doc": "<p>The list of available datasets.</p>\n", "annotation": ": list[torchvision.datasets.vision.VisionDataset]", "default_value": "[&lt;class &#x27;clarena.stl_datasets.raw.ahdd.ArabicHandwrittenDigits&#x27;&gt;, &lt;class &#x27;torchvision.datasets.cifar.CIFAR10&#x27;&gt;, &lt;class &#x27;torchvision.datasets.cifar.CIFAR100&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.cub2002011.CUB2002011&#x27;&gt;, &lt;class &#x27;torchvision.datasets.caltech.Caltech101&#x27;&gt;, &lt;class &#x27;torchvision.datasets.caltech.Caltech256&#x27;&gt;, &lt;class &#x27;torchvision.datasets.celeba.CelebA&#x27;&gt;, &lt;class &#x27;torchvision.datasets.country211.Country211&#x27;&gt;, &lt;class &#x27;torchvision.datasets.dtd.DTD&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.emnist.EMNISTBalanced&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.emnist.EMNISTByClass&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.emnist.EMNISTByMerge&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.emnist.EMNISTDigits&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.emnist.EMNISTLetters&#x27;&gt;, &lt;class &#x27;torchvision.datasets.eurosat.EuroSAT&#x27;&gt;, &lt;class &#x27;torchvision.datasets.fer2013.FER2013&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.fgvc_aircraft.FGVCAircraftFamily&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.fgvc_aircraft.FGVCAircraftManufacturer&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.fgvc_aircraft.FGVCAircraftVariant&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.facescrub.FaceScrub10&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.facescrub.FaceScrub100&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.facescrub.FaceScrubFromHAT&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.facescrub.FaceScrub20&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.facescrub.FaceScrub50&#x27;&gt;, &lt;class &#x27;torchvision.datasets.mnist.FashionMNIST&#x27;&gt;, &lt;class &#x27;torchvision.datasets.flowers102.Flowers102&#x27;&gt;, &lt;class &#x27;torchvision.datasets.food101.Food101&#x27;&gt;, &lt;class &#x27;torchvision.datasets.gtsrb.GTSRB&#x27;&gt;, &lt;class &#x27;torchvision.datasets.mnist.KMNIST&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.kannada_mnist.KannadaMNIST&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.linnaeus5.Linnaeus5_128&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.linnaeus5.Linnaeus5_256&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.linnaeus5.Linnaeus5_32&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.linnaeus5.Linnaeus5_64&#x27;&gt;, &lt;class &#x27;torchvision.datasets.mnist.MNIST&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.notmnist.NotMNIST&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.notmnist.NotMNISTFromHAT&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.oxford_iiit_pet.OxfordIIITPet2&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.oxford_iiit_pet.OxfordIIITPet37&#x27;&gt;, &lt;class &#x27;torchvision.datasets.pcam.PCAM&#x27;&gt;, &lt;class &#x27;torchvision.datasets.rendered_sst2.RenderedSST2&#x27;&gt;, &lt;class &#x27;torchvision.datasets.semeion.SEMEION&#x27;&gt;, &lt;class &#x27;torchvision.datasets.sun397.SUN397&#x27;&gt;, &lt;class &#x27;torchvision.datasets.svhn.SVHN&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.sign_language_mnist.SignLanguageMNIST&#x27;&gt;, &lt;class &#x27;torchvision.datasets.stanford_cars.StanfordCars&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.traffic_signs.TrafficSignsFromHAT&#x27;&gt;, &lt;class &#x27;tinyimagenet.TinyImageNet&#x27;&gt;, &lt;class &#x27;torchvision.datasets.usps.USPS&#x27;&gt;]"}, {"fullname": "clarena.cl_datasets.combined.Combined.test_percentage", "modulename": "clarena.cl_datasets.combined", "qualname": "Combined.test_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some data into test data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.combined.Combined.validation_percentage", "modulename": "clarena.cl_datasets.combined", "qualname": "Combined.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.combined.Combined.prepare_data", "modulename": "clarena.cl_datasets.combined", "qualname": "Combined.prepare_data", "kind": "function", "doc": "<p>Download the original datasets if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.combined.Combined.train_and_val_dataset", "modulename": "clarena.cl_datasets.combined", "qualname": "Combined.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.combined.Combined.test_dataset", "modulename": "clarena.cl_datasets.combined", "qualname": "Combined.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_ahdd", "modulename": "clarena.cl_datasets.permuted_ahdd", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted Arabic Handwritten Digits dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_ahdd.PermutedArabicHandwrittenDigits", "modulename": "clarena.cl_datasets.permuted_ahdd", "qualname": "PermutedArabicHandwrittenDigits", "kind": "class", "doc": "<p>Permuted Arabic Handwritten Digits dataset. The <a href=\"https://www.kaggle.com/datasets/mloey1/ahdd1\">Arabic Handwritten Digits dataset</a> is a collection of handwritten Arabic digits (0-9). It consists of 60,000 training and 10,000 test images of handwritten Arabic digits (10 classes), each 28x28 grayscale image (similar to MNIST).</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_ahdd.PermutedArabicHandwrittenDigits.__init__", "modulename": "clarena.cl_datasets.permuted_ahdd", "qualname": "PermutedArabicHandwrittenDigits.__init__", "kind": "function", "doc": "<p>Initialize the dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Arabic Handwritten Digits data 'ArabicHandwrittenDigits/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation, should be one of the following:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_ahdd.PermutedArabicHandwrittenDigits.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_ahdd", "qualname": "PermutedArabicHandwrittenDigits.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;clarena.stl_datasets.raw.ahdd.ArabicHandwrittenDigits&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_ahdd.PermutedArabicHandwrittenDigits.validation_percentage", "modulename": "clarena.cl_datasets.permuted_ahdd", "qualname": "PermutedArabicHandwrittenDigits.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_ahdd.PermutedArabicHandwrittenDigits.prepare_data", "modulename": "clarena.cl_datasets.permuted_ahdd", "qualname": "PermutedArabicHandwrittenDigits.prepare_data", "kind": "function", "doc": "<p>Download the original Arabic Handwritten Digits dataset if haven't. Because the original dataset is published on Kaggle, we need to download it manually. This function will not download the original dataset automatically.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_ahdd.PermutedArabicHandwrittenDigits.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_ahdd", "qualname": "PermutedArabicHandwrittenDigits.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_ahdd.PermutedArabicHandwrittenDigits.test_dataset", "modulename": "clarena.cl_datasets.permuted_ahdd", "qualname": "PermutedArabicHandwrittenDigits.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_caltech101", "modulename": "clarena.cl_datasets.permuted_caltech101", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted Caltech 101 dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_caltech101.PermutedCaltech101", "modulename": "clarena.cl_datasets.permuted_caltech101", "qualname": "PermutedCaltech101", "kind": "class", "doc": "<p>Permuted Caltech 101 dataset. The <a href=\"https://data.caltech.edu/records/mzrjq-6wc02\">Caltech 101 dataset</a> is a collection of pictures of objects. It consists of 9,146 images of 101 classes, each color image.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_caltech101.PermutedCaltech101.__init__", "modulename": "clarena.cl_datasets.permuted_caltech101", "qualname": "PermutedCaltech101.__init__", "kind": "function", "doc": "<p>Initialize the dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Caltech data 'Caltech/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>test_percentage</strong> (<code>float</code>): the percentage to randomly split some data into test data.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation, should be one of the following:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">test_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_caltech101.PermutedCaltech101.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_caltech101", "qualname": "PermutedCaltech101.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.caltech.Caltech101&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_caltech101.PermutedCaltech101.test_percentage", "modulename": "clarena.cl_datasets.permuted_caltech101", "qualname": "PermutedCaltech101.test_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some data into test data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_caltech101.PermutedCaltech101.validation_percentage", "modulename": "clarena.cl_datasets.permuted_caltech101", "qualname": "PermutedCaltech101.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_caltech101.PermutedCaltech101.prepare_data", "modulename": "clarena.cl_datasets.permuted_caltech101", "qualname": "PermutedCaltech101.prepare_data", "kind": "function", "doc": "<p>Download the original Caltech 101 dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_caltech101.PermutedCaltech101.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_caltech101", "qualname": "PermutedCaltech101.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_caltech101.PermutedCaltech101.test_dataset", "modulename": "clarena.cl_datasets.permuted_caltech101", "qualname": "PermutedCaltech101.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_caltech256", "modulename": "clarena.cl_datasets.permuted_caltech256", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted Caltech 256 dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_caltech256.PermutedCaltech256", "modulename": "clarena.cl_datasets.permuted_caltech256", "qualname": "PermutedCaltech256", "kind": "class", "doc": "<p>Permuted Caltech 256 dataset. The <a href=\"https://data.caltech.edu/records/nyy15-4j048\">Caltech 256 dataset</a> is a collection of pictures of objects. It consists of 30,607 images of 256 classes, each color image.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_caltech256.PermutedCaltech256.__init__", "modulename": "clarena.cl_datasets.permuted_caltech256", "qualname": "PermutedCaltech256.__init__", "kind": "function", "doc": "<p>Initialize the dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Caltech data 'Caltech/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>test_percentage</strong> (<code>float</code>): the percentage to randomly split some data into test data.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation, should be one of the following:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">test_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_caltech256.PermutedCaltech256.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_caltech256", "qualname": "PermutedCaltech256.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.caltech.Caltech256&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_caltech256.PermutedCaltech256.test_percentage", "modulename": "clarena.cl_datasets.permuted_caltech256", "qualname": "PermutedCaltech256.test_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some data into test data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_caltech256.PermutedCaltech256.validation_percentage", "modulename": "clarena.cl_datasets.permuted_caltech256", "qualname": "PermutedCaltech256.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_caltech256.PermutedCaltech256.prepare_data", "modulename": "clarena.cl_datasets.permuted_caltech256", "qualname": "PermutedCaltech256.prepare_data", "kind": "function", "doc": "<p>Download the original Caltech 256 dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_caltech256.PermutedCaltech256.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_caltech256", "qualname": "PermutedCaltech256.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_caltech256.PermutedCaltech256.test_dataset", "modulename": "clarena.cl_datasets.permuted_caltech256", "qualname": "PermutedCaltech256.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_celeba", "modulename": "clarena.cl_datasets.permuted_celeba", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted CelebA dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_celeba.PermutedCelebA", "modulename": "clarena.cl_datasets.permuted_celeba", "qualname": "PermutedCelebA", "kind": "class", "doc": "<p>Permuted CelebA dataset. The <a href=\"https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\">CelebFaces Attributes Dataset (CelebA)</a> is a large-scale celebrity faces dataset. It consists of 202,599 face images of 10,177 celebrity identities (classes), each 178x218 color image.</p>\n\n<p>Note that the original CelebA dataset is not a classification dataset but a attributes dataset. We only use the identity of each face as the class label for classification.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_celeba.PermutedCelebA.__init__", "modulename": "clarena.cl_datasets.permuted_celeba", "qualname": "PermutedCelebA.__init__", "kind": "function", "doc": "<p>Initialize the dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original CelebA data 'CelebA/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation, should be one of the following:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_celeba.PermutedCelebA.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_celeba", "qualname": "PermutedCelebA.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.celeba.CelebA&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_celeba.PermutedCelebA.prepare_data", "modulename": "clarena.cl_datasets.permuted_celeba", "qualname": "PermutedCelebA.prepare_data", "kind": "function", "doc": "<p>Download the original CelebA dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_celeba.PermutedCelebA.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_celeba", "qualname": "PermutedCelebA.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_celeba.PermutedCelebA.test_dataset", "modulename": "clarena.cl_datasets.permuted_celeba", "qualname": "PermutedCelebA.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_cifar10", "modulename": "clarena.cl_datasets.permuted_cifar10", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted CIFAR-10 dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_cifar10.PermutedCIFAR10", "modulename": "clarena.cl_datasets.permuted_cifar10", "qualname": "PermutedCIFAR10", "kind": "class", "doc": "<p>Permuted CIFAR-10 dataset. The <a href=\"https://www.cs.toronto.edu/~kriz/cifar.html\">CIFAR-10 dataset</a> is a subset of the <a href=\"https://people.csail.mit.edu/torralba/tinyimages/\">80 million tiny images dataset</a>. It consists of 50,000 training and 10,000 test images of 10 classes, each 32x32 color image.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_cifar10.PermutedCIFAR10.__init__", "modulename": "clarena.cl_datasets.permuted_cifar10", "qualname": "PermutedCIFAR10.__init__", "kind": "function", "doc": "<p>Initialize the dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original CIFAR-10 data 'cifar-10-python/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation, should be one of the following:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_cifar10.PermutedCIFAR10.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_cifar10", "qualname": "PermutedCIFAR10.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.cifar.CIFAR10&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_cifar10.PermutedCIFAR10.validation_percentage", "modulename": "clarena.cl_datasets.permuted_cifar10", "qualname": "PermutedCIFAR10.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_cifar10.PermutedCIFAR10.prepare_data", "modulename": "clarena.cl_datasets.permuted_cifar10", "qualname": "PermutedCIFAR10.prepare_data", "kind": "function", "doc": "<p>Download the original CIFAR dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_cifar10.PermutedCIFAR10.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_cifar10", "qualname": "PermutedCIFAR10.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_cifar10.PermutedCIFAR10.test_dataset", "modulename": "clarena.cl_datasets.permuted_cifar10", "qualname": "PermutedCIFAR10.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_cifar100", "modulename": "clarena.cl_datasets.permuted_cifar100", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted CIFAR-100 dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_cifar100.PermutedCIFAR100", "modulename": "clarena.cl_datasets.permuted_cifar100", "qualname": "PermutedCIFAR100", "kind": "class", "doc": "<p>Permuted CIFAR-100 dataset. The <a href=\"https://www.cs.toronto.edu/~kriz/cifar.html\">CIFAR-100 dataset</a> is a subset of the <a href=\"https://people.csail.mit.edu/torralba/tinyimages/\">80 million tiny images dataset</a>. It consists of 50,000 training and 10,000 test images of 100 classes, each 32x32 color image.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_cifar100.PermutedCIFAR100.__init__", "modulename": "clarena.cl_datasets.permuted_cifar100", "qualname": "PermutedCIFAR100.__init__", "kind": "function", "doc": "<p>Initialize the dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original CIFAR-100 data 'cifar-100-python/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation, should be one of the following:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_cifar100.PermutedCIFAR100.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_cifar100", "qualname": "PermutedCIFAR100.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.cifar.CIFAR100&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_cifar100.PermutedCIFAR100.validation_percentage", "modulename": "clarena.cl_datasets.permuted_cifar100", "qualname": "PermutedCIFAR100.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_cifar100.PermutedCIFAR100.prepare_data", "modulename": "clarena.cl_datasets.permuted_cifar100", "qualname": "PermutedCIFAR100.prepare_data", "kind": "function", "doc": "<p>Download the original CIFAR dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_cifar100.PermutedCIFAR100.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_cifar100", "qualname": "PermutedCIFAR100.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_cifar100.PermutedCIFAR100.test_dataset", "modulename": "clarena.cl_datasets.permuted_cifar100", "qualname": "PermutedCIFAR100.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_country211", "modulename": "clarena.cl_datasets.permuted_country211", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted Country211 dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_country211.PermutedCountry211", "modulename": "clarena.cl_datasets.permuted_country211", "qualname": "PermutedCountry211", "kind": "class", "doc": "<p>Permuted Country211 dataset. The <a href=\"https://github.com/openai/CLIP/blob/main/data/country211.md\">Country211 dataset</a> is a collection of geolocation pictures of different countries. It consists of 31,650 training, 10,550 validation, and 21,100 test images of 211 countries (classes), each 256x256 color image.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_country211.PermutedCountry211.__init__", "modulename": "clarena.cl_datasets.permuted_country211", "qualname": "PermutedCountry211.__init__", "kind": "function", "doc": "<p>Initialize the dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Country211 data 'Country211/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation, should be one of the following:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_country211.PermutedCountry211.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_country211", "qualname": "PermutedCountry211.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.country211.Country211&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_country211.PermutedCountry211.prepare_data", "modulename": "clarena.cl_datasets.permuted_country211", "qualname": "PermutedCountry211.prepare_data", "kind": "function", "doc": "<p>Download the original Country211 dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_country211.PermutedCountry211.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_country211", "qualname": "PermutedCountry211.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_country211.PermutedCountry211.test_dataset", "modulename": "clarena.cl_datasets.permuted_country211", "qualname": "PermutedCountry211.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_cub2002011", "modulename": "clarena.cl_datasets.permuted_cub2002011", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted CUB-200-2011 dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_cub2002011.PermutedCUB2002011", "modulename": "clarena.cl_datasets.permuted_cub2002011", "qualname": "PermutedCUB2002011", "kind": "class", "doc": "<p>Permuted CUB-200-2011 dataset. The <a href=\"https://www.vision.caltech.edu/datasets/cub_200_2011/\">CUB (Caltech-UCSD Birds)-200-2011)</a> is a bird image dataset. It consists of 100,000 training, 10,000 validation, 10,000 test images of 200 bird species (classes), each 64x64 color image.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_cub2002011.PermutedCUB2002011.__init__", "modulename": "clarena.cl_datasets.permuted_cub2002011", "qualname": "PermutedCUB2002011.__init__", "kind": "function", "doc": "<p>Initialize the dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original CUB-200-2011 data 'CUB_200_2011/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation, should be one of the following:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_cub2002011.PermutedCUB2002011.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_cub2002011", "qualname": "PermutedCUB2002011.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;clarena.stl_datasets.raw.cub2002011.CUB2002011&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_cub2002011.PermutedCUB2002011.validation_percentage", "modulename": "clarena.cl_datasets.permuted_cub2002011", "qualname": "PermutedCUB2002011.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_cub2002011.PermutedCUB2002011.prepare_data", "modulename": "clarena.cl_datasets.permuted_cub2002011", "qualname": "PermutedCUB2002011.prepare_data", "kind": "function", "doc": "<p>Download the original CUB-200-2011 dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_cub2002011.PermutedCUB2002011.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_cub2002011", "qualname": "PermutedCUB2002011.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_cub2002011.PermutedCUB2002011.test_dataset", "modulename": "clarena.cl_datasets.permuted_cub2002011", "qualname": "PermutedCUB2002011.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_dtd", "modulename": "clarena.cl_datasets.permuted_dtd", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted DTD dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_dtd.PermutedDTD", "modulename": "clarena.cl_datasets.permuted_dtd", "qualname": "PermutedDTD", "kind": "class", "doc": "<p>Permuted DTD dataset. The <a href=\"https://www.robots.ox.ac.uk/~vgg/data/dtd/\">DTD dataset</a> is a collection of describable texture pictures. It consists of 5,640 images of 47 kinds of textures (classes), each 300x300-640x640 color image.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_dtd.PermutedDTD.__init__", "modulename": "clarena.cl_datasets.permuted_dtd", "qualname": "PermutedDTD.__init__", "kind": "function", "doc": "<p>Initialize the dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original DTD data 'DTD/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation, should be one of the following:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_dtd.PermutedDTD.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_dtd", "qualname": "PermutedDTD.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.dtd.DTD&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_dtd.PermutedDTD.prepare_data", "modulename": "clarena.cl_datasets.permuted_dtd", "qualname": "PermutedDTD.prepare_data", "kind": "function", "doc": "<p>Download the original DTD dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_dtd.PermutedDTD.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_dtd", "qualname": "PermutedDTD.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_dtd.PermutedDTD.test_dataset", "modulename": "clarena.cl_datasets.permuted_dtd", "qualname": "PermutedDTD.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_emnist", "modulename": "clarena.cl_datasets.permuted_emnist", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted EMNIST dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_emnist.PermutedEMNIST", "modulename": "clarena.cl_datasets.permuted_emnist", "qualname": "PermutedEMNIST", "kind": "class", "doc": "<p>Permuted EMNIST dataset. The <a href=\"https://www.nist.gov/itl/products-and-services/emnist-dataset/\">EMNIST dataset</a> is a collection of handwritten letters and digits (including A-Z, a-z, 0-9). It consists of 814,255 images in 62 classes, each 28x28 grayscale image.</p>\n\n<p>EMNIST has 6 different splits: <code>byclass</code>, <code>bymerge</code>, <code>balanced</code>, <code>letters</code>, <code>digits</code> and <code>mnist</code>, each containing a different subset of the original collection. We support all of them in Permuted EMNIST.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_emnist.PermutedEMNIST.__init__", "modulename": "clarena.cl_datasets.permuted_emnist", "qualname": "PermutedEMNIST.__init__", "kind": "function", "doc": "<p>Initialize the dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original EMNIST data 'EMNIST/' live.</li>\n<li><strong>split</strong> (<code>str</code>): the original EMNIST dataset has 6 different splits: <code>byclass</code>, <code>bymerge</code>, <code>balanced</code>, <code>letters</code>, <code>digits</code> and <code>mnist</code>. This argument specifies which one to use.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation, should be one of the following:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_emnist.PermutedEMNIST.split", "modulename": "clarena.cl_datasets.permuted_emnist", "qualname": "PermutedEMNIST.split", "kind": "variable", "doc": "<p>Store the split of the original EMNIST dataset. It can be <code>byclass</code>, <code>bymerge</code>, <code>balanced</code>, <code>letters</code>, <code>digits</code> or <code>mnist</code>.</p>\n", "annotation": ": str"}, {"fullname": "clarena.cl_datasets.permuted_emnist.PermutedEMNIST.validation_percentage", "modulename": "clarena.cl_datasets.permuted_emnist", "qualname": "PermutedEMNIST.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_emnist.PermutedEMNIST.prepare_data", "modulename": "clarena.cl_datasets.permuted_emnist", "qualname": "PermutedEMNIST.prepare_data", "kind": "function", "doc": "<p>Download the original EMNIST dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_emnist.PermutedEMNIST.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_emnist", "qualname": "PermutedEMNIST.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_emnist.PermutedEMNIST.test_dataset", "modulename": "clarena.cl_datasets.permuted_emnist", "qualname": "PermutedEMNIST.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_eurosat", "modulename": "clarena.cl_datasets.permuted_eurosat", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted EuroSAT dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_eurosat.PermutedEuroSAT", "modulename": "clarena.cl_datasets.permuted_eurosat", "qualname": "PermutedEuroSAT", "kind": "class", "doc": "<p>Permuted EuroSAT dataset. The <a href=\"https://github.com/phelber/eurosat\">EuroSAT dataset</a> is a collection of satellite images of lands. It consists of 27,000 images of 10 classes, each 64x64 color image.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_eurosat.PermutedEuroSAT.__init__", "modulename": "clarena.cl_datasets.permuted_eurosat", "qualname": "PermutedEuroSAT.__init__", "kind": "function", "doc": "<p>Initialize the dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Caltech data 'EuroSAT/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>test_percentage</strong> (<code>float</code>): the percentage to randomly split some data into test data.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation, should be one of the following:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">test_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_eurosat.PermutedEuroSAT.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_eurosat", "qualname": "PermutedEuroSAT.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.eurosat.EuroSAT&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_eurosat.PermutedEuroSAT.test_percentage", "modulename": "clarena.cl_datasets.permuted_eurosat", "qualname": "PermutedEuroSAT.test_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some data into test data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_eurosat.PermutedEuroSAT.validation_percentage", "modulename": "clarena.cl_datasets.permuted_eurosat", "qualname": "PermutedEuroSAT.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_eurosat.PermutedEuroSAT.prepare_data", "modulename": "clarena.cl_datasets.permuted_eurosat", "qualname": "PermutedEuroSAT.prepare_data", "kind": "function", "doc": "<p>Download the original EuroSAT dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_eurosat.PermutedEuroSAT.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_eurosat", "qualname": "PermutedEuroSAT.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_eurosat.PermutedEuroSAT.test_dataset", "modulename": "clarena.cl_datasets.permuted_eurosat", "qualname": "PermutedEuroSAT.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_facescrub", "modulename": "clarena.cl_datasets.permuted_facescrub", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted FaceScrub dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_facescrub.PermutedFaceScrub", "modulename": "clarena.cl_datasets.permuted_facescrub", "qualname": "PermutedFaceScrub", "kind": "class", "doc": "<p>Permuted FaceScrub dataset. The <a href=\"https://vintage.winklerbros.net/facescrub.html\">original FaceScrub dataset</a> is a collection of human face images. It consists 106,863 images of 530 people (classes), each high resolution color image.</p>\n\n<p>To make it simple, <a href=\"https://github.com/nkundiushuti/facescrub_subset\">this version</a> uses subset of the official <a href=\"http://megaface.cs.washington.edu/participate/challenge.html\">Megaface FaceScrub challenge</a>, cropped and resized to 32x32. We have <a href=\"https://github.com/nkundiushuti/facescrub_subset/blob/master/data/facescrub_10.zip\">FaceScrub-10</a>, <a href=\"https://github.com/nkundiushuti/facescrub_subset/blob/master/data/facescrub_20.zip\">FaceScrub-20</a>, <a href=\"https://github.com/nkundiushuti/facescrub_subset/blob/master/data/facescrub_50.zip\">FaceScrub-50</a>, <a href=\"https://github.com/nkundiushuti/facescrub_subset/blob/master/data/facescrub_100.zip\">FaceScrub-100</a> datasets where the number of classes are 10, 20, 50 and 100 respectively.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_facescrub.PermutedFaceScrub.__init__", "modulename": "clarena.cl_datasets.permuted_facescrub", "qualname": "PermutedFaceScrub.__init__", "kind": "function", "doc": "<p>Initialize the dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original FaceScrub data 'FaceScrub/' live.</li>\n<li><strong>size</strong> (<code>str</code>): the size of the dataset, should be one of the following:\n<ol>\n<li>'10': 10 classes (10 people).</li>\n<li>'20': 20 classes (20 people).</li>\n<li>'50': 50 classes (50 people).</li>\n<li>'100': 100 classes (100 people).</li>\n</ol></li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation, should be one of the following:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">size</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_facescrub.PermutedFaceScrub.validation_percentage", "modulename": "clarena.cl_datasets.permuted_facescrub", "qualname": "PermutedFaceScrub.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_facescrub.PermutedFaceScrub.prepare_data", "modulename": "clarena.cl_datasets.permuted_facescrub", "qualname": "PermutedFaceScrub.prepare_data", "kind": "function", "doc": "<p>Download the original FaceScrub dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_facescrub.PermutedFaceScrub.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_facescrub", "qualname": "PermutedFaceScrub.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_facescrub.PermutedFaceScrub.test_dataset", "modulename": "clarena.cl_datasets.permuted_facescrub", "qualname": "PermutedFaceScrub.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_fashionmnist", "modulename": "clarena.cl_datasets.permuted_fashionmnist", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted Fashion-MNIST dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_fashionmnist.PermutedFashionMNIST", "modulename": "clarena.cl_datasets.permuted_fashionmnist", "qualname": "PermutedFashionMNIST", "kind": "class", "doc": "<p>Permuted Fashion-MNIST dataset. The <a href=\"https://github.com/zalandoresearch/fashion-mnist\">Fashion-MNIST dataset</a> is a collection of fashion images. It consists of 60,000 training and 10,000 test images of 10 types of clothing (classes), each 28x28 grayscale image (similar to MNIST).</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_fashionmnist.PermutedFashionMNIST.__init__", "modulename": "clarena.cl_datasets.permuted_fashionmnist", "qualname": "PermutedFashionMNIST.__init__", "kind": "function", "doc": "<p>Initialize the dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Fashion-MNIST data 'FashionMNIST/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation, should be one of the following:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_fashionmnist.PermutedFashionMNIST.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_fashionmnist", "qualname": "PermutedFashionMNIST.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.mnist.FashionMNIST&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_fashionmnist.PermutedFashionMNIST.validation_percentage", "modulename": "clarena.cl_datasets.permuted_fashionmnist", "qualname": "PermutedFashionMNIST.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_fashionmnist.PermutedFashionMNIST.prepare_data", "modulename": "clarena.cl_datasets.permuted_fashionmnist", "qualname": "PermutedFashionMNIST.prepare_data", "kind": "function", "doc": "<p>Download the original Fashion-MNIST dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_fashionmnist.PermutedFashionMNIST.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_fashionmnist", "qualname": "PermutedFashionMNIST.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_fashionmnist.PermutedFashionMNIST.test_dataset", "modulename": "clarena.cl_datasets.permuted_fashionmnist", "qualname": "PermutedFashionMNIST.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_fer2013", "modulename": "clarena.cl_datasets.permuted_fer2013", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted FER2013 dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_fer2013.PermutedFER2013", "modulename": "clarena.cl_datasets.permuted_fer2013", "qualname": "PermutedFER2013", "kind": "class", "doc": "<p>Permuted FER2013 dataset. The <a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0893608014002159\">FER2013 dataset</a> is a collection of facial expression images. It consists of 35,887 images of 7 facial expressions (classes), each 48x48 grayscale image.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_fer2013.PermutedFER2013.__init__", "modulename": "clarena.cl_datasets.permuted_fer2013", "qualname": "PermutedFER2013.__init__", "kind": "function", "doc": "<p>Initialize the dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original FER2013 data 'FER2013/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation, should be one of the following:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_fer2013.PermutedFER2013.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_fer2013", "qualname": "PermutedFER2013.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.fer2013.FER2013&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_fer2013.PermutedFER2013.validation_percentage", "modulename": "clarena.cl_datasets.permuted_fer2013", "qualname": "PermutedFER2013.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_fer2013.PermutedFER2013.prepare_data", "modulename": "clarena.cl_datasets.permuted_fer2013", "qualname": "PermutedFER2013.prepare_data", "kind": "function", "doc": "<p>Download the original FER2013 dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_fer2013.PermutedFER2013.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_fer2013", "qualname": "PermutedFER2013.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_fer2013.PermutedFER2013.test_dataset", "modulename": "clarena.cl_datasets.permuted_fer2013", "qualname": "PermutedFER2013.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_fgvc_aircraft", "modulename": "clarena.cl_datasets.permuted_fgvc_aircraft", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted FGVC-Aircraft dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_fgvc_aircraft.PermutedFGVCAircraft", "modulename": "clarena.cl_datasets.permuted_fgvc_aircraft", "qualname": "PermutedFGVCAircraft", "kind": "class", "doc": "<p>Permuted FGVC-Aircraft dataset. The <a href=\"https://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/\">FGVC-Aircraft dataset</a> is a collection of aircraft images. It consists of 10,200 images, each color image.</p>\n\n<p>FGVC-Aircraft has 3 different class labels by variant, family and manufacturer, which has 102, 70, 41 classes respectively. We support all of them in Permuted FGVC-Aircraft.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_fgvc_aircraft.PermutedFGVCAircraft.__init__", "modulename": "clarena.cl_datasets.permuted_fgvc_aircraft", "qualname": "PermutedFGVCAircraft.__init__", "kind": "function", "doc": "<p>Initialize the dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original FGVCAircraft data 'FGVCAircraft/' live.</li>\n<li><strong>annotation_level</strong> (<code>str</code>): The annotation level, supports 'variant', 'family' and 'manufacturer'.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation, should be one of the following:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">annotation_level</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_fgvc_aircraft.PermutedFGVCAircraft.annotation_level", "modulename": "clarena.cl_datasets.permuted_fgvc_aircraft", "qualname": "PermutedFGVCAircraft.annotation_level", "kind": "variable", "doc": "<p>The annotation level, supports 'variant', 'family' and 'manufacturer'.</p>\n", "annotation": ": str"}, {"fullname": "clarena.cl_datasets.permuted_fgvc_aircraft.PermutedFGVCAircraft.prepare_data", "modulename": "clarena.cl_datasets.permuted_fgvc_aircraft", "qualname": "PermutedFGVCAircraft.prepare_data", "kind": "function", "doc": "<p>Download the original FGVC-Aircraft dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_fgvc_aircraft.PermutedFGVCAircraft.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_fgvc_aircraft", "qualname": "PermutedFGVCAircraft.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_fgvc_aircraft.PermutedFGVCAircraft.test_dataset", "modulename": "clarena.cl_datasets.permuted_fgvc_aircraft", "qualname": "PermutedFGVCAircraft.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_flowers102", "modulename": "clarena.cl_datasets.permuted_flowers102", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted Oxford 102 Flower dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_flowers102.PermutedFlowers102", "modulename": "clarena.cl_datasets.permuted_flowers102", "qualname": "PermutedFlowers102", "kind": "class", "doc": "<p>Permuted Oxford 102 Flower dataset. The <a href=\"https://www.robots.ox.ac.uk/~vgg/data/flowers/102/\">Oxford 102 Flower dataset</a> is a collection of flower pictures. It consists of 8,189 images of 102 kinds of flowers (classes), each color image.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_flowers102.PermutedFlowers102.__init__", "modulename": "clarena.cl_datasets.permuted_flowers102", "qualname": "PermutedFlowers102.__init__", "kind": "function", "doc": "<p>Initialize the dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Oxford 102 Flower data 'Flower102/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation, should be one of the following:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_flowers102.PermutedFlowers102.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_flowers102", "qualname": "PermutedFlowers102.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.flowers102.Flowers102&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_flowers102.PermutedFlowers102.prepare_data", "modulename": "clarena.cl_datasets.permuted_flowers102", "qualname": "PermutedFlowers102.prepare_data", "kind": "function", "doc": "<p>Download the original Oxford 102 Flower dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_flowers102.PermutedFlowers102.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_flowers102", "qualname": "PermutedFlowers102.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_flowers102.PermutedFlowers102.test_dataset", "modulename": "clarena.cl_datasets.permuted_flowers102", "qualname": "PermutedFlowers102.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_food101", "modulename": "clarena.cl_datasets.permuted_food101", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted Food-101 dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_food101.PermutedFood101", "modulename": "clarena.cl_datasets.permuted_food101", "qualname": "PermutedFood101", "kind": "class", "doc": "<p>Permuted Food-101 dataset. The <a href=\"https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/\">Food-101 dataset</a> is a collection of food images. It consists of 101,000 images of 101 classes, each color image.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_food101.PermutedFood101.__init__", "modulename": "clarena.cl_datasets.permuted_food101", "qualname": "PermutedFood101.__init__", "kind": "function", "doc": "<p>Initialize the dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Food-101 data 'Food101/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation, should be one of the following:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_food101.PermutedFood101.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_food101", "qualname": "PermutedFood101.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.food101.Food101&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_food101.PermutedFood101.validation_percentage", "modulename": "clarena.cl_datasets.permuted_food101", "qualname": "PermutedFood101.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_food101.PermutedFood101.prepare_data", "modulename": "clarena.cl_datasets.permuted_food101", "qualname": "PermutedFood101.prepare_data", "kind": "function", "doc": "<p>Download the original Food-101 dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_food101.PermutedFood101.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_food101", "qualname": "PermutedFood101.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_food101.PermutedFood101.test_dataset", "modulename": "clarena.cl_datasets.permuted_food101", "qualname": "PermutedFood101.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_gtsrb", "modulename": "clarena.cl_datasets.permuted_gtsrb", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted GTSRB dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_gtsrb.PermutedGTSRB", "modulename": "clarena.cl_datasets.permuted_gtsrb", "qualname": "PermutedGTSRB", "kind": "class", "doc": "<p>Permuted GTSRB dataset. The <a href=\"http://yann.lecun.com/exdb/mnist/\">GTSRB dataset</a> is a collection of traffic sign images. It consists of 51,839 images of 43 different traffic signs (classes), each color image.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_gtsrb.PermutedGTSRB.__init__", "modulename": "clarena.cl_datasets.permuted_gtsrb", "qualname": "PermutedGTSRB.__init__", "kind": "function", "doc": "<p>Initialize the dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original GTSRB data 'GTSRB/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation, should be one of the following:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_gtsrb.PermutedGTSRB.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_gtsrb", "qualname": "PermutedGTSRB.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.gtsrb.GTSRB&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_gtsrb.PermutedGTSRB.validation_percentage", "modulename": "clarena.cl_datasets.permuted_gtsrb", "qualname": "PermutedGTSRB.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_gtsrb.PermutedGTSRB.prepare_data", "modulename": "clarena.cl_datasets.permuted_gtsrb", "qualname": "PermutedGTSRB.prepare_data", "kind": "function", "doc": "<p>Download the original GTSRB dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_gtsrb.PermutedGTSRB.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_gtsrb", "qualname": "PermutedGTSRB.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_gtsrb.PermutedGTSRB.test_dataset", "modulename": "clarena.cl_datasets.permuted_gtsrb", "qualname": "PermutedGTSRB.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_imagenette", "modulename": "clarena.cl_datasets.permuted_imagenette", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted Imagenette dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_imagenette.PermutedImagenette", "modulename": "clarena.cl_datasets.permuted_imagenette", "qualname": "PermutedImagenette", "kind": "class", "doc": "<p>Permuted Imagenette dataset. The <a href=\"https://github.com/fastai/imagenette\">Imagenette dataset</a> is a subset of 10 easily classified classes from <a href=\"https://www.image-net.org\">Imagenet</a>. It provides full sizes (as Imagenet), and resized 320x320 and 160x160. We support all of them in Permuted Imagenette.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_imagenette.PermutedImagenette.__init__", "modulename": "clarena.cl_datasets.permuted_imagenette", "qualname": "PermutedImagenette.__init__", "kind": "function", "doc": "<p>Initialize the dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Imagenette data 'Imagenette/' live.</li>\n<li><strong>size</strong> (<code>str</code>): image size type. Supports \"full\" (default), \"320px\", and \"160px\".</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation, should be one of the following:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">size</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_imagenette.PermutedImagenette.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_imagenette", "qualname": "PermutedImagenette.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.imagenette.Imagenette&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_imagenette.PermutedImagenette.size", "modulename": "clarena.cl_datasets.permuted_imagenette", "qualname": "PermutedImagenette.size", "kind": "variable", "doc": "<p>Store the size type of image.</p>\n", "annotation": ": str"}, {"fullname": "clarena.cl_datasets.permuted_imagenette.PermutedImagenette.validation_percentage", "modulename": "clarena.cl_datasets.permuted_imagenette", "qualname": "PermutedImagenette.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_imagenette.PermutedImagenette.prepare_data", "modulename": "clarena.cl_datasets.permuted_imagenette", "qualname": "PermutedImagenette.prepare_data", "kind": "function", "doc": "<p>Download the original Imagenette dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_imagenette.PermutedImagenette.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_imagenette", "qualname": "PermutedImagenette.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_imagenette.PermutedImagenette.test_dataset", "modulename": "clarena.cl_datasets.permuted_imagenette", "qualname": "PermutedImagenette.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_kannadamnist", "modulename": "clarena.cl_datasets.permuted_kannadamnist", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted Kannada-MNIST dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_kannadamnist.PermutedKannadaMNIST", "modulename": "clarena.cl_datasets.permuted_kannadamnist", "qualname": "PermutedKannadaMNIST", "kind": "class", "doc": "<p>Permuted Kannada-MNIST dataset. The <a href=\"https://github.com/vinayprabhu/Kannada_MNIST\">Kannada-MNIST dataset</a> is a collection of handwritten Kannada digits (0-9). It consists of 60,000 training and 10,000 test images of handwritten Kannada digits (10 classes), each 28x28 grayscale image (similar to MNIST).</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_kannadamnist.PermutedKannadaMNIST.__init__", "modulename": "clarena.cl_datasets.permuted_kannadamnist", "qualname": "PermutedKannadaMNIST.__init__", "kind": "function", "doc": "<p>Initialize the dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Kannada-MNIST data 'KannadaMNIST/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation, should be one of the following:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_kannadamnist.PermutedKannadaMNIST.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_kannadamnist", "qualname": "PermutedKannadaMNIST.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;clarena.stl_datasets.raw.kannada_mnist.KannadaMNIST&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_kannadamnist.PermutedKannadaMNIST.validation_percentage", "modulename": "clarena.cl_datasets.permuted_kannadamnist", "qualname": "PermutedKannadaMNIST.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_kannadamnist.PermutedKannadaMNIST.prepare_data", "modulename": "clarena.cl_datasets.permuted_kannadamnist", "qualname": "PermutedKannadaMNIST.prepare_data", "kind": "function", "doc": "<p>Download the original Kannada-MNIST dataset if haven't. Because the original dataset is published on Kaggle, we need to download it manually. This function will not download the original dataset automatically.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_kannadamnist.PermutedKannadaMNIST.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_kannadamnist", "qualname": "PermutedKannadaMNIST.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_kannadamnist.PermutedKannadaMNIST.test_dataset", "modulename": "clarena.cl_datasets.permuted_kannadamnist", "qualname": "PermutedKannadaMNIST.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_kmnist", "modulename": "clarena.cl_datasets.permuted_kmnist", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted Kuzushiji-MNIST dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_kmnist.PermutedKMNIST", "modulename": "clarena.cl_datasets.permuted_kmnist", "qualname": "PermutedKMNIST", "kind": "class", "doc": "<p>Permuted Kuzushiji-MNIST dataset. The <a href=\"https://github.com/rois-codh/kmnist\">Kuzushiji-MNIST dataset</a> is a collection of Japanese Kuzushiji character images. It consists of 60,000 training and 10,000 test images of Japanese Kuzushiji images (10 classes), each 28x28 grayscale image (similar to MNIST).</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_kmnist.PermutedKMNIST.__init__", "modulename": "clarena.cl_datasets.permuted_kmnist", "qualname": "PermutedKMNIST.__init__", "kind": "function", "doc": "<p>Initialize the dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Kuzushiji-MNIST data 'KMNIST/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation, should be one of the following:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_kmnist.PermutedKMNIST.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_kmnist", "qualname": "PermutedKMNIST.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.mnist.KMNIST&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_kmnist.PermutedKMNIST.validation_percentage", "modulename": "clarena.cl_datasets.permuted_kmnist", "qualname": "PermutedKMNIST.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_kmnist.PermutedKMNIST.prepare_data", "modulename": "clarena.cl_datasets.permuted_kmnist", "qualname": "PermutedKMNIST.prepare_data", "kind": "function", "doc": "<p>Download the original Kuzushiji-MNIST dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_kmnist.PermutedKMNIST.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_kmnist", "qualname": "PermutedKMNIST.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_kmnist.PermutedKMNIST.test_dataset", "modulename": "clarena.cl_datasets.permuted_kmnist", "qualname": "PermutedKMNIST.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_linnaeus5", "modulename": "clarena.cl_datasets.permuted_linnaeus5", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted Linnaeus 5 dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_linnaeus5.PermutedLinnaeus5", "modulename": "clarena.cl_datasets.permuted_linnaeus5", "qualname": "PermutedLinnaeus5", "kind": "class", "doc": "<p>Permuted Linnaeus 5 dataset. The <a href=\"https://chaladze.com/l5/\">Linnaeus 5 dataset</a> is a collection of flower images. It consists of 8,000 images of 5 flower species (classes). It provides 256x256, 128x128, 64x64, and 32x32 color images. We support all of them in Permuted Linnaeus 5.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_linnaeus5.PermutedLinnaeus5.__init__", "modulename": "clarena.cl_datasets.permuted_linnaeus5", "qualname": "PermutedLinnaeus5.__init__", "kind": "function", "doc": "<p>Initialize the dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Linnaeus 5 data 'Linnaeus5/' live.</li>\n<li><strong>resolution</strong> (<code>str</code>): Image resolution, one of [\"256\", \"128\", \"64\", \"32\"].</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation, should be one of the following:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">resolution</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_linnaeus5.PermutedLinnaeus5.resolution", "modulename": "clarena.cl_datasets.permuted_linnaeus5", "qualname": "PermutedLinnaeus5.resolution", "kind": "variable", "doc": "<p>Store the resolution of the original dataset.</p>\n", "annotation": ": str"}, {"fullname": "clarena.cl_datasets.permuted_linnaeus5.PermutedLinnaeus5.validation_percentage", "modulename": "clarena.cl_datasets.permuted_linnaeus5", "qualname": "PermutedLinnaeus5.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_linnaeus5.PermutedLinnaeus5.prepare_data", "modulename": "clarena.cl_datasets.permuted_linnaeus5", "qualname": "PermutedLinnaeus5.prepare_data", "kind": "function", "doc": "<p>Download the original Linnaeus 5 dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_linnaeus5.PermutedLinnaeus5.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_linnaeus5", "qualname": "PermutedLinnaeus5.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_linnaeus5.PermutedLinnaeus5.test_dataset", "modulename": "clarena.cl_datasets.permuted_linnaeus5", "qualname": "PermutedLinnaeus5.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_mnist", "modulename": "clarena.cl_datasets.permuted_mnist", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted MNIST dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_mnist.PermutedMNIST", "modulename": "clarena.cl_datasets.permuted_mnist", "qualname": "PermutedMNIST", "kind": "class", "doc": "<p>Permuted MNIST dataset. The <a href=\"http://yann.lecun.com/exdb/mnist/\">MNIST dataset</a> is a collection of handwritten digits. It consists of 60,000 training and 10,000 test images of handwritten digit images (10 classes), each 28x28 grayscale image.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_mnist.PermutedMNIST.__init__", "modulename": "clarena.cl_datasets.permuted_mnist", "qualname": "PermutedMNIST.__init__", "kind": "function", "doc": "<p>Initialize the dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original MNIST data 'MNIST/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation, should be one of the following:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_mnist.PermutedMNIST.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_mnist", "qualname": "PermutedMNIST.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.mnist.MNIST&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_mnist.PermutedMNIST.validation_percentage", "modulename": "clarena.cl_datasets.permuted_mnist", "qualname": "PermutedMNIST.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_mnist.PermutedMNIST.prepare_data", "modulename": "clarena.cl_datasets.permuted_mnist", "qualname": "PermutedMNIST.prepare_data", "kind": "function", "doc": "<p>Download the original MNIST dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_mnist.PermutedMNIST.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_mnist", "qualname": "PermutedMNIST.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_mnist.PermutedMNIST.test_dataset", "modulename": "clarena.cl_datasets.permuted_mnist", "qualname": "PermutedMNIST.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_notmnist", "modulename": "clarena.cl_datasets.permuted_notmnist", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted NotMNIST dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_notmnist.PermutedNotMNIST", "modulename": "clarena.cl_datasets.permuted_notmnist", "qualname": "PermutedNotMNIST", "kind": "class", "doc": "<p>Permuted NotMNIST dataset. The <a href=\"https://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html\">NotMNIST dataset</a> is a collection of letters (A-J). Permuted MNIST dataset. This version uses the smaller set, which consists of about 19,000 images of 10 classes, each 28x28 grayscale image.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_notmnist.PermutedNotMNIST.__init__", "modulename": "clarena.cl_datasets.permuted_notmnist", "qualname": "PermutedNotMNIST.__init__", "kind": "function", "doc": "<p>Initialize the dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original NotMNIST data 'NotMNIST/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation, should be one of the following:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_notmnist.PermutedNotMNIST.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_notmnist", "qualname": "PermutedNotMNIST.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;clarena.stl_datasets.raw.notmnist.NotMNIST&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_notmnist.PermutedNotMNIST.validation_percentage", "modulename": "clarena.cl_datasets.permuted_notmnist", "qualname": "PermutedNotMNIST.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_notmnist.PermutedNotMNIST.prepare_data", "modulename": "clarena.cl_datasets.permuted_notmnist", "qualname": "PermutedNotMNIST.prepare_data", "kind": "function", "doc": "<p>Download the original NotMNIST dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_notmnist.PermutedNotMNIST.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_notmnist", "qualname": "PermutedNotMNIST.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_notmnist.PermutedNotMNIST.test_dataset", "modulename": "clarena.cl_datasets.permuted_notmnist", "qualname": "PermutedNotMNIST.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_oxford_iiit_pet", "modulename": "clarena.cl_datasets.permuted_oxford_iiit_pet", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted Oxford-IIIT Pet dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_oxford_iiit_pet.PermutedOxfordIIITPet", "modulename": "clarena.cl_datasets.permuted_oxford_iiit_pet", "qualname": "PermutedOxfordIIITPet", "kind": "class", "doc": "<p>Permuted Oxford-IIIT Pet dataset. The <a href=\"https://www.robots.ox.ac.uk/~vgg/data/pets/\">Oxford-IIIT Pet dataset</a> is a collection of cat and dog pictures. It consists of 7,349 images of 37 breeds (classes), each color image. It also provides a binary classification version with 2 classes (cat or dog). We support both versions in Permuted Oxford-IIIT Pet.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_oxford_iiit_pet.PermutedOxfordIIITPet.__init__", "modulename": "clarena.cl_datasets.permuted_oxford_iiit_pet", "qualname": "PermutedOxfordIIITPet.__init__", "kind": "function", "doc": "<p>Initialize the dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Oxford-IIIT Pet data 'OxfordIIITPet/' live.</li>\n<li><strong>target_type</strong> (<code>str</code>): the target type, should be one of the following:\n<ol>\n<li>'category': Label for one of the 37 pet categories.</li>\n<li>'binary-category': Binary label for cat or dog.</li>\n</ol></li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation, should be one of the following:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">target_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_oxford_iiit_pet.PermutedOxfordIIITPet.target_type", "modulename": "clarena.cl_datasets.permuted_oxford_iiit_pet", "qualname": "PermutedOxfordIIITPet.target_type", "kind": "variable", "doc": "<p>Store the target type.</p>\n", "annotation": ": str"}, {"fullname": "clarena.cl_datasets.permuted_oxford_iiit_pet.PermutedOxfordIIITPet.validation_percentage", "modulename": "clarena.cl_datasets.permuted_oxford_iiit_pet", "qualname": "PermutedOxfordIIITPet.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_oxford_iiit_pet.PermutedOxfordIIITPet.prepare_data", "modulename": "clarena.cl_datasets.permuted_oxford_iiit_pet", "qualname": "PermutedOxfordIIITPet.prepare_data", "kind": "function", "doc": "<p>Download the original Oxford-IIIT Pet dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_oxford_iiit_pet.PermutedOxfordIIITPet.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_oxford_iiit_pet", "qualname": "PermutedOxfordIIITPet.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_oxford_iiit_pet.PermutedOxfordIIITPet.test_dataset", "modulename": "clarena.cl_datasets.permuted_oxford_iiit_pet", "qualname": "PermutedOxfordIIITPet.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_pcam", "modulename": "clarena.cl_datasets.permuted_pcam", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted PCAM dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_pcam.PermutedPCAM", "modulename": "clarena.cl_datasets.permuted_pcam", "qualname": "PermutedPCAM", "kind": "class", "doc": "<p>Permuted PCAM dataset. The <a href=\"https://github.com/basveeling/pcam\">PCAM dataset</a> is a collection of medical images of breast cancer. It consists of 327,680 images in 2 classes (benign and malignant), each 96x96 color image.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_pcam.PermutedPCAM.__init__", "modulename": "clarena.cl_datasets.permuted_pcam", "qualname": "PermutedPCAM.__init__", "kind": "function", "doc": "<p>Initialize the dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original PCAM data 'PCAM/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation, should be one of the following:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_pcam.PermutedPCAM.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_pcam", "qualname": "PermutedPCAM.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.pcam.PCAM&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_pcam.PermutedPCAM.prepare_data", "modulename": "clarena.cl_datasets.permuted_pcam", "qualname": "PermutedPCAM.prepare_data", "kind": "function", "doc": "<p>Download the original PCAM dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_pcam.PermutedPCAM.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_pcam", "qualname": "PermutedPCAM.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_pcam.PermutedPCAM.test_dataset", "modulename": "clarena.cl_datasets.permuted_pcam", "qualname": "PermutedPCAM.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_renderedsst2", "modulename": "clarena.cl_datasets.permuted_renderedsst2", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted Rendered SST2 dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_renderedsst2.PermutedRenderedSST2", "modulename": "clarena.cl_datasets.permuted_renderedsst2", "qualname": "PermutedRenderedSST2", "kind": "class", "doc": "<p>Permuted Rendered SST2 dataset. The <a href=\"https://github.com/openai/CLIP/blob/main/data/rendered-sst2.md\">Rendered SST2 dataset</a> is a collection of optical character recognition images. It consists of 9,613 images in 2 classes (positive and negative sentiment), each 448x448 color image.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_renderedsst2.PermutedRenderedSST2.__init__", "modulename": "clarena.cl_datasets.permuted_renderedsst2", "qualname": "PermutedRenderedSST2.__init__", "kind": "function", "doc": "<p>Initialize the dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Rendered SST2 data 'RenderedSST2/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation, should be one of the following:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_renderedsst2.PermutedRenderedSST2.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_renderedsst2", "qualname": "PermutedRenderedSST2.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.rendered_sst2.RenderedSST2&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_renderedsst2.PermutedRenderedSST2.prepare_data", "modulename": "clarena.cl_datasets.permuted_renderedsst2", "qualname": "PermutedRenderedSST2.prepare_data", "kind": "function", "doc": "<p>Download the original Rendered SST2 dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_renderedsst2.PermutedRenderedSST2.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_renderedsst2", "qualname": "PermutedRenderedSST2.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_renderedsst2.PermutedRenderedSST2.test_dataset", "modulename": "clarena.cl_datasets.permuted_renderedsst2", "qualname": "PermutedRenderedSST2.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_sign_language_mnist", "modulename": "clarena.cl_datasets.permuted_sign_language_mnist", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted Sign Language MNIST dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_sign_language_mnist.PermutedSignLanguageMNIST", "modulename": "clarena.cl_datasets.permuted_sign_language_mnist", "qualname": "PermutedSignLanguageMNIST", "kind": "class", "doc": "<p>Permuted Sign Language MNIST dataset. The <a href=\"https://www.kaggle.com/datasets/datamunge/sign-language-mnist\">Sign Language MNIST dataset</a> is a collection of hand gesture images representing ASL letters (A-Y, excluding J). It consists of 34,627 images of 24 classes, each 28x28 grayscale image.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_sign_language_mnist.PermutedSignLanguageMNIST.__init__", "modulename": "clarena.cl_datasets.permuted_sign_language_mnist", "qualname": "PermutedSignLanguageMNIST.__init__", "kind": "function", "doc": "<p>Initialize the dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Sign Language MNIST data 'SignLanguageMNIST/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation, should be one of the following:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_sign_language_mnist.PermutedSignLanguageMNIST.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_sign_language_mnist", "qualname": "PermutedSignLanguageMNIST.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;clarena.stl_datasets.raw.sign_language_mnist.SignLanguageMNIST&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_sign_language_mnist.PermutedSignLanguageMNIST.validation_percentage", "modulename": "clarena.cl_datasets.permuted_sign_language_mnist", "qualname": "PermutedSignLanguageMNIST.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_sign_language_mnist.PermutedSignLanguageMNIST.prepare_data", "modulename": "clarena.cl_datasets.permuted_sign_language_mnist", "qualname": "PermutedSignLanguageMNIST.prepare_data", "kind": "function", "doc": "<p>Download the original Sign Language MNIST dataset if haven't. Because the original dataset is published on Kaggle, we need to download it manually. This function will not download the original dataset automatically.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_sign_language_mnist.PermutedSignLanguageMNIST.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_sign_language_mnist", "qualname": "PermutedSignLanguageMNIST.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_sign_language_mnist.PermutedSignLanguageMNIST.test_dataset", "modulename": "clarena.cl_datasets.permuted_sign_language_mnist", "qualname": "PermutedSignLanguageMNIST.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_stanfordcars", "modulename": "clarena.cl_datasets.permuted_stanfordcars", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted Stanford Cars dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_stanfordcars.PermutedStanfordCars", "modulename": "clarena.cl_datasets.permuted_stanfordcars", "qualname": "PermutedStanfordCars", "kind": "class", "doc": "<p>Permuted Stanford Cars dataset. The <a href=\"https://pytorch.org/vision/stable/generated/torchvision.datasets.StanfordCars.html#torchvision.datasets.StanfordCars/\">Stanford Cars dataset</a> is a collection of car images. It consists of 16,185 images in 196 classes, each color image.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_stanfordcars.PermutedStanfordCars.__init__", "modulename": "clarena.cl_datasets.permuted_stanfordcars", "qualname": "PermutedStanfordCars.__init__", "kind": "function", "doc": "<p>Initialize the dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Stanford Cars data 'StanfordCars/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation, should be one of the following:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_stanfordcars.PermutedStanfordCars.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_stanfordcars", "qualname": "PermutedStanfordCars.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.stanford_cars.StanfordCars&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_stanfordcars.PermutedStanfordCars.validation_percentage", "modulename": "clarena.cl_datasets.permuted_stanfordcars", "qualname": "PermutedStanfordCars.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_stanfordcars.PermutedStanfordCars.prepare_data", "modulename": "clarena.cl_datasets.permuted_stanfordcars", "qualname": "PermutedStanfordCars.prepare_data", "kind": "function", "doc": "<p>Download the original Stanford Cars dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_stanfordcars.PermutedStanfordCars.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_stanfordcars", "qualname": "PermutedStanfordCars.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_stanfordcars.PermutedStanfordCars.test_dataset", "modulename": "clarena.cl_datasets.permuted_stanfordcars", "qualname": "PermutedStanfordCars.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_sun397", "modulename": "clarena.cl_datasets.permuted_sun397", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted SUN397 dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_sun397.PermutedSUN397", "modulename": "clarena.cl_datasets.permuted_sun397", "qualname": "PermutedSUN397", "kind": "class", "doc": "<p>Permuted SUN397 dataset. The <a href=\"https://vision.princeton.edu/projects/2010/SUN\">SUN397 dataset</a> is a collection of scene images. It consists of 108,754 images of 397 classes, each color image.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_sun397.PermutedSUN397.__init__", "modulename": "clarena.cl_datasets.permuted_sun397", "qualname": "PermutedSUN397.__init__", "kind": "function", "doc": "<p>Initialize the dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original SUN397 data 'SUN397/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>test_percentage</strong> (<code>float</code>): the percentage to randomly split some data into test data.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation, should be one of the following:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">test_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_sun397.PermutedSUN397.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_sun397", "qualname": "PermutedSUN397.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.sun397.SUN397&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_sun397.PermutedSUN397.test_percentage", "modulename": "clarena.cl_datasets.permuted_sun397", "qualname": "PermutedSUN397.test_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some data into test data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_sun397.PermutedSUN397.validation_percentage", "modulename": "clarena.cl_datasets.permuted_sun397", "qualname": "PermutedSUN397.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_sun397.PermutedSUN397.prepare_data", "modulename": "clarena.cl_datasets.permuted_sun397", "qualname": "PermutedSUN397.prepare_data", "kind": "function", "doc": "<p>Download the original SUN397 dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_sun397.PermutedSUN397.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_sun397", "qualname": "PermutedSUN397.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_sun397.PermutedSUN397.test_dataset", "modulename": "clarena.cl_datasets.permuted_sun397", "qualname": "PermutedSUN397.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_svhn", "modulename": "clarena.cl_datasets.permuted_svhn", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted SVHN dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_svhn.PermutedSVHN", "modulename": "clarena.cl_datasets.permuted_svhn", "qualname": "PermutedSVHN", "kind": "class", "doc": "<p>Permuted SVHN dataset. The <a href=\"http://ufldl.stanford.edu/housenumbers/\">SVHN dataset</a> is a collection of street view house number images. It consists 73,257 training and 26,032 test images of 10 classes, each 32x32 color image.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_svhn.PermutedSVHN.__init__", "modulename": "clarena.cl_datasets.permuted_svhn", "qualname": "PermutedSVHN.__init__", "kind": "function", "doc": "<p>Initialize the dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original SVHN data 'SVHN/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation, should be one of the following:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_svhn.PermutedSVHN.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_svhn", "qualname": "PermutedSVHN.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.svhn.SVHN&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_svhn.PermutedSVHN.validation_percentage", "modulename": "clarena.cl_datasets.permuted_svhn", "qualname": "PermutedSVHN.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_svhn.PermutedSVHN.prepare_data", "modulename": "clarena.cl_datasets.permuted_svhn", "qualname": "PermutedSVHN.prepare_data", "kind": "function", "doc": "<p>Download the original SVHN dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_svhn.PermutedSVHN.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_svhn", "qualname": "PermutedSVHN.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_svhn.PermutedSVHN.test_dataset", "modulename": "clarena.cl_datasets.permuted_svhn", "qualname": "PermutedSVHN.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_tinyimagenet", "modulename": "clarena.cl_datasets.permuted_tinyimagenet", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted TinyImageNet dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_tinyimagenet.PermutedTinyImageNet", "modulename": "clarena.cl_datasets.permuted_tinyimagenet", "qualname": "PermutedTinyImageNet", "kind": "class", "doc": "<p>Permuted TinyImageNet dataset. The <a href=\"http://vision.stanford.edu/teaching/cs231n/reports/2015/pdfs/yle_project.pdf\">TinyImageNet dataset</a> is smaller, more manageable version of the <a href=\"https://www.image-net.org\">Imagenet dataset</a>. It consists of 100,000 training, 10,000 validation and 10,000 test images of 200 classes, each 64x64 color image.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_tinyimagenet.PermutedTinyImageNet.__init__", "modulename": "clarena.cl_datasets.permuted_tinyimagenet", "qualname": "PermutedTinyImageNet.__init__", "kind": "function", "doc": "<p>Initialize the dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original TinyImageNet data 'tiny-imagenet-200/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation, should be one of the following:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_tinyimagenet.PermutedTinyImageNet.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_tinyimagenet", "qualname": "PermutedTinyImageNet.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;tinyimagenet.TinyImageNet&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_tinyimagenet.PermutedTinyImageNet.validation_percentage", "modulename": "clarena.cl_datasets.permuted_tinyimagenet", "qualname": "PermutedTinyImageNet.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_tinyimagenet.PermutedTinyImageNet.prepare_data", "modulename": "clarena.cl_datasets.permuted_tinyimagenet", "qualname": "PermutedTinyImageNet.prepare_data", "kind": "function", "doc": "<p>Download the original TinyImageNet dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_tinyimagenet.PermutedTinyImageNet.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_tinyimagenet", "qualname": "PermutedTinyImageNet.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_tinyimagenet.PermutedTinyImageNet.test_dataset", "modulename": "clarena.cl_datasets.permuted_tinyimagenet", "qualname": "PermutedTinyImageNet.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_usps", "modulename": "clarena.cl_datasets.permuted_usps", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted USPS dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_usps.PermutedUSPS", "modulename": "clarena.cl_datasets.permuted_usps", "qualname": "PermutedUSPS", "kind": "class", "doc": "<p>Permuted USPS dataset. The <a href=\"https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html#usps\">USPS dataset</a> is a collection of handwritten digits. It consists of 9,298 handwritten digit images (10 classes), each 16x16 grayscale image.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_usps.PermutedUSPS.__init__", "modulename": "clarena.cl_datasets.permuted_usps", "qualname": "PermutedUSPS.__init__", "kind": "function", "doc": "<p>Initialize the dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original USPS data 'USPS/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation, should be one of the following:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_usps.PermutedUSPS.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_usps", "qualname": "PermutedUSPS.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.usps.USPS&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_usps.PermutedUSPS.validation_percentage", "modulename": "clarena.cl_datasets.permuted_usps", "qualname": "PermutedUSPS.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_usps.PermutedUSPS.prepare_data", "modulename": "clarena.cl_datasets.permuted_usps", "qualname": "PermutedUSPS.prepare_data", "kind": "function", "doc": "<p>Download the original USPS dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_usps.PermutedUSPS.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_usps", "qualname": "PermutedUSPS.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_usps.PermutedUSPS.test_dataset", "modulename": "clarena.cl_datasets.permuted_usps", "qualname": "PermutedUSPS.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.split_cifar10", "modulename": "clarena.cl_datasets.split_cifar10", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Split CIFAR-10 dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.split_cifar10.SplitCIFAR10", "modulename": "clarena.cl_datasets.split_cifar10", "qualname": "SplitCIFAR10", "kind": "class", "doc": "<p>Split CIFAR-10 dataset. The <a href=\"https://www.cs.toronto.edu/~kriz/cifar.html\">CIFAR-10 dataset</a> is a subset of the <a href=\"https://people.csail.mit.edu/torralba/tinyimages/\">80 million tiny images dataset</a>. It consists of 50,000 training and 10,000 test images of 10 classes, each 32x32 color image.</p>\n", "bases": "clarena.cl_datasets.base.CLSplitDataset"}, {"fullname": "clarena.cl_datasets.split_cifar10.SplitCIFAR10.__init__", "modulename": "clarena.cl_datasets.split_cifar10", "qualname": "SplitCIFAR10.__init__", "kind": "function", "doc": "<p>Initialize the dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original CIFAR-10 data 'cifar-10-python/' live.</li>\n<li><strong>class_split</strong> (<code>dict[int, list[int]]</code>): the dict of classes for each task. The keys are task IDs ane the values are lists of class labels (integers starting from 0) to split for each task.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): The percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">class_split</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.split_cifar10.SplitCIFAR10.original_dataset_python_class", "modulename": "clarena.cl_datasets.split_cifar10", "qualname": "SplitCIFAR10.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.cifar.CIFAR10&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.split_cifar10.SplitCIFAR10.validation_percentage", "modulename": "clarena.cl_datasets.split_cifar10", "qualname": "SplitCIFAR10.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.split_cifar10.SplitCIFAR10.prepare_data", "modulename": "clarena.cl_datasets.split_cifar10", "qualname": "SplitCIFAR10.prepare_data", "kind": "function", "doc": "<p>Download the original CIFAR-10 dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.split_cifar10.SplitCIFAR10.get_subset_of_classes", "modulename": "clarena.cl_datasets.split_cifar10", "qualname": "SplitCIFAR10.get_subset_of_classes", "kind": "function", "doc": "<p>Get a subset of classes from the dataset of current classes of <code>self.task_id</code>. It is used when constructing the split. It must be implemented by subclasses.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>dataset</strong> (<code>Dataset</code>): the dataset to retrieve subset from.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>subset</strong> (<code>Dataset</code>): the subset of classes from the dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">dataset</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.split_cifar10.SplitCIFAR10.train_and_val_dataset", "modulename": "clarena.cl_datasets.split_cifar10", "qualname": "SplitCIFAR10.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.split_cifar10.SplitCIFAR10.test_dataset", "modulename": "clarena.cl_datasets.split_cifar10", "qualname": "SplitCIFAR10.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.split_cifar100", "modulename": "clarena.cl_datasets.split_cifar100", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Split CIFAR-100 dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.split_cifar100.SplitCIFAR100", "modulename": "clarena.cl_datasets.split_cifar100", "qualname": "SplitCIFAR100", "kind": "class", "doc": "<p>Split CIFAR-100 dataset. The <a href=\"https://www.cs.toronto.edu/~kriz/cifar.html\">CIFAR-100 dataset</a> is a subset of the <a href=\"https://people.csail.mit.edu/torralba/tinyimages/\">80 million tiny images dataset</a>. It consists of 50,000 training and 10,000 test images of 100 classes, each 32x32 color image.</p>\n", "bases": "clarena.cl_datasets.base.CLSplitDataset"}, {"fullname": "clarena.cl_datasets.split_cifar100.SplitCIFAR100.__init__", "modulename": "clarena.cl_datasets.split_cifar100", "qualname": "SplitCIFAR100.__init__", "kind": "function", "doc": "<p>Initialize the dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original CIFAR-100 data 'cifar-100-python/' live.</li>\n<li><strong>class_split</strong> (<code>dict[int, list[int]]</code>): the dict of classes for each task. The keys are task IDs ane the values are lists of class labels (integers starting from 0) to split for each task.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): The percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">class_split</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.split_cifar100.SplitCIFAR100.original_dataset_python_class", "modulename": "clarena.cl_datasets.split_cifar100", "qualname": "SplitCIFAR100.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.cifar.CIFAR100&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.split_cifar100.SplitCIFAR100.validation_percentage", "modulename": "clarena.cl_datasets.split_cifar100", "qualname": "SplitCIFAR100.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.split_cifar100.SplitCIFAR100.prepare_data", "modulename": "clarena.cl_datasets.split_cifar100", "qualname": "SplitCIFAR100.prepare_data", "kind": "function", "doc": "<p>Download the original CIFAR-100 dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.split_cifar100.SplitCIFAR100.get_subset_of_classes", "modulename": "clarena.cl_datasets.split_cifar100", "qualname": "SplitCIFAR100.get_subset_of_classes", "kind": "function", "doc": "<p>Get a subset of classes from the dataset of current classes of <code>self.task_id</code>. It is used when constructing the split. It must be implemented by subclasses.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>dataset</strong> (<code>Dataset</code>): the dataset to retrieve subset from.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>subset</strong> (<code>Dataset</code>): the subset of classes from the dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">dataset</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.split_cifar100.SplitCIFAR100.train_and_val_dataset", "modulename": "clarena.cl_datasets.split_cifar100", "qualname": "SplitCIFAR100.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.split_cifar100.SplitCIFAR100.test_dataset", "modulename": "clarena.cl_datasets.split_cifar100", "qualname": "SplitCIFAR100.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.split_cub2002011", "modulename": "clarena.cl_datasets.split_cub2002011", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Split CUB-200-2011 dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.split_cub2002011.SplitCUB2002011", "modulename": "clarena.cl_datasets.split_cub2002011", "qualname": "SplitCUB2002011", "kind": "class", "doc": "<p>Split CUB-200-2011 dataset. The <a href=\"https://www.vision.caltech.edu/datasets/cub_200_2011/\">CUB (Caltech-UCSD Birds)-200-2011)</a> is a bird image dataset. It consists of 100,000 training, 10,000 validation, 10,000 test images of 200 bird species (classes), each 64x64 color image.</p>\n", "bases": "clarena.cl_datasets.base.CLSplitDataset"}, {"fullname": "clarena.cl_datasets.split_cub2002011.SplitCUB2002011.__init__", "modulename": "clarena.cl_datasets.split_cub2002011", "qualname": "SplitCUB2002011.__init__", "kind": "function", "doc": "<p>Initialize the dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original CUB-200-2011 data 'CUB_200_2011/' live.</li>\n<li><strong>class_split</strong> (<code>dict[int, list[int]]</code>): the dict of classes for each task. The keys are task IDs ane the values are lists of class labels (integers starting from 0) to split for each task.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): The percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">class_split</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.split_cub2002011.SplitCUB2002011.original_dataset_python_class", "modulename": "clarena.cl_datasets.split_cub2002011", "qualname": "SplitCUB2002011.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;clarena.stl_datasets.raw.cub2002011.CUB2002011&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.split_cub2002011.SplitCUB2002011.validation_percentage", "modulename": "clarena.cl_datasets.split_cub2002011", "qualname": "SplitCUB2002011.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.split_cub2002011.SplitCUB2002011.prepare_data", "modulename": "clarena.cl_datasets.split_cub2002011", "qualname": "SplitCUB2002011.prepare_data", "kind": "function", "doc": "<p>Download the original CUB-200-2011 dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.split_cub2002011.SplitCUB2002011.get_subset_of_classes", "modulename": "clarena.cl_datasets.split_cub2002011", "qualname": "SplitCUB2002011.get_subset_of_classes", "kind": "function", "doc": "<p>Get a subset of classes from the dataset of current classes of <code>self.task_id</code>. It is used when constructing the split. It must be implemented by subclasses.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>dataset</strong> (<code>Dataset</code>): the dataset to retrieve subset from.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>subset</strong> (<code>Dataset</code>): the subset of classes from the dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">dataset</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.split_cub2002011.SplitCUB2002011.train_and_val_dataset", "modulename": "clarena.cl_datasets.split_cub2002011", "qualname": "SplitCUB2002011.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.split_cub2002011.SplitCUB2002011.test_dataset", "modulename": "clarena.cl_datasets.split_cub2002011", "qualname": "SplitCUB2002011.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.split_mnist", "modulename": "clarena.cl_datasets.split_mnist", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Split MNIST dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.split_mnist.SplitMNIST", "modulename": "clarena.cl_datasets.split_mnist", "qualname": "SplitMNIST", "kind": "class", "doc": "<p>Split MNIST dataset. The <a href=\"http://yann.lecun.com/exdb/mnist/\">MNIST dataset</a> is a collection of handwritten digits. It consists of 60,000 training and 10,000 test images of handwritten digit images (10 classes), each 28x28 grayscale image.</p>\n", "bases": "clarena.cl_datasets.base.CLSplitDataset"}, {"fullname": "clarena.cl_datasets.split_mnist.SplitMNIST.__init__", "modulename": "clarena.cl_datasets.split_mnist", "qualname": "SplitMNIST.__init__", "kind": "function", "doc": "<p>Initialize the dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original MNIST data 'MNIST/' live.</li>\n<li><strong>class_split</strong> (<code>dict[int, list[int]]</code>): the dict of classes for each task. The keys are task IDs ane the values are lists of class labels (integers starting from 0) to split for each task.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): The percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">class_split</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.split_mnist.SplitMNIST.original_dataset_python_class", "modulename": "clarena.cl_datasets.split_mnist", "qualname": "SplitMNIST.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.mnist.MNIST&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.split_mnist.SplitMNIST.validation_percentage", "modulename": "clarena.cl_datasets.split_mnist", "qualname": "SplitMNIST.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.split_mnist.SplitMNIST.prepare_data", "modulename": "clarena.cl_datasets.split_mnist", "qualname": "SplitMNIST.prepare_data", "kind": "function", "doc": "<p>Download the original MNIST dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.split_mnist.SplitMNIST.get_subset_of_classes", "modulename": "clarena.cl_datasets.split_mnist", "qualname": "SplitMNIST.get_subset_of_classes", "kind": "function", "doc": "<p>Get a subset of classes from the dataset of current classes of <code>self.task_id</code>. It is used when constructing the split. It must be implemented by subclasses.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>dataset</strong> (<code>Dataset</code>): the dataset to retrieve subset from.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>subset</strong> (<code>Dataset</code>): the subset of classes from the dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">dataset</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.split_mnist.SplitMNIST.train_and_val_dataset", "modulename": "clarena.cl_datasets.split_mnist", "qualname": "SplitMNIST.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.split_mnist.SplitMNIST.test_dataset", "modulename": "clarena.cl_datasets.split_mnist", "qualname": "SplitMNIST.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.split_tinyimagenet", "modulename": "clarena.cl_datasets.split_tinyimagenet", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Split TinyImageNet dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.split_tinyimagenet.SplitTinyImageNet", "modulename": "clarena.cl_datasets.split_tinyimagenet", "qualname": "SplitTinyImageNet", "kind": "class", "doc": "<p>Split TinyImageNet dataset. The <a href=\"http://vision.stanford.edu/teaching/cs231n/reports/2015/pdfs/yle_project.pdf\">TinyImageNet dataset</a> is smaller, more manageable version of the <a href=\"https://www.image-net.org\">Imagenet dataset</a>. It consists of 100,000 training, 10,000 validation and 10,000 test images of 200 classes, each 64x64 color image.</p>\n", "bases": "clarena.cl_datasets.base.CLSplitDataset"}, {"fullname": "clarena.cl_datasets.split_tinyimagenet.SplitTinyImageNet.__init__", "modulename": "clarena.cl_datasets.split_tinyimagenet", "qualname": "SplitTinyImageNet.__init__", "kind": "function", "doc": "<p>Initialize the dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original TinyImageNet data 'tiny-imagenet-200/' live.</li>\n<li><strong>class_split</strong> (<code>dict[int, list[int]]</code>): the dict of classes for each task. The keys are task IDs ane the values are lists of class labels (integers starting from 0) to split for each task.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): The percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">class_split</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.split_tinyimagenet.SplitTinyImageNet.original_dataset_python_class", "modulename": "clarena.cl_datasets.split_tinyimagenet", "qualname": "SplitTinyImageNet.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;tinyimagenet.TinyImageNet&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.split_tinyimagenet.SplitTinyImageNet.validation_percentage", "modulename": "clarena.cl_datasets.split_tinyimagenet", "qualname": "SplitTinyImageNet.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.split_tinyimagenet.SplitTinyImageNet.prepare_data", "modulename": "clarena.cl_datasets.split_tinyimagenet", "qualname": "SplitTinyImageNet.prepare_data", "kind": "function", "doc": "<p>Download the original TinyImagenet dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.split_tinyimagenet.SplitTinyImageNet.get_subset_of_classes", "modulename": "clarena.cl_datasets.split_tinyimagenet", "qualname": "SplitTinyImageNet.get_subset_of_classes", "kind": "function", "doc": "<p>Get a subset of classes from the dataset of current classes of <code>self.task_id</code>. It is used when constructing the split.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>dataset</strong> (<code>ImageFolder</code>): the dataset to retrieve subset from.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>subset</strong> (<code>ImageFolder</code>): the subset of classes from the dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">dataset</span><span class=\"p\">:</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">datasets</span><span class=\"o\">.</span><span class=\"n\">folder</span><span class=\"o\">.</span><span class=\"n\">ImageFolder</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">datasets</span><span class=\"o\">.</span><span class=\"n\">folder</span><span class=\"o\">.</span><span class=\"n\">ImageFolder</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.split_tinyimagenet.SplitTinyImageNet.train_and_val_dataset", "modulename": "clarena.cl_datasets.split_tinyimagenet", "qualname": "SplitTinyImageNet.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_dataset</strong> (<code>Dataset</code>): the training dataset of task <code>self.task_id</code>.</li>\n<li><strong>val_dataset</strong> (<code>Dataset</code>): the validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.split_tinyimagenet.SplitTinyImageNet.test_dataset", "modulename": "clarena.cl_datasets.split_tinyimagenet", "qualname": "SplitTinyImageNet.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments", "modulename": "clarena.experiments", "kind": "module", "doc": "<h1 id=\"experiments\">Experiments</h1>\n\n<p>This submodule provides <strong>experiment classes</strong> that manage the overall configs and process of different experiments.</p>\n\n<p>We provide:</p>\n\n<ul>\n<li><code>CLMainTrainExperiment</code>: continual learning main experiment.</li>\n<li><code>CLMainEvalExperiment</code>: evaluating trained continual learning main experiment.</li>\n<li><code>CLEvalExperiment</code>: full evaluating trained continual learning experiment.</li>\n<li><code>CULMainTrainExperiment</code>: continual unlearning main experiment.</li>\n<li><code>CULEvalExperiment</code>: full evaluating trained continual unlearning experiment.</li>\n<li><code>MTLTrainExperiment</code>: multi-task learning experiment.</li>\n<li><code>MTLEvalExperiment</code>: evaluating trained multi-task learning experiment.</li>\n<li><code>STLTrainExperiment</code>: single-task learning experiment.</li>\n<li><code>STLEvalExperiment</code>: evaluating trained single-task learning experiment.</li>\n</ul>\n\n<p>The <code>clarena</code> commands are binded to one or multiple experiments. For example, <code>clarena train clmain</code> corresponds to experiment <code>CLMainTrain</code>; <code>clarena full cul</code> correspond to multiple experiments including <code>CULMainTrain</code>, some reference experiments that use <code>CLMainTrain</code>, and <code>CULEval</code>.</p>\n\n<p>Please note that this is an API documantation. Please refer to the main documentation pages for more information about the <code>clarena</code> commands:</p>\n\n<ul>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/FAQs\"><strong>Full Usage of <code>clarena</code> Command</strong></a></li>\n</ul>\n"}, {"fullname": "clarena.experiments.cl_full_metrics_calculation", "modulename": "clarena.experiments.cl_full_metrics_calculation", "kind": "module", "doc": "<p>The submodule in <code>experiments</code> for calculating full metrics of continual learning.</p>\n"}, {"fullname": "clarena.experiments.cl_full_metrics_calculation.CLFullMetricsCalculation", "modulename": "clarena.experiments.cl_full_metrics_calculation", "qualname": "CLFullMetricsCalculation", "kind": "class", "doc": "<p>The base class for full calculating full metrics of continual learning.</p>\n\n<p>These metrics are beyond those can be evaluated and calculated after a single experiment, for example, accuracy and loss in CL main and other reference experiments. They include:</p>\n\n<ul>\n<li>Backward Transfer (BWT): requires accuracy of CL main.</li>\n<li>Forward Transfer (FWT): requires accuracy of CL main and reference independent learning.</li>\n<li>Forgetting Rate (FR): requires accuracy of CL main, reference joint learning and reference random stratified model.</li>\n</ul>\n"}, {"fullname": "clarena.experiments.cl_full_metrics_calculation.CLFullMetricsCalculation.__init__", "modulename": "clarena.experiments.cl_full_metrics_calculation", "qualname": "CLFullMetricsCalculation.__init__", "kind": "function", "doc": "<p>Initializes the <code>CLFullMetricsCalculationExperiment</code> with a configuration.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span>)</span>"}, {"fullname": "clarena.experiments.cl_full_metrics_calculation.CLFullMetricsCalculation.cfg", "modulename": "clarena.experiments.cl_full_metrics_calculation", "qualname": "CLFullMetricsCalculation.cfg", "kind": "variable", "doc": "<p>The complete config dict.</p>\n", "annotation": ": omegaconf.dictconfig.DictConfig"}, {"fullname": "clarena.experiments.cl_full_metrics_calculation.CLFullMetricsCalculation.calculate_tasks", "modulename": "clarena.experiments.cl_full_metrics_calculation", "qualname": "CLFullMetricsCalculation.calculate_tasks", "kind": "variable", "doc": "<p>The list of tasks that the metrics is averaged on. Parsed from config and used in the metrics calculation loop.</p>\n", "annotation": ": list[int]"}, {"fullname": "clarena.experiments.cl_full_metrics_calculation.CLFullMetricsCalculation.main_acc_csv_path", "modulename": "clarena.experiments.cl_full_metrics_calculation", "qualname": "CLFullMetricsCalculation.main_acc_csv_path", "kind": "variable", "doc": "<p>The path of CL main accuracy CSV file to read.</p>\n", "annotation": ": str"}, {"fullname": "clarena.experiments.cl_full_metrics_calculation.CLFullMetricsCalculation.output_dir", "modulename": "clarena.experiments.cl_full_metrics_calculation", "qualname": "CLFullMetricsCalculation.output_dir", "kind": "variable", "doc": "<p>The output directory to store the logs and checkpoints. Parsed from config and help any output operation to locate the correct directory.</p>\n", "annotation": ": str"}, {"fullname": "clarena.experiments.cl_full_metrics_calculation.CLFullMetricsCalculation.bwt_save_dir", "modulename": "clarena.experiments.cl_full_metrics_calculation", "qualname": "CLFullMetricsCalculation.bwt_save_dir", "kind": "variable", "doc": "<p>The directory where data and figures of BWT metric will be saved. Better inside the output folder.</p>\n", "annotation": ": str"}, {"fullname": "clarena.experiments.cl_full_metrics_calculation.CLFullMetricsCalculation.bwt_csv_path", "modulename": "clarena.experiments.cl_full_metrics_calculation", "qualname": "CLFullMetricsCalculation.bwt_csv_path", "kind": "variable", "doc": "<p>The path to save backward transfer (BWT) metrics CSV file.</p>\n", "annotation": ": str"}, {"fullname": "clarena.experiments.cl_full_metrics_calculation.CLFullMetricsCalculation.sanity_check", "modulename": "clarena.experiments.cl_full_metrics_calculation", "qualname": "CLFullMetricsCalculation.sanity_check", "kind": "function", "doc": "<p>Check the sanity of the config dict <code>self.cfg</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.cl_full_metrics_calculation.CLFullMetricsCalculation.run", "modulename": "clarena.experiments.cl_full_metrics_calculation", "qualname": "CLFullMetricsCalculation.run", "kind": "function", "doc": "<p>The main method to run the continual learning full calculation.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.cl_full_metrics_calculation.CLFullMetricsCalculation.calculate_and_save_bwt_to_csv", "modulename": "clarena.experiments.cl_full_metrics_calculation", "qualname": "CLFullMetricsCalculation.calculate_and_save_bwt_to_csv", "kind": "function", "doc": "<p>Calculate the backward transfer (BWT) from the main accuracy CSV file and save it to a CSV file.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>acc_main_csv_path</strong> (<code>str</code>): the path to the main accuracy CSV file.</li>\n<li><strong>calculate_tasks</strong> (<code>list[int]</code>): the list of tasks to calculate the BWT.</li>\n<li><strong>csv_path</strong> (<code>str</code>): the path to save the BWT CSV file.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">acc_main_csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">calculate_tasks</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.cl_full_metrics_calculation.CLFullMetricsCalculation.calculate_and_save_fwt_to_csv", "modulename": "clarena.experiments.cl_full_metrics_calculation", "qualname": "CLFullMetricsCalculation.calculate_and_save_fwt_to_csv", "kind": "function", "doc": "<p>Calculate the forward transfer (FWT) from the main accuracy CSV file and save it to a CSV file.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>acc_main_csv_path</strong> (<code>str</code>): the path to the main accuracy CSV file.</li>\n<li><strong>acc_refil_csv_path</strong> (<code>str</code>): the path to the reference independent learning accuracy CSV file.</li>\n<li><strong>calculate_tasks</strong> (<code>list[int]</code>): the list of tasks to calculate the FWT.</li>\n<li><strong>csv_path</strong> (<code>str</code>): the path to save the FWT CSV file.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">acc_main_csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">acc_refil_csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">calculate_tasks</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.cl_full_metrics_calculation.CLFullMetricsCalculation.calculate_and_save_fr_to_csv", "modulename": "clarena.experiments.cl_full_metrics_calculation", "qualname": "CLFullMetricsCalculation.calculate_and_save_fr_to_csv", "kind": "function", "doc": "<p>Calculate the forgetting rate (FR) from the main accuracy CSV file and save it to a CSV file.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>acc_main_csv_path</strong> (<code>str</code>): the path to the main accuracy CSV file.</li>\n<li><strong>acc_refjl_csv_path</strong> (<code>str</code>): the path to the reference joint learning accuracy CSV file.</li>\n<li><strong>acc_refrandom_csv_path</strong> (<code>str</code>): the path to the reference random stratified model accuracy CSV file.</li>\n<li><strong>calculate_tasks</strong> (<code>list[int]</code>): the list of tasks to calculate the FR.</li>\n<li><strong>csv_path</strong> (<code>str</code>): the path to save the FR CSV file.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">acc_main_csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">acc_refjl_csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">acc_refrandom_csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">calculate_tasks</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.cl_full_metrics_calculation.CLFullMetricsCalculation.plot_bwt_curve_from_csv", "modulename": "clarena.experiments.cl_full_metrics_calculation", "qualname": "CLFullMetricsCalculation.plot_bwt_curve_from_csv", "kind": "function", "doc": "<p>Plot the backward transfer (BWT) curve from saved CSV file and save the plot to the designated directory.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>csv_path</strong> (<code>str</code>): the path to the CSV file where the <code>utils.save_bwt_to_csv()</code> saved the BWT metric.</li>\n<li><strong>plot_path</strong> (<code>str</code>): the path to save plot. Better same as the output directory of the experiment. E.g. './outputs/expr_name/1970-01-01_00-00-00/results/bwt.png'.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">plot_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.cl_full_metrics_calculation.CLFullMetricsCalculation.plot_fwt_curve_from_csv", "modulename": "clarena.experiments.cl_full_metrics_calculation", "qualname": "CLFullMetricsCalculation.plot_fwt_curve_from_csv", "kind": "function", "doc": "<p>Plot the forward transfer (FWT) curve from saved CSV file and save the plot to the designated directory.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>csv_path</strong> (<code>str</code>): the path to the CSV file where the <code>utils.save_fwt_to_csv()</code> saved the FWT metric.</li>\n<li><strong>plot_path</strong> (<code>str</code>): the path to save plot. Better same as the output directory of the experiment. E.g. './outputs/expr_name/1970-01-01_00-00-00/results/fwt.png'.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">plot_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.clmain_eval", "modulename": "clarena.experiments.clmain_eval", "kind": "module", "doc": "<p>The submodule in <code>experiments</code> for evaluating trained continual learning main experiment.</p>\n"}, {"fullname": "clarena.experiments.clmain_eval.CLMainEval", "modulename": "clarena.experiments.clmain_eval", "qualname": "CLMainEval", "kind": "class", "doc": "<p>The base class for evaluating trained continual learning main experiment.</p>\n\n<p>This runs evaluation (<code>test</code> of the <code>Trainer</code>) on a trained continual learning model read from saved model file, without any training loop.</p>\n"}, {"fullname": "clarena.experiments.clmain_eval.CLMainEval.__init__", "modulename": "clarena.experiments.clmain_eval", "qualname": "CLMainEval.__init__", "kind": "function", "doc": "<p>Initializes the CL evaluation object with a evaluation configuration.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span>)</span>"}, {"fullname": "clarena.experiments.clmain_eval.CLMainEval.cfg", "modulename": "clarena.experiments.clmain_eval", "qualname": "CLMainEval.cfg", "kind": "variable", "doc": "<p>Store the complete config dict for any future reference.</p>\n", "annotation": ": omegaconf.dictconfig.DictConfig"}, {"fullname": "clarena.experiments.clmain_eval.CLMainEval.cl_paradigm", "modulename": "clarena.experiments.clmain_eval", "qualname": "CLMainEval.cl_paradigm", "kind": "variable", "doc": "<p>Store the continual learning paradigm, either 'TIL' (Task-Incremental Learning) or 'CIL' (Class-Incremental Learning). Parsed from config and used to set up CL dataset.</p>\n", "annotation": ": str"}, {"fullname": "clarena.experiments.clmain_eval.CLMainEval.eval_tasks", "modulename": "clarena.experiments.clmain_eval", "qualname": "CLMainEval.eval_tasks", "kind": "variable", "doc": "<p>Store the list of tasks to be evaluated. Parsed from config and used in the evaluation loop.</p>\n", "annotation": ": list[int]"}, {"fullname": "clarena.experiments.clmain_eval.CLMainEval.global_seed", "modulename": "clarena.experiments.clmain_eval", "qualname": "CLMainEval.global_seed", "kind": "variable", "doc": "<p>Store the global seed for the entire experiment. Parsed from config and used to seed all random number generators.</p>\n", "annotation": ": int"}, {"fullname": "clarena.experiments.clmain_eval.CLMainEval.output_dir", "modulename": "clarena.experiments.clmain_eval", "qualname": "CLMainEval.output_dir", "kind": "variable", "doc": "<p>Store the output directory to store the logs and checkpoints. Parsed from config and help any output operation to locate the correct directory.</p>\n", "annotation": ": str"}, {"fullname": "clarena.experiments.clmain_eval.CLMainEval.main_model_path", "modulename": "clarena.experiments.clmain_eval", "qualname": "CLMainEval.main_model_path", "kind": "variable", "doc": "<p>Store the path to the model file to load the main model from. Parsed from config and used to load the main model for evaluation.</p>\n", "annotation": ": str"}, {"fullname": "clarena.experiments.clmain_eval.CLMainEval.cl_dataset", "modulename": "clarena.experiments.clmain_eval", "qualname": "CLMainEval.cl_dataset", "kind": "variable", "doc": "<p>CL dataset object. Instantiate in <code>instantiate_cl_dataset()</code>.</p>\n", "annotation": ": clarena.cl_datasets.base.CLDataset"}, {"fullname": "clarena.experiments.clmain_eval.CLMainEval.trainer", "modulename": "clarena.experiments.clmain_eval", "qualname": "CLMainEval.trainer", "kind": "variable", "doc": "<p>Trainer object. Instantiate in <code>instantiate_trainer()</code>.</p>\n", "annotation": ": lightning.pytorch.trainer.trainer.Trainer"}, {"fullname": "clarena.experiments.clmain_eval.CLMainEval.lightning_loggers", "modulename": "clarena.experiments.clmain_eval", "qualname": "CLMainEval.lightning_loggers", "kind": "variable", "doc": "<p>The list of initialized lightning loggers objects for current task <code>self.task_id</code>. Instantiate in <code>instantiate_lightning_loggers()</code>.</p>\n", "annotation": ": list[lightning.pytorch.loggers.logger.Logger]"}, {"fullname": "clarena.experiments.clmain_eval.CLMainEval.callbacks", "modulename": "clarena.experiments.clmain_eval", "qualname": "CLMainEval.callbacks", "kind": "variable", "doc": "<p>The list of initialized callbacks objects for the evaluation. Instantiate in <code>instantiate_callbacks()</code>.</p>\n", "annotation": ": list[lightning.pytorch.callbacks.callback.Callback]"}, {"fullname": "clarena.experiments.clmain_eval.CLMainEval.task_id", "modulename": "clarena.experiments.clmain_eval", "qualname": "CLMainEval.task_id", "kind": "variable", "doc": "<p>Task ID counter indicating which task is being processed. Self updated during the task loop. Starting from 1.</p>\n", "annotation": ": int"}, {"fullname": "clarena.experiments.clmain_eval.CLMainEval.processed_task_ids", "modulename": "clarena.experiments.clmain_eval", "qualname": "CLMainEval.processed_task_ids", "kind": "variable", "doc": "<p>Task IDs that have been processed in the experiment.</p>\n", "annotation": ": list[int]"}, {"fullname": "clarena.experiments.clmain_eval.CLMainEval.sanity_check", "modulename": "clarena.experiments.clmain_eval", "qualname": "CLMainEval.sanity_check", "kind": "function", "doc": "<p>Check the sanity of the config dict <code>self.cfg</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.clmain_eval.CLMainEval.instantiate_cl_dataset", "modulename": "clarena.experiments.clmain_eval", "qualname": "CLMainEval.instantiate_cl_dataset", "kind": "function", "doc": "<p>Instantiate the CL dataset object from cl_dataset config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>cl_dataset_cfg</strong> (<code>DictConfig</code>): the cl_dataset config dict.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">cl_dataset_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.clmain_eval.CLMainEval.instantiate_trainer", "modulename": "clarena.experiments.clmain_eval", "qualname": "CLMainEval.instantiate_trainer", "kind": "function", "doc": "<p>Instantiate the trainer object for task <code>task_id</code> from trainer config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>trainer_cfg</strong> (<code>DictConfig</code>): the trainer config dict. It can be a dict containing trainer config for each task; otherwise, it's an uniform trainer config for all tasks (but different objects).</li>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">trainer_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.clmain_eval.CLMainEval.instantiate_lightning_loggers", "modulename": "clarena.experiments.clmain_eval", "qualname": "CLMainEval.instantiate_lightning_loggers", "kind": "function", "doc": "<p>Instantiate the list of lightning loggers objects for task <code>task_id</code> from lightning_loggers config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>lightning_loggers_cfg</strong> (<code>DictConfig</code>): the lightning_loggers config dict. All tasks share the same lightning_loggers config but different objects.</li>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">lightning_loggers_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span>,</span><span class=\"param\">\t<span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.clmain_eval.CLMainEval.instantiate_callbacks", "modulename": "clarena.experiments.clmain_eval", "qualname": "CLMainEval.instantiate_callbacks", "kind": "function", "doc": "<p>Instantiate the list of callbacks objects from metrics and other callbacks config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>metrics_cfg</strong> (<code>DictConfig</code>): the metrics config dict. All tasks share the same callbacks config but different objects.</li>\n<li><strong>callbacks_cfg</strong> (<code>DictConfig</code>): the callbacks config dict. All tasks share the same callbacks config but different objects.</li>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">metrics_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span>,</span><span class=\"param\">\t<span class=\"n\">callbacks_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span>,</span><span class=\"param\">\t<span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.clmain_eval.CLMainEval.set_global_seed", "modulename": "clarena.experiments.clmain_eval", "qualname": "CLMainEval.set_global_seed", "kind": "function", "doc": "<p>Set the global seed for the entire experiment.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.clmain_eval.CLMainEval.setup_task_id", "modulename": "clarena.experiments.clmain_eval", "qualname": "CLMainEval.setup_task_id", "kind": "function", "doc": "<p>Set up current task_id in the beginning of the continual learning process of a new task.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): current task_id.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.clmain_eval.CLMainEval.instantiate_global", "modulename": "clarena.experiments.clmain_eval", "qualname": "CLMainEval.instantiate_global", "kind": "function", "doc": "<p>Instantiate dataset.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.clmain_eval.CLMainEval.setup_global", "modulename": "clarena.experiments.clmain_eval", "qualname": "CLMainEval.setup_global", "kind": "function", "doc": "<p>Let CL dataset know the CL paradigm to define its CL class map.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.clmain_eval.CLMainEval.instantiate_task_specific", "modulename": "clarena.experiments.clmain_eval", "qualname": "CLMainEval.instantiate_task_specific", "kind": "function", "doc": "<p>Instantiate task-specific components for the task <code>task_id</code> from <code>self.cfg</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.clmain_eval.CLMainEval.setup_task_specific", "modulename": "clarena.experiments.clmain_eval", "qualname": "CLMainEval.setup_task_specific", "kind": "function", "doc": "<p>Setup task-specific components to get ready for the task <code>task_id</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.clmain_eval.CLMainEval.run", "modulename": "clarena.experiments.clmain_eval", "qualname": "CLMainEval.run", "kind": "function", "doc": "<p>The main method to run the continual learning evaluation experiment.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.clmain_train", "modulename": "clarena.experiments.clmain_train", "kind": "module", "doc": "<p>The submodule in <code>experiments</code> for training continual learning main experiment.</p>\n"}, {"fullname": "clarena.experiments.clmain_train.CLMainTrain", "modulename": "clarena.experiments.clmain_train", "qualname": "CLMainTrain", "kind": "class", "doc": "<p>The base class for training continual learning main experiment.</p>\n\n<p>This module provides the CLMainTrain class which orchestrates the entire\ncontinual learning experiment lifecycle including dataset setup, model\ninstantiation, training, and evaluation across sequential tasks.</p>\n"}, {"fullname": "clarena.experiments.clmain_train.CLMainTrain.__init__", "modulename": "clarena.experiments.clmain_train", "qualname": "CLMainTrain.__init__", "kind": "function", "doc": "<p>Initialize the <code>CLMainTrain</code> object with a continual learning main experiment configuration.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>cfg</strong> (<code>DictConfig</code>): the complete config dict for the continual learning main experiment.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span>)</span>"}, {"fullname": "clarena.experiments.clmain_train.CLMainTrain.cfg", "modulename": "clarena.experiments.clmain_train", "qualname": "CLMainTrain.cfg", "kind": "variable", "doc": "<p>The complete config dict.</p>\n", "annotation": ": omegaconf.dictconfig.DictConfig"}, {"fullname": "clarena.experiments.clmain_train.CLMainTrain.cl_paradigm", "modulename": "clarena.experiments.clmain_train", "qualname": "CLMainTrain.cl_paradigm", "kind": "variable", "doc": "<p>The continual learning paradigm.</p>\n", "annotation": ": str"}, {"fullname": "clarena.experiments.clmain_train.CLMainTrain.train_tasks", "modulename": "clarena.experiments.clmain_train", "qualname": "CLMainTrain.train_tasks", "kind": "variable", "doc": "<p>The list of task IDs to train.</p>\n", "annotation": ": list[int]"}, {"fullname": "clarena.experiments.clmain_train.CLMainTrain.eval_after_tasks", "modulename": "clarena.experiments.clmain_train", "qualname": "CLMainTrain.eval_after_tasks", "kind": "variable", "doc": "<p>If task ID $t$ is in this list, run the evaluation process for all seen tasks after training task $t$.</p>\n", "annotation": ": list[int]"}, {"fullname": "clarena.experiments.clmain_train.CLMainTrain.global_seed", "modulename": "clarena.experiments.clmain_train", "qualname": "CLMainTrain.global_seed", "kind": "variable", "doc": "<p>The global seed for the entire experiment.</p>\n", "annotation": ": int"}, {"fullname": "clarena.experiments.clmain_train.CLMainTrain.output_dir", "modulename": "clarena.experiments.clmain_train", "qualname": "CLMainTrain.output_dir", "kind": "variable", "doc": "<p>The folder name for storing the experiment results.</p>\n", "annotation": ": str"}, {"fullname": "clarena.experiments.clmain_train.CLMainTrain.cl_dataset", "modulename": "clarena.experiments.clmain_train", "qualname": "CLMainTrain.cl_dataset", "kind": "variable", "doc": "<p>CL dataset object.</p>\n", "annotation": ": clarena.cl_datasets.base.CLDataset"}, {"fullname": "clarena.experiments.clmain_train.CLMainTrain.backbone", "modulename": "clarena.experiments.clmain_train", "qualname": "CLMainTrain.backbone", "kind": "variable", "doc": "<p>Backbone network object.</p>\n", "annotation": ": clarena.backbones.base.CLBackbone"}, {"fullname": "clarena.experiments.clmain_train.CLMainTrain.heads", "modulename": "clarena.experiments.clmain_train", "qualname": "CLMainTrain.heads", "kind": "variable", "doc": "<p>CL output heads object.</p>\n", "annotation": ": clarena.heads.heads_til.HeadsTIL | clarena.heads.heads_cil.HeadsCIL"}, {"fullname": "clarena.experiments.clmain_train.CLMainTrain.model", "modulename": "clarena.experiments.clmain_train", "qualname": "CLMainTrain.model", "kind": "variable", "doc": "<p>CL model object.</p>\n", "annotation": ": clarena.cl_algorithms.base.CLAlgorithm"}, {"fullname": "clarena.experiments.clmain_train.CLMainTrain.optimizer_t", "modulename": "clarena.experiments.clmain_train", "qualname": "CLMainTrain.optimizer_t", "kind": "variable", "doc": "<p>Optimizer object for current task <code>self.task_id</code>.</p>\n", "annotation": ": torch.optim.optimizer.Optimizer"}, {"fullname": "clarena.experiments.clmain_train.CLMainTrain.lr_scheduler_t", "modulename": "clarena.experiments.clmain_train", "qualname": "CLMainTrain.lr_scheduler_t", "kind": "variable", "doc": "<p>Learning rate scheduler object for current task <code>self.task_id</code>. Instantiate in <code>instantiate_lr_scheduler()</code>.</p>\n", "annotation": ": torch.optim.lr_scheduler.LRScheduler"}, {"fullname": "clarena.experiments.clmain_train.CLMainTrain.trainer_t", "modulename": "clarena.experiments.clmain_train", "qualname": "CLMainTrain.trainer_t", "kind": "variable", "doc": "<p>Trainer object for current task <code>self.task_id</code>. Instantiate in <code>instantiate_trainer()</code>.</p>\n", "annotation": ": lightning.pytorch.trainer.trainer.Trainer"}, {"fullname": "clarena.experiments.clmain_train.CLMainTrain.lightning_loggers_t", "modulename": "clarena.experiments.clmain_train", "qualname": "CLMainTrain.lightning_loggers_t", "kind": "variable", "doc": "<p>The list of initialized lightning loggers objects for current task <code>self.task_id</code>. Instantiate in <code>instantiate_lightning_loggers()</code>.</p>\n", "annotation": ": list[lightning.pytorch.loggers.logger.Logger]"}, {"fullname": "clarena.experiments.clmain_train.CLMainTrain.callbacks_t", "modulename": "clarena.experiments.clmain_train", "qualname": "CLMainTrain.callbacks_t", "kind": "variable", "doc": "<p>The list of initialized other callbacks objects for current task <code>self.task_id</code>. Instantiate in <code>instantiate_callbacks()</code>.</p>\n", "annotation": ": list[lightning.pytorch.callbacks.callback.Callback]"}, {"fullname": "clarena.experiments.clmain_train.CLMainTrain.task_id", "modulename": "clarena.experiments.clmain_train", "qualname": "CLMainTrain.task_id", "kind": "variable", "doc": "<p>Task ID counter indicating which task is being processed. Self updated during the task loop. Valid from 1 to <code>cl_dataset.num_tasks</code>.</p>\n", "annotation": ": int"}, {"fullname": "clarena.experiments.clmain_train.CLMainTrain.processed_task_ids", "modulename": "clarena.experiments.clmain_train", "qualname": "CLMainTrain.processed_task_ids", "kind": "variable", "doc": "<p>Task IDs that have been processed in the experiment.</p>\n", "annotation": ": list[int]"}, {"fullname": "clarena.experiments.clmain_train.CLMainTrain.sanity_check", "modulename": "clarena.experiments.clmain_train", "qualname": "CLMainTrain.sanity_check", "kind": "function", "doc": "<p>Check the sanity of the config dict <code>self.cfg</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.clmain_train.CLMainTrain.instantiate_cl_dataset", "modulename": "clarena.experiments.clmain_train", "qualname": "CLMainTrain.instantiate_cl_dataset", "kind": "function", "doc": "<p>Instantiate the CL dataset object from <code>cl_dataset_cfg</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>cl_dataset_cfg</strong> (<code>DictConfig</code>): the cl_dataset config dict.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">cl_dataset_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.clmain_train.CLMainTrain.instantiate_backbone", "modulename": "clarena.experiments.clmain_train", "qualname": "CLMainTrain.instantiate_backbone", "kind": "function", "doc": "<p>Instantiate the CL backbone network object from backbone config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone_cfg</strong> (<code>DictConfig</code>): the backbone config dict.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">backbone_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.clmain_train.CLMainTrain.instantiate_heads", "modulename": "clarena.experiments.clmain_train", "qualname": "CLMainTrain.instantiate_heads", "kind": "function", "doc": "<p>Instantiate the CL output heads object according to field <code>cl_paradigm</code> and backbone <code>output_dim</code> in the config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>cl_paradigm</strong> (<code>str</code>): the CL paradigm, either 'TIL' or 'CIL'.</li>\n<li><strong>input_dim</strong> (<code>int</code>): the input dimension of the heads. Must be equal to the <code>output_dim</code> of the connected backbone.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">cl_paradigm</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">input_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.clmain_train.CLMainTrain.instantiate_cl_algorithm", "modulename": "clarena.experiments.clmain_train", "qualname": "CLMainTrain.instantiate_cl_algorithm", "kind": "function", "doc": "<p>Instantiate the cl_algorithm object from cl_algorithm config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>cl_algorithm_cfg</strong> (<code>DictConfig</code>): the cl_algorithm config dict.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">cl_algorithm_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.clmain_train.CLMainTrain.instantiate_optimizer", "modulename": "clarena.experiments.clmain_train", "qualname": "CLMainTrain.instantiate_optimizer", "kind": "function", "doc": "<p>Instantiate the optimizer object for task <code>task_id</code> from optimizer config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>optimizer_cfg</strong> (<code>DictConfig</code>): the optimizer config dict. It can be a dict containing optimizer config for each task; otherwise, it's an uniform optimizer config for all tasks (but different objects).</li>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">optimizer_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span>,</span><span class=\"param\">\t<span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.clmain_train.CLMainTrain.instantiate_lr_scheduler", "modulename": "clarena.experiments.clmain_train", "qualname": "CLMainTrain.instantiate_lr_scheduler", "kind": "function", "doc": "<p>Instantiate the learning rate scheduler object for task <code>task_id</code> from lr_scheduler config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>lr_scheduler_cfg</strong> (<code>DictConfig</code>): the learning rate scheduler config dict. It can be a dict containing learning rate scheduler config for each task; otherwise, it's an uniform learning rate scheduler config for all tasks (but different objects).</li>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">lr_scheduler_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span>,</span><span class=\"param\">\t<span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.clmain_train.CLMainTrain.instantiate_trainer", "modulename": "clarena.experiments.clmain_train", "qualname": "CLMainTrain.instantiate_trainer", "kind": "function", "doc": "<p>Instantiate the trainer object for task <code>task_id</code> from trainer config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>trainer_cfg</strong> (<code>DictConfig</code>): the trainer config dict. It can be a dict containing trainer config for each task; otherwise, it's an uniform trainer config for all tasks (but different objects).</li>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">trainer_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.clmain_train.CLMainTrain.instantiate_lightning_loggers", "modulename": "clarena.experiments.clmain_train", "qualname": "CLMainTrain.instantiate_lightning_loggers", "kind": "function", "doc": "<p>Instantiate the list of lightning loggers objects for task <code>task_id</code> from lightning_loggers config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>lightning_loggers_cfg</strong> (<code>DictConfig</code>): the lightning_loggers config dict. All tasks share the same lightning_loggers config but different objects.</li>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">lightning_loggers_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span>,</span><span class=\"param\">\t<span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.clmain_train.CLMainTrain.instantiate_callbacks", "modulename": "clarena.experiments.clmain_train", "qualname": "CLMainTrain.instantiate_callbacks", "kind": "function", "doc": "<p>Instantiate the list of callbacks objects from metrics and other callbacks config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>metrics_cfg</strong> (<code>DictConfig</code>): the metrics config dict. All tasks share the same callbacks config but different objects.</li>\n<li><strong>callbacks_cfg</strong> (<code>DictConfig</code>): the callbacks config dict. All tasks share the same callbacks config but different objects.</li>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">metrics_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span>,</span><span class=\"param\">\t<span class=\"n\">callbacks_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span>,</span><span class=\"param\">\t<span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.clmain_train.CLMainTrain.set_global_seed", "modulename": "clarena.experiments.clmain_train", "qualname": "CLMainTrain.set_global_seed", "kind": "function", "doc": "<p>Set the global seed for the entire experiment.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.clmain_train.CLMainTrain.setup_task_id", "modulename": "clarena.experiments.clmain_train", "qualname": "CLMainTrain.setup_task_id", "kind": "function", "doc": "<p>Set up current task_id in the beginning of the continual learning process of a new task.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): current task_id.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.clmain_train.CLMainTrain.instantiate_global", "modulename": "clarena.experiments.clmain_train", "qualname": "CLMainTrain.instantiate_global", "kind": "function", "doc": "<p>Instantiate global components for the entire CL experiment from <code>self.cfg</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.clmain_train.CLMainTrain.setup_global", "modulename": "clarena.experiments.clmain_train", "qualname": "CLMainTrain.setup_global", "kind": "function", "doc": "<p>Let CL dataset know the CL paradigm to define its CL class map.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.clmain_train.CLMainTrain.instantiate_task_specific", "modulename": "clarena.experiments.clmain_train", "qualname": "CLMainTrain.instantiate_task_specific", "kind": "function", "doc": "<p>Instantiate task-specific components for the current task <code>self.task_id</code> from <code>self.cfg</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.clmain_train.CLMainTrain.setup_task_specific", "modulename": "clarena.experiments.clmain_train", "qualname": "CLMainTrain.setup_task_specific", "kind": "function", "doc": "<p>Setup task-specific components to get ready for the current task <code>self.task_id</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.clmain_train.CLMainTrain.run", "modulename": "clarena.experiments.clmain_train", "qualname": "CLMainTrain.run", "kind": "function", "doc": "<p>The main method to run the continual learning experiment.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.cul_full_eval", "modulename": "clarena.experiments.cul_full_eval", "kind": "module", "doc": "<p>The submodule in <code>experiments</code> for full evaluating trained continual unlearning experiment.</p>\n"}, {"fullname": "clarena.experiments.cul_full_eval.CULFullEval", "modulename": "clarena.experiments.cul_full_eval", "qualname": "CULFullEval", "kind": "class", "doc": "<p>The base class for full evaluating trained continual unlearning experiment.</p>\n\n<p>This runs evaluation (<code>test</code> of the <code>Trainer</code>) on a trained continual unlearning and reference model read from saved model file, without any training loop.</p>\n"}, {"fullname": "clarena.experiments.cul_full_eval.CULFullEval.__init__", "modulename": "clarena.experiments.cul_full_eval", "qualname": "CULFullEval.__init__", "kind": "function", "doc": "<p>Initializes the CUL evaluation object with a evaluation configuration.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span>)</span>"}, {"fullname": "clarena.experiments.cul_full_eval.CULFullEval.cfg", "modulename": "clarena.experiments.cul_full_eval", "qualname": "CULFullEval.cfg", "kind": "variable", "doc": "<p>The complete config dict.</p>\n", "annotation": ": omegaconf.dictconfig.DictConfig"}, {"fullname": "clarena.experiments.cul_full_eval.CULFullEval.main_model_path", "modulename": "clarena.experiments.cul_full_eval", "qualname": "CULFullEval.main_model_path", "kind": "variable", "doc": "<p>The path to the model file to load the main model from.</p>\n", "annotation": ": str"}, {"fullname": "clarena.experiments.cul_full_eval.CULFullEval.refretrain_model_path", "modulename": "clarena.experiments.cul_full_eval", "qualname": "CULFullEval.refretrain_model_path", "kind": "variable", "doc": "<p>The path to the model file to load the reference retraining model from.</p>\n", "annotation": ": str"}, {"fullname": "clarena.experiments.cul_full_eval.CULFullEval.reforiginal_model_path", "modulename": "clarena.experiments.cul_full_eval", "qualname": "CULFullEval.reforiginal_model_path", "kind": "variable", "doc": "<p>The path to the model file to load the reference original model from.</p>\n", "annotation": ": str"}, {"fullname": "clarena.experiments.cul_full_eval.CULFullEval.output_dir", "modulename": "clarena.experiments.cul_full_eval", "qualname": "CULFullEval.output_dir", "kind": "variable", "doc": "<p>The main output directory to store the logs and checkpoints. Parsed from config and help any output operation to locate the correct directory.</p>\n", "annotation": ": str"}, {"fullname": "clarena.experiments.cul_full_eval.CULFullEval.cl_paradigm", "modulename": "clarena.experiments.cul_full_eval", "qualname": "CULFullEval.cl_paradigm", "kind": "variable", "doc": "<p>The continual learning paradigm, either 'TIL' (Task-Incremental Learning) or 'CIL' (Class-Incremental Learning).</p>\n", "annotation": ": str"}, {"fullname": "clarena.experiments.cul_full_eval.CULFullEval.dd_eval_tasks", "modulename": "clarena.experiments.cul_full_eval", "qualname": "CULFullEval.dd_eval_tasks", "kind": "variable", "doc": "<p>The list of tasks to be evaluated for JSD. Parsed from config and used in the evaluation loop.</p>\n", "annotation": ": list[int]"}, {"fullname": "clarena.experiments.cul_full_eval.CULFullEval.ad_eval_tasks", "modulename": "clarena.experiments.cul_full_eval", "qualname": "CULFullEval.ad_eval_tasks", "kind": "variable", "doc": "<p>The list of tasks to be evaluated for AD. Parsed from config and used in the evaluation loop.</p>\n", "annotation": ": list[int]"}, {"fullname": "clarena.experiments.cul_full_eval.CULFullEval.global_seed", "modulename": "clarena.experiments.cul_full_eval", "qualname": "CULFullEval.global_seed", "kind": "variable", "doc": "<p>Store the global seed for the entire experiment. Parsed from config and used to seed all random number generators.</p>\n", "annotation": ": int"}, {"fullname": "clarena.experiments.cul_full_eval.CULFullEval.cl_dataset", "modulename": "clarena.experiments.cul_full_eval", "qualname": "CULFullEval.cl_dataset", "kind": "variable", "doc": "<p>CL dataset object. Instantiate in <code>instantiate_cl_dataset()</code>.</p>\n", "annotation": ": clarena.cl_datasets.base.CLDataset"}, {"fullname": "clarena.experiments.cul_full_eval.CULFullEval.evaluation_module", "modulename": "clarena.experiments.cul_full_eval", "qualname": "CULFullEval.evaluation_module", "kind": "variable", "doc": "<p>Evaluation module for continual unlearning. Instantiate in <code>instantiate_evaluation_module()</code>.</p>\n", "annotation": ": clarena.utils.eval.CULEvaluation"}, {"fullname": "clarena.experiments.cul_full_eval.CULFullEval.trainer", "modulename": "clarena.experiments.cul_full_eval", "qualname": "CULFullEval.trainer", "kind": "variable", "doc": "<p>Trainer object. Instantiate in <code>instantiate_trainer()</code>.</p>\n", "annotation": ": lightning.pytorch.trainer.trainer.Trainer"}, {"fullname": "clarena.experiments.cul_full_eval.CULFullEval.callbacks", "modulename": "clarena.experiments.cul_full_eval", "qualname": "CULFullEval.callbacks", "kind": "variable", "doc": "<p>The list of initialized callbacks objects for the evaluation. Instantiate in <code>instantiate_callbacks()</code>.</p>\n", "annotation": ": list[lightning.pytorch.callbacks.callback.Callback]"}, {"fullname": "clarena.experiments.cul_full_eval.CULFullEval.task_id", "modulename": "clarena.experiments.cul_full_eval", "qualname": "CULFullEval.task_id", "kind": "variable", "doc": "<p>Task ID counter indicating which task is being processed. Self updated during the task loop. Starting from 1.</p>\n", "annotation": ": int"}, {"fullname": "clarena.experiments.cul_full_eval.CULFullEval.processed_task_ids", "modulename": "clarena.experiments.cul_full_eval", "qualname": "CULFullEval.processed_task_ids", "kind": "variable", "doc": "<p>Task IDs that have been processed in the experiment.</p>\n", "annotation": ": list[int]"}, {"fullname": "clarena.experiments.cul_full_eval.CULFullEval.sanity_check", "modulename": "clarena.experiments.cul_full_eval", "qualname": "CULFullEval.sanity_check", "kind": "function", "doc": "<p>Check the sanity of the config dict <code>self.cfg</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.cul_full_eval.CULFullEval.instantiate_cl_dataset", "modulename": "clarena.experiments.cul_full_eval", "qualname": "CULFullEval.instantiate_cl_dataset", "kind": "function", "doc": "<p>Instantiate the CL dataset object from cl_dataset config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>cl_dataset_cfg</strong> (<code>DictConfig</code>): the cl_dataset config dict.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">cl_dataset_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.cul_full_eval.CULFullEval.instantiate_evaluation_module", "modulename": "clarena.experiments.cul_full_eval", "qualname": "CULFullEval.instantiate_evaluation_module", "kind": "function", "doc": "<p>Instantiate the evaluation module object.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.cul_full_eval.CULFullEval.instantiate_trainer", "modulename": "clarena.experiments.cul_full_eval", "qualname": "CULFullEval.instantiate_trainer", "kind": "function", "doc": "<p>Instantiate the trainer object from trainer config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>trainer_cfg</strong> (<code>DictConfig</code>): the trainer config dict. All tasks share the same trainer config but different objects.</li>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">trainer_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.cul_full_eval.CULFullEval.instantiate_callbacks", "modulename": "clarena.experiments.cul_full_eval", "qualname": "CULFullEval.instantiate_callbacks", "kind": "function", "doc": "<p>Instantiate the list of callbacks objects from metrics and other callbacks config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>metrics_cfg</strong> (<code>DictConfig</code>): the metrics config dict. All tasks share the same callbacks config but different objects.</li>\n<li><strong>callbacks_cfg</strong> (<code>DictConfig</code>): the callbacks config dict. All tasks share the same callbacks config but different objects.</li>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">metrics_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span>,</span><span class=\"param\">\t<span class=\"n\">callbacks_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span>,</span><span class=\"param\">\t<span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.cul_full_eval.CULFullEval.set_global_seed", "modulename": "clarena.experiments.cul_full_eval", "qualname": "CULFullEval.set_global_seed", "kind": "function", "doc": "<p>Set the global seed for the entire experiment.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.cul_full_eval.CULFullEval.setup_task_id", "modulename": "clarena.experiments.cul_full_eval", "qualname": "CULFullEval.setup_task_id", "kind": "function", "doc": "<p>Set up current task_id in the beginning of the continual learning process of a new task.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): current task_id.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.cul_full_eval.CULFullEval.instantiate_global", "modulename": "clarena.experiments.cul_full_eval", "qualname": "CULFullEval.instantiate_global", "kind": "function", "doc": "<p>Instantiate dataset.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.cul_full_eval.CULFullEval.setup_global", "modulename": "clarena.experiments.cul_full_eval", "qualname": "CULFullEval.setup_global", "kind": "function", "doc": "<p>Let CL dataset know the CL paradigm to define its CL class map.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.cul_full_eval.CULFullEval.instantiate_task_specific", "modulename": "clarena.experiments.cul_full_eval", "qualname": "CULFullEval.instantiate_task_specific", "kind": "function", "doc": "<p>Instantiate task-specific components for the task <code>task_id</code> from <code>self.cfg</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.cul_full_eval.CULFullEval.setup_task_specific", "modulename": "clarena.experiments.cul_full_eval", "qualname": "CULFullEval.setup_task_specific", "kind": "function", "doc": "<p>Setup task-specific components to get ready for the task <code>task_id</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.cul_full_eval.CULFullEval.run", "modulename": "clarena.experiments.cul_full_eval", "qualname": "CULFullEval.run", "kind": "function", "doc": "<p>Run the evaluation: calls trainer.test() on the loaded model\nand the instantiated CLDataModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.culmain_train", "modulename": "clarena.experiments.culmain_train", "kind": "module", "doc": "<p>The submodule in <code>experiments</code> for continual unlearning main experiment.</p>\n"}, {"fullname": "clarena.experiments.culmain_train.CULMainTrain", "modulename": "clarena.experiments.culmain_train", "qualname": "CULMainTrain", "kind": "class", "doc": "<p>The base class for continual unlearning main experiment.</p>\n", "bases": "clarena.experiments.clmain_train.CLMainTrain"}, {"fullname": "clarena.experiments.culmain_train.CULMainTrain.__init__", "modulename": "clarena.experiments.culmain_train", "qualname": "CULMainTrain.__init__", "kind": "function", "doc": "<p>Initializes the CUL experiment object with a experiment configuration.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>cfg</strong> (<code>DictConfig</code>): the complete config dict for the CUL experiment.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span>)</span>"}, {"fullname": "clarena.experiments.culmain_train.CULMainTrain.unlearning_algorithm", "modulename": "clarena.experiments.culmain_train", "qualname": "CULMainTrain.unlearning_algorithm", "kind": "variable", "doc": "<p>Continual unlearning algorithm object. Instantiate in <code>instantiate_unlearning_algorithm()</code>.</p>\n", "annotation": ": clarena.unlearning_algorithms.base.CULAlgorithm"}, {"fullname": "clarena.experiments.culmain_train.CULMainTrain.unlearning_requests", "modulename": "clarena.experiments.culmain_train", "qualname": "CULMainTrain.unlearning_requests", "kind": "variable", "doc": "<p>The unlearning requests for each task in the experiment. Keys are IDs of the tasks that request unlearning after their learning, and values are the list of the previous tasks to be unlearned. Parsed from config and used in the tasks loop.</p>\n", "annotation": ": dict[int, list[int]]"}, {"fullname": "clarena.experiments.culmain_train.CULMainTrain.unlearned_task_ids", "modulename": "clarena.experiments.culmain_train", "qualname": "CULMainTrain.unlearned_task_ids", "kind": "variable", "doc": "<p>The list of task IDs that have been unlearned in the experiment. Updated in the tasks loop when unlearning requests are made.</p>\n", "annotation": ": set[int]"}, {"fullname": "clarena.experiments.culmain_train.CULMainTrain.permanent_mark", "modulename": "clarena.experiments.culmain_train", "qualname": "CULMainTrain.permanent_mark", "kind": "variable", "doc": "<p>Whether a task is permanent for each task in the experiment. If a task is permanent, it will not be unlearned i.e. not shown in future unlearning requests. This applies to some unlearning algorithms that need to know whether a task is permanent.</p>\n", "annotation": ": dict[int, bool]"}, {"fullname": "clarena.experiments.culmain_train.CULMainTrain.sanity_check", "modulename": "clarena.experiments.culmain_train", "qualname": "CULMainTrain.sanity_check", "kind": "function", "doc": "<p>Check the sanity of the config dict <code>self.cfg</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.culmain_train.CULMainTrain.instantiate_unlearning_algorithm", "modulename": "clarena.experiments.culmain_train", "qualname": "CULMainTrain.instantiate_unlearning_algorithm", "kind": "function", "doc": "<p>Instantiate the unlearning_algorithm object from unlearning_algorithm config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>unlearning_algorithm_cfg</strong> (<code>DictConfig</code>): the unlearning_algorithm config dict.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">unlearning_algorithm_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.culmain_train.CULMainTrain.instantiate_global", "modulename": "clarena.experiments.culmain_train", "qualname": "CULMainTrain.instantiate_global", "kind": "function", "doc": "<p>Instantiate global components for the entire CUL experiment from <code>self.cfg</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.culmain_train.CULMainTrain.setup_task_specific", "modulename": "clarena.experiments.culmain_train", "qualname": "CULMainTrain.setup_task_specific", "kind": "function", "doc": "<p>Setup task-specific components to get ready for the current task <code>self.task_id</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.culmain_train.CULMainTrain.run_task", "modulename": "clarena.experiments.culmain_train", "qualname": "CULMainTrain.run_task", "kind": "function", "doc": "<p>Run the continual learning process for a single task <code>self.task_id</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.mtl_eval", "modulename": "clarena.experiments.mtl_eval", "kind": "module", "doc": "<p>The submodule in <code>experiments</code> for evaluating trained multi-task learning experiment.</p>\n"}, {"fullname": "clarena.experiments.mtl_eval.MTLEval", "modulename": "clarena.experiments.mtl_eval", "qualname": "MTLEval", "kind": "class", "doc": "<p>The base class for evaluating trained multi-task learning experiment.</p>\n\n<p>This runs evaluation on a trained multi-task learning model read from saved model file, without any training loop.</p>\n"}, {"fullname": "clarena.experiments.mtl_eval.MTLEval.__init__", "modulename": "clarena.experiments.mtl_eval", "qualname": "MTLEval.__init__", "kind": "function", "doc": "<p>Initializes the MTL evaluation object with a evaluation configuration.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span>)</span>"}, {"fullname": "clarena.experiments.mtl_eval.MTLEval.cfg", "modulename": "clarena.experiments.mtl_eval", "qualname": "MTLEval.cfg", "kind": "variable", "doc": "<p>The complete config dict.</p>\n", "annotation": ": omegaconf.dictconfig.DictConfig"}, {"fullname": "clarena.experiments.mtl_eval.MTLEval.eval_tasks", "modulename": "clarena.experiments.mtl_eval", "qualname": "MTLEval.eval_tasks", "kind": "variable", "doc": "<p>Store the list of tasks to be evaluated. Parsed from config and used in the evaluation loop.</p>\n", "annotation": ": list[int]"}, {"fullname": "clarena.experiments.mtl_eval.MTLEval.global_seed", "modulename": "clarena.experiments.mtl_eval", "qualname": "MTLEval.global_seed", "kind": "variable", "doc": "<p>Store the global seed for the entire experiment. Parsed from config and used to seed all random number generators.</p>\n", "annotation": ": int"}, {"fullname": "clarena.experiments.mtl_eval.MTLEval.output_dir", "modulename": "clarena.experiments.mtl_eval", "qualname": "MTLEval.output_dir", "kind": "variable", "doc": "<p>Store the output directory to store the logs and checkpoints. Parsed from config and help any output operation to locate the correct directory.</p>\n", "annotation": ": str"}, {"fullname": "clarena.experiments.mtl_eval.MTLEval.model_path", "modulename": "clarena.experiments.mtl_eval", "qualname": "MTLEval.model_path", "kind": "variable", "doc": "<p>Store the model path to load the model from. Parsed from config and used to load the model for evaluation.</p>\n", "annotation": ": str"}, {"fullname": "clarena.experiments.mtl_eval.MTLEval.mtl_dataset", "modulename": "clarena.experiments.mtl_eval", "qualname": "MTLEval.mtl_dataset", "kind": "variable", "doc": "<p>MTL dataset object. Instantiate in <code>instantiate_mtl_dataset()</code>. One of <code>mtl_dataset</code> and <code>cl_dataset</code> must exist.</p>\n", "annotation": ": clarena.mtl_datasets.base.MTLDataset"}, {"fullname": "clarena.experiments.mtl_eval.MTLEval.cl_dataset", "modulename": "clarena.experiments.mtl_eval", "qualname": "MTLEval.cl_dataset", "kind": "variable", "doc": "<p>CL dataset object to construct MTL dataset. One of <code>mtl_dataset</code> and <code>cl_dataset</code> must exist.</p>\n", "annotation": ": clarena.cl_datasets.base.CLDataset"}, {"fullname": "clarena.experiments.mtl_eval.MTLEval.trainer", "modulename": "clarena.experiments.mtl_eval", "qualname": "MTLEval.trainer", "kind": "variable", "doc": "<p>Trainer object. Instantiate in <code>instantiate_trainer()</code>.</p>\n", "annotation": ": lightning.pytorch.trainer.trainer.Trainer"}, {"fullname": "clarena.experiments.mtl_eval.MTLEval.lightning_loggers", "modulename": "clarena.experiments.mtl_eval", "qualname": "MTLEval.lightning_loggers", "kind": "variable", "doc": "<p>The list of initialized lightning loggers objects for current task <code>self.task_id</code>. Instantiate in <code>instantiate_lightning_loggers()</code>.</p>\n", "annotation": ": list[lightning.pytorch.loggers.logger.Logger]"}, {"fullname": "clarena.experiments.mtl_eval.MTLEval.callbacks", "modulename": "clarena.experiments.mtl_eval", "qualname": "MTLEval.callbacks", "kind": "variable", "doc": "<p>The list of initialized callbacks objects for the evaluation. Instantiate in <code>instantiate_callbacks()</code>.</p>\n", "annotation": ": list[lightning.pytorch.callbacks.callback.Callback]"}, {"fullname": "clarena.experiments.mtl_eval.MTLEval.sanity_check", "modulename": "clarena.experiments.mtl_eval", "qualname": "MTLEval.sanity_check", "kind": "function", "doc": "<p>Check the sanity of the config dict <code>self.cfg</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.mtl_eval.MTLEval.instantiate_mtl_dataset", "modulename": "clarena.experiments.mtl_eval", "qualname": "MTLEval.instantiate_mtl_dataset", "kind": "function", "doc": "<p>Instantiate the MTL dataset object from mtl_dataset config, or construct the MTL dataset from cl_dataset_config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>mtl_dataset_cfg</strong> (<code>DictConfig</code>): the mtl_dataset config dict. When cl_dataset_cfg is not None, it should be None.</li>\n<li><strong>cl_dataset_cfg</strong> (<code>DictConfig</code>): the cl_dataset config dict used to construct the MTL dataset from the CL dataset. When mtl_dataset_cfg is not None, it should be None.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">mtl_dataset_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">cl_dataset_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span> <span class=\"o\">|</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.mtl_eval.MTLEval.instantiate_trainer", "modulename": "clarena.experiments.mtl_eval", "qualname": "MTLEval.instantiate_trainer", "kind": "function", "doc": "<p>Instantiate the trainer object from trainer config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>trainer_cfg</strong> (<code>DictConfig</code>): the trainer config dict.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">trainer_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.mtl_eval.MTLEval.instantiate_lightning_loggers", "modulename": "clarena.experiments.mtl_eval", "qualname": "MTLEval.instantiate_lightning_loggers", "kind": "function", "doc": "<p>Instantiate the list of lightning loggers objects from lightning_loggers config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>lightning_loggers_cfg</strong> (<code>DictConfig</code>): the lightning_loggers config dict.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">lightning_loggers_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.mtl_eval.MTLEval.instantiate_callbacks", "modulename": "clarena.experiments.mtl_eval", "qualname": "MTLEval.instantiate_callbacks", "kind": "function", "doc": "<p>Instantiate the list of callbacks objects from metrics and other callbacks config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>metrics_cfg</strong> (<code>DictConfig</code>): the metrics config dict.</li>\n<li><strong>callbacks_cfg</strong> (<code>DictConfig</code>): the callbacks config dict.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">metrics_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span>,</span><span class=\"param\">\t<span class=\"n\">callbacks_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.mtl_eval.MTLEval.set_global_seed", "modulename": "clarena.experiments.mtl_eval", "qualname": "MTLEval.set_global_seed", "kind": "function", "doc": "<p>Set the global seed for the entire experiment.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.mtl_eval.MTLEval.instantiate", "modulename": "clarena.experiments.mtl_eval", "qualname": "MTLEval.instantiate", "kind": "function", "doc": "<p>Instantiate components for the MTL experiment from <code>self.cfg</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.mtl_eval.MTLEval.setup", "modulename": "clarena.experiments.mtl_eval", "qualname": "MTLEval.setup", "kind": "function", "doc": "<p>Setup.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.mtl_eval.MTLEval.run", "modulename": "clarena.experiments.mtl_eval", "qualname": "MTLEval.run", "kind": "function", "doc": "<p>The main method to run the multi-task evaluation learning experiment.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.mtl_train", "modulename": "clarena.experiments.mtl_train", "kind": "module", "doc": "<p>The submodule in <code>experiments</code> for multi-task learning experiment.</p>\n\n<p>This module contains the <code>MTLTrain</code> class, which is the main entry point for running multi-task learning experiments in the Continual Learning Arena. It handles the instantiation of various components such as datasets, backbones, heads, algorithms, optimizers, learning rate schedulers, trainers, loggers, and callbacks.</p>\n"}, {"fullname": "clarena.experiments.mtl_train.MTLTrain", "modulename": "clarena.experiments.mtl_train", "qualname": "MTLTrain", "kind": "class", "doc": "<p>The base class for multi-task learning experiment.</p>\n\n<p>This module contains the <code>MTLTrain</code> class, which is the main entry point for running multi-task learning experiments in the Continual Learning Arena. It handles the instantiation of various components such as datasets, backbones, heads, algorithms, optimizers, learning rate schedulers, trainers, loggers, and callbacks.</p>\n"}, {"fullname": "clarena.experiments.mtl_train.MTLTrain.__init__", "modulename": "clarena.experiments.mtl_train", "qualname": "MTLTrain.__init__", "kind": "function", "doc": "<p>Initializes the MTL experiment object with a multi-task learning configuration.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>cfg</strong> (<code>DictConfig</code>): the complete config dict for the MTL experiment.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span>)</span>"}, {"fullname": "clarena.experiments.mtl_train.MTLTrain.cfg", "modulename": "clarena.experiments.mtl_train", "qualname": "MTLTrain.cfg", "kind": "variable", "doc": "<p>The complete config dict.</p>\n", "annotation": ": omegaconf.dictconfig.DictConfig"}, {"fullname": "clarena.experiments.mtl_train.MTLTrain.train_tasks", "modulename": "clarena.experiments.mtl_train", "qualname": "MTLTrain.train_tasks", "kind": "variable", "doc": "<p>The list of tasks to be jointly trained.</p>\n", "annotation": ": list[int]"}, {"fullname": "clarena.experiments.mtl_train.MTLTrain.eval_tasks", "modulename": "clarena.experiments.mtl_train", "qualname": "MTLTrain.eval_tasks", "kind": "variable", "doc": "<p>The list of tasks to be evaluated.</p>\n", "annotation": ": list[int]"}, {"fullname": "clarena.experiments.mtl_train.MTLTrain.global_seed", "modulename": "clarena.experiments.mtl_train", "qualname": "MTLTrain.global_seed", "kind": "variable", "doc": "<p>The global seed for the entire experiment.</p>\n", "annotation": ": int"}, {"fullname": "clarena.experiments.mtl_train.MTLTrain.output_dir", "modulename": "clarena.experiments.mtl_train", "qualname": "MTLTrain.output_dir", "kind": "variable", "doc": "<p>The folder name for storing the experiment results.</p>\n", "annotation": ": str"}, {"fullname": "clarena.experiments.mtl_train.MTLTrain.mtl_dataset", "modulename": "clarena.experiments.mtl_train", "qualname": "MTLTrain.mtl_dataset", "kind": "variable", "doc": "<p>MTL dataset object. Instantiate in <code>instantiate_mtl_dataset()</code>. One of <code>mtl_dataset</code> and <code>cl_dataset</code> must exist.</p>\n", "annotation": ": clarena.mtl_datasets.base.MTLDataset"}, {"fullname": "clarena.experiments.mtl_train.MTLTrain.cl_dataset", "modulename": "clarena.experiments.mtl_train", "qualname": "MTLTrain.cl_dataset", "kind": "variable", "doc": "<p>CL dataset object to construct MTL dataset. One of <code>mtl_dataset</code> and <code>cl_dataset</code> must exist.</p>\n", "annotation": ": clarena.cl_datasets.base.CLDataset"}, {"fullname": "clarena.experiments.mtl_train.MTLTrain.backbone", "modulename": "clarena.experiments.mtl_train", "qualname": "MTLTrain.backbone", "kind": "variable", "doc": "<p>Backbone network object. Instantiate in <code>instantiate_backbone()</code>.</p>\n", "annotation": ": clarena.backbones.base.CLBackbone"}, {"fullname": "clarena.experiments.mtl_train.MTLTrain.heads", "modulename": "clarena.experiments.mtl_train", "qualname": "MTLTrain.heads", "kind": "variable", "doc": "<p>MTL output heads object. Instantiate in <code>instantiate_heads()</code>.</p>\n", "annotation": ": clarena.heads.heads_mtl.HeadsMTL"}, {"fullname": "clarena.experiments.mtl_train.MTLTrain.model", "modulename": "clarena.experiments.mtl_train", "qualname": "MTLTrain.model", "kind": "variable", "doc": "<p>MTL model object. Instantiate in <code>instantiate_mtl_algorithm()</code>.</p>\n", "annotation": ": clarena.mtl_algorithms.base.MTLAlgorithm"}, {"fullname": "clarena.experiments.mtl_train.MTLTrain.optimizer", "modulename": "clarena.experiments.mtl_train", "qualname": "MTLTrain.optimizer", "kind": "variable", "doc": "<p>Optimizer object. Instantiate in <code>instantiate_optimizer()</code>.</p>\n", "annotation": ": torch.optim.optimizer.Optimizer"}, {"fullname": "clarena.experiments.mtl_train.MTLTrain.lr_scheduler", "modulename": "clarena.experiments.mtl_train", "qualname": "MTLTrain.lr_scheduler", "kind": "variable", "doc": "<p>Learning rate scheduler object. Instantiate in <code>instantiate_lr_scheduler()</code>.</p>\n", "annotation": ": torch.optim.lr_scheduler.LRScheduler"}, {"fullname": "clarena.experiments.mtl_train.MTLTrain.trainer", "modulename": "clarena.experiments.mtl_train", "qualname": "MTLTrain.trainer", "kind": "variable", "doc": "<p>Trainer object. Instantiate in <code>instantiate_trainer()</code>.</p>\n", "annotation": ": lightning.pytorch.trainer.trainer.Trainer"}, {"fullname": "clarena.experiments.mtl_train.MTLTrain.lightning_loggers", "modulename": "clarena.experiments.mtl_train", "qualname": "MTLTrain.lightning_loggers", "kind": "variable", "doc": "<p>The list of initialized lightning loggers objects. Instantiate in <code>instantiate_lightning_loggers()</code>.</p>\n", "annotation": ": list[lightning.pytorch.loggers.logger.Logger]"}, {"fullname": "clarena.experiments.mtl_train.MTLTrain.callbacks", "modulename": "clarena.experiments.mtl_train", "qualname": "MTLTrain.callbacks", "kind": "variable", "doc": "<p>The list of initialized callbacks objects. Instantiate in <code>instantiate_callbacks()</code>.</p>\n", "annotation": ": list[lightning.pytorch.callbacks.callback.Callback]"}, {"fullname": "clarena.experiments.mtl_train.MTLTrain.sanity_check", "modulename": "clarena.experiments.mtl_train", "qualname": "MTLTrain.sanity_check", "kind": "function", "doc": "<p>Check the sanity of the config dict <code>self.cfg</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.mtl_train.MTLTrain.instantiate_mtl_dataset", "modulename": "clarena.experiments.mtl_train", "qualname": "MTLTrain.instantiate_mtl_dataset", "kind": "function", "doc": "<p>Instantiate the MTL dataset object from mtl_dataset config, or construct the MTL dataset from cl_dataset_config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>mtl_dataset_cfg</strong> (<code>DictConfig</code>): the mtl_dataset config dict. When cl_dataset_cfg is not None, it should be None.</li>\n<li><strong>cl_dataset_cfg</strong> (<code>DictConfig</code>): the cl_dataset config dict used to construct the MTL dataset from the CL dataset. When mtl_dataset_cfg is not None, it should be None.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">mtl_dataset_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">cl_dataset_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span> <span class=\"o\">|</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.mtl_train.MTLTrain.instantiate_backbone", "modulename": "clarena.experiments.mtl_train", "qualname": "MTLTrain.instantiate_backbone", "kind": "function", "doc": "<p>Instantiate the MTL backbone network object from backbone config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone_cfg</strong> (<code>DictConfig</code>): the backbone config dict.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">backbone_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.mtl_train.MTLTrain.instantiate_heads", "modulename": "clarena.experiments.mtl_train", "qualname": "MTLTrain.instantiate_heads", "kind": "function", "doc": "<p>Instantiate the MTL output heads object according to field  backbone <code>output_dim</code> in the config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_dim</strong> (<code>int</code>): the input dimension of the heads. Must be equal to the <code>output_dim</code> of the connected backbone.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">input_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.mtl_train.MTLTrain.instantiate_mtl_algorithm", "modulename": "clarena.experiments.mtl_train", "qualname": "MTLTrain.instantiate_mtl_algorithm", "kind": "function", "doc": "<p>Instantiate the mtl_algorithm object from mtl_algorithm config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>mtl_algorithm_cfg</strong> (<code>DictConfig</code>): the mtl_algorithm config dict.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">mtl_algorithm_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.mtl_train.MTLTrain.instantiate_optimizer", "modulename": "clarena.experiments.mtl_train", "qualname": "MTLTrain.instantiate_optimizer", "kind": "function", "doc": "<p>Instantiate the optimizer object from optimizer config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>optimizer_cfg</strong> (<code>DictConfig</code>): the optimizer config dict.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">optimizer_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.mtl_train.MTLTrain.instantiate_lr_scheduler", "modulename": "clarena.experiments.mtl_train", "qualname": "MTLTrain.instantiate_lr_scheduler", "kind": "function", "doc": "<p>Instantiate the learning rate scheduler object from lr_scheduler config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>lr_scheduler_cfg</strong> (<code>DictConfig</code>): the learning rate scheduler config dict.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">lr_scheduler_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.mtl_train.MTLTrain.instantiate_trainer", "modulename": "clarena.experiments.mtl_train", "qualname": "MTLTrain.instantiate_trainer", "kind": "function", "doc": "<p>Instantiate the trainer object from trainer config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>trainer_cfg</strong> (<code>DictConfig</code>): the trainer config dict.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">trainer_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.mtl_train.MTLTrain.instantiate_lightning_loggers", "modulename": "clarena.experiments.mtl_train", "qualname": "MTLTrain.instantiate_lightning_loggers", "kind": "function", "doc": "<p>Instantiate the list of lightning loggers objects from lightning_loggers config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>lightning_loggers_cfg</strong> (<code>DictConfig</code>): the lightning_loggers config dict.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">lightning_loggers_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.mtl_train.MTLTrain.instantiate_callbacks", "modulename": "clarena.experiments.mtl_train", "qualname": "MTLTrain.instantiate_callbacks", "kind": "function", "doc": "<p>Instantiate the list of callbacks objects from metrics and other callbacks config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>metrics_cfg</strong> (<code>DictConfig</code>): the metrics config dict.</li>\n<li><strong>callbacks_cfg</strong> (<code>DictConfig</code>): the callbacks config dict.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">metrics_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span>,</span><span class=\"param\">\t<span class=\"n\">callbacks_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.mtl_train.MTLTrain.set_global_seed", "modulename": "clarena.experiments.mtl_train", "qualname": "MTLTrain.set_global_seed", "kind": "function", "doc": "<p>Set the global seed for the entire experiment.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.mtl_train.MTLTrain.instantiate", "modulename": "clarena.experiments.mtl_train", "qualname": "MTLTrain.instantiate", "kind": "function", "doc": "<p>Instantiate components for the MTL experiment from <code>self.cfg</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.mtl_train.MTLTrain.setup", "modulename": "clarena.experiments.mtl_train", "qualname": "MTLTrain.setup", "kind": "function", "doc": "<p>Setup.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.mtl_train.MTLTrain.run", "modulename": "clarena.experiments.mtl_train", "qualname": "MTLTrain.run", "kind": "function", "doc": "<p>The main method to run the multi-task learning experiment.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.stl_eval", "modulename": "clarena.experiments.stl_eval", "kind": "module", "doc": "<p>The submodule in <code>experiments</code> for evaluating trained single-task learning experiment.</p>\n"}, {"fullname": "clarena.experiments.stl_eval.STLEval", "modulename": "clarena.experiments.stl_eval", "qualname": "STLEval", "kind": "class", "doc": "<p>The base class for evaluating trained single-task learning experiment.</p>\n\n<p>This runs evaluation on a trained single-task learning model read from saved model file, without any training loop.</p>\n"}, {"fullname": "clarena.experiments.stl_eval.STLEval.__init__", "modulename": "clarena.experiments.stl_eval", "qualname": "STLEval.__init__", "kind": "function", "doc": "<p>Initializes the STL evaluation object with a evaluation configuration.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span>)</span>"}, {"fullname": "clarena.experiments.stl_eval.STLEval.cfg", "modulename": "clarena.experiments.stl_eval", "qualname": "STLEval.cfg", "kind": "variable", "doc": "<p>The complete config dict.</p>\n", "annotation": ": omegaconf.dictconfig.DictConfig"}, {"fullname": "clarena.experiments.stl_eval.STLEval.global_seed", "modulename": "clarena.experiments.stl_eval", "qualname": "STLEval.global_seed", "kind": "variable", "doc": "<p>Store the global seed for the entire experiment. Parsed from config and used to seed all random number generators.</p>\n", "annotation": ": int"}, {"fullname": "clarena.experiments.stl_eval.STLEval.output_dir", "modulename": "clarena.experiments.stl_eval", "qualname": "STLEval.output_dir", "kind": "variable", "doc": "<p>Store the output directory to store the logs and checkpoints. Parsed from config and help any output operation to locate the correct directory.</p>\n", "annotation": ": str"}, {"fullname": "clarena.experiments.stl_eval.STLEval.model_path", "modulename": "clarena.experiments.stl_eval", "qualname": "STLEval.model_path", "kind": "variable", "doc": "<p>Store the model path to load the model from. Parsed from config and used to load the model for evaluation.</p>\n", "annotation": ": str"}, {"fullname": "clarena.experiments.stl_eval.STLEval.stl_dataset", "modulename": "clarena.experiments.stl_eval", "qualname": "STLEval.stl_dataset", "kind": "variable", "doc": "<p>STL dataset object. Instantiate in <code>instantiate_stl_dataset()</code>.</p>\n", "annotation": ": clarena.stl_datasets.base.STLDataset"}, {"fullname": "clarena.experiments.stl_eval.STLEval.trainer", "modulename": "clarena.experiments.stl_eval", "qualname": "STLEval.trainer", "kind": "variable", "doc": "<p>Trainer object. Instantiate in <code>instantiate_trainer()</code>.</p>\n", "annotation": ": lightning.pytorch.trainer.trainer.Trainer"}, {"fullname": "clarena.experiments.stl_eval.STLEval.lightning_loggers", "modulename": "clarena.experiments.stl_eval", "qualname": "STLEval.lightning_loggers", "kind": "variable", "doc": "<p>The list of initialized lightning loggers objects for current task <code>self.task_id</code>. Instantiate in <code>instantiate_lightning_loggers()</code>.</p>\n", "annotation": ": list[lightning.pytorch.loggers.logger.Logger]"}, {"fullname": "clarena.experiments.stl_eval.STLEval.callbacks", "modulename": "clarena.experiments.stl_eval", "qualname": "STLEval.callbacks", "kind": "variable", "doc": "<p>The list of initialized callbacks objects for the evaluation. Instantiate in <code>instantiate_callbacks()</code>.</p>\n", "annotation": ": list[lightning.pytorch.callbacks.callback.Callback]"}, {"fullname": "clarena.experiments.stl_eval.STLEval.sanity_check", "modulename": "clarena.experiments.stl_eval", "qualname": "STLEval.sanity_check", "kind": "function", "doc": "<p>Check the sanity of the config dict <code>self.cfg</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.stl_eval.STLEval.instantiate_stl_dataset", "modulename": "clarena.experiments.stl_eval", "qualname": "STLEval.instantiate_stl_dataset", "kind": "function", "doc": "<p>Instantiate the STL dataset object from stl_dataset config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>stl_dataset_cfg</strong> (<code>DictConfig</code>): the stl_dataset config dict.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">stl_dataset_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.stl_eval.STLEval.instantiate_trainer", "modulename": "clarena.experiments.stl_eval", "qualname": "STLEval.instantiate_trainer", "kind": "function", "doc": "<p>Instantiate the trainer object from trainer config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>trainer_cfg</strong> (<code>DictConfig</code>): the trainer config dict.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">trainer_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.stl_eval.STLEval.instantiate_lightning_loggers", "modulename": "clarena.experiments.stl_eval", "qualname": "STLEval.instantiate_lightning_loggers", "kind": "function", "doc": "<p>Instantiate the list of lightning loggers objects from lightning_loggers config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>lightning_loggers_cfg</strong> (<code>DictConfig</code>): the lightning_loggers config dict.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">lightning_loggers_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.stl_eval.STLEval.instantiate_callbacks", "modulename": "clarena.experiments.stl_eval", "qualname": "STLEval.instantiate_callbacks", "kind": "function", "doc": "<p>Instantiate the list of callbacks objects from metrics and other callbacks config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>metrics_cfg</strong> (<code>DictConfig</code>): the metrics config dict.</li>\n<li><strong>callbacks_cfg</strong> (<code>DictConfig</code>): the callbacks config dict.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">metrics_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span>,</span><span class=\"param\">\t<span class=\"n\">callbacks_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.stl_eval.STLEval.set_global_seed", "modulename": "clarena.experiments.stl_eval", "qualname": "STLEval.set_global_seed", "kind": "function", "doc": "<p>Set the global seed for the entire experiment.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.stl_eval.STLEval.instantiate", "modulename": "clarena.experiments.stl_eval", "qualname": "STLEval.instantiate", "kind": "function", "doc": "<p>Instantiate components for the STL experiment from <code>self.cfg</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.stl_eval.STLEval.setup", "modulename": "clarena.experiments.stl_eval", "qualname": "STLEval.setup", "kind": "function", "doc": "<p>Setup.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.stl_eval.STLEval.run", "modulename": "clarena.experiments.stl_eval", "qualname": "STLEval.run", "kind": "function", "doc": "<p>The main method to run the single-task learning experiment.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.stl_train", "modulename": "clarena.experiments.stl_train", "kind": "module", "doc": "<p>The submodule in <code>experiments</code> for single-task learning experiment.</p>\n"}, {"fullname": "clarena.experiments.stl_train.STLTrain", "modulename": "clarena.experiments.stl_train", "qualname": "STLTrain", "kind": "class", "doc": "<p>The base class for single-task learning experiment.</p>\n"}, {"fullname": "clarena.experiments.stl_train.STLTrain.__init__", "modulename": "clarena.experiments.stl_train", "qualname": "STLTrain.__init__", "kind": "function", "doc": "<p>Initializes the STL experiment object with a single-task learning configuration.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>cfg</strong> (<code>DictConfig</code>): the complete config dict for the STL experiment.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span>)</span>"}, {"fullname": "clarena.experiments.stl_train.STLTrain.cfg", "modulename": "clarena.experiments.stl_train", "qualname": "STLTrain.cfg", "kind": "variable", "doc": "<p>The complete config dict.</p>\n", "annotation": ": omegaconf.dictconfig.DictConfig"}, {"fullname": "clarena.experiments.stl_train.STLTrain.global_seed", "modulename": "clarena.experiments.stl_train", "qualname": "STLTrain.global_seed", "kind": "variable", "doc": "<p>The global seed for the entire experiment.</p>\n", "annotation": ": int"}, {"fullname": "clarena.experiments.stl_train.STLTrain.stl_dataset", "modulename": "clarena.experiments.stl_train", "qualname": "STLTrain.stl_dataset", "kind": "variable", "doc": "<p>STL dataset object. Instantiate in <code>instantiate_stl_dataset()</code>.</p>\n", "annotation": ": clarena.stl_datasets.base.STLDataset"}, {"fullname": "clarena.experiments.stl_train.STLTrain.backbone", "modulename": "clarena.experiments.stl_train", "qualname": "STLTrain.backbone", "kind": "variable", "doc": "<p>Backbone network object. Instantiate in <code>instantiate_backbone()</code>.</p>\n", "annotation": ": clarena.backbones.base.Backbone"}, {"fullname": "clarena.experiments.stl_train.STLTrain.head", "modulename": "clarena.experiments.stl_train", "qualname": "STLTrain.head", "kind": "variable", "doc": "<p>STL output heads object. Instantiate in <code>instantiate_head()</code>.</p>\n", "annotation": ": clarena.heads.head_stl.HeadSTL"}, {"fullname": "clarena.experiments.stl_train.STLTrain.model", "modulename": "clarena.experiments.stl_train", "qualname": "STLTrain.model", "kind": "variable", "doc": "<p>STL model object. Instantiate in <code>instantiate_stl_algorithm()</code>.</p>\n", "annotation": ": clarena.stl_algorithms.base.STLAlgorithm"}, {"fullname": "clarena.experiments.stl_train.STLTrain.optimizer", "modulename": "clarena.experiments.stl_train", "qualname": "STLTrain.optimizer", "kind": "variable", "doc": "<p>Optimizer object. Instantiate in <code>instantiate_optimizer()</code>.</p>\n", "annotation": ": torch.optim.optimizer.Optimizer"}, {"fullname": "clarena.experiments.stl_train.STLTrain.lr_scheduler", "modulename": "clarena.experiments.stl_train", "qualname": "STLTrain.lr_scheduler", "kind": "variable", "doc": "<p>Learning rate scheduler object. Instantiate in <code>instantiate_lr_scheduler()</code>.</p>\n", "annotation": ": torch.optim.lr_scheduler.LRScheduler"}, {"fullname": "clarena.experiments.stl_train.STLTrain.trainer", "modulename": "clarena.experiments.stl_train", "qualname": "STLTrain.trainer", "kind": "variable", "doc": "<p>Trainer object. Instantiate in <code>instantiate_trainer()</code>.</p>\n", "annotation": ": lightning.pytorch.trainer.trainer.Trainer"}, {"fullname": "clarena.experiments.stl_train.STLTrain.lightning_loggers", "modulename": "clarena.experiments.stl_train", "qualname": "STLTrain.lightning_loggers", "kind": "variable", "doc": "<p>The list of initialized lightning loggers objects. Instantiate in <code>instantiate_lightning_loggers()</code>.</p>\n", "annotation": ": list[lightning.pytorch.loggers.logger.Logger]"}, {"fullname": "clarena.experiments.stl_train.STLTrain.callbacks", "modulename": "clarena.experiments.stl_train", "qualname": "STLTrain.callbacks", "kind": "variable", "doc": "<p>The list of initialized callbacks objects. Instantiate in <code>instantiate_callbacks()</code>.</p>\n", "annotation": ": list[lightning.pytorch.callbacks.callback.Callback]"}, {"fullname": "clarena.experiments.stl_train.STLTrain.sanity_check", "modulename": "clarena.experiments.stl_train", "qualname": "STLTrain.sanity_check", "kind": "function", "doc": "<p>Check the sanity of the config dict <code>self.cfg</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.stl_train.STLTrain.instantiate_stl_dataset", "modulename": "clarena.experiments.stl_train", "qualname": "STLTrain.instantiate_stl_dataset", "kind": "function", "doc": "<p>Instantiate the STL dataset object from stl_dataset config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>stl_dataset_cfg</strong> (<code>DictConfig</code>): the stl_dataset config dict.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">stl_dataset_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.stl_train.STLTrain.instantiate_backbone", "modulename": "clarena.experiments.stl_train", "qualname": "STLTrain.instantiate_backbone", "kind": "function", "doc": "<p>Instantiate the MTL backbone network object from backbone config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone_cfg</strong> (<code>DictConfig</code>): the backbone config dict.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">backbone_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.stl_train.STLTrain.instantiate_head", "modulename": "clarena.experiments.stl_train", "qualname": "STLTrain.instantiate_head", "kind": "function", "doc": "<p>Instantiate the STL output head object according to <code>output_dim</code> in the config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_dim</strong> (<code>int</code>): the input dimension of the heads. Must be equal to the <code>output_dim</code> of the connected backbone.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">input_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.stl_train.STLTrain.instantiate_stl_algorithm", "modulename": "clarena.experiments.stl_train", "qualname": "STLTrain.instantiate_stl_algorithm", "kind": "function", "doc": "<p>Instantiate the stl_algorithm object from stl_algorithm config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>stl_algorithm_cfg</strong> (<code>DictConfig</code>): the stl_algorithm config dict.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">stl_algorithm_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.stl_train.STLTrain.instantiate_optimizer", "modulename": "clarena.experiments.stl_train", "qualname": "STLTrain.instantiate_optimizer", "kind": "function", "doc": "<p>Instantiate the optimizer object from optimizer config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>optimizer_cfg</strong> (<code>DictConfig</code>): the optimizer config dict.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">optimizer_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.stl_train.STLTrain.instantiate_lr_scheduler", "modulename": "clarena.experiments.stl_train", "qualname": "STLTrain.instantiate_lr_scheduler", "kind": "function", "doc": "<p>Instantiate the learning rate scheduler object from lr_scheduler config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>lr_scheduler_cfg</strong> (<code>DictConfig</code>): the learning rate scheduler config dict.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">lr_scheduler_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.stl_train.STLTrain.instantiate_trainer", "modulename": "clarena.experiments.stl_train", "qualname": "STLTrain.instantiate_trainer", "kind": "function", "doc": "<p>Instantiate the trainer object from trainer config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>trainer_cfg</strong> (<code>DictConfig</code>): the trainer config dict.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">trainer_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.stl_train.STLTrain.instantiate_lightning_loggers", "modulename": "clarena.experiments.stl_train", "qualname": "STLTrain.instantiate_lightning_loggers", "kind": "function", "doc": "<p>Instantiate the list of lightning loggers objects from lightning_loggers config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>lightning_loggers_cfg</strong> (<code>DictConfig</code>): the lightning_loggers config dict.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">lightning_loggers_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.stl_train.STLTrain.instantiate_callbacks", "modulename": "clarena.experiments.stl_train", "qualname": "STLTrain.instantiate_callbacks", "kind": "function", "doc": "<p>Instantiate the list of callbacks objects from metrics and other callbacks config.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>metrics_cfg</strong> (<code>DictConfig</code>): the metrics config dict.</li>\n<li><strong>callbacks_cfg</strong> (<code>DictConfig</code>): the callbacks config dict.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">metrics_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span>,</span><span class=\"param\">\t<span class=\"n\">callbacks_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.stl_train.STLTrain.set_global_seed", "modulename": "clarena.experiments.stl_train", "qualname": "STLTrain.set_global_seed", "kind": "function", "doc": "<p>Set the global seed for the entire experiment.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.stl_train.STLTrain.instantiate", "modulename": "clarena.experiments.stl_train", "qualname": "STLTrain.instantiate", "kind": "function", "doc": "<p>Instantiate components for the JL experiment from <code>self.cfg</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.stl_train.STLTrain.setup", "modulename": "clarena.experiments.stl_train", "qualname": "STLTrain.setup", "kind": "function", "doc": "<p>Setup.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.experiments.stl_train.STLTrain.run", "modulename": "clarena.experiments.stl_train", "qualname": "STLTrain.run", "kind": "function", "doc": "<p>The main method to run the single-task learning experiment.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.heads", "modulename": "clarena.heads", "kind": "module", "doc": "<h1 id=\"output-heads\">Output Heads</h1>\n\n<p>This submodule provides the <strong>output heads</strong> in CLArena.</p>\n\n<p>There are two types of continual learning / unlearning heads in CLArena: <code>HeadsTIL</code> and <code>HeadsCIL</code>, corresponding to two CL paradigms respectively: Task-Incremental Learning (TIL) and Class-Incremental Learning (CIL). For Multi-Task Learning (MTL), we have <code>HeadsMTL</code> which is a collection of independent heads for each task.</p>\n\n<p>Please note that this is an API documantation. Please refer to the main documentation pages for more information about the heads.</p>\n\n<ul>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/configure-your-experiment/experiment-index-config\"><strong>Configure CL Paradigm in Experiment Index Config</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/posts/continual-learning-beginners-guide#sec-CL-classification\">**A Beginners' Guide to Continual Learning (Multi-head Classifier)</a></li>\n</ul>\n"}, {"fullname": "clarena.heads.HeadsTIL", "modulename": "clarena.heads", "qualname": "HeadsTIL", "kind": "class", "doc": "<p>The output heads for Task-Incremental Learning (TIL). Independent head assigned to each TIL task takes the output from backbone network and forwards it into logits for predicting classes of the task.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "clarena.heads.HeadsTIL.__init__", "modulename": "clarena.heads", "qualname": "HeadsTIL.__init__", "kind": "function", "doc": "<p>Initializes TIL heads object with no heads.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_dim</strong> (<code>int</code>): the input dimension of the heads. Must be equal to the <code>output_dim</code> of the connected backbone.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span>)</span>"}, {"fullname": "clarena.heads.HeadsTIL.heads", "modulename": "clarena.heads", "qualname": "HeadsTIL.heads", "kind": "variable", "doc": "<p>TIL output heads are stored independently in a <code>ModuleDict</code>. Keys are task IDs and values are the corresponding <code>nn.Linear</code> heads. We use <code>ModuleDict</code> rather than <code>dict</code> to make sure <code>LightningModule</code> can track these model parameters for the purpose of, such as automatically to device, recorded in model summaries.</p>\n\n<p>Note that the task IDs must be string type in order to let <code>LightningModule</code> identify this part of the model.</p>\n", "annotation": ": torch.nn.modules.container.ModuleDict"}, {"fullname": "clarena.heads.HeadsTIL.head_t", "modulename": "clarena.heads", "qualname": "HeadsTIL.head_t", "kind": "variable", "doc": "<p>The output head for the current task. It is created when the task arrives and stored in <code>self.heads</code>.</p>\n", "annotation": ": torch.nn.modules.linear.Linear | None"}, {"fullname": "clarena.heads.HeadsTIL.input_dim", "modulename": "clarena.heads", "qualname": "HeadsTIL.input_dim", "kind": "variable", "doc": "<p>Store the input dimension of the heads. Used when creating new heads.</p>\n", "annotation": ": int"}, {"fullname": "clarena.heads.HeadsTIL.task_id", "modulename": "clarena.heads", "qualname": "HeadsTIL.task_id", "kind": "variable", "doc": "<p>Task ID counter indicating which task is being processed. Self updated during the task loop. Starting from 1.</p>\n", "annotation": ": int"}, {"fullname": "clarena.heads.HeadsTIL.setup_task_id", "modulename": "clarena.heads", "qualname": "HeadsTIL.setup_task_id", "kind": "function", "doc": "<p>Create the output head when task <code>task_id</code> arrives if there's no. This must be done before <code>forward()</code> is called.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID.</li>\n<li><strong>num_classes_t</strong> (<code>int</code>): the number of classes in the task.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span>, </span><span class=\"param\"><span class=\"n\">num_classes_t</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.heads.HeadsTIL.get_head", "modulename": "clarena.heads", "qualname": "HeadsTIL.get_head", "kind": "function", "doc": "<p>Get the output head for task <code>task_id</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>head_t</strong> (<code>nn.Linear</code>): the output head for task <code>task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">linear</span><span class=\"o\">.</span><span class=\"n\">Linear</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.heads.HeadsTIL.forward", "modulename": "clarena.heads", "qualname": "HeadsTIL.forward", "kind": "function", "doc": "<p>The forward pass for data from task <code>task_id</code>. A head is selected according to the task_id and the feature is passed through the head.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>feature</strong> (<code>Tensor</code>): the feature tensor from the backbone network.</li>\n<li><strong>task_id</strong> (<code>int</code>): the task ID where the data are from, which is provided by task-incremental setting.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>logits</strong> (<code>Tensor</code>): the output logits tensor.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">feature</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.heads.HeadsCIL", "modulename": "clarena.heads", "qualname": "HeadsCIL", "kind": "class", "doc": "<p>The output heads for Class-Incremental Learning (CIL). Head of all classes from CIL tasks takes the output from backbone network and forwards it into logits for predicting classes of all tasks.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "clarena.heads.HeadsCIL.__init__", "modulename": "clarena.heads", "qualname": "HeadsCIL.__init__", "kind": "function", "doc": "<p>Initializes a CIL heads object with no heads.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_dim</strong> (<code>int</code>): the input dimension of the heads. Must be equal to the <code>output_dim</code> of the connected backbone.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span>)</span>"}, {"fullname": "clarena.heads.HeadsCIL.heads", "modulename": "clarena.heads", "qualname": "HeadsCIL.heads", "kind": "variable", "doc": "<p>CIL output heads are stored in a <code>ModuleDict</code>. Keys are task IDs and values are the corresponding <code>nn.Linear</code> heads. We use <code>ModuleDict</code> rather than <code>dict</code> to make sure <code>LightningModule</code> can track these model parameters for the purpose of, such as automatically to device, recorded in model summaries.</p>\n", "annotation": ": torch.nn.modules.container.ModuleDict"}, {"fullname": "clarena.heads.HeadsCIL.input_dim", "modulename": "clarena.heads", "qualname": "HeadsCIL.input_dim", "kind": "variable", "doc": "<p>The input dimension of the heads. Used when creating new heads.</p>\n", "annotation": ": int"}, {"fullname": "clarena.heads.HeadsCIL.task_id", "modulename": "clarena.heads", "qualname": "HeadsCIL.task_id", "kind": "variable", "doc": "<p>Task ID counter indicating which task is being processed. Self updated during the task loop. Starting from 1.</p>\n", "annotation": ": int"}, {"fullname": "clarena.heads.HeadsCIL.processed_task_ids", "modulename": "clarena.heads", "qualname": "HeadsCIL.processed_task_ids", "kind": "variable", "doc": "<p>Task IDs that have been processed in the experiment.</p>\n", "annotation": ": list[int]"}, {"fullname": "clarena.heads.HeadsCIL.setup_task_id", "modulename": "clarena.heads", "qualname": "HeadsCIL.setup_task_id", "kind": "function", "doc": "<p>Create the output head when task <code>task_id</code> arrives if there's no. This must be done before <code>forward()</code> is called.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID.</li>\n<li><strong>num_classes_t</strong> (<code>int</code>): the number of classes in the task.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span>, </span><span class=\"param\"><span class=\"n\">num_classes_t</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.heads.HeadsCIL.forward", "modulename": "clarena.heads", "qualname": "HeadsCIL.forward", "kind": "function", "doc": "<p>The forward pass for data. The information of which <code>task_id</code> the data are from is not provided. The head for all classes is selected and the feature is passed.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>feature</strong> (<code>Tensor</code>): the feature tensor from the backbone network.</li>\n<li><strong>task_id</strong> (<code>int</code> or <code>None</code>): the task ID where the data are from. In CIL, it is just a placeholder for API consistence with the TIL heads but never used. Best practices are not to provide this argument and leave it as the default value.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>logits</strong> (<code>Tensor</code>): the output logits tensor.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">feature</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.heads.HeadsMTL", "modulename": "clarena.heads", "qualname": "HeadsMTL", "kind": "class", "doc": "<p>The output heads for Multi-Task Learning (MTL). Independent head assigned to each task takes the output from backbone network and forwards it into logits for predicting classes of the task.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "clarena.heads.HeadsMTL.__init__", "modulename": "clarena.heads", "qualname": "HeadsMTL.__init__", "kind": "function", "doc": "<p>Initializes MTL heads object.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_dim</strong> (<code>int</code>): the input dimension of the heads. Must be equal to the <code>output_dim</code> of the connected backbone.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span>)</span>"}, {"fullname": "clarena.heads.HeadsMTL.heads", "modulename": "clarena.heads", "qualname": "HeadsMTL.heads", "kind": "variable", "doc": "<p>MTL output heads are stored independently in a <code>ModuleDict</code>. Keys are task IDs and values are the corresponding <code>nn.Linear</code> heads. We use <code>ModuleDict</code> rather than <code>dict</code> to make sure <code>LightningModule</code> can track these model parameters for the purpose of, such as automatically to device, recorded in model summaries.</p>\n\n<p>Note that the task IDs must be string type in order to let <code>LightningModule</code> identify this part of the model.</p>\n", "annotation": ": torch.nn.modules.container.ModuleDict"}, {"fullname": "clarena.heads.HeadsMTL.input_dim", "modulename": "clarena.heads", "qualname": "HeadsMTL.input_dim", "kind": "variable", "doc": "<p>Store the input dimension of the heads. Used when creating new heads.</p>\n", "annotation": ": int"}, {"fullname": "clarena.heads.HeadsMTL.setup_tasks", "modulename": "clarena.heads", "qualname": "HeadsMTL.setup_tasks", "kind": "function", "doc": "<p>Create the output heads. This must be done before <code>forward()</code> is called.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>list[int]</code>): the target task IDs.</li>\n<li><strong>num_classes</strong> (<code>dict[int, int]</code>): the number of classes in each task. Keys are task IDs and values are the number of classes for the corresponding task.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_ids</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">num_classes</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.heads.HeadsMTL.get_head", "modulename": "clarena.heads", "qualname": "HeadsMTL.get_head", "kind": "function", "doc": "<p>Get the output head for task <code>task_id</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>head_t</strong> (<code>nn.Linear</code>): the output head for task <code>task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">linear</span><span class=\"o\">.</span><span class=\"n\">Linear</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.heads.HeadsMTL.forward", "modulename": "clarena.heads", "qualname": "HeadsMTL.forward", "kind": "function", "doc": "<p>The forward pass for data from task <code>task_id</code>. A head is selected according to the task_id and the feature is passed through the head.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>feature</strong> (<code>Tensor</code>): the feature tensor from the backbone network.</li>\n<li><strong>task_ids</strong> (<code>int</code> | <code>Tensor</code>): the task ID(s) for the input data. If the input batch is from the same task, this can be a single integer.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>logits</strong> (<code>Tensor</code>): the output logits tensor.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">feature</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">task_ids</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.heads.HeadSTL", "modulename": "clarena.heads", "qualname": "HeadSTL", "kind": "class", "doc": "<p>The output head for Single-Task Learning (STL).</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "clarena.heads.HeadSTL.__init__", "modulename": "clarena.heads", "qualname": "HeadSTL.__init__", "kind": "function", "doc": "<p>Initializes STL head object.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_dim</strong> (<code>int</code>): the input dimension of the head. Must be equal to the <code>output_dim</code> of the connected backbone.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span>)</span>"}, {"fullname": "clarena.heads.HeadSTL.head", "modulename": "clarena.heads", "qualname": "HeadSTL.head", "kind": "variable", "doc": "<p>STL output head.</p>\n", "annotation": ": torch.nn.modules.linear.Linear"}, {"fullname": "clarena.heads.HeadSTL.input_dim", "modulename": "clarena.heads", "qualname": "HeadSTL.input_dim", "kind": "variable", "doc": "<p>Store the input dimension of the head. Used when creating new head.</p>\n", "annotation": ": int"}, {"fullname": "clarena.heads.HeadSTL.setup_task", "modulename": "clarena.heads", "qualname": "HeadSTL.setup_task", "kind": "function", "doc": "<p>Create the output head. This must be done before <code>forward()</code> is called.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>num_classes</strong> (<code>int</code>): the number of classes in the task.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">num_classes</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.heads.HeadSTL.forward", "modulename": "clarena.heads", "qualname": "HeadSTL.forward", "kind": "function", "doc": "<p>The forward pass for data.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>feature</strong> (<code>Tensor</code>): the feature tensor from the backbone network.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>logits</strong> (<code>Tensor</code>): the output logits tensor.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">feature</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics", "modulename": "clarena.metrics", "kind": "module", "doc": "<h1 id=\"metrics\">Metrics</h1>\n\n<p>This submodule provides the <strong>metrics</strong> in CLArena. This includes:</p>\n\n<ul>\n<li>Callbacks that control each metric's calculation, logging and visualization process. They are implemented as subclasses of <code>MetricCallback</code>.</li>\n<li>Custom metrics that can be used in continual learning experiments. They are implemented as classes in <code>torchmetrics</code>.</li>\n</ul>\n\n<p>Please note that this is an API documentation. Please refer to the main documentation pages for more information about the metrics and how to configure and implement them:</p>\n\n<ul>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/configure-your-experiment/metrics\"><strong>Configure Metrics</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/implement-your-cl-modules/metrics\"><strong>Implement Your Metrics</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/posts/continual-learning-metrics\"><strong>A Summary of Continual Learning Metrics</strong></a></li>\n</ul>\n"}, {"fullname": "clarena.metrics.MetricCallback", "modulename": "clarena.metrics", "qualname": "MetricCallback", "kind": "class", "doc": "<p>The base class for all metrics callbacks in CLArena.</p>\n\n<p>This class is a placeholder for future metrics callbacks that can be used in continual learning experiments.\nIt is not intended to be instantiated directly, but rather to be subclassed by specific metrics callbacks.</p>\n", "bases": "lightning.pytorch.callbacks.callback.Callback"}, {"fullname": "clarena.metrics.MetricCallback.__init__", "modulename": "clarena.metrics", "qualname": "MetricCallback.__init__", "kind": "function", "doc": "<p>Initialize the <code>MetricCallback</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>save_dir</strong> (<code>str</code>): The directory where data and figures of metrics will be saved. Better inside the output folder.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">save_dir</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span>)</span>"}, {"fullname": "clarena.metrics.MetricCallback.save_dir", "modulename": "clarena.metrics", "qualname": "MetricCallback.save_dir", "kind": "variable", "doc": "<p>Store the directory where data and figures of metrics will be saved.</p>\n", "annotation": ": str"}, {"fullname": "clarena.metrics.CLAccuracy", "modulename": "clarena.metrics", "qualname": "CLAccuracy", "kind": "class", "doc": "<p>Provides all actions that are related to CL accuracy metric, which include:</p>\n\n<ul>\n<li>Defining, initializing and recording accuracy metric.</li>\n<li>Logging training and validation accuracy metric to Lightning loggers in real time.</li>\n<li>Saving test accuracy metric to files.</li>\n<li>Visualizing test accuracy metric as plots.</li>\n</ul>\n\n<p>The callback is able to produce the following outputs:</p>\n\n<ul>\n<li>CSV files for test accuracy (lower triangular) matrix and average accuracy. See <a href=\"https://pengxiang-wang.com/posts/continual-learning-metrics.html#sec-test-performance-of-previous-tasks\">here</a> for details.</li>\n<li>Coloured plot for test accuracy (lower triangular) matrix. See <a href=\"https://pengxiang-wang.com/posts/continual-learning-metrics.html#sec-test-performance-of-previous-tasks\">here</a> for details.</li>\n<li>Curve plots for test average accuracy over different training tasks. See <a href=\"https://pengxiang-wang.com/posts/continual-learning-metrics.html#sec-average-test-performance-over-tasks\">here</a> for details.</li>\n</ul>\n\n<p>Please refer to the <a href=\"https://pengxiang-wang.com/posts/continual-learning-metrics\">A Summary of Continual Learning Metrics</a> to learn about this metric.</p>\n", "bases": "clarena.metrics.base.MetricCallback"}, {"fullname": "clarena.metrics.CLAccuracy.__init__", "modulename": "clarena.metrics", "qualname": "CLAccuracy.__init__", "kind": "function", "doc": "<p>Initialize the <code>CLAccuracy</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>save_dir</strong> (<code>str</code>): The directory where data and figures of metrics will be saved. Better inside the output folder.</li>\n<li><strong>test_acc_csv_name</strong> (<code>str</code>): file name to save test accuracy matrix and average accuracy as CSV file.</li>\n<li><strong>test_acc_matrix_plot_name</strong> (<code>str</code> | <code>None</code>): file name to save accuracy matrix plot. If <code>None</code>, no file will be saved.</li>\n<li><strong>test_ave_acc_plot_name</strong> (<code>str</code> | <code>None</code>): file name to save average accuracy as curve plot over different training tasks. If <code>None</code>, no file will be saved.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">save_dir</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">test_acc_csv_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;acc.csv&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">test_acc_matrix_plot_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">test_ave_acc_plot_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.metrics.CLAccuracy.test_acc_csv_path", "modulename": "clarena.metrics", "qualname": "CLAccuracy.test_acc_csv_path", "kind": "variable", "doc": "<p>The path to save test accuracy matrix and average accuracy CSV file.</p>\n", "annotation": ": str"}, {"fullname": "clarena.metrics.CLAccuracy.acc_training_epoch", "modulename": "clarena.metrics", "qualname": "CLAccuracy.acc_training_epoch", "kind": "variable", "doc": "<p>Classification accuracy of training epoch. Accumulated and calculated from the training batches. See <a href=\"https://pengxiang-wang.com/posts/continual-learning-metrics.html#sec-performance-of-training-epoch\">here</a> for details.</p>\n", "annotation": ": clarena.utils.metrics.MeanMetricBatch"}, {"fullname": "clarena.metrics.CLAccuracy.acc_val", "modulename": "clarena.metrics", "qualname": "CLAccuracy.acc_val", "kind": "variable", "doc": "<p>Validation classification accuracy of the model after training epoch. Accumulated and calculated from the validation batches. See <a href=\"https://pengxiang-wang.com/posts/continual-learning-metrics.html#sec-validation-performace\">here</a> for details.</p>\n", "annotation": ": clarena.utils.metrics.MeanMetricBatch"}, {"fullname": "clarena.metrics.CLAccuracy.acc_test", "modulename": "clarena.metrics", "qualname": "CLAccuracy.acc_test", "kind": "variable", "doc": "<p>Test classification accuracy of the current model (<code>self.task_id</code>) on current and previous tasks. Accumulated and calculated from the test batches. Keys are task IDs and values are the corresponding metrics. It is the last row of the lower triangular matrix. See <a href=\"https://pengxiang-wang.com/posts/continual-learning-metrics.html#sec-test-performance-of-previous-tasks\">here</a> for details.</p>\n", "annotation": ": dict[int, clarena.utils.metrics.MeanMetricBatch]"}, {"fullname": "clarena.metrics.CLAccuracy.task_id", "modulename": "clarena.metrics", "qualname": "CLAccuracy.task_id", "kind": "variable", "doc": "<p>Task ID counter indicating which task is being processed. Self updated during the task loop. Valid from 1 to <code>cl_dataset.num_tasks</code>.</p>\n", "annotation": ": int"}, {"fullname": "clarena.metrics.CLAccuracy.on_fit_start", "modulename": "clarena.metrics", "qualname": "CLAccuracy.on_fit_start", "kind": "function", "doc": "<p>Initialize training and validation metrics.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.CLAccuracy.on_train_batch_end", "modulename": "clarena.metrics", "qualname": "CLAccuracy.on_train_batch_end", "kind": "function", "doc": "<p>Record training metrics from training batch, log metrics of training batch and accumulated metrics of the epoch to Lightning loggers.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the training step, the returns of the <code>training_step()</code> method in the <code>CLAlgorithm</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the training data batch.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.CLAccuracy.on_train_epoch_end", "modulename": "clarena.metrics", "qualname": "CLAccuracy.on_train_epoch_end", "kind": "function", "doc": "<p>Log metrics of training epoch to plot learning curves and reset the metrics accumulation at the end of training epoch.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.CLAccuracy.on_validation_batch_end", "modulename": "clarena.metrics", "qualname": "CLAccuracy.on_validation_batch_end", "kind": "function", "doc": "<p>Accumulating metrics from validation batch. We don't need to log and monitor the metrics of validation batches.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the validation step, which is the returns of the <code>validation_step()</code> method in the <code>CLAlgorithm</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the validation data batch.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.CLAccuracy.on_validation_epoch_end", "modulename": "clarena.metrics", "qualname": "CLAccuracy.on_validation_epoch_end", "kind": "function", "doc": "<p>Log validation metrics to plot learning curves.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.CLAccuracy.on_test_start", "modulename": "clarena.metrics", "qualname": "CLAccuracy.on_test_start", "kind": "function", "doc": "<p>Initialize the metrics for testing each seen task in the beginning of a task's testing.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.CLAccuracy.on_test_batch_end", "modulename": "clarena.metrics", "qualname": "CLAccuracy.on_test_batch_end", "kind": "function", "doc": "<p>Accumulating metrics from test batch. We don't need to log and monitor the metrics of test batches.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the test step, which is the returns of the <code>test_step()</code> method in the <code>CLAlgorithm</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the test data batch.</li>\n<li><strong>dataloader_idx</strong> (<code>int</code>): the task ID of seen tasks to be tested. A default value of 0 is given otherwise the LightningModule will raise a <code>RuntimeError</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">dataloader_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.CLAccuracy.on_test_epoch_end", "modulename": "clarena.metrics", "qualname": "CLAccuracy.on_test_epoch_end", "kind": "function", "doc": "<p>Save and plot test metrics at the end of test.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.CLAccuracy.update_test_acc_to_csv", "modulename": "clarena.metrics", "qualname": "CLAccuracy.update_test_acc_to_csv", "kind": "function", "doc": "<p>Update the test accuracy metrics of seen tasks at the last line to an existing CSV file. A new file will be created if not existing.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>after_training_task_id</strong> (<code>int</code>): the task ID after training.</li>\n<li><strong>csv_path</strong> (<code>str</code>): save the test metric to path. E.g. './outputs/expr_name/1970-01-01_00-00-00/results/acc.csv'.</li>\n<li><strong>skipped_task_ids_for_ave</strong> (<code>list[int]</code> | <code>None</code>): the task IDs that are skipped to calculate average accuracy. If <code>None</code>, no task is skipped. Default: <code>None</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">after_training_task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">skipped_task_ids_for_ave</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.CLAccuracy.plot_test_acc_matrix_from_csv", "modulename": "clarena.metrics", "qualname": "CLAccuracy.plot_test_acc_matrix_from_csv", "kind": "function", "doc": "<p>Plot the test accuracy matrix from saved CSV file and save the plot to the designated directory.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>csv_path</strong> (<code>str</code>): the path to the CSV file where the <code>utils.update_test_acc_to_csv()</code> saved the test accuracy metric.</li>\n<li><strong>plot_path</strong> (<code>str</code>): the path to save plot. Better same as the output directory of the experiment. E.g. './outputs/expr_name/1970-01-01_00-00-00/acc_matrix.png'.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">plot_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.CLAccuracy.plot_test_ave_acc_curve_from_csv", "modulename": "clarena.metrics", "qualname": "CLAccuracy.plot_test_ave_acc_curve_from_csv", "kind": "function", "doc": "<p>Plot the test average accuracy curve over different training tasks from saved CSV file and save the plot to the designated directory.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>csv_path</strong> (<code>str</code>): the path to the CSV file where the <code>utils.update_test_acc_to_csv()</code> saved the test accuracy metric.</li>\n<li><strong>plot_path</strong> (<code>str</code>): the path to save plot. Better same as the output directory of the experiment. E.g. './outputs/expr_name/1970-01-01_00-00-00/ave_acc.png'.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">plot_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.CLLoss", "modulename": "clarena.metrics", "qualname": "CLLoss", "kind": "class", "doc": "<p>Provides all actions that are related to CL loss metrics, which include:</p>\n\n<ul>\n<li>Defining, initializing and recording loss metrics.</li>\n<li>Logging training and validation loss metrics to Lightning loggers in real time.</li>\n<li>Saving test loss metrics to files.</li>\n<li>Visualizing test loss metrics as plots.</li>\n</ul>\n\n<p>The callback is able to produce the following outputs:</p>\n\n<ul>\n<li>CSV files for classification loss (lower triangular) matrix and average classification loss. See <a href=\"https://pengxiang-wang.com/posts/continual-learning-metrics.html#sec-test-performance-of-previous-tasks\">here</a> for details.</li>\n<li>Coloured plot for test classification loss (lower triangular) matrix. See <a href=\"https://pengxiang-wang.com/posts/continual-learning-metrics.html#sec-test-performance-of-previous-tasks\">here</a> for details.</li>\n<li>Curve plots for test average classification loss over different training tasks. See <a href=\"https://pengxiang-wang.com/posts/continual-learning-metrics.html#sec-average-test-performance-over-tasks\">here</a> for details.</li>\n</ul>\n\n<p>Please refer to the <a href=\"https://pengxiang-wang.com/posts/continual-learning-metrics\">A Summary of Continual Learning Metrics</a> to learn about this metric.</p>\n", "bases": "clarena.metrics.base.MetricCallback"}, {"fullname": "clarena.metrics.CLLoss.__init__", "modulename": "clarena.metrics", "qualname": "CLLoss.__init__", "kind": "function", "doc": "<p>Initialize the <code>CLLoss</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>save_dir</strong> (<code>str</code>): The directory where data and figures of metrics will be saved. Better inside the output folder.</li>\n<li><strong>test_loss_cls_csv_name</strong>(<code>str</code>): file name to save classification loss matrix and average classification loss as CSV file.</li>\n<li><strong>test_loss_cls_matrix_plot_name</strong> (<code>str</code> | <code>None</code>): file name to save classification loss matrix plot. If <code>None</code>, no file will be saved.</li>\n<li><strong>test_ave_loss_cls_plot_name</strong> (<code>str</code> | <code>None</code>): file name to save average classification loss as curve plot over different training tasks. If <code>None</code>, no file will be saved.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">save_dir</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">test_loss_cls_csv_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;loss_cls.csv&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">test_loss_cls_matrix_plot_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">test_ave_loss_cls_plot_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.metrics.CLLoss.test_loss_cls_csv_path", "modulename": "clarena.metrics", "qualname": "CLLoss.test_loss_cls_csv_path", "kind": "variable", "doc": "<p>Store the path to save test classification loss matrix and average classification loss CSV file.</p>\n", "annotation": ": str"}, {"fullname": "clarena.metrics.CLLoss.loss_cls_training_epoch", "modulename": "clarena.metrics", "qualname": "CLLoss.loss_cls_training_epoch", "kind": "variable", "doc": "<p>Classification loss of training epoch. Accumulated and calculated from the training batches. See <a href=\"https://pengxiang-wang.com/posts/continual-learning-metrics.html#sec-performance-of-training-epoch\">here</a> for details.</p>\n", "annotation": ": clarena.utils.metrics.MeanMetricBatch"}, {"fullname": "clarena.metrics.CLLoss.loss_training_epoch", "modulename": "clarena.metrics", "qualname": "CLLoss.loss_training_epoch", "kind": "variable", "doc": "<p>Total loss of training epoch. Accumulated and calculated from the training batches. See <a href=\"https://pengxiang-wang.com/posts/continual-learning-metrics.html#sec-performance-of-training-epoch\">here</a> for details.</p>\n", "annotation": ": clarena.utils.metrics.MeanMetricBatch"}, {"fullname": "clarena.metrics.CLLoss.loss_cls_val", "modulename": "clarena.metrics", "qualname": "CLLoss.loss_cls_val", "kind": "variable", "doc": "<p>Validation classification of the model loss after training epoch. Accumulated and calculated from the validation batches. See <a href=\"https://pengxiang-wang.com/posts/continual-learning-metrics.html#sec-validation-performace\">here</a> for details.</p>\n", "annotation": ": clarena.utils.metrics.MeanMetricBatch"}, {"fullname": "clarena.metrics.CLLoss.loss_cls_test", "modulename": "clarena.metrics", "qualname": "CLLoss.loss_cls_test", "kind": "variable", "doc": "<p>Test classification loss of the current model (<code>self.task_id</code>) on current and previous tasks. Accumulated and calculated from the test batches. Keys are task IDs and values are the corresponding metrics. It is the last row of the lower triangular matrix. See <a href=\"https://pengxiang-wang.com/posts/continual-learning-metrics.html#sec-test-performance-of-previous-tasks\">here</a> for details.</p>\n", "annotation": ": dict[int, clarena.utils.metrics.MeanMetricBatch]"}, {"fullname": "clarena.metrics.CLLoss.task_id", "modulename": "clarena.metrics", "qualname": "CLLoss.task_id", "kind": "variable", "doc": "<p>Task ID counter indicating which task is being processed. Self updated during the task loop. Starting from 1.</p>\n", "annotation": ": int"}, {"fullname": "clarena.metrics.CLLoss.on_fit_start", "modulename": "clarena.metrics", "qualname": "CLLoss.on_fit_start", "kind": "function", "doc": "<p>Initialize training and validation metrics.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.CLLoss.on_train_batch_end", "modulename": "clarena.metrics", "qualname": "CLLoss.on_train_batch_end", "kind": "function", "doc": "<p>Record training metrics from training batch, log metrics of training batch and accumulated metrics of the epoch to Lightning loggers.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the training step, the returns of the <code>training_step()</code> method in the <code>CLAlgorithm</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the training data batch.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.CLLoss.on_train_epoch_end", "modulename": "clarena.metrics", "qualname": "CLLoss.on_train_epoch_end", "kind": "function", "doc": "<p>Log metrics of training epoch to plot learning curves and reset the metrics accumulation at the end of training epoch.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.CLLoss.on_validation_batch_end", "modulename": "clarena.metrics", "qualname": "CLLoss.on_validation_batch_end", "kind": "function", "doc": "<p>Accumulating metrics from validation batch. We don't need to log and monitor the metrics of validation batches.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the validation step, which is the returns of the <code>validation_step()</code> method in the <code>CLAlgorithm</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the validation data batch.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.CLLoss.on_validation_epoch_end", "modulename": "clarena.metrics", "qualname": "CLLoss.on_validation_epoch_end", "kind": "function", "doc": "<p>Log validation metrics to plot learning curves.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.CLLoss.on_test_start", "modulename": "clarena.metrics", "qualname": "CLLoss.on_test_start", "kind": "function", "doc": "<p>Initialize the metrics for testing each seen task in the beginning of a task's testing.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.CLLoss.on_test_batch_end", "modulename": "clarena.metrics", "qualname": "CLLoss.on_test_batch_end", "kind": "function", "doc": "<p>Accumulating metrics from test batch. We don't need to log and monitor the metrics of test batches.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the test step, which is the returns of the <code>test_step()</code> method in the <code>CLAlgorithm</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the test data batch.</li>\n<li><strong>dataloader_idx</strong> (<code>int</code>): the task ID of seen tasks to be tested. A default value of 0 is given otherwise the LightningModule will raise a <code>RuntimeError</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">dataloader_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.CLLoss.on_test_epoch_end", "modulename": "clarena.metrics", "qualname": "CLLoss.on_test_epoch_end", "kind": "function", "doc": "<p>Save and plot test metrics at the end of test.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.CLLoss.update_test_loss_cls_to_csv", "modulename": "clarena.metrics", "qualname": "CLLoss.update_test_loss_cls_to_csv", "kind": "function", "doc": "<p>Update the test classification loss metrics of seen tasks at the last line to an existing CSV file. A new file will be created if not existing.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>after_training_task_id</strong> (<code>int</code>): the task ID after training.</li>\n<li><strong>csv_path</strong> (<code>str</code>): save the test metric to path. E.g. './outputs/expr_name/1970-01-01_00-00-00/results/loss_cls.csv'.</li>\n<li><strong>skipped_task_ids_for_ave</strong> (<code>list[int]</code> | <code>None</code>): the task IDs that are skipped to calculate average accuracy. If <code>None</code>, no task is skipped. Default: <code>None</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">after_training_task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">skipped_task_ids_for_ave</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.CLLoss.plot_test_loss_cls_matrix_from_csv", "modulename": "clarena.metrics", "qualname": "CLLoss.plot_test_loss_cls_matrix_from_csv", "kind": "function", "doc": "<p>Plot the test classification loss matrix from saved CSV file and save the plot to the designated directory.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>csv_path</strong> (<code>str</code>): the path to the CSV file where the <code>utils.update_loss_cls_to_csv()</code> saved the test classification loss metric.</li>\n<li><strong>plot_path</strong> (<code>str</code>): the path to save plot. Better same as the output directory of the experiment. E.g. './outputs/expr_name/1970-01-01_00-00-00/loss_cls_matrix.png'.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">plot_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.CLLoss.plot_test_ave_loss_cls_curve_from_csv", "modulename": "clarena.metrics", "qualname": "CLLoss.plot_test_ave_loss_cls_curve_from_csv", "kind": "function", "doc": "<p>Plot the test average classfication loss curve over different training tasks from saved CSV file and save the plot to the designated directory.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>csv_path</strong> (<code>str</code>): the path to the CSV file where the <code>utils.update_test_acc_to_csv()</code> saved the test classfication loss metric.</li>\n<li><strong>plot_path</strong> (<code>str</code>): the path to save plot. Better same as the output directory of the experiment. E.g. './outputs/expr_name/1970-01-01_00-00-00/ave_loss_cls.png'.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">plot_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.CULDistributionDistance", "modulename": "clarena.metrics", "qualname": "CULDistributionDistance", "kind": "class", "doc": "<p>Provides all actions that are related to CUL distribution distance (DD) metric, which include:</p>\n\n<ul>\n<li>Defining, initializing and recording DD metric.</li>\n<li>Saving DD metric to files.</li>\n<li>Visualizing DD metric as plots.</li>\n</ul>\n\n<p>The callback is able to produce the following outputs:</p>\n\n<ul>\n<li>CSV files for DD in each task.</li>\n<li>Coloured plot for DD in each task.</li>\n</ul>\n\n<p>Note that this callback is designed to be used with the <code>CULEvaluation</code> module, which is a special evaluation module for continual unlearning. It is not a typical test step in the algorithm, but rather a test protocol that evaluates the performance of the model on unlearned tasks.</p>\n", "bases": "clarena.metrics.base.MetricCallback"}, {"fullname": "clarena.metrics.CULDistributionDistance.__init__", "modulename": "clarena.metrics", "qualname": "CULDistributionDistance.__init__", "kind": "function", "doc": "<p>Initialize the <code>CULDistributionDistance</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>save_dir</strong> (<code>str</code>): The directory where data and figures of metrics will be saved. Better inside the output folder.</li>\n<li><strong>distribution_distance_type</strong> (<code>str</code>): the type of distribution distance to use. Should be one of the following:\n<ul>\n<li>'euclidean': Eulidean distance.</li>\n<li>'cosine': Cosine distance.</li>\n<li>'manhattan': Manhattan distance.</li>\n</ul></li>\n<li><strong>distribution_distance_csv_name</strong> (<code>str</code>): file name to save test distribution distance metrics as CSV file.</li>\n<li><strong>distribution_distance_plot_name</strong> (<code>str</code> | <code>None</code>): file name to save test distribution distance metrics as plot. If <code>None</code>, no plot will be saved.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">save_dir</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">distribution_distance_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">distribution_distance_csv_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;dd.csv&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">distribution_distance_plot_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.metrics.CULDistributionDistance.distribution_distance_type", "modulename": "clarena.metrics", "qualname": "CULDistributionDistance.distribution_distance_type", "kind": "variable", "doc": "<p>Store the type of distribution distance to use.</p>\n", "annotation": ": str"}, {"fullname": "clarena.metrics.CULDistributionDistance.distribution_distance_csv_path", "modulename": "clarena.metrics", "qualname": "CULDistributionDistance.distribution_distance_csv_path", "kind": "variable", "doc": "<p>Store the path to save the test distribution distance metrics CSV file.</p>\n", "annotation": ": str"}, {"fullname": "clarena.metrics.CULDistributionDistance.distribution_distance", "modulename": "clarena.metrics", "qualname": "CULDistributionDistance.distribution_distance", "kind": "variable", "doc": "<p>Distribution distance unlearning metrics for each seen task. Accumulated and calculated from the test batches. Keys are task IDs and values are the corresponding metrics.</p>\n", "annotation": ": dict[str, clarena.utils.metrics.MeanMetricBatch]"}, {"fullname": "clarena.metrics.CULDistributionDistance.task_id", "modulename": "clarena.metrics", "qualname": "CULDistributionDistance.task_id", "kind": "variable", "doc": "<p>Task ID counter indicating which task is being processed. Self updated during the task loop. Starting from 1.</p>\n", "annotation": ": int"}, {"fullname": "clarena.metrics.CULDistributionDistance.on_test_start", "modulename": "clarena.metrics", "qualname": "CULDistributionDistance.on_test_start", "kind": "function", "doc": "<p>Initialize the metrics for testing each seen task in the beginning of a task's testing.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">eval</span><span class=\"o\">.</span><span class=\"n\">CULEvaluation</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.CULDistributionDistance.on_test_batch_end", "modulename": "clarena.metrics", "qualname": "CULDistributionDistance.on_test_batch_end", "kind": "function", "doc": "<p>Accumulating metrics from test batch. We don't need to log and monitor the metrics of test batches.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the test step, which is the returns of the <code>test_step()</code> method in the <code>CULEvaluation</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the validation data batch.</li>\n<li><strong>dataloader_idx</strong> (<code>int</code>): the task ID of seen tasks to be tested. A default value of 0 is given otherwise the LightningModule will raise a <code>RuntimeError</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">eval</span><span class=\"o\">.</span><span class=\"n\">CULEvaluation</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">dataloader_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.CULDistributionDistance.on_test_epoch_end", "modulename": "clarena.metrics", "qualname": "CULDistributionDistance.on_test_epoch_end", "kind": "function", "doc": "<p>Save and plot test metrics at the end of test.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">eval</span><span class=\"o\">.</span><span class=\"n\">CULEvaluation</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.CULDistributionDistance.update_unlearning_test_distance_to_csv", "modulename": "clarena.metrics", "qualname": "CULDistributionDistance.update_unlearning_test_distance_to_csv", "kind": "function", "doc": "<p>Update the unlearning test distance metrics of unlearning tasks to CSV file.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>distance_metric</strong> (<code>dict[str, MeanMetricBatch]</code>): the distance metric of unlearned tasks. Accumulated and calculated from the unlearning test batches.</li>\n<li><strong>csv_path</strong> (<code>str</code>): save the test metric to path. E.g. './outputs/expr_name/1970-01-01_00-00-00/results/unlearning_test_after_task_X/distance.csv'.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">distance_metric</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">metrics</span><span class=\"o\">.</span><span class=\"n\">MeanMetricBatch</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.CULDistributionDistance.plot_unlearning_test_distance_from_csv", "modulename": "clarena.metrics", "qualname": "CULDistributionDistance.plot_unlearning_test_distance_from_csv", "kind": "function", "doc": "<p>Plot the unlearning test distance matrix over different unlearned tasks from saved CSV file and save the plot to the designated directory.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>csv_path</strong> (<code>str</code>): the path to the CSV file where the <code>utils.save_unlearning_test_distance_to_csv()</code> saved the unlearning test distance metric.</li>\n<li><strong>plot_path</strong> (<code>str</code>): the path to save plot. Better same as the output directory of the experiment. E.g. './outputs/expr_name/1970-01-01_00-00-00/results/unlearning_test_after_task_X/distance.png'.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">plot_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.CULAccuracyDifference", "modulename": "clarena.metrics", "qualname": "CULAccuracyDifference", "kind": "class", "doc": "<p>Provides all actions that are related to CUL accuracy difference (AD) metric, which include:</p>\n\n<ul>\n<li>Defining, initializing and recording AD metric.</li>\n<li>Saving AD metric to files.</li>\n<li>Visualizing AD metric as plots.</li>\n</ul>\n\n<p>The callback is able to produce the following outputs:</p>\n\n<ul>\n<li>CSV files for AD in each task.</li>\n<li>Coloured plot for AD in each task.</li>\n</ul>\n\n<p>Note that this callback is designed to be used with the <code>CULEvaluation</code> module, which is a special evaluation module for continual unlearning. It is not a typical test step in the algorithm, but rather a test protocol that evaluates the performance of the model on unlearned tasks.</p>\n", "bases": "clarena.metrics.base.MetricCallback"}, {"fullname": "clarena.metrics.CULAccuracyDifference.__init__", "modulename": "clarena.metrics", "qualname": "CULAccuracyDifference.__init__", "kind": "function", "doc": "<p>Initialize the <code>CULAccuracyDifference</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>save_dir</strong> (<code>str</code>): The directory where data and figures of metrics will be saved. Better inside the output folder.</li>\n<li><strong>accuracy_difference_csv_name</strong> (<code>str</code>): file name to save test accuracy difference metrics as CSV file.</li>\n<li><strong>accuracy_difference_plot_name</strong> (<code>str</code> | <code>None</code>): file name to save test accuracy difference metrics as plot. If <code>None</code>, no plot will be saved.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">save_dir</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">accuracy_difference_csv_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;ad.csv&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">accuracy_difference_plot_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.metrics.CULAccuracyDifference.accuracy_difference_csv_path", "modulename": "clarena.metrics", "qualname": "CULAccuracyDifference.accuracy_difference_csv_path", "kind": "variable", "doc": "<p>Store the path to save the test accuracy difference metrics CSV file.</p>\n", "annotation": ": str"}, {"fullname": "clarena.metrics.CULAccuracyDifference.accuracy_difference", "modulename": "clarena.metrics", "qualname": "CULAccuracyDifference.accuracy_difference", "kind": "variable", "doc": "<p>Accuracy difference (between main and full model) metrics for each seen task. Accumulated and calculated from the test batches. Keys are task IDs and values are the corresponding metrics.</p>\n", "annotation": ": dict[str, clarena.utils.metrics.MeanMetricBatch]"}, {"fullname": "clarena.metrics.CULAccuracyDifference.task_id", "modulename": "clarena.metrics", "qualname": "CULAccuracyDifference.task_id", "kind": "variable", "doc": "<p>Task ID counter indicating which task is being processed. Self updated during the task loop. Starting from 1.</p>\n", "annotation": ": int"}, {"fullname": "clarena.metrics.CULAccuracyDifference.on_test_start", "modulename": "clarena.metrics", "qualname": "CULAccuracyDifference.on_test_start", "kind": "function", "doc": "<p>Initialize the metrics for testing each seen task in the beginning of a task's testing.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">eval</span><span class=\"o\">.</span><span class=\"n\">CULEvaluation</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.CULAccuracyDifference.on_test_batch_end", "modulename": "clarena.metrics", "qualname": "CULAccuracyDifference.on_test_batch_end", "kind": "function", "doc": "<p>Accumulating metrics from test batch. We don't need to log and monitor the metrics of test batches.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the test step, which is the returns of the <code>test_step()</code> method in the <code>CULEvaluation</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the validation data batch.</li>\n<li><strong>dataloader_idx</strong> (<code>int</code>): the task ID of seen tasks to be tested. A default value of 0 is given otherwise the LightningModule will raise a <code>RuntimeError</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">eval</span><span class=\"o\">.</span><span class=\"n\">CULEvaluation</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">dataloader_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.CULAccuracyDifference.on_test_epoch_end", "modulename": "clarena.metrics", "qualname": "CULAccuracyDifference.on_test_epoch_end", "kind": "function", "doc": "<p>Save and plot test metrics at the end of test.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">eval</span><span class=\"o\">.</span><span class=\"n\">CULEvaluation</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.CULAccuracyDifference.update_unlearning_accuracy_difference_to_csv", "modulename": "clarena.metrics", "qualname": "CULAccuracyDifference.update_unlearning_accuracy_difference_to_csv", "kind": "function", "doc": "<p>Update the unlearning accuracy difference metrics of unlearning tasks to CSV file.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>accuracy_difference_metric</strong> (<code>dict[str, MeanMetricBatch]</code>): the accuracy difference metric. Accumulated and calculated from the unlearning test batches.</li>\n<li><strong>csv_path</strong> (<code>str</code>): save the test metric to path. E.g. './outputs/expr_name/1970-01-01_00-00-00/results/unlearning_test_after_task_X/distance.csv'.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">accuracy_difference_metric</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">metrics</span><span class=\"o\">.</span><span class=\"n\">MeanMetricBatch</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.CULAccuracyDifference.plot_unlearning_accuracy_difference_from_csv", "modulename": "clarena.metrics", "qualname": "CULAccuracyDifference.plot_unlearning_accuracy_difference_from_csv", "kind": "function", "doc": "<p>Plot the unlearning accuracy difference matrix over different unlearned tasks from saved CSV file and save the plot to the designated directory.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>csv_path</strong> (<code>str</code>): the path to the CSV file where the <code>utils.save_unlearning_test_distance_to_csv()</code> saved the unlearning test distance metric.</li>\n<li><strong>plot_path</strong> (<code>str</code>): the path to save plot. Better same as the output directory of the experiment. E.g. './outputs/expr_name/1970-01-01_00-00-00/results/unlearning_test_after_task_X/distance.png'.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">plot_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.HATAdjustmentRate", "modulename": "clarena.metrics", "qualname": "HATAdjustmentRate", "kind": "class", "doc": "<p>Provides all actions that are related to adjustment rate of <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task)</a> algorithm and its extensions, which include:</p>\n\n<ul>\n<li>Visualizing adjustment rate during training as figures.</li>\n</ul>\n\n<p>The callback is able to produce the following outputs:</p>\n\n<ul>\n<li>Figures of training adjustment rate.</li>\n</ul>\n", "bases": "clarena.metrics.base.MetricCallback"}, {"fullname": "clarena.metrics.HATAdjustmentRate.__init__", "modulename": "clarena.metrics", "qualname": "HATAdjustmentRate.__init__", "kind": "function", "doc": "<p>Initialize the <code>HATAdjustmentRate</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>save_dir</strong> (<code>str</code> | <code>None</code>): The directory to save the adjustment rate figures. Better inside the output folder.</li>\n<li><strong>plot_adjustment_rate_every_n_steps</strong> (<code>int</code> | <code>None</code>): the frequency of plotting adjustment rate figures in terms of number of batches during training.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">save_dir</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">plot_adjustment_rate_every_n_steps</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.metrics.HATAdjustmentRate.plot_adjustment_rate_every_n_steps", "modulename": "clarena.metrics", "qualname": "HATAdjustmentRate.plot_adjustment_rate_every_n_steps", "kind": "variable", "doc": "<p>Store the frequency of plotting adjustment rate in terms of number of batches.</p>\n", "annotation": ": int"}, {"fullname": "clarena.metrics.HATAdjustmentRate.task_id", "modulename": "clarena.metrics", "qualname": "HATAdjustmentRate.task_id", "kind": "variable", "doc": "<p>Task ID counter indicating which task is being processed. Self updated during the task loop. Starting from 1.</p>\n", "annotation": ": int"}, {"fullname": "clarena.metrics.HATAdjustmentRate.on_fit_start", "modulename": "clarena.metrics", "qualname": "HATAdjustmentRate.on_fit_start", "kind": "function", "doc": "<p>Get the current task ID in the beginning of a task's fitting (training and validation). Sanity check the <code>pl_module</code> to be <code>HAT</code>.</p>\n\n<p><strong>Raises:</strong>\n-<strong>TypeError</strong>: when the <code>pl_module</code> is not <code>HAT</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.HATAdjustmentRate.on_train_batch_end", "modulename": "clarena.metrics", "qualname": "HATAdjustmentRate.on_train_batch_end", "kind": "function", "doc": "<p>Plot adjustment rate after training batch.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the training step, which is the returns of the <code>training_step()</code> method in the <code>CLAlgorithm</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the training data batch.</li>\n<li><strong>batch_idx</strong> (<code>int</code>): the index of the current batch. This is for the file name of mask figures.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.HATAdjustmentRate.plot_hat_adjustment_rate", "modulename": "clarena.metrics", "qualname": "HATAdjustmentRate.plot_hat_adjustment_rate", "kind": "function", "doc": "<p>Plot adjustment rate in <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task)</a>) algorithm. This includes the adjustment rate weight and adjustment rate bias (if applicable).</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>adjustment_rate</strong> (<code>dict[str, Tensor]</code>): the adjustment rate. Key (<code>str</code>) is layer name, value (<code>Tensor</code>) is the adjustment rate tensor. If it's adjustment rate weight, it has size same as weights. If it's adjustment rate bias, it has size same as biases.</li>\n<li><strong>weight_or_bias</strong> (<code>str</code>): the type of adjustment rate. It can be either 'weight' or 'bias'. This is to form the plot name.</li>\n<li><strong>step</strong> (<code>int</code>): the training step (batch index) of the adjustment rate to be plotted. This is to form the plot name. Keep <code>None</code> for not showing the step in the plot name.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">adjustment_rate</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">weight_or_bias</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">step</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.HATNetworkCapacity", "modulename": "clarena.metrics", "qualname": "HATNetworkCapacity", "kind": "class", "doc": "<p>Provides all actions that are related to network capacity of <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task)</a> algorithm and its extensions, which include:</p>\n\n<ul>\n<li>Logging network capacity during training. See the \"Evaluation Metrics\" section in chapter 4.1 in <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a> for more details about network capacity.</li>\n</ul>\n", "bases": "clarena.metrics.base.MetricCallback"}, {"fullname": "clarena.metrics.HATNetworkCapacity.__init__", "modulename": "clarena.metrics", "qualname": "HATNetworkCapacity.__init__", "kind": "function", "doc": "<p>Initialize the <code>HATNetworkCapacity</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>save_dir</strong> (<code>str</code>): the directory to save the mask figures. Better inside the output folder.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">save_dir</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span>)</span>"}, {"fullname": "clarena.metrics.HATNetworkCapacity.task_id", "modulename": "clarena.metrics", "qualname": "HATNetworkCapacity.task_id", "kind": "variable", "doc": "<p>Task ID counter indicating which task is being processed. Self updated during the task loop. Starting from 1.</p>\n", "annotation": ": int"}, {"fullname": "clarena.metrics.HATNetworkCapacity.on_fit_start", "modulename": "clarena.metrics", "qualname": "HATNetworkCapacity.on_fit_start", "kind": "function", "doc": "<p>Get the current task ID in the beginning of a task's fitting (training and validation). Sanity check the <code>pl_module</code> to be <code>HAT</code>.</p>\n\n<p><strong>Raises:</strong>\n-<strong>TypeError</strong>: when the <code>pl_module</code> is not <code>HAT</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.HATNetworkCapacity.on_train_batch_end", "modulename": "clarena.metrics", "qualname": "HATNetworkCapacity.on_train_batch_end", "kind": "function", "doc": "<p>Plot training mask, adjustment rate and log network capacity after training batch.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the training step, which is the returns of the <code>training_step()</code> method in the <code>CLAlgorithm</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the training data batch.</li>\n<li><strong>batch_idx</strong> (<code>int</code>): the index of the current batch. This is for the file name of mask figures.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.HATMasks", "modulename": "clarena.metrics", "qualname": "HATMasks", "kind": "class", "doc": "<p>Provides all actions that are related to masks of <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task)</a> algorithm and its extensions, which include:</p>\n\n<ul>\n<li>Visualizing mask and cumulative mask figures during training and testing as figures.</li>\n</ul>\n\n<p>The callback is able to produce the following outputs:</p>\n\n<ul>\n<li>Figures of both training and test, masks and cumulative masks.</li>\n</ul>\n", "bases": "clarena.metrics.base.MetricCallback"}, {"fullname": "clarena.metrics.HATMasks.__init__", "modulename": "clarena.metrics", "qualname": "HATMasks.__init__", "kind": "function", "doc": "<p>Initialize the <code>HATMasks</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>save_dir</strong> (<code>str</code>): the directory to save the mask figures. Better inside the output folder.</li>\n<li><strong>test_masks_dir_name</strong> (<code>str</code> | <code>None</code>): the relative path to <code>save_dir</code> to save the test mask figures. If <code>None</code>, no file will be saved.</li>\n<li><strong>test_cumulative_masks_dir_name</strong> (<code>str</code> | <code>None</code>): the directory to save the test cumulative mask figures. If <code>None</code>, no file will be saved.</li>\n<li><strong>training_masks_dir_name</strong> (<code>str</code> | <code>None</code>): the directory to save the training mask figures. If <code>None</code>, no file will be saved.</li>\n<li><strong>plot_training_mask_every_n_steps</strong> (<code>int</code> | <code>None</code>): the frequency of plotting training mask figures in terms of number of batches during training. Only applies when <code>training_masks_dir_name</code> is not <code>None</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">save_dir</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">test_masks_dir_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">test_cumulative_masks_dir_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">training_masks_dir_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">plot_training_mask_every_n_steps</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.metrics.HATMasks.plot_training_mask_every_n_steps", "modulename": "clarena.metrics", "qualname": "HATMasks.plot_training_mask_every_n_steps", "kind": "variable", "doc": "<p>Store the frequency of plotting training masks in terms of number of batches.</p>\n", "annotation": ": int"}, {"fullname": "clarena.metrics.HATMasks.task_id", "modulename": "clarena.metrics", "qualname": "HATMasks.task_id", "kind": "variable", "doc": "<p>Task ID counter indicating which task is being processed. Self updated during the task loop. Starting from 1.</p>\n", "annotation": ": int"}, {"fullname": "clarena.metrics.HATMasks.on_fit_start", "modulename": "clarena.metrics", "qualname": "HATMasks.on_fit_start", "kind": "function", "doc": "<p>Get the current task ID in the beginning of a task's fitting (training and validation). Sanity check the <code>pl_module</code> to be <code>HAT</code>.</p>\n\n<p><strong>Raises:</strong>\n-<strong>TypeError</strong>: when the <code>pl_module</code> is not <code>HAT</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.HATMasks.on_train_batch_end", "modulename": "clarena.metrics", "qualname": "HATMasks.on_train_batch_end", "kind": "function", "doc": "<p>Plot training mask after training batch.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the training step, which is the returns of the <code>training_step()</code> method in the <code>CLAlgorithm</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the training data batch.</li>\n<li><strong>batch_idx</strong> (<code>int</code>): the index of the current batch. This is for the file name of mask figures.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.HATMasks.on_test_start", "modulename": "clarena.metrics", "qualname": "HATMasks.on_test_start", "kind": "function", "doc": "<p>Plot test mask and cumulative mask figures.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.HATMasks.plot_hat_mask", "modulename": "clarena.metrics", "qualname": "HATMasks.plot_hat_mask", "kind": "function", "doc": "<p>Plot mask in <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task)</a>) algorithm. This includes the mask and cumulative mask.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>mask</strong> (<code>dict[str, Tensor]</code>): the hard attention (whose values are 0 or 1) mask. Key (<code>str</code>) is layer name, value (<code>Tensor</code>) is the mask tensor. The mask tensor has size (number of units, ).</li>\n<li><strong>plot_dir</strong> (<code>str</code>): the directory to save plot. Better same as the output directory of the experiment.</li>\n<li><strong>task_id</strong> (<code>int</code>): the task ID of the mask to be plotted. This is to form the plot name.</li>\n<li><strong>step</strong> (<code>int</code>): the training step (batch index) of the mask to be plotted. Apply to the training mask only. This is to form the plot name. Keep <code>None</code> for not showing the step in the plot name.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">mask</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">plot_dir</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">step</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.MTLAccuracy", "modulename": "clarena.metrics", "qualname": "MTLAccuracy", "kind": "class", "doc": "<p>Provides all actions that are related to MTL accuracy metrics, which include:</p>\n\n<ul>\n<li>Defining, initializing and recording accuracy metric.</li>\n<li>Logging training and validation accuracy metric to Lightning loggers in real time.</li>\n<li>Saving test accuracy metric to files.</li>\n<li>Visualizing test accuracy metric as plots.</li>\n</ul>\n\n<p>The callback is able to produce the following outputs:</p>\n\n<ul>\n<li>CSV files for test accuracy of all tasks and average accuracy.</li>\n<li>Bar charts for test accuracy of all tasks.</li>\n</ul>\n", "bases": "clarena.metrics.base.MetricCallback"}, {"fullname": "clarena.metrics.MTLAccuracy.__init__", "modulename": "clarena.metrics", "qualname": "MTLAccuracy.__init__", "kind": "function", "doc": "<p>Initialize the <code>MTLAccuracy</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>save_dir</strong> (<code>str</code>): The directory where data and figures of metrics will be saved. Better inside the output folder.</li>\n<li><strong>test_acc_csv_name</strong> (<code>str</code>): file name to save test accuracy of all tasks and average accuracy as CSV file.</li>\n<li><strong>test_acc_plot_name</strong> (<code>str</code> | <code>None</code>): file name to save accuracy plot. If <code>None</code>, no file will be saved.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">save_dir</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">test_acc_csv_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;acc.csv&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">test_acc_plot_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.metrics.MTLAccuracy.test_acc_csv_path", "modulename": "clarena.metrics", "qualname": "MTLAccuracy.test_acc_csv_path", "kind": "variable", "doc": "<p>The path to save test accuracy of all tasks and average accuracy CSV file.</p>\n", "annotation": ": str"}, {"fullname": "clarena.metrics.MTLAccuracy.acc_training_epoch", "modulename": "clarena.metrics", "qualname": "MTLAccuracy.acc_training_epoch", "kind": "variable", "doc": "<p>Classification accuracy of training epoch. Accumulated and calculated from the training batches.</p>\n", "annotation": ": clarena.utils.metrics.MeanMetricBatch"}, {"fullname": "clarena.metrics.MTLAccuracy.acc_val", "modulename": "clarena.metrics", "qualname": "MTLAccuracy.acc_val", "kind": "variable", "doc": "<p>Validation classification accuracy of the model after training epoch. Accumulated and calculated from the validation batches. Keys are task IDs and values are the corresponding metrics.</p>\n", "annotation": ": dict[int, clarena.utils.metrics.MeanMetricBatch]"}, {"fullname": "clarena.metrics.MTLAccuracy.acc_test", "modulename": "clarena.metrics", "qualname": "MTLAccuracy.acc_test", "kind": "variable", "doc": "<p>Test classification accuracy of all tasks. Accumulated and calculated from the test batches. Keys are task IDs and values are the corresponding metrics.</p>\n", "annotation": ": dict[int, clarena.utils.metrics.MeanMetricBatch]"}, {"fullname": "clarena.metrics.MTLAccuracy.on_fit_start", "modulename": "clarena.metrics", "qualname": "MTLAccuracy.on_fit_start", "kind": "function", "doc": "<p>Initialize training and validation metrics.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">mtl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">MTLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.MTLAccuracy.on_train_batch_end", "modulename": "clarena.metrics", "qualname": "MTLAccuracy.on_train_batch_end", "kind": "function", "doc": "<p>Record training metrics from training batch, log metrics of training batch and accumulated metrics of the epoch to Lightning loggers.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the training step, the returns of the <code>training_step()</code> method in the <code>MTLAlgorithm</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the training data batch.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">mtl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">MTLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.MTLAccuracy.on_train_epoch_end", "modulename": "clarena.metrics", "qualname": "MTLAccuracy.on_train_epoch_end", "kind": "function", "doc": "<p>Log metrics of training epoch to plot learning curves and reset the metrics accumulation at the end of training epoch.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">mtl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">MTLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.MTLAccuracy.on_validation_batch_end", "modulename": "clarena.metrics", "qualname": "MTLAccuracy.on_validation_batch_end", "kind": "function", "doc": "<p>Accumulating metrics from validation batch. We don't need to log and monitor the metrics of validation batches.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the validation step, which is the returns of the <code>validation_step()</code> method in the <code>MTLAlgorithm</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the validation data batch.</li>\n<li><strong>dataloader_idx</strong> (<code>int</code>): the task ID of the validation dataloader. A default value of 0 is given otherwise the LightningModule will raise a <code>RuntimeError</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">mtl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">MTLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">dataloader_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.MTLAccuracy.on_test_start", "modulename": "clarena.metrics", "qualname": "MTLAccuracy.on_test_start", "kind": "function", "doc": "<p>Initialize the metrics for testing each seen task in the beginning of a task's testing.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">mtl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">MTLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.MTLAccuracy.on_test_batch_end", "modulename": "clarena.metrics", "qualname": "MTLAccuracy.on_test_batch_end", "kind": "function", "doc": "<p>Accumulating metrics from test batch. We don't need to log and monitor the metrics of test batches.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the test step, which is the returns of the <code>test_step()</code> method in the <code>MTLAlgorithm</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the validation data batch.</li>\n<li><strong>dataloader_idx</strong> (<code>int</code>): the task ID of seen tasks to be tested. A default value of 0 is given otherwise the LightningModule will raise a <code>RuntimeError</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">mtl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">MTLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">dataloader_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.MTLAccuracy.on_test_epoch_end", "modulename": "clarena.metrics", "qualname": "MTLAccuracy.on_test_epoch_end", "kind": "function", "doc": "<p>Save and plot test metrics at the end of test.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">mtl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">MTLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.MTLAccuracy.save_test_acc_to_csv", "modulename": "clarena.metrics", "qualname": "MTLAccuracy.save_test_acc_to_csv", "kind": "function", "doc": "<p>Save the test accuracy metrics of all tasks in multi-task learning to an CSV file.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>csv_path</strong> (<code>str</code>): save the test metric to path. E.g. './outputs/expr_name/1970-01-01_00-00-00/results/acc.csv'.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.MTLAccuracy.plot_test_acc_from_csv", "modulename": "clarena.metrics", "qualname": "MTLAccuracy.plot_test_acc_from_csv", "kind": "function", "doc": "<p>Plot the test accuracy bar chart of all tasks in multi-task learning from saved CSV file and save the plot to the designated directory.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>csv_path</strong> (<code>str</code>): the path to the csv file where the <code>utils.save_test_acc_csv()</code> saved the test accuracy metric.</li>\n<li><strong>plot_path</strong> (<code>str</code>): the path to save plot. Better same as the output directory of the experiment. E.g. './outputs/expr_name/1970-01-01_00-00-00/acc.png'.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">plot_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.MTLLoss", "modulename": "clarena.metrics", "qualname": "MTLLoss", "kind": "class", "doc": "<p>Provides all actions that are related to MTL loss metrics, which include:</p>\n\n<ul>\n<li>Defining, initializing and recording loss metrics.</li>\n<li>Logging training and validation loss metrics to Lightning loggers in real time.</li>\n<li>Saving test loss metrics to files.</li>\n<li>Visualizing test loss metrics as plots.</li>\n</ul>\n\n<p>The callback is able to produce the following outputs:</p>\n\n<ul>\n<li>CSV files for test classification loss of all tasks and average classification loss.</li>\n<li>Bar charts for test classification loss of all tasks.</li>\n</ul>\n", "bases": "clarena.metrics.base.MetricCallback"}, {"fullname": "clarena.metrics.MTLLoss.__init__", "modulename": "clarena.metrics", "qualname": "MTLLoss.__init__", "kind": "function", "doc": "<p>Initialize the <code>MTLLoss</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>save_dir</strong> (<code>str</code>): The directory where data and figures of metrics will be saved. Better inside the output folder.</li>\n<li><strong>test_loss_cls_csv_name</strong>(<code>str</code>): file name to save classification loss of all tasks and average classification loss as CSV file.</li>\n<li><strong>test_loss_cls_plot_name</strong> (<code>str</code> | <code>None</code>): file name to save classification loss plot. If <code>None</code>, no file will be saved.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">save_dir</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">test_loss_cls_csv_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;loss_cls.csv&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">test_loss_cls_plot_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.metrics.MTLLoss.test_loss_cls_csv_path", "modulename": "clarena.metrics", "qualname": "MTLLoss.test_loss_cls_csv_path", "kind": "variable", "doc": "<p>The path to save test classification loss of all tasks and average classification loss CSV file.</p>\n", "annotation": ": str"}, {"fullname": "clarena.metrics.MTLLoss.loss_cls_training_epoch", "modulename": "clarena.metrics", "qualname": "MTLLoss.loss_cls_training_epoch", "kind": "variable", "doc": "<p>Classification loss of training epoch. Accumulated and calculated from the training batches.</p>\n", "annotation": ": clarena.utils.metrics.MeanMetricBatch"}, {"fullname": "clarena.metrics.MTLLoss.loss_cls_val", "modulename": "clarena.metrics", "qualname": "MTLLoss.loss_cls_val", "kind": "variable", "doc": "<p>Validation classification loss of the model after training epoch. Accumulated and calculated from the validation batches. Keys are task IDs and values are the corresponding metrics.</p>\n", "annotation": ": dict[int, clarena.utils.metrics.MeanMetricBatch]"}, {"fullname": "clarena.metrics.MTLLoss.loss_cls_test", "modulename": "clarena.metrics", "qualname": "MTLLoss.loss_cls_test", "kind": "variable", "doc": "<p>Test classification loss of all tasks. Accumulated and calculated from the test batches. Keys are task IDs and values are the corresponding metrics.</p>\n", "annotation": ": dict[int, clarena.utils.metrics.MeanMetricBatch]"}, {"fullname": "clarena.metrics.MTLLoss.on_fit_start", "modulename": "clarena.metrics", "qualname": "MTLLoss.on_fit_start", "kind": "function", "doc": "<p>Initialize training and validation metrics.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">mtl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">MTLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.MTLLoss.on_train_batch_end", "modulename": "clarena.metrics", "qualname": "MTLLoss.on_train_batch_end", "kind": "function", "doc": "<p>Record training metrics from training batch, log metrics of training batch and accumulated metrics of the epoch to Lightning loggers.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the training step, the returns of the <code>training_step()</code> method in the <code>MTLAlgorithm</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the training data batch.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">mtl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">MTLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.MTLLoss.on_train_epoch_end", "modulename": "clarena.metrics", "qualname": "MTLLoss.on_train_epoch_end", "kind": "function", "doc": "<p>Log metrics of training epoch to plot learning curves and reset the metrics accumulation at the end of training epoch.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">mtl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">MTLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.MTLLoss.on_validation_batch_end", "modulename": "clarena.metrics", "qualname": "MTLLoss.on_validation_batch_end", "kind": "function", "doc": "<p>Accumulating metrics from validation batch. We don't need to log and monitor the metrics of validation batches.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the validation step, which is the returns of the <code>validation_step()</code> method in the <code>MTLAlgorithm</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the validation data batch.</li>\n<li><strong>dataloader_idx</strong> (<code>int</code>): the task ID of the validation dataloader. A default value of 0 is given otherwise the LightningModule will raise a <code>RuntimeError</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">mtl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">MTLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">dataloader_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.MTLLoss.on_validation_epoch_end", "modulename": "clarena.metrics", "qualname": "MTLLoss.on_validation_epoch_end", "kind": "function", "doc": "<p>Log validation metrics to plot learning curves.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">mtl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">MTLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.MTLLoss.on_test_start", "modulename": "clarena.metrics", "qualname": "MTLLoss.on_test_start", "kind": "function", "doc": "<p>Initialize the metrics for testing each seen task in the beginning of a task's testing.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">mtl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">MTLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.MTLLoss.on_test_batch_end", "modulename": "clarena.metrics", "qualname": "MTLLoss.on_test_batch_end", "kind": "function", "doc": "<p>Accumulating metrics from test batch. We don't need to log and monitor the metrics of test batches.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the test step, which is the returns of the <code>test_step()</code> method in the <code>MTLAlgorithm</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the validation data batch.</li>\n<li><strong>dataloader_idx</strong> (<code>int</code>): the task ID of seen tasks to be tested. A default value of 0 is given otherwise the LightningModule will raise a <code>RuntimeError</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">mtl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">MTLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">dataloader_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.MTLLoss.on_test_epoch_end", "modulename": "clarena.metrics", "qualname": "MTLLoss.on_test_epoch_end", "kind": "function", "doc": "<p>Save and plot test metrics at the end of test.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">mtl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">MTLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.MTLLoss.save_test_loss_cls_to_csv", "modulename": "clarena.metrics", "qualname": "MTLLoss.save_test_loss_cls_to_csv", "kind": "function", "doc": "<p>Save the test classification loss metrics of all tasks in multi-task learning to an CSV file.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>csv_path</strong> (<code>str</code>): save the test metric to path. E.g. './outputs/expr_name/1970-01-01_00-00-00/results/loss_cls.csv'.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.MTLLoss.plot_test_loss_cls_from_csv", "modulename": "clarena.metrics", "qualname": "MTLLoss.plot_test_loss_cls_from_csv", "kind": "function", "doc": "<p>Plot the test classification loss bar chart of all tasks in multi-task learning from saved CSV file and save the plot to the designated directory.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>csv_path</strong> (<code>str</code>): the path to the csv file where the <code>utils.save_test_acc_csv()</code> saved the test classification loss metric.</li>\n<li><strong>plot_path</strong> (<code>str</code>): the path to save plot. Better same as the output directory of the experiment. E.g. './outputs/expr_name/1970-01-01_00-00-00/loss_cls.png'.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">plot_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.STLAccuracy", "modulename": "clarena.metrics", "qualname": "STLAccuracy", "kind": "class", "doc": "<p>Provides all actions that are related to STL accuracy metric, which include:</p>\n\n<ul>\n<li>Defining, initializing and recording accuracy metric.</li>\n<li>Logging training and validation accuracy metric to Lightning loggers in real time.</li>\n</ul>\n\n<p>Saving test accuracy metric to files.</p>\n\n<ul>\n<li>The callback is able to produce the following outputs:</li>\n<li>CSV files for test accuracy.</li>\n</ul>\n", "bases": "clarena.metrics.base.MetricCallback"}, {"fullname": "clarena.metrics.STLAccuracy.__init__", "modulename": "clarena.metrics", "qualname": "STLAccuracy.__init__", "kind": "function", "doc": "<p>Initialize the <code>STLAccuracy</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>save_dir</strong> (<code>str</code>): The directory where data and figures of metrics will be saved. Better inside the output folder.</li>\n<li><strong>test_acc_csv_name</strong> (<code>str</code>): file name to save test accuracy of all tasks and average accuracy as CSV file.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">save_dir</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">test_acc_csv_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;acc.csv&#39;</span></span>)</span>"}, {"fullname": "clarena.metrics.STLAccuracy.test_acc_csv_path", "modulename": "clarena.metrics", "qualname": "STLAccuracy.test_acc_csv_path", "kind": "variable", "doc": "<p>The path to save test accuracy of all tasks and average accuracy CSV file.</p>\n", "annotation": ": str"}, {"fullname": "clarena.metrics.STLAccuracy.acc_training_epoch", "modulename": "clarena.metrics", "qualname": "STLAccuracy.acc_training_epoch", "kind": "variable", "doc": "<p>Classification accuracy of training epoch. Accumulated and calculated from the training batches.</p>\n", "annotation": ": clarena.utils.metrics.MeanMetricBatch"}, {"fullname": "clarena.metrics.STLAccuracy.acc_val", "modulename": "clarena.metrics", "qualname": "STLAccuracy.acc_val", "kind": "variable", "doc": "<p>Validation classification accuracy of the model after training epoch. Accumulated and calculated from the validation batches.</p>\n", "annotation": ": clarena.utils.metrics.MeanMetricBatch"}, {"fullname": "clarena.metrics.STLAccuracy.acc_test", "modulename": "clarena.metrics", "qualname": "STLAccuracy.acc_test", "kind": "variable", "doc": "<p>Test classification accuracy. Accumulated and calculated from the test batches.</p>\n", "annotation": ": clarena.utils.metrics.MeanMetricBatch"}, {"fullname": "clarena.metrics.STLAccuracy.on_fit_start", "modulename": "clarena.metrics", "qualname": "STLAccuracy.on_fit_start", "kind": "function", "doc": "<p>Initialize training and validation metrics.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">stl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">STLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.STLAccuracy.on_train_batch_end", "modulename": "clarena.metrics", "qualname": "STLAccuracy.on_train_batch_end", "kind": "function", "doc": "<p>Record training metrics from training batch, log metrics of training batch and accumulated metrics of the epoch to Lightning loggers.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the training step, the returns of the <code>training_step()</code> method in the <code>STLAlgorithm</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the training data batch.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">stl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">STLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.STLAccuracy.on_train_epoch_end", "modulename": "clarena.metrics", "qualname": "STLAccuracy.on_train_epoch_end", "kind": "function", "doc": "<p>Log metrics of training epoch to plot learning curves and reset the metrics accumulation at the end of training epoch.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">stl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">STLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.STLAccuracy.on_validation_batch_end", "modulename": "clarena.metrics", "qualname": "STLAccuracy.on_validation_batch_end", "kind": "function", "doc": "<p>Accumulating metrics from validation batch. We don't need to log and monitor the metrics of validation batches.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the validation step, which is the returns of the <code>validation_step()</code> method in the <code>STLAlgorithm</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the validation data batch.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">stl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">STLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.STLAccuracy.on_validation_epoch_end", "modulename": "clarena.metrics", "qualname": "STLAccuracy.on_validation_epoch_end", "kind": "function", "doc": "<p>Log validation metrics to plot learning curves.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">stl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">STLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.STLAccuracy.on_test_start", "modulename": "clarena.metrics", "qualname": "STLAccuracy.on_test_start", "kind": "function", "doc": "<p>Initialize the testing metrics.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">stl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">STLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.STLAccuracy.on_test_batch_end", "modulename": "clarena.metrics", "qualname": "STLAccuracy.on_test_batch_end", "kind": "function", "doc": "<p>Accumulating metrics from test batch. We don't need to log and monitor the metrics of test batches.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the test step, which is the returns of the <code>test_step()</code> method in the <code>STLAlgorithm</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the test data batch.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">stl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">STLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.STLAccuracy.on_test_epoch_end", "modulename": "clarena.metrics", "qualname": "STLAccuracy.on_test_epoch_end", "kind": "function", "doc": "<p>Save and plot test metrics at the end of test.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">stl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">STLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.STLAccuracy.save_test_acc_to_csv", "modulename": "clarena.metrics", "qualname": "STLAccuracy.save_test_acc_to_csv", "kind": "function", "doc": "<p>Save the test accuracy metrics of all tasks in single-task learning to an CSV file.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>csv_path</strong> (<code>str</code>): save the test metric to path. E.g. './outputs/expr_name/1970-01-01_00-00-00/results/acc.csv'.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.STLLoss", "modulename": "clarena.metrics", "qualname": "STLLoss", "kind": "class", "doc": "<p>Provides all actions that are related to STL loss metrics, which include:</p>\n\n<ul>\n<li>Defining, initializing and recording loss metrics.</li>\n<li>Logging training and validation loss metrics to Lightning loggers in real time.</li>\n</ul>\n\n<p>Saving test loss metrics to files.</p>\n\n<ul>\n<li>The callback is able to produce the following outputs:</li>\n<li>CSV files for test classification loss.</li>\n</ul>\n", "bases": "clarena.metrics.base.MetricCallback"}, {"fullname": "clarena.metrics.STLLoss.__init__", "modulename": "clarena.metrics", "qualname": "STLLoss.__init__", "kind": "function", "doc": "<p>Initialize the <code>STLLoss</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>save_dir</strong> (<code>str</code>): The directory where data and figures of metrics will be saved. Better inside the output folder.</li>\n<li><strong>test_loss_cls_csv_name</strong>(<code>str</code>): file name to save classification loss of all tasks and average classification loss as CSV file.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">save_dir</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">test_loss_cls_csv_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;loss_cls.csv&#39;</span></span>)</span>"}, {"fullname": "clarena.metrics.STLLoss.test_loss_cls_csv_path", "modulename": "clarena.metrics", "qualname": "STLLoss.test_loss_cls_csv_path", "kind": "variable", "doc": "<p>Store the path to save test classification loss of all tasks and average classification loss CSV file.</p>\n", "annotation": ": str"}, {"fullname": "clarena.metrics.STLLoss.loss_cls_training_epoch", "modulename": "clarena.metrics", "qualname": "STLLoss.loss_cls_training_epoch", "kind": "variable", "doc": "<p>Classification loss of training epoch. Accumulated and calculated from the training batches.</p>\n", "annotation": ": clarena.utils.metrics.MeanMetricBatch"}, {"fullname": "clarena.metrics.STLLoss.loss_cls_val", "modulename": "clarena.metrics", "qualname": "STLLoss.loss_cls_val", "kind": "variable", "doc": "<p>Validation classification of the model loss after training epoch. Accumulated and calculated from the validation batches.</p>\n", "annotation": ": clarena.utils.metrics.MeanMetricBatch"}, {"fullname": "clarena.metrics.STLLoss.loss_cls_test", "modulename": "clarena.metrics", "qualname": "STLLoss.loss_cls_test", "kind": "variable", "doc": "<p>Test classification loss. Accumulated and calculated from the test batches.</p>\n", "annotation": ": clarena.utils.metrics.MeanMetricBatch"}, {"fullname": "clarena.metrics.STLLoss.on_fit_start", "modulename": "clarena.metrics", "qualname": "STLLoss.on_fit_start", "kind": "function", "doc": "<p>Initialize training and validation metrics.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">stl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">STLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.STLLoss.on_train_batch_end", "modulename": "clarena.metrics", "qualname": "STLLoss.on_train_batch_end", "kind": "function", "doc": "<p>Record training metrics from training batch, log metrics of training batch and accumulated metrics of the epoch to Lightning loggers.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the training step, the returns of the <code>training_step()</code> method in the <code>STLAlgorithm</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the training data batch.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">stl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">STLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.STLLoss.on_train_epoch_end", "modulename": "clarena.metrics", "qualname": "STLLoss.on_train_epoch_end", "kind": "function", "doc": "<p>Log metrics of training epoch to plot learning curves and reset the metrics accumulation at the end of training epoch.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">stl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">STLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.STLLoss.on_validation_batch_end", "modulename": "clarena.metrics", "qualname": "STLLoss.on_validation_batch_end", "kind": "function", "doc": "<p>Accumulating metrics from validation batch. We don't need to log and monitor the metrics of validation batches.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the validation step, which is the returns of the <code>validation_step()</code> method in the <code>STLAlgorithm</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the validation data batch.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">stl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">STLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.STLLoss.on_validation_epoch_end", "modulename": "clarena.metrics", "qualname": "STLLoss.on_validation_epoch_end", "kind": "function", "doc": "<p>Log validation metrics to plot learning curves.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">stl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">STLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.STLLoss.on_test_start", "modulename": "clarena.metrics", "qualname": "STLLoss.on_test_start", "kind": "function", "doc": "<p>Initialize the metrics for testing each seen task in the beginning of a task's testing.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">stl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">STLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.STLLoss.on_test_batch_end", "modulename": "clarena.metrics", "qualname": "STLLoss.on_test_batch_end", "kind": "function", "doc": "<p>Accumulating metrics from test batch. We don't need to log and monitor the metrics of test batches.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the test step, which is the returns of the <code>test_step()</code> method in the <code>STLAlgorithm</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the test data batch.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">stl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">STLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.STLLoss.on_test_epoch_end", "modulename": "clarena.metrics", "qualname": "STLLoss.on_test_epoch_end", "kind": "function", "doc": "<p>Save and plot test metrics at the end of test.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">stl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">STLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.STLLoss.save_test_loss_cls_to_csv", "modulename": "clarena.metrics", "qualname": "STLLoss.save_test_loss_cls_to_csv", "kind": "function", "doc": "<p>Save the test classification loss metrics in single-task learning to an CSV file.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>csv_path</strong> (<code>str</code>): save the test metric to path. E.g. './outputs/expr_name/1970-01-01_00-00-00/results/loss_cls.csv'.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_algorithms", "modulename": "clarena.mtl_algorithms", "kind": "module", "doc": "<h1 id=\"multi-task-learning-algorithms\">Multi-Task Learning Algorithms</h1>\n\n<p>This submodule provides the <strong>multi-task learning algorithms</strong> in CLArena.</p>\n\n<p>The algorithms are implemented as subclasses of <code>MTLAlgorithm</code>.</p>\n\n<p>Please note that this is an API documantation. Please refer to the main documentation pages for more information about the backbone networks and how to configure and implement them:</p>\n\n<ul>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/configure-your-experiment/cl-algorithm\"><strong>Configure MTL Algorithm</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/implement-your-cl-modules/cl-algorithm\"><strong>Implement Your CL Algorithm Class</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/posts/continual-learning-beginners-guide#sec-methodology\"><strong>A Beginners' Guide to Continual Learning (Methodology Overview)</strong></a></li>\n</ul>\n"}, {"fullname": "clarena.mtl_algorithms.base", "modulename": "clarena.mtl_algorithms.base", "kind": "module", "doc": "<p>The submodule in <code>mtl_algorithms</code> for MTL algorithm bases.</p>\n"}, {"fullname": "clarena.mtl_algorithms.base.MTLAlgorithm", "modulename": "clarena.mtl_algorithms.base", "qualname": "MTLAlgorithm", "kind": "class", "doc": "<p>The class of multi-task learning, inherited from <code>LightningModule</code>.</p>\n", "bases": "lightning.pytorch.core.module.LightningModule"}, {"fullname": "clarena.mtl_algorithms.base.MTLAlgorithm.__init__", "modulename": "clarena.mtl_algorithms.base", "qualname": "MTLAlgorithm.__init__", "kind": "function", "doc": "<p>Initialize the MTL algorithm with the network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone</strong> (<code>Backbone</code>): backbone network.</li>\n<li><strong>heads</strong> (<code>HeadsMTL</code>): output heads.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">Backbone</span>,</span><span class=\"param\">\t<span class=\"n\">heads</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_mtl</span><span class=\"o\">.</span><span class=\"n\">HeadsMTL</span></span>)</span>"}, {"fullname": "clarena.mtl_algorithms.base.MTLAlgorithm.backbone", "modulename": "clarena.mtl_algorithms.base", "qualname": "MTLAlgorithm.backbone", "kind": "variable", "doc": "<p>Store the backbone network.</p>\n", "annotation": ": clarena.backbones.base.Backbone"}, {"fullname": "clarena.mtl_algorithms.base.MTLAlgorithm.heads", "modulename": "clarena.mtl_algorithms.base", "qualname": "MTLAlgorithm.heads", "kind": "variable", "doc": "<p>Store the output heads.</p>\n", "annotation": ": clarena.heads.heads_mtl.HeadsMTL"}, {"fullname": "clarena.mtl_algorithms.base.MTLAlgorithm.optimizer", "modulename": "clarena.mtl_algorithms.base", "qualname": "MTLAlgorithm.optimizer", "kind": "variable", "doc": "<p>Store the optimizer object (partially initialized) for the backpropagation of task <code>self.task_id</code>. Will be equipped with parameters in <code>configure_optimizers()</code>.</p>\n", "annotation": ": torch.optim.optimizer.Optimizer"}, {"fullname": "clarena.mtl_algorithms.base.MTLAlgorithm.lr_scheduler", "modulename": "clarena.mtl_algorithms.base", "qualname": "MTLAlgorithm.lr_scheduler", "kind": "variable", "doc": "<p>Store the learning rate scheduler for the optimizer. If <code>None</code>, no scheduler is used.</p>\n", "annotation": ": torch.optim.lr_scheduler.LRScheduler | None"}, {"fullname": "clarena.mtl_algorithms.base.MTLAlgorithm.criterion", "modulename": "clarena.mtl_algorithms.base", "qualname": "MTLAlgorithm.criterion", "kind": "variable", "doc": "<p>The loss function bewteen the output logits and the target labels. Default is cross-entropy loss.</p>\n"}, {"fullname": "clarena.mtl_algorithms.base.MTLAlgorithm.if_forward_func_return_logits_only", "modulename": "clarena.mtl_algorithms.base", "qualname": "MTLAlgorithm.if_forward_func_return_logits_only", "kind": "variable", "doc": "<p>Whether the <code>forward()</code> method returns logits only. If <code>False</code>, it returns a dictionary containing logits and other information. Default is <code>False</code>.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.mtl_algorithms.base.MTLAlgorithm.sanity_check", "modulename": "clarena.mtl_algorithms.base", "qualname": "MTLAlgorithm.sanity_check", "kind": "function", "doc": "<p>Check the sanity of the arguments.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_algorithms.base.MTLAlgorithm.setup_tasks", "modulename": "clarena.mtl_algorithms.base", "qualname": "MTLAlgorithm.setup_tasks", "kind": "function", "doc": "<p>Setup tasks for the MTL algorithm. This must be done before <code>forward()</code> method is called.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>optimizer</strong> (<code>Optimizer</code>): the optimizer object (partially initialized).</li>\n<li><strong>lr_scheduler</strong> (<code>LRScheduler</code> | None): the learning rate scheduler for the optimizer. If <code>None</code>, no scheduler is used.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">optimizer</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">Optimizer</span>,</span><span class=\"param\">\t<span class=\"n\">lr_scheduler</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">lr_scheduler</span><span class=\"o\">.</span><span class=\"n\">LRScheduler</span> <span class=\"o\">|</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_algorithms.base.MTLAlgorithm.get_val_task_id_from_dataloader_idx", "modulename": "clarena.mtl_algorithms.base", "qualname": "MTLAlgorithm.get_val_task_id_from_dataloader_idx", "kind": "function", "doc": "<p>Get the validation task ID from the dataloader index.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>dataloader_idx</strong> (<code>int</code>): the dataloader index.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_task_id</strong> (<code>str</code>): the test task ID.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">dataloader_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">int</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_algorithms.base.MTLAlgorithm.get_test_task_id_from_dataloader_idx", "modulename": "clarena.mtl_algorithms.base", "qualname": "MTLAlgorithm.get_test_task_id_from_dataloader_idx", "kind": "function", "doc": "<p>Get the test task ID from the dataloader index.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>dataloader_idx</strong> (<code>int</code>): the dataloader index.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_task_id</strong> (<code>str</code>): the test task ID.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">dataloader_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">int</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_algorithms.base.MTLAlgorithm.forward", "modulename": "clarena.mtl_algorithms.base", "qualname": "MTLAlgorithm.forward", "kind": "function", "doc": "<p>The forward pass for data. Note that it is nothing to do with <code>forward()</code> method in <code>nn.Module</code>.</p>\n\n<p>This forward pass does not accept input batch in different tasks. Please make sure the input batch is from the same task. If you want to use this forward pass for different tasks, please divide the input batch by tasks and call this forward pass for each task separately.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): The input tensor from data.</li>\n<li><strong>task_ids</strong> (<code>int</code> | <code>Tensor</code>): the task ID(s) for the input data. If the input batch is from the same task, this can be a single integer.</li>\n<li><strong>stage</strong> (<code>str</code>): the stage of the forward pass, should be one of the following:\n<ol>\n<li>'train': training stage.</li>\n<li>'validation': validation stage.</li>\n<li>'test': testing stage.</li>\n</ol></li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>logits</strong> (<code>Tensor</code>): the output logits tensor.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">task_ids</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_algorithms.base.MTLAlgorithm.configure_optimizers", "modulename": "clarena.mtl_algorithms.base", "qualname": "MTLAlgorithm.configure_optimizers", "kind": "function", "doc": "<p>Configure optimizer hooks by Lightning.\nSee <a href=\"https://lightning.ai/docs/pytorch/stable/common/lightning_module.html#configure-optimizers\">Lightning docs</a> for more details.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">Optimizer</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_algorithms.joint_learning", "modulename": "clarena.mtl_algorithms.joint_learning", "kind": "module", "doc": "<p>The submodule in <code>mtl_algorithms</code> for joint learning algorithm.</p>\n"}, {"fullname": "clarena.mtl_algorithms.joint_learning.JointLearning", "modulename": "clarena.mtl_algorithms.joint_learning", "qualname": "JointLearning", "kind": "class", "doc": "<p>Joint learning algorithm.</p>\n\n<p>The most naive way for multi-task learning. It directly trains all tasks.</p>\n", "bases": "clarena.mtl_algorithms.base.MTLAlgorithm"}, {"fullname": "clarena.mtl_algorithms.joint_learning.JointLearning.__init__", "modulename": "clarena.mtl_algorithms.joint_learning", "qualname": "JointLearning.__init__", "kind": "function", "doc": "<p>Initialize the JointLearning algorithm with the network. It has no additional hyperparameters.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone</strong> (<code>CLBackbone</code>): backbone network.</li>\n<li><strong>heads</strong> (<code>HeadsMTL</code>): output heads.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLBackbone</span>,</span><span class=\"param\">\t<span class=\"n\">heads</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_mtl</span><span class=\"o\">.</span><span class=\"n\">HeadsMTL</span></span>)</span>"}, {"fullname": "clarena.mtl_algorithms.joint_learning.JointLearning.training_step", "modulename": "clarena.mtl_algorithms.joint_learning", "qualname": "JointLearning.training_step", "kind": "function", "doc": "<p>Training step.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of training data, which can be from any mixed tasks. Must include task IDs in the batch.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and accuracy from this training step. Keys (<code>str</code>) are the metrics names, and values (<code>Tensor</code>) are the metrics. Must include the key 'loss' which is total loss in the case of automatic optimization, according to PyTorch Lightning docs.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_algorithms.joint_learning.JointLearning.validation_step", "modulename": "clarena.mtl_algorithms.joint_learning", "qualname": "JointLearning.validation_step", "kind": "function", "doc": "<p>Validation step. This is done task by task rather than mixing the tasks in batches.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of validation data.</li>\n<li><strong>dataloader_idx</strong> (<code>int</code>): the task ID of seen tasks to be tested. A default value of 0 is given otherwise the LightningModule will raise a <code>RuntimeError</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and accuracy from this validation step. Keys (<code>str</code>) are the metrics names, and values (<code>Tensor</code>) are the metrics.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">dataloader_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_algorithms.joint_learning.JointLearning.test_step", "modulename": "clarena.mtl_algorithms.joint_learning", "qualname": "JointLearning.test_step", "kind": "function", "doc": "<p>Test step. This is done task by task rather than mixing the tasks in batches.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of test data.</li>\n<li><strong>dataloader_idx</strong> (<code>int</code>): the task ID of seen tasks to be tested. A default value of 0 is given otherwise the LightningModule will raise a <code>RuntimeError</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and accuracy from this test step. Keys (<code>str</code>) are the metrics names, and values (<code>Tensor</code>) are the metrics.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">dataloader_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_datasets", "modulename": "clarena.mtl_datasets", "kind": "module", "doc": "<h1 id=\"continual-learning-datasets\">Continual Learning Datasets</h1>\n\n<p>This submodule provides the <strong>continual learning datasets</strong> that can be used in CLArena.</p>\n\n<p>The datasets are implemented as subclasses of <code>CLDataset</code> classes, which are the base class for all continual learning datasets in CLArena.</p>\n\n<ul>\n<li><code>CLDataset</code>: The base class for continual learning datasets.</li>\n<li><code>CLPermutedDataset</code>: The base class for permuted continual learning datasets. A child class of <code>CLDataset</code>.</li>\n<li><code>CLSplitDataset</code>: The base class for split continual learning datasets. A child class of <code>CLDataset</code>.</li>\n<li><code>CLCombinedDataset</code>: The base class for combined continual learning datasets. A child class of <code>CLDataset</code>.</li>\n</ul>\n\n<p>Please note that this is an API documantation. Please refer to the main documentation pages for more information about the CL datasets and how to configure and implement them:</p>\n\n<ul>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/configure-your-experiment/cl-dataset\"><strong>Configure CL Dataset</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/implement-your-cl-modules/cl-dataset\"><strong>Implement Your CL Dataset Class</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/posts/continual-learning-beginners-guide#sec-CL-dataset\"><strong>A Beginners' Guide to Continual Learning (CL Dataset)</strong></a></li>\n</ul>\n"}, {"fullname": "clarena.mtl_datasets.base", "modulename": "clarena.mtl_datasets.base", "kind": "module", "doc": "<p>The submodule in <code>mtl_datasets</code> for MTL dataset bases.</p>\n"}, {"fullname": "clarena.mtl_datasets.base.MTLDataset", "modulename": "clarena.mtl_datasets.base", "qualname": "MTLDataset", "kind": "class", "doc": "<p>The base class of multi-task learning datasets, inherited from <code>LightningDataModule</code>.</p>\n", "bases": "lightning.pytorch.core.datamodule.LightningDataModule"}, {"fullname": "clarena.mtl_datasets.base.MTLDataset.__init__", "modulename": "clarena.mtl_datasets.base", "qualname": "MTLDataset.__init__", "kind": "function", "doc": "<p>Initialize the MTL dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code> | <code>list[str]</code>): the root directory where the original data files for constructing the MTL dataset physically live. If <code>list[str]</code>, it should be a list of strings, each string is the root directory for each task.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the MTL dataset.</li>\n<li><strong>sampling_strategy</strong> (<code>str</code>): the sampling strategy that construct training batch from each task's dataset. Should be one of the following:\n<ul>\n<li>'mixed': mixed sampling strategy, which samples from all tasks' datasets.</li>\n</ul></li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">sampling_strategy</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;mixed&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.mtl_datasets.base.MTLDataset.root", "modulename": "clarena.mtl_datasets.base", "qualname": "MTLDataset.root", "kind": "variable", "doc": "<p>The dict of root directories of the original data files for each task.</p>\n", "annotation": ": dict[int, str]"}, {"fullname": "clarena.mtl_datasets.base.MTLDataset.num_tasks", "modulename": "clarena.mtl_datasets.base", "qualname": "MTLDataset.num_tasks", "kind": "variable", "doc": "<p>The maximum number of tasks supported by the dataset.</p>\n", "annotation": ": int"}, {"fullname": "clarena.mtl_datasets.base.MTLDataset.sampling_strategy", "modulename": "clarena.mtl_datasets.base", "qualname": "MTLDataset.sampling_strategy", "kind": "variable", "doc": "<p>The sampling strategy for constructing training batch from each task's dataset.</p>\n", "annotation": ": str"}, {"fullname": "clarena.mtl_datasets.base.MTLDataset.batch_size", "modulename": "clarena.mtl_datasets.base", "qualname": "MTLDataset.batch_size", "kind": "variable", "doc": "<p>The batch size for dataloaders.</p>\n", "annotation": ": int"}, {"fullname": "clarena.mtl_datasets.base.MTLDataset.num_workers", "modulename": "clarena.mtl_datasets.base", "qualname": "MTLDataset.num_workers", "kind": "variable", "doc": "<p>The number of workers for dataloaders.</p>\n", "annotation": ": int"}, {"fullname": "clarena.mtl_datasets.base.MTLDataset.custom_transforms", "modulename": "clarena.mtl_datasets.base", "qualname": "MTLDataset.custom_transforms", "kind": "variable", "doc": "<p>The dict of custom transforms for each task.</p>\n", "annotation": ": dict[int, typing.Union[typing.Callable, torchvision.transforms.transforms.Compose, NoneType]]"}, {"fullname": "clarena.mtl_datasets.base.MTLDataset.repeat_channels", "modulename": "clarena.mtl_datasets.base", "qualname": "MTLDataset.repeat_channels", "kind": "variable", "doc": "<p>The dict of number of channels to repeat for each task.</p>\n", "annotation": ": dict[int, int | None]"}, {"fullname": "clarena.mtl_datasets.base.MTLDataset.to_tensor", "modulename": "clarena.mtl_datasets.base", "qualname": "MTLDataset.to_tensor", "kind": "variable", "doc": "<p>The dict of to_tensor flag for each task.</p>\n", "annotation": ": dict[int, bool]"}, {"fullname": "clarena.mtl_datasets.base.MTLDataset.resize", "modulename": "clarena.mtl_datasets.base", "qualname": "MTLDataset.resize", "kind": "variable", "doc": "<p>The dict of sizes to resize to for each task.</p>\n", "annotation": ": dict[int, tuple[int, int] | None]"}, {"fullname": "clarena.mtl_datasets.base.MTLDataset.dataset_train", "modulename": "clarena.mtl_datasets.base", "qualname": "MTLDataset.dataset_train", "kind": "variable", "doc": "<p>The dictionary to store training dataset object of each task. Keys are task IDs and values are the dataset objects. Can be PyTorch Dataset objects or any other dataset objects.</p>\n", "annotation": ": dict[int, typing.Any]"}, {"fullname": "clarena.mtl_datasets.base.MTLDataset.dataset_val", "modulename": "clarena.mtl_datasets.base", "qualname": "MTLDataset.dataset_val", "kind": "variable", "doc": "<p>The dictionary to store validation dataset object of each task. Keys are task IDs and values are the dataset objects. Can be PyTorch Dataset objects or any other dataset objects.</p>\n", "annotation": ": dict[int, typing.Any]"}, {"fullname": "clarena.mtl_datasets.base.MTLDataset.dataset_test", "modulename": "clarena.mtl_datasets.base", "qualname": "MTLDataset.dataset_test", "kind": "variable", "doc": "<p>The dictionary to store test dataset object of each task. Keys are task IDs and values are the dataset objects. Can be PyTorch Dataset objects or any other dataset objects.</p>\n", "annotation": ": dict[int, typing.Any]"}, {"fullname": "clarena.mtl_datasets.base.MTLDataset.mean", "modulename": "clarena.mtl_datasets.base", "qualname": "MTLDataset.mean", "kind": "variable", "doc": "<p>Tthe list of mean values for normalization for all tasks. Used when constructing the transforms.</p>\n", "annotation": ": dict[int, float]"}, {"fullname": "clarena.mtl_datasets.base.MTLDataset.std", "modulename": "clarena.mtl_datasets.base", "qualname": "MTLDataset.std", "kind": "variable", "doc": "<p>The list of standard deviation values for normalization for all tasks. Used when constructing the transforms.</p>\n", "annotation": ": dict[int, float]"}, {"fullname": "clarena.mtl_datasets.base.MTLDataset.train_tasks", "modulename": "clarena.mtl_datasets.base", "qualname": "MTLDataset.train_tasks", "kind": "variable", "doc": "<p>\"The list of task IDs to be trained. It should be a list of integers, each integer is the task ID.</p>\n", "annotation": ": list[int]"}, {"fullname": "clarena.mtl_datasets.base.MTLDataset.eval_tasks", "modulename": "clarena.mtl_datasets.base", "qualname": "MTLDataset.eval_tasks", "kind": "variable", "doc": "<p>The list of task IDs to be evaluated. It should be a list of integers, each integer is the task ID.</p>\n", "annotation": ": list[int]"}, {"fullname": "clarena.mtl_datasets.base.MTLDataset.sanity_check", "modulename": "clarena.mtl_datasets.base", "qualname": "MTLDataset.sanity_check", "kind": "function", "doc": "<p>Check the sanity of the arguments.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_datasets.base.MTLDataset.get_class_map", "modulename": "clarena.mtl_datasets.base", "qualname": "MTLDataset.get_class_map", "kind": "function", "doc": "<p>Get the mapping of classes of task <code>task_id</code>. It must be implemented by subclasses.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): The task ID to query class map.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>class_map</strong>(<code>dict[str | int, int]</code>): the class map of the task. Key is original class label, value is integer class label for multi-task learning. The mapped class labels of each task should be continuous integers from 0 to the number of classes.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_datasets.base.MTLDataset.prepare_data", "modulename": "clarena.mtl_datasets.base", "qualname": "MTLDataset.prepare_data", "kind": "function", "doc": "<p>Use this to download and prepare data. It must be implemented by subclasses, regulated by <code>LightningDatamodule</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_datasets.base.MTLDataset.setup", "modulename": "clarena.mtl_datasets.base", "qualname": "MTLDataset.setup", "kind": "function", "doc": "<p>Set up the dataset for different stages.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>stage</strong> (<code>str</code>): the stage of the experiment. Should be one of the following:\n<ul>\n<li>'fit': training and validation dataset should be assigned to <code>self.dataset_train</code> and <code>self.dataset_val</code>.</li>\n<li>'test': test dataset should be assigned to <code>self.dataset_test</code>.</li>\n</ul></li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_datasets.base.MTLDataset.setup_tasks", "modulename": "clarena.mtl_datasets.base", "qualname": "MTLDataset.setup_tasks", "kind": "function", "doc": "<p>Set up the tasks for the dataset.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>train_tasks</strong> (<code>list[int]</code>): the list of task IDs to be trained. It should be a list of integers, each integer is the task ID. This is used when constructing the dataloader.</li>\n<li><strong>eval_tasks</strong> (<code>list[int]</code>): the list of task IDs to be evaluated. It should be a list of integers, each integer is the task ID. This is used when constructing the dataloader.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">train_tasks</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">eval_tasks</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_datasets.base.MTLDataset.train_and_val_transforms", "modulename": "clarena.mtl_datasets.base", "qualname": "MTLDataset.train_and_val_transforms", "kind": "function", "doc": "<p>Transforms generator for train and validation dataset of task <code>task_id</code> incorporating the custom transforms with basic transforms like <code>normalization</code> and <code>ToTensor()</code>. It is a handy tool to use in subclasses when constructing the dataset.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the task ID of training and validation dataset to get the transforms for.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_transforms</strong> (<code>transforms.Compose</code>): the composed training transforms.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_datasets.base.MTLDataset.test_transforms", "modulename": "clarena.mtl_datasets.base", "qualname": "MTLDataset.test_transforms", "kind": "function", "doc": "<p>Transforms generator for test dataset of task <code>task_id</code>. Only basic transforms like <code>normalization</code> and <code>ToTensor()</code> are included. It is a handy tool to use in subclasses when constructing the dataset.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the task ID of test dataset to get the transforms for.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_transforms</strong> (<code>transforms.Compose</code>): the composed training transforms.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_datasets.base.MTLDataset.train_and_val_dataset", "modulename": "clarena.mtl_datasets.base", "qualname": "MTLDataset.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>task_id</code>. It must be implemented by subclasses.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the task ID to get the training and validation dataset for.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>Any</code>): the train and validation dataset of task <code>task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Any</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_datasets.base.MTLDataset.test_dataset", "modulename": "clarena.mtl_datasets.base", "qualname": "MTLDataset.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>task_id</code>. It must be implemented by subclasses.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the task ID to get the test dataset for.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Any</code>): the test dataset of task <code>task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Any</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_datasets.base.MTLDataset.train_dataloader", "modulename": "clarena.mtl_datasets.base", "qualname": "MTLDataset.train_dataloader", "kind": "function", "doc": "<p>DataLoader generator for stage train. It is automatically called before training.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_dataloader</strong> (<code>Dataloader</code>): the train DataLoader of task.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_datasets.base.MTLDataset.val_dataloader", "modulename": "clarena.mtl_datasets.base", "qualname": "MTLDataset.val_dataloader", "kind": "function", "doc": "<p>DataLoader generator for the validation stage. It is automatically called before validation.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>val_dataloader</strong> (<code>dict[int, DataLoader]</code>): the validation DataLoader.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_datasets.base.MTLDataset.test_dataloader", "modulename": "clarena.mtl_datasets.base", "qualname": "MTLDataset.test_dataloader", "kind": "function", "doc": "<p>DataLoader generator for stage test. It is automatically called before testing.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataloader</strong> (<code>dict[int, DataLoader]</code>): the test DataLoader.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_datasets.base.MTLDatasetFromRaw", "modulename": "clarena.mtl_datasets.base", "qualname": "MTLDatasetFromRaw", "kind": "class", "doc": "<p>Multi-task learning datasets constructed from raw data files. Inherits from <code>MTLDataset</code>.</p>\n\n<p>This is usually for constructing the reference joint learning experiment for multi-task learning.</p>\n", "bases": "MTLDataset"}, {"fullname": "clarena.mtl_datasets.base.MTLDatasetFromRaw.__init__", "modulename": "clarena.mtl_datasets.base", "qualname": "MTLDatasetFromRaw.__init__", "kind": "function", "doc": "<p>Initialize the <code>MTLDatasetFromRaw</code> object.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>datasets</strong> (<code>dict[int, str]</code>): the dict of dataset class paths for each task. The keys are task IDs and the values are the dataset class paths (as strings) to use for each task.</li>\n<li><strong>root</strong> (<code>str</code> | <code>dict[int, str]</code>): the root directory where the original data files for constructing the MTL dataset physically live. If <code>dict[int, str]</code>, it should be a dict of task IDs and their corresponding root directories.</li>\n<li><strong>sampling_strategy</strong> (<code>str</code>): the sampling strategy that construct training batch from each task's dataset. Should be one of the following:\n<ul>\n<li>'mixed': mixed sampling strategy, which samples from all tasks' datasets.</li>\n</ul></li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize. If it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">datasets</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">sampling_strategy</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;mixed&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.mtl_datasets.base.MTLDatasetFromRaw.original_dataset_python_classes", "modulename": "clarena.mtl_datasets.base", "qualname": "MTLDatasetFromRaw.original_dataset_python_classes", "kind": "variable", "doc": "<p>The dict of dataset classes for each task.</p>\n", "annotation": ": dict[int, torch.utils.data.dataset.Dataset]"}, {"fullname": "clarena.mtl_datasets.base.MTLDatasetFromRaw.original_dataset_python_class_t", "modulename": "clarena.mtl_datasets.base", "qualname": "MTLDatasetFromRaw.original_dataset_python_class_t", "kind": "variable", "doc": "<p>The dataset class for the current task <code>self.task_id</code>.</p>\n", "annotation": ": torch.utils.data.dataset.Dataset"}, {"fullname": "clarena.mtl_datasets.base.MTLDatasetFromRaw.original_dataset_constants_t", "modulename": "clarena.mtl_datasets.base", "qualname": "MTLDatasetFromRaw.original_dataset_constants_t", "kind": "variable", "doc": "<p>The original dataset constants class for the current task <code>self.task_id</code>.</p>\n", "annotation": ": type[clarena.stl_datasets.raw.constants.DatasetConstants]"}, {"fullname": "clarena.mtl_datasets.base.MTLDatasetFromRaw.get_class_map", "modulename": "clarena.mtl_datasets.base", "qualname": "MTLDatasetFromRaw.get_class_map", "kind": "function", "doc": "<p>Get the mapping of classes of task <code>task_id</code> to fit continual learning settings <code>self.cl_paradigm</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the task ID to query the CL class map.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>cl_class_map</strong> (<code>dict[str | int, int]</code>): the CL class map of the task. Key is the original class label, value is the integer class label for continual learning.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_datasets.base.MTLDatasetFromRaw.setup_tasks", "modulename": "clarena.mtl_datasets.base", "qualname": "MTLDatasetFromRaw.setup_tasks", "kind": "function", "doc": "<p>Set up the tasks for the dataset.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>train_tasks</strong> (<code>list[int]</code>): the list of task IDs to be trained. It should be a list of integers, each integer is the task ID. This is used when constructing the dataloader.</li>\n<li><strong>eval_tasks</strong> (<code>list[int]</code>): the list of task IDs to be evaluated. It should be a list of integers, each integer is the task ID. This is used when constructing the dataloader.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">train_tasks</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">eval_tasks</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_datasets.base.MTLDatasetFromCL", "modulename": "clarena.mtl_datasets.base", "qualname": "MTLDatasetFromCL", "kind": "class", "doc": "<p>Multi-task learning datasets constructed from the CL datasets. Inherits from <code>MTLDataset</code>.</p>\n\n<p>This is usually for constructing the reference joint learning experiment for continual learning.</p>\n", "bases": "MTLDataset"}, {"fullname": "clarena.mtl_datasets.base.MTLDatasetFromCL.__init__", "modulename": "clarena.mtl_datasets.base", "qualname": "MTLDatasetFromCL.__init__", "kind": "function", "doc": "<p>Initialize the <code>MTLDatasetFromCL</code> object.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>cl_dataset</strong> (<code>CLDataset</code>): the CL dataset object to be used for constructing the MTL dataset.</li>\n<li><strong>sampling_strategy</strong> (<code>str</code>): the sampling strategy that construct training batch from each task's dataset. Should be one of the following:\n<ul>\n<li>'mixed': mixed sampling strategy, which samples from all tasks' datasets.</li>\n</ul></li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">cl_dataset</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_datasets</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLDataset</span>,</span><span class=\"param\">\t<span class=\"n\">sampling_strategy</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;mixed&#39;</span></span>)</span>"}, {"fullname": "clarena.mtl_datasets.base.MTLDatasetFromCL.cl_dataset", "modulename": "clarena.mtl_datasets.base", "qualname": "MTLDatasetFromCL.cl_dataset", "kind": "variable", "doc": "<p>The CL dataset for constructing the MTL dataset.</p>\n", "annotation": ": clarena.cl_datasets.base.CLDataset"}, {"fullname": "clarena.mtl_datasets.base.MTLDatasetFromCL.prepare_data", "modulename": "clarena.mtl_datasets.base", "qualname": "MTLDatasetFromCL.prepare_data", "kind": "function", "doc": "<p>Download and prepare data.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_datasets.base.MTLDatasetFromCL.setup", "modulename": "clarena.mtl_datasets.base", "qualname": "MTLDatasetFromCL.setup", "kind": "function", "doc": "<p>Set up the dataset for different stages.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>stage</strong> (<code>str</code>): the stage of the experiment. Should be one of the following:\n<ul>\n<li>'fit': training and validation dataset should be assigned to <code>self.dataset_train</code> and <code>self.dataset_val</code>.</li>\n<li>'test': test dataset should be assigned to <code>self.dataset_test</code>.</li>\n</ul></li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_datasets.base.MTLDatasetFromCL.get_class_map", "modulename": "clarena.mtl_datasets.base", "qualname": "MTLDatasetFromCL.get_class_map", "kind": "function", "doc": "<p>Get the mapping of classes of task <code>task_id</code>. It is inherited from <code>CLDataset</code> and returns the class map of the task.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): The task ID to query class map.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>class_map</strong>(<code>dict[str | int, int]</code>): the class map of the task. Key is original class label, value is integer class label for multi-task learning. The mapped class labels of each task should be continuous integers from 0 to the number of classes.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_datasets.base.TaskLabelledDataset", "modulename": "clarena.mtl_datasets.base", "qualname": "TaskLabelledDataset", "kind": "class", "doc": "<p>The dataset class that labels the a task's dataset with the given task ID. It is used to label the dataset with the task ID for MTL experiment.</p>\n", "bases": "typing.Generic[+_T_co]"}, {"fullname": "clarena.mtl_datasets.base.TaskLabelledDataset.__init__", "modulename": "clarena.mtl_datasets.base", "qualname": "TaskLabelledDataset.__init__", "kind": "function", "doc": "<p>Initialize the task labelled dataset object.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>dataset</strong> (<code>Dataset</code>): the dataset to be labelled.</li>\n<li><strong>task_id</strong> (<code>int</code>): the task ID to be labelled.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">dataset</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span>)</span>"}, {"fullname": "clarena.mtl_datasets.base.TaskLabelledDataset.dataset", "modulename": "clarena.mtl_datasets.base", "qualname": "TaskLabelledDataset.dataset", "kind": "variable", "doc": "<p>Store the dataset object.</p>\n", "annotation": ": torch.utils.data.dataset.Dataset"}, {"fullname": "clarena.mtl_datasets.base.TaskLabelledDataset.task_id", "modulename": "clarena.mtl_datasets.base", "qualname": "TaskLabelledDataset.task_id", "kind": "variable", "doc": "<p>Store the task ID.</p>\n", "annotation": ": int"}, {"fullname": "clarena.mtl_datasets.base.label_dataset_task", "modulename": "clarena.mtl_datasets.base", "qualname": "label_dataset_task", "kind": "function", "doc": "<p>Label the dataset with the given task ID by wrapping it with a dataset that returns (x, y, task_id) tuples.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>dataset</strong> (<code>Dataset</code>): the dataset to be labelled.</li>\n<li><strong>task_id</strong> (<code>int</code>): the task ID to be labelled.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>task_labelled_dataset</strong> (<code>Dataset</code>): the labelled dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">dataset</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>,</span><span class=\"param\">\t<span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_datasets.multi_domain_sentiment", "modulename": "clarena.mtl_datasets.multi_domain_sentiment", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for CelebA dataset used for multi-task learning.</p>\n"}, {"fullname": "clarena.mtl_datasets.multi_domain_sentiment.MultiDomainSentiment", "modulename": "clarena.mtl_datasets.multi_domain_sentiment", "qualname": "MultiDomainSentiment", "kind": "class", "doc": "<p>CelebA dataset used for multi-task learning. The <a href=\"https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\">CelebFaces Attributes Dataset (CelebA)</a> is a large-scale celebrity faces dataset. It consists of 202,599 face images of 10,177 celebrity identities, each 178x218 color image. Each image is annotated with 40 attribute labels. We construct a multi-task learning dataset from CelebA by treating each attribute as a task.</p>\n", "bases": "clarena.mtl_datasets.base.MTLDataset"}, {"fullname": "clarena.mtl_datasets.multi_domain_sentiment.MultiDomainSentiment.__init__", "modulename": "clarena.mtl_datasets.multi_domain_sentiment", "qualname": "MultiDomainSentiment.__init__", "kind": "function", "doc": "<p>Initialize the MultiTaskCelebA dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original CelebA data 'CelebA/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the MTL dataset.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>list[int]</code>): The batch size in train, val, test dataloader. If <code>list[str]</code>, it should be a list of integers, each integer is the batch size for each task.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>list[int]</code>): the number of workers for dataloaders. If <code>list[str]</code>, it should be a list of integers, each integer is the num of workers for each task.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or list of them): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform. <code>ToTensor()</code>, normalize, permute and so on are not included. If it is a list, each item is the custom transforms for each task.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | list of them): the number of channels to repeat for each task. Default is None, which means no repeat. If not None, it should be an integer. If it is a list, each item is the number of channels to repeat for each task.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>list[bool]</code>): whether to include <code>ToTensor()</code> transform. Default is True.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or list of them): the size to resize the images to. Default is None, which means no resize. If not None, it should be a tuple of two integers. If it is a list, each item is the size to resize for each task.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation, should be one of the following:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>list[int]</code> or <code>None</code>): the seeds for permutation operations used to construct tasks. Make sure it has the same number of seeds as <code>num_tasks</code>. Default is None, which creates a list of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.mtl_datasets.multi_domain_sentiment.MultiDomainSentiment.prepare_data", "modulename": "clarena.mtl_datasets.multi_domain_sentiment", "qualname": "MultiDomainSentiment.prepare_data", "kind": "function", "doc": "<p>Download the original CelebA dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_datasets.multi_domain_sentiment.MultiDomainSentiment.train_and_val_dataset", "modulename": "clarena.mtl_datasets.multi_domain_sentiment", "qualname": "MultiDomainSentiment.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_datasets.multi_domain_sentiment.MultiDomainSentiment.test_dataset", "modulename": "clarena.mtl_datasets.multi_domain_sentiment", "qualname": "MultiDomainSentiment.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_algorithms", "modulename": "clarena.stl_algorithms", "kind": "module", "doc": "<h1 id=\"multi-task-learning-algorithms\">Multi-Task Learning Algorithms</h1>\n\n<p>This submodule provides the <strong>multi-task learning algorithms</strong> in CLArena.</p>\n\n<p>The algorithms are implemented as subclasses of <code>MTLAlgorithm</code>.</p>\n\n<p>Please note that this is an API documantation. Please refer to the main documentation pages for more information about the backbone networks and how to configure and implement them:</p>\n\n<ul>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/configure-your-experiment/cl-algorithm\"><strong>Configure MTL Algorithm</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/implement-your-cl-modules/cl-algorithm\"><strong>Implement Your CL Algorithm Class</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/posts/continual-learning-beginners-guide#sec-methodology\"><strong>A Beginners' Guide to Continual Learning (Methodology Overview)</strong></a></li>\n</ul>\n"}, {"fullname": "clarena.stl_algorithms.base", "modulename": "clarena.stl_algorithms.base", "kind": "module", "doc": "<p>The submodule in <code>stl_algorithms</code> for single-task learning algorithm bases.</p>\n"}, {"fullname": "clarena.stl_algorithms.base.STLAlgorithm", "modulename": "clarena.stl_algorithms.base", "qualname": "STLAlgorithm", "kind": "class", "doc": "<p>The class of single-task learning.</p>\n", "bases": "lightning.pytorch.core.module.LightningModule"}, {"fullname": "clarena.stl_algorithms.base.STLAlgorithm.__init__", "modulename": "clarena.stl_algorithms.base", "qualname": "STLAlgorithm.__init__", "kind": "function", "doc": "<p>Initialize the multi-task learning algorithm with the network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone</strong> (<code>Backbone</code>): backbone network.</li>\n<li><strong>head</strong> (<code>HeadsTIL</code> | <code>HeadsCIL</code>): output head.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">Backbone</span>,</span><span class=\"param\">\t<span class=\"n\">head</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_mtl</span><span class=\"o\">.</span><span class=\"n\">HeadsMTL</span></span>)</span>"}, {"fullname": "clarena.stl_algorithms.base.STLAlgorithm.backbone", "modulename": "clarena.stl_algorithms.base", "qualname": "STLAlgorithm.backbone", "kind": "variable", "doc": "<p>Store the backbone network.</p>\n", "annotation": ": clarena.backbones.base.Backbone"}, {"fullname": "clarena.stl_algorithms.base.STLAlgorithm.head", "modulename": "clarena.stl_algorithms.base", "qualname": "STLAlgorithm.head", "kind": "variable", "doc": "<p>Store the output head.</p>\n", "annotation": ": clarena.heads.heads_mtl.HeadsMTL"}, {"fullname": "clarena.stl_algorithms.base.STLAlgorithm.optimizer", "modulename": "clarena.stl_algorithms.base", "qualname": "STLAlgorithm.optimizer", "kind": "variable", "doc": "<p>Store the optimizer object (partially initialized) for the backpropagation of task <code>self.task_id</code>. Will be equipped with parameters in <code>configure_optimizers()</code>.</p>\n", "annotation": ": torch.optim.optimizer.Optimizer"}, {"fullname": "clarena.stl_algorithms.base.STLAlgorithm.lr_scheduler", "modulename": "clarena.stl_algorithms.base", "qualname": "STLAlgorithm.lr_scheduler", "kind": "variable", "doc": "<p>Store the learning rate scheduler for the optimizer. If <code>None</code>, no scheduler is used.</p>\n", "annotation": ": torch.optim.lr_scheduler.LRScheduler | None"}, {"fullname": "clarena.stl_algorithms.base.STLAlgorithm.criterion", "modulename": "clarena.stl_algorithms.base", "qualname": "STLAlgorithm.criterion", "kind": "variable", "doc": "<p>The loss function bewteen the output logits and the target labels. Default is cross-entropy loss.</p>\n"}, {"fullname": "clarena.stl_algorithms.base.STLAlgorithm.if_forward_func_return_logits_only", "modulename": "clarena.stl_algorithms.base", "qualname": "STLAlgorithm.if_forward_func_return_logits_only", "kind": "variable", "doc": "<p>Whether the <code>forward()</code> method returns logits only. If <code>False</code>, it returns a dictionary containing logits and other information. Default is <code>False</code>.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.stl_algorithms.base.STLAlgorithm.sanity_check", "modulename": "clarena.stl_algorithms.base", "qualname": "STLAlgorithm.sanity_check", "kind": "function", "doc": "<p>Check the sanity of the arguments.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_algorithms.base.STLAlgorithm.setup_task", "modulename": "clarena.stl_algorithms.base", "qualname": "STLAlgorithm.setup_task", "kind": "function", "doc": "<p>Setup the components for the STL algorithm. This must be done before <code>forward()</code> method is called.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>optimizer</strong> (<code>Optimizer</code>): the optimizer object (partially initialized).</li>\n<li><strong>lr_scheduler</strong> (<code>LRScheduler</code> | None): the learning rate scheduler for the optimizer. If <code>None</code>, no scheduler is used.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">optimizer</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">Optimizer</span>,</span><span class=\"param\">\t<span class=\"n\">lr_scheduler</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">lr_scheduler</span><span class=\"o\">.</span><span class=\"n\">LRScheduler</span> <span class=\"o\">|</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_algorithms.base.STLAlgorithm.forward", "modulename": "clarena.stl_algorithms.base", "qualname": "STLAlgorithm.forward", "kind": "function", "doc": "<p>The forward pass for data from task <code>task_id</code>. Note that it is nothing to do with <code>forward()</code> method in <code>nn.Module</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): The input tensor from data.</li>\n<li><strong>stage</strong> (<code>str</code>): the stage of the forward pass, should be one of the following:\n<ol>\n<li>'train': training stage.</li>\n<li>'validation': validation stage.</li>\n<li>'test': testing stage.</li>\n</ol></li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>logits</strong> (<code>Tensor</code>): the output logits tensor.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_algorithms.base.STLAlgorithm.configure_optimizers", "modulename": "clarena.stl_algorithms.base", "qualname": "STLAlgorithm.configure_optimizers", "kind": "function", "doc": "<p>Configure optimizer hooks by Lightning.\nSee <a href=\"https://lightning.ai/docs/pytorch/stable/common/lightning_module.html#configure-optimizers\">Lightning docs</a> for more details.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">Optimizer</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_algorithms.single_learning", "modulename": "clarena.stl_algorithms.single_learning", "kind": "module", "doc": "<p>The submodule in <code>stl_algorithms</code> for single learning algorithm.</p>\n"}, {"fullname": "clarena.stl_algorithms.single_learning.SingleLearning", "modulename": "clarena.stl_algorithms.single_learning", "qualname": "SingleLearning", "kind": "class", "doc": "<p>Single learning algorithm.</p>\n\n<p>The most naive way for single-task learning. It directly trains the task.</p>\n", "bases": "clarena.stl_algorithms.base.STLAlgorithm"}, {"fullname": "clarena.stl_algorithms.single_learning.SingleLearning.__init__", "modulename": "clarena.stl_algorithms.single_learning", "qualname": "SingleLearning.__init__", "kind": "function", "doc": "<p>Initialize the SingleLearning algorithm with the network. It has no additional hyperparameters.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone</strong> (<code>CLBackbone</code>): backbone network.</li>\n<li><strong>head</strong> (<code>HeadsMTL</code>): output head.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">Backbone</span>,</span><span class=\"param\">\t<span class=\"n\">head</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">head_stl</span><span class=\"o\">.</span><span class=\"n\">HeadSTL</span></span>)</span>"}, {"fullname": "clarena.stl_algorithms.single_learning.SingleLearning.training_step", "modulename": "clarena.stl_algorithms.single_learning", "qualname": "SingleLearning.training_step", "kind": "function", "doc": "<p>Training step for single learning.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of training data.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and accuracy from this training step. Keys (<code>str</code>) are the metrics names, and values (<code>Tensor</code>) are the metrics. Must include the key 'loss' which is total loss in the case of automatic optimization, according to PyTorch Lightning docs.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_algorithms.single_learning.SingleLearning.validation_step", "modulename": "clarena.stl_algorithms.single_learning", "qualname": "SingleLearning.validation_step", "kind": "function", "doc": "<p>Validation step for single learning.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of validation data.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and accuracy from this validation step. Keys (<code>str</code>) are the metrics names, and values (<code>Tensor</code>) are the metrics.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_algorithms.single_learning.SingleLearning.test_step", "modulename": "clarena.stl_algorithms.single_learning", "qualname": "SingleLearning.test_step", "kind": "function", "doc": "<p>Test step for single learning.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of test data.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and accuracy from this test step. Keys (<code>str</code>) are the metrics names, and values (<code>Tensor</code>) are the metrics.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets", "modulename": "clarena.stl_datasets", "kind": "module", "doc": "<h1 id=\"single-task-learning-datasets\">Single-Task Learning Datasets</h1>\n\n<p>This submodule provides the <strong>single-task learning datasets</strong> that can be used in CLArena.</p>\n\n<p>The datasets are implemented as subclasses of <code>STLDataset</code> classes, which are the base class for all single-task learning datasets in CLArena.</p>\n\n<p>Please note that this is an API documantation. Please refer to the main documentation pages for more information about the STL datasets and how to configure and implement them:</p>\n\n<ul>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/single-task-learning/configure-main-experiment/STL-dataset.qmd\"><strong>Configure STL Dataset</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-lear ning-arena/docs/implement-your-cl-modules/cl-dataset\"><strong>Implement Your CL Dataset Class</strong></a></li>\n</ul>\n"}, {"fullname": "clarena.stl_datasets.base", "modulename": "clarena.stl_datasets.base", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for STL dataset bases.</p>\n"}, {"fullname": "clarena.stl_datasets.base.STLDataset", "modulename": "clarena.stl_datasets.base", "qualname": "STLDataset", "kind": "class", "doc": "<p>The base class of single-task learning datasets, inherited from <code>LightningDataModule</code>.</p>\n", "bases": "lightning.pytorch.core.datamodule.LightningDataModule"}, {"fullname": "clarena.stl_datasets.base.STLDataset.__init__", "modulename": "clarena.stl_datasets.base", "qualname": "STLDataset.__init__", "kind": "function", "doc": "<p>Initialize the STL dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original data files for constructing the STL dataset physically live.</li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform. <code>ToTensor()</code>, normalize, permute and so on are not included.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code>): the number of channels to repeat for each task. Default is None, which means no repeat. If not None, it should be an integer.</li>\n<li><strong>to_tensor</strong> (<code>bool</code>): whether to include <code>ToTensor()</code> transform. Default is True.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or list of them): the size to resize the images to. Default is None, which means no resize. If not None, it should be a tuple of two integers.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.base.STLDataset.root", "modulename": "clarena.stl_datasets.base", "qualname": "STLDataset.root", "kind": "variable", "doc": "<p>The root directory of the original data files.</p>\n", "annotation": ": str"}, {"fullname": "clarena.stl_datasets.base.STLDataset.batch_size", "modulename": "clarena.stl_datasets.base", "qualname": "STLDataset.batch_size", "kind": "variable", "doc": "<p>The batch size for dataloaders.</p>\n", "annotation": ": int"}, {"fullname": "clarena.stl_datasets.base.STLDataset.num_workers", "modulename": "clarena.stl_datasets.base", "qualname": "STLDataset.num_workers", "kind": "variable", "doc": "<p>The number of workers for dataloaders.</p>\n", "annotation": ": int"}, {"fullname": "clarena.stl_datasets.base.STLDataset.custom_transforms", "modulename": "clarena.stl_datasets.base", "qualname": "STLDataset.custom_transforms", "kind": "variable", "doc": "<p>The custom transforms.</p>\n", "annotation": ": Union[Callable, torchvision.transforms.transforms.Compose]"}, {"fullname": "clarena.stl_datasets.base.STLDataset.repeat_channels", "modulename": "clarena.stl_datasets.base", "qualname": "STLDataset.repeat_channels", "kind": "variable", "doc": "<p>The number of channels to repeat.</p>\n", "annotation": ": int | None"}, {"fullname": "clarena.stl_datasets.base.STLDataset.to_tensor", "modulename": "clarena.stl_datasets.base", "qualname": "STLDataset.to_tensor", "kind": "variable", "doc": "<p>The to_tensor flag.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.stl_datasets.base.STLDataset.resize", "modulename": "clarena.stl_datasets.base", "qualname": "STLDataset.resize", "kind": "variable", "doc": "<p>The size to resize.</p>\n", "annotation": ": tuple[int, int] | None"}, {"fullname": "clarena.stl_datasets.base.STLDataset.num_classes", "modulename": "clarena.stl_datasets.base", "qualname": "STLDataset.num_classes", "kind": "variable", "doc": "<p>The number of classes in each task.</p>\n", "annotation": ": int"}, {"fullname": "clarena.stl_datasets.base.STLDataset.class_map", "modulename": "clarena.stl_datasets.base", "qualname": "STLDataset.class_map", "kind": "variable", "doc": "<p>The class map for the current task <code>self.task_id</code>. The key is the integer class label, and the value is the original class label. It is used to get the original class label from the integer class label.</p>\n", "annotation": ": dict[int, str | int]"}, {"fullname": "clarena.stl_datasets.base.STLDataset.dataset_train", "modulename": "clarena.stl_datasets.base", "qualname": "STLDataset.dataset_train", "kind": "variable", "doc": "<p>Training dataset object. Can be PyTorch Dataset objects or any other dataset objects.</p>\n", "annotation": ": Any"}, {"fullname": "clarena.stl_datasets.base.STLDataset.dataset_val", "modulename": "clarena.stl_datasets.base", "qualname": "STLDataset.dataset_val", "kind": "variable", "doc": "<p>Validation dataset object. Can be PyTorch Dataset objects or any other dataset objects.</p>\n", "annotation": ": Any"}, {"fullname": "clarena.stl_datasets.base.STLDataset.dataset_test", "modulename": "clarena.stl_datasets.base", "qualname": "STLDataset.dataset_test", "kind": "variable", "doc": "<p>Test dataset object. Can be PyTorch Dataset objects or any other dataset objects.</p>\n", "annotation": ": Any"}, {"fullname": "clarena.stl_datasets.base.STLDataset.mean", "modulename": "clarena.stl_datasets.base", "qualname": "STLDataset.mean", "kind": "variable", "doc": "<p>Mean value for normalization. Used when constructing the transforms.</p>\n", "annotation": ": float"}, {"fullname": "clarena.stl_datasets.base.STLDataset.std", "modulename": "clarena.stl_datasets.base", "qualname": "STLDataset.std", "kind": "variable", "doc": "<p>Standard deviation value for normalization. Used when constructing the transforms.</p>\n", "annotation": ": float"}, {"fullname": "clarena.stl_datasets.base.STLDataset.sanity_check", "modulename": "clarena.stl_datasets.base", "qualname": "STLDataset.sanity_check", "kind": "function", "doc": "<p>Check the sanity of the arguments.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.base.STLDataset.get_class_map", "modulename": "clarena.stl_datasets.base", "qualname": "STLDataset.get_class_map", "kind": "function", "doc": "<p>Get the mapping of classes of task <code>task_id</code>. It must be implemented by subclasses.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>class_map</strong>(<code>dict[str | int, int]</code>): the class map of the task. Key is original class label, value is integer class label for single-task learning.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.base.STLDataset.prepare_data", "modulename": "clarena.stl_datasets.base", "qualname": "STLDataset.prepare_data", "kind": "function", "doc": "<p>Use this to download and prepare data. It must be implemented by subclasses, regulated by <code>LightningDatamodule</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.base.STLDataset.setup", "modulename": "clarena.stl_datasets.base", "qualname": "STLDataset.setup", "kind": "function", "doc": "<p>Set up the dataset for different stages.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>stage</strong> (<code>str</code>): the stage of the experiment. Should be one of the following:\n<ul>\n<li>'fit': training and validation dataset should be assigned to <code>self.dataset_train</code> and <code>self.dataset_val</code>.</li>\n<li>'test': test dataset should be assigned to <code>self.dataset_test</code>.</li>\n</ul></li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.base.STLDataset.setup_task", "modulename": "clarena.stl_datasets.base", "qualname": "STLDataset.setup_task", "kind": "function", "doc": "<p>Set up the tasks for the dataset.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>train_tasks</strong> (<code>list[int]</code>): the list of task IDs to be trained. It should be a list of integers, each integer is the task ID. This is used when constructing the dataloader.</li>\n<li><strong>eval_tasks</strong> (<code>list[int]</code>): the list of task IDs to be evaluated. It should be a list of integers, each integer is the task ID. This is used when constructing the dataloader.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.base.STLDataset.train_and_val_transforms", "modulename": "clarena.stl_datasets.base", "qualname": "STLDataset.train_and_val_transforms", "kind": "function", "doc": "<p>Transforms generator for train and validation dataset incorporating the custom transforms with basic transforms like <code>normalization</code> and <code>ToTensor()</code>. It is a handy tool to use in subclasses when constructing the dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_transforms</strong> (<code>transforms.Compose</code>): the composed training transforms.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.base.STLDataset.test_transforms", "modulename": "clarena.stl_datasets.base", "qualname": "STLDataset.test_transforms", "kind": "function", "doc": "<p>Transforms generator for test dataset. Only basic transforms like <code>normalization</code> and <code>ToTensor()</code> are included. It is a handy tool to use in subclasses when constructing the dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_transforms</strong> (<code>transforms.Compose</code>): the composed training transforms.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.base.STLDataset.train_and_val_dataset", "modulename": "clarena.stl_datasets.base", "qualname": "STLDataset.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset. It must be implemented by subclasses.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>Any</code>): the train and validation dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Any</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.base.STLDataset.test_dataset", "modulename": "clarena.stl_datasets.base", "qualname": "STLDataset.test_dataset", "kind": "function", "doc": "<p>Get the test dataset. It must be implemented by subclasses.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the task ID to get the test dataset.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Any</code>): the test dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Any</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.base.STLDataset.train_dataloader", "modulename": "clarena.stl_datasets.base", "qualname": "STLDataset.train_dataloader", "kind": "function", "doc": "<p>DataLoader generator for stage train. It is automatically called before training.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_dataloader</strong> (<code>Dataloader</code>): the train DataLoader.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.base.STLDataset.val_dataloader", "modulename": "clarena.stl_datasets.base", "qualname": "STLDataset.val_dataloader", "kind": "function", "doc": "<p>DataLoader generator for the validation stage. It is automatically called before validation.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>val_dataloader</strong> (<code>DataLoader</code>): the validation DataLoader.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.base.STLDataset.test_dataloader", "modulename": "clarena.stl_datasets.base", "qualname": "STLDataset.test_dataloader", "kind": "function", "doc": "<p>DataLoader generator for stage test. It is automatically called before testing.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataloader</strong> (<code>Dataloader</code>): the test DataLoader.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.base.STLDatasetFromRaw", "modulename": "clarena.stl_datasets.base", "qualname": "STLDatasetFromRaw", "kind": "class", "doc": "<p>The base class of single-task learning datasets from raw data, inherited from <code>STLDataset</code>.</p>\n\n<p>It is used to construct the STL dataset from raw data files.</p>\n", "bases": "STLDataset"}, {"fullname": "clarena.stl_datasets.base.STLDatasetFromRaw.__init__", "modulename": "clarena.stl_datasets.base", "qualname": "STLDatasetFromRaw.__init__", "kind": "function", "doc": "<p>Initialize the STL dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original data files for constructing the STL dataset physically live.</li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform. <code>ToTensor()</code>, normalize, permute and so on are not included.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code>): the number of channels to repeat for each task. Default is None, which means no repeat. If not None, it should be an integer.</li>\n<li><strong>to_tensor</strong> (<code>bool</code>): whether to include <code>ToTensor()</code> transform. Default is True.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or list of them): the size to resize the images to. Default is None, which means no resize. If not None, it should be a tuple of two integers.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.base.STLDatasetFromRaw.original_dataset_python_class", "modulename": "clarena.stl_datasets.base", "qualname": "STLDatasetFromRaw.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class. It must be provided in subclasses.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]"}, {"fullname": "clarena.stl_datasets.base.STLDatasetFromRaw.original_dataset_constants", "modulename": "clarena.stl_datasets.base", "qualname": "STLDatasetFromRaw.original_dataset_constants", "kind": "variable", "doc": "<p>The original dataset constants class.</p>\n", "annotation": ": type[clarena.stl_datasets.raw.constants.DatasetConstants]"}, {"fullname": "clarena.stl_datasets.base.STLDatasetFromRaw.get_class_map", "modulename": "clarena.stl_datasets.base", "qualname": "STLDatasetFromRaw.get_class_map", "kind": "function", "doc": "<p>Get the mapping of classes of task <code>task_id</code>. It must be implemented by subclasses.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>class_map</strong>(<code>dict[str | int, int]</code>): the class map of the task. Key is original class label, value is integer class label for single-task learning.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.base.STLDatasetFromRaw.setup_task", "modulename": "clarena.stl_datasets.base", "qualname": "STLDatasetFromRaw.setup_task", "kind": "function", "doc": "<p>Set up the tasks for the dataset.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>train_tasks</strong> (<code>list[int]</code>): the list of task IDs to be trained. It should be a list of integers, each integer is the task ID. This is used when constructing the dataloader.</li>\n<li><strong>eval_tasks</strong> (<code>list[int]</code>): the list of task IDs to be evaluated. It should be a list of integers, each integer is the task ID. This is used when constructing the dataloader.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.mnist", "modulename": "clarena.stl_datasets.mnist", "kind": "module", "doc": "<p>The submodule in <code>stl_datasets</code> for MNIST dataset.</p>\n"}, {"fullname": "clarena.stl_datasets.mnist.STLMNIST", "modulename": "clarena.stl_datasets.mnist", "qualname": "STLMNIST", "kind": "class", "doc": "<p>MNIST dataset. The <a href=\"http://yann.lecun.com/exdb/mnist/\">MNIST dataset</a> is a collection of handwritten digits. It consists of 60,000 training and 10,000 test images of handwritten digit images (10 classes), each 28x28 grayscale image.</p>\n", "bases": "clarena.stl_datasets.base.STLDatasetFromRaw"}, {"fullname": "clarena.stl_datasets.mnist.STLMNIST.__init__", "modulename": "clarena.stl_datasets.mnist", "qualname": "STLMNIST.__init__", "kind": "function", "doc": "<p>Initialize the dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original MNIST data 'MNIST/' live.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform. <code>ToTensor()</code>, normalize, permute and so on are not included.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code>): the number of channels to repeat for each task. Default is None, which means no repeat. If not None, it should be an integer.</li>\n<li><strong>to_tensor</strong> (<code>bool</code>): whether to include <code>ToTensor()</code> transform. Default is True.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or list of them): the size to resize the images to. Default is None, which means no resize. If not None, it should be a tuple of two integers.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.mnist.STLMNIST.original_dataset_python_class", "modulename": "clarena.stl_datasets.mnist", "qualname": "STLMNIST.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.mnist.MNIST&#x27;&gt;"}, {"fullname": "clarena.stl_datasets.mnist.STLMNIST.validation_percentage", "modulename": "clarena.stl_datasets.mnist", "qualname": "STLMNIST.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.stl_datasets.mnist.STLMNIST.prepare_data", "modulename": "clarena.stl_datasets.mnist", "qualname": "STLMNIST.prepare_data", "kind": "function", "doc": "<p>Download the original MNIST dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.mnist.STLMNIST.train_and_val_dataset", "modulename": "clarena.stl_datasets.mnist", "qualname": "STLMNIST.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.mnist.STLMNIST.test_dataset", "modulename": "clarena.stl_datasets.mnist", "qualname": "STLMNIST.test_dataset", "kind": "function", "doc": "<p>Get the test dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.raw", "modulename": "clarena.stl_datasets.raw", "kind": "module", "doc": "<h1 id=\"original-datasets\">Original Datasets</h1>\n\n<p>This submodule provides the <strong>original datasets</strong> that are used for constructing continual learning datasets.</p>\n\n<p>The datasets are implemented as subclasses of PyTorch <code>Dataset</code> classes.</p>\n"}, {"fullname": "clarena.stl_datasets.raw.ahdd", "modulename": "clarena.stl_datasets.raw.ahdd", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets.original</code> for the original Arabic Handwritten Digits dataset.</p>\n"}, {"fullname": "clarena.stl_datasets.raw.ahdd.ArabicHandwrittenDigits", "modulename": "clarena.stl_datasets.raw.ahdd", "qualname": "ArabicHandwrittenDigits", "kind": "class", "doc": "<p>Arabic Handwritten Digits Dataset (CSV version). The <a href=\"https://www.kaggle.com/datasets/mloey1/ahdd1\">Arabic Handwritten Digits dataset</a> is a collection of handwritten Arabic digits (0-9). The dataset contains 60,000 training and 10,000 testing samples of handwritten Arabic digits (0-9), each 28x28 grayscale image.</p>\n", "bases": "typing.Generic[+_T_co]"}, {"fullname": "clarena.stl_datasets.raw.ahdd.ArabicHandwrittenDigits.__init__", "modulename": "clarena.stl_datasets.raw.ahdd", "qualname": "ArabicHandwrittenDigits.__init__", "kind": "function", "doc": "<p>Initialize the Arabic Handwritten Digits CSV dataset.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code> | <code>Path</code>): Root directory containing extracted CSV files.</li>\n<li><strong>train</strong> (<code>bool</code>): If True, use training set; else use test set.</li>\n<li><strong>transform</strong> (<code>callable</code> | <code>None</code>): Image transformation function.</li>\n<li><strong>target_transform</strong> (<code>callable</code> | <code>None</code>): Target transformation function.</li>\n<li><strong>download</strong> (<code>bool</code>): Not implemented. Must place files manually.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"n\">pathlib</span><span class=\"o\">.</span><span class=\"n\">Path</span>,</span><span class=\"param\">\t<span class=\"n\">train</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">target_transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.raw.ahdd.ArabicHandwrittenDigits.base_folder", "modulename": "clarena.stl_datasets.raw.ahdd", "qualname": "ArabicHandwrittenDigits.base_folder", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;Arabic Handwritten Digits Dataset CSV&#x27;"}, {"fullname": "clarena.stl_datasets.raw.ahdd.ArabicHandwrittenDigits.train_images_file", "modulename": "clarena.stl_datasets.raw.ahdd", "qualname": "ArabicHandwrittenDigits.train_images_file", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;csvTrainImages 60k x 784.csv&#x27;"}, {"fullname": "clarena.stl_datasets.raw.ahdd.ArabicHandwrittenDigits.train_labels_file", "modulename": "clarena.stl_datasets.raw.ahdd", "qualname": "ArabicHandwrittenDigits.train_labels_file", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;csvTrainLabel 60k x 1.csv&#x27;"}, {"fullname": "clarena.stl_datasets.raw.ahdd.ArabicHandwrittenDigits.test_images_file", "modulename": "clarena.stl_datasets.raw.ahdd", "qualname": "ArabicHandwrittenDigits.test_images_file", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;csvTestImages 10k x 784.csv&#x27;"}, {"fullname": "clarena.stl_datasets.raw.ahdd.ArabicHandwrittenDigits.test_labels_file", "modulename": "clarena.stl_datasets.raw.ahdd", "qualname": "ArabicHandwrittenDigits.test_labels_file", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;csvTestLabel 10k x 1.csv&#x27;"}, {"fullname": "clarena.stl_datasets.raw.ahdd.ArabicHandwrittenDigits.train", "modulename": "clarena.stl_datasets.raw.ahdd", "qualname": "ArabicHandwrittenDigits.train", "kind": "variable", "doc": "<p>Flag indicating whether to use training or testing dataset.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.stl_datasets.raw.ahdd.ArabicHandwrittenDigits.images", "modulename": "clarena.stl_datasets.raw.ahdd", "qualname": "ArabicHandwrittenDigits.images", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "clarena.stl_datasets.raw.ahdd.ArabicHandwrittenDigits.labels", "modulename": "clarena.stl_datasets.raw.ahdd", "qualname": "ArabicHandwrittenDigits.labels", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "clarena.stl_datasets.raw.ahdd.ArabicHandwrittenDigits.class_names", "modulename": "clarena.stl_datasets.raw.ahdd", "qualname": "ArabicHandwrittenDigits.class_names", "kind": "variable", "doc": "<p>Digit classes from 0 to 9.</p>\n", "annotation": ": list[str]"}, {"fullname": "clarena.stl_datasets.raw.cub2002011", "modulename": "clarena.stl_datasets.raw.cub2002011", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets.original</code> for the original CUB-200-2011 dataset.</p>\n"}, {"fullname": "clarena.stl_datasets.raw.cub2002011.CUB2002011", "modulename": "clarena.stl_datasets.raw.cub2002011", "qualname": "CUB2002011", "kind": "class", "doc": "<p>CUB-200-2011 dataset. <a href=\"https://www.vision.caltech.edu/datasets/cub_200_2011/\">CUB(Caltech-UCSD Birds)-200-2011)</a> is a bird image dataset. It consists of 120,000 64x64 colour images in 200 classes, with 500 training, 50 validation and 50 test examples per class.</p>\n", "bases": "typing.Generic[+_T_co]"}, {"fullname": "clarena.stl_datasets.raw.cub2002011.CUB2002011.__init__", "modulename": "clarena.stl_datasets.raw.cub2002011", "qualname": "CUB2002011.__init__", "kind": "function", "doc": "<p>Initialize the CUB-200-2011 dataset.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code> | <code>Path</code>): Root directory of the dataset.</li>\n<li><strong>train</strong> (<code>bool</code> | <code>None</code>): If True, creates dataset from training set, otherwise creates from test set.</li>\n<li><strong>transform</strong> (<code>callable</code> | <code>None</code>): A function/transform that  takes in an PIL image and returns a transformed version. E.g, <code>transforms.RandomCrop</code></li>\n<li><strong>target_transform</strong> (<code>callable</code> | <code>None</code>): A function/transform that takes in the target and transforms it.</li>\n<li><strong>download</strong> (<code>bool</code>): If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"n\">pathlib</span><span class=\"o\">.</span><span class=\"n\">Path</span>,</span><span class=\"param\">\t<span class=\"n\">train</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">target_transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.raw.cub2002011.CUB2002011.base_folder", "modulename": "clarena.stl_datasets.raw.cub2002011", "qualname": "CUB2002011.base_folder", "kind": "variable", "doc": "<p>The folder name where the main data (images) are stored.</p>\n", "annotation": ": str", "default_value": "&#x27;CUB_200_2011/images&#x27;"}, {"fullname": "clarena.stl_datasets.raw.cub2002011.CUB2002011.url", "modulename": "clarena.stl_datasets.raw.cub2002011", "qualname": "CUB2002011.url", "kind": "variable", "doc": "<p>The url to download the dataset from.</p>\n", "default_value": "&#x27;http://www.vision.caltech.edu/visipedia-data/CUB-200-2011/CUB_200_2011.tgz&#x27;"}, {"fullname": "clarena.stl_datasets.raw.cub2002011.CUB2002011.file_id", "modulename": "clarena.stl_datasets.raw.cub2002011", "qualname": "CUB2002011.file_id", "kind": "variable", "doc": "<p>The file id to download the dataset from Google Drive.</p>\n", "default_value": "&#x27;1hbzc_P1FuxMkcabkgn9ZKinBwW683j45&#x27;"}, {"fullname": "clarena.stl_datasets.raw.cub2002011.CUB2002011.filename", "modulename": "clarena.stl_datasets.raw.cub2002011", "qualname": "CUB2002011.filename", "kind": "variable", "doc": "<p>The filename of the dataset.</p>\n", "default_value": "&#x27;CUB_200_2011.tgz&#x27;"}, {"fullname": "clarena.stl_datasets.raw.cub2002011.CUB2002011.tgz_md5", "modulename": "clarena.stl_datasets.raw.cub2002011", "qualname": "CUB2002011.tgz_md5", "kind": "variable", "doc": "<p>The md5 hash of the dataset tar file.</p>\n", "default_value": "&#x27;97eceeb196236b17998738112f37df78&#x27;"}, {"fullname": "clarena.stl_datasets.raw.cub2002011.CUB2002011.data", "modulename": "clarena.stl_datasets.raw.cub2002011", "qualname": "CUB2002011.data", "kind": "variable", "doc": "<p>Store the metadata of the dataset. The metadata includes the image file paths and the class labels.</p>\n", "annotation": ": pandas.core.frame.DataFrame"}, {"fullname": "clarena.stl_datasets.raw.cub2002011.CUB2002011.class_names", "modulename": "clarena.stl_datasets.raw.cub2002011", "qualname": "CUB2002011.class_names", "kind": "variable", "doc": "<p>Store the class names of the dataset.</p>\n", "annotation": ": list[str]"}, {"fullname": "clarena.stl_datasets.raw.cub2002011.CUB2002011.loader", "modulename": "clarena.stl_datasets.raw.cub2002011", "qualname": "CUB2002011.loader", "kind": "variable", "doc": "<p>Store the loader function to load the images.</p>\n", "annotation": ": str"}, {"fullname": "clarena.stl_datasets.raw.cub2002011.CUB2002011.train", "modulename": "clarena.stl_datasets.raw.cub2002011", "qualname": "CUB2002011.train", "kind": "variable", "doc": "<p>Store the flag to indicate whether to load training or test dataset.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.stl_datasets.raw.cub2002011.CUB2002011.download", "modulename": "clarena.stl_datasets.raw.cub2002011", "qualname": "CUB2002011.download", "kind": "function", "doc": "<p>Download the MNIST data if it doesn't exist already.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.raw.emnist", "modulename": "clarena.stl_datasets.raw.emnist", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets.original</code> for the original EMNIST dataset.</p>\n"}, {"fullname": "clarena.stl_datasets.raw.emnist.EMNISTByClass", "modulename": "clarena.stl_datasets.raw.emnist", "qualname": "EMNISTByClass", "kind": "class", "doc": "<p>EMNIST ByClass dataset. The <a href=\"https://www.nist.gov/itl/products-and-services/emnist-dataset/\">original EMNIST dataset</a> is a collection of handwritten letters and digits (including A-Z, a-z, 0-9). The ByClass split consists of 814,255 28x28 grayscale images in 62 unbalanced classes.</p>\n", "bases": "typing.Generic[+_T_co]"}, {"fullname": "clarena.stl_datasets.raw.emnist.EMNISTByClass.__init__", "modulename": "clarena.stl_datasets.raw.emnist", "qualname": "EMNISTByClass.__init__", "kind": "function", "doc": "<p>Initialize the EMNIST ByClass dataset.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): Root directory of the dataset where <code>EMNIST/raw/train-images-idx3-ubyte</code> and <code>EMNIST/raw/t10k-images-idx3-ubyte</code> exist.</li>\n<li><strong>train</strong> (<code>bool</code> | <code>None</code>): If True, creates dataset from training set, otherwise creates from test set.</li>\n<li><strong>transform</strong> (<code>callable</code> | <code>None</code>): A function/transform that  takes in an PIL image and returns a transformed version. E.g, <code>transforms.RandomCrop</code></li>\n<li><strong>target_transform</strong> (<code>callable</code> | <code>None</code>): A function/transform that takes in the target and transforms it.</li>\n<li><strong>download</strong> (<code>bool</code>): If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"n\">pathlib</span><span class=\"o\">.</span><span class=\"n\">Path</span>,</span><span class=\"param\">\t<span class=\"n\">train</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">target_transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.raw.emnist.EMNISTByMerge", "modulename": "clarena.stl_datasets.raw.emnist", "qualname": "EMNISTByMerge", "kind": "class", "doc": "<p>EMNIST ByMerge dataset. The <a href=\"https://www.nist.gov/itl/products-and-services/emnist-dataset/\">original EMNIST dataset</a> is a collection of handwritten letters and digits (including A-Z, a-z, 0-9). The ByMerge split consists of 814,255 28x28 grayscale images in 47 unbalanced classes.</p>\n", "bases": "typing.Generic[+_T_co]"}, {"fullname": "clarena.stl_datasets.raw.emnist.EMNISTByMerge.__init__", "modulename": "clarena.stl_datasets.raw.emnist", "qualname": "EMNISTByMerge.__init__", "kind": "function", "doc": "<p>Initialize the EMNIST ByMerge dataset.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): Root directory of the dataset where <code>EMNIST/raw/train-images-idx3-ubyte</code> and <code>EMNIST/raw/t10k-images-idx3-ubyte</code> exist.</li>\n<li><strong>train</strong> (<code>bool</code> | <code>None</code>): If True, creates dataset from training set, otherwise creates from test set.</li>\n<li><strong>transform</strong> (<code>callable</code> | <code>None</code>): A function/transform that  takes in an PIL image and returns a transformed version. E.g, <code>transforms.RandomCrop</code></li>\n<li><strong>target_transform</strong> (<code>callable</code> | <code>None</code>): A function/transform that takes in the target and transforms it.</li>\n<li><strong>download</strong> (<code>bool</code>): If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"n\">pathlib</span><span class=\"o\">.</span><span class=\"n\">Path</span>,</span><span class=\"param\">\t<span class=\"n\">train</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">target_transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.raw.emnist.EMNISTBalanced", "modulename": "clarena.stl_datasets.raw.emnist", "qualname": "EMNISTBalanced", "kind": "class", "doc": "<p>EMNIST Balanced dataset. The <a href=\"https://www.nist.gov/itl/products-and-services/emnist-dataset/\">original EMNIST dataset</a> is a collection of handwritten letters and digits (including A-Z, a-z, 0-9). The Balanced split consists of 131,600 28x28 grayscale images in 47 balanced classes.</p>\n", "bases": "typing.Generic[+_T_co]"}, {"fullname": "clarena.stl_datasets.raw.emnist.EMNISTBalanced.__init__", "modulename": "clarena.stl_datasets.raw.emnist", "qualname": "EMNISTBalanced.__init__", "kind": "function", "doc": "<p>Initialize the EMNIST Balanced dataset.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): Root directory of the dataset where <code>EMNIST/raw/train-images-idx3-ubyte</code> and <code>EMNIST/raw/t10k-images-idx3-ubyte</code> exist.</li>\n<li><strong>train</strong> (<code>bool</code> | <code>None</code>): If True, creates dataset from training set, otherwise creates from test set.</li>\n<li><strong>transform</strong> (<code>callable</code> | <code>None</code>): A function/transform that  takes in an PIL image and returns a transformed version. E.g, <code>transforms.RandomCrop</code></li>\n<li><strong>target_transform</strong> (<code>callable</code> | <code>None</code>): A function/transform that takes in the target and transforms it.</li>\n<li><strong>download</strong> (<code>bool</code>): If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"n\">pathlib</span><span class=\"o\">.</span><span class=\"n\">Path</span>,</span><span class=\"param\">\t<span class=\"n\">train</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">target_transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.raw.emnist.EMNISTLetters", "modulename": "clarena.stl_datasets.raw.emnist", "qualname": "EMNISTLetters", "kind": "class", "doc": "<p>EMNIST Letters dataset. The <a href=\"https://www.nist.gov/itl/products-and-services/emnist-dataset/\">original EMNIST dataset</a> is a collection of handwritten letters and digits (including A-Z, a-z, 0-9). The Letters split consists of 145,600 28x28 grayscale letters images in 26 balanced classes.</p>\n", "bases": "typing.Generic[+_T_co]"}, {"fullname": "clarena.stl_datasets.raw.emnist.EMNISTLetters.__init__", "modulename": "clarena.stl_datasets.raw.emnist", "qualname": "EMNISTLetters.__init__", "kind": "function", "doc": "<p>Initialize the EMNIST Letters dataset.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): Root directory of the dataset where <code>EMNIST/raw/train-images-idx3-ubyte</code> and <code>EMNIST/raw/t10k-images-idx3-ubyte</code> exist.</li>\n<li><strong>train</strong> (<code>bool</code> | <code>None</code>): If True, creates dataset from training set, otherwise creates from test set.</li>\n<li><strong>transform</strong> (<code>callable</code> | <code>None</code>): A function/transform that  takes in an PIL image and returns a transformed version. E.g, <code>transforms.RandomCrop</code></li>\n<li><strong>target_transform</strong> (<code>callable</code> | <code>None</code>): A function/transform that takes in the target and transforms it.</li>\n<li><strong>download</strong> (<code>bool</code>): If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"n\">pathlib</span><span class=\"o\">.</span><span class=\"n\">Path</span>,</span><span class=\"param\">\t<span class=\"n\">train</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">target_transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.raw.emnist.EMNISTDigits", "modulename": "clarena.stl_datasets.raw.emnist", "qualname": "EMNISTDigits", "kind": "class", "doc": "<p>EMNIST Digits dataset. The <a href=\"https://www.nist.gov/itl/products-and-services/emnist-dataset/\">original EMNIST dataset</a> is a collection of handwritten letters and digits (including A-Z, a-z, 0-9). The Digits split consists of 280,000 28x28 grayscale digits images in 10 balanced classes.</p>\n", "bases": "typing.Generic[+_T_co]"}, {"fullname": "clarena.stl_datasets.raw.emnist.EMNISTDigits.__init__", "modulename": "clarena.stl_datasets.raw.emnist", "qualname": "EMNISTDigits.__init__", "kind": "function", "doc": "<p>Initialize the EMNIST Digits dataset.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): Root directory of the dataset where <code>EMNIST/raw/train-images-idx3-ubyte</code> and <code>EMNIST/raw/t10k-images-idx3-ubyte</code> exist.</li>\n<li><strong>train</strong> (<code>bool</code> | <code>None</code>): If True, creates dataset from training set, otherwise creates from test set.</li>\n<li><strong>transform</strong> (<code>callable</code> | <code>None</code>): A function/transform that  takes in an PIL image and returns a transformed version. E.g, <code>transforms.RandomCrop</code></li>\n<li><strong>target_transform</strong> (<code>callable</code> | <code>None</code>): A function/transform that takes in the target and transforms it.</li>\n<li><strong>download</strong> (<code>bool</code>): If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"n\">pathlib</span><span class=\"o\">.</span><span class=\"n\">Path</span>,</span><span class=\"param\">\t<span class=\"n\">train</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">target_transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.raw.facescrub", "modulename": "clarena.stl_datasets.raw.facescrub", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets.original</code> for the original FaceScrub subset dataset.</p>\n"}, {"fullname": "clarena.stl_datasets.raw.facescrub.FaceScrub10", "modulename": "clarena.stl_datasets.raw.facescrub", "qualname": "FaceScrub10", "kind": "class", "doc": "<p>FaceScrub-10 dataset. <a href=\"https://github.com/nkundiushuti/facescrub_subset/\">FaceScrub-10</a> is a 10-class subset of the official <a href=\"http://megaface.cs.washington.edu/participate/challenge.html\">Megaface FaceScrub challenge</a>, cropped and resized to 32x32.</p>\n", "bases": "typing.Generic[+_T_co]"}, {"fullname": "clarena.stl_datasets.raw.facescrub.FaceScrub10.__init__", "modulename": "clarena.stl_datasets.raw.facescrub", "qualname": "FaceScrub10.__init__", "kind": "function", "doc": "<p>Initialize the FaceScrub10 dataset.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code> | <code>Path</code>): Root directory of the dataset.</li>\n<li><strong>train</strong> (<code>bool</code>): If True, load the training set. Otherwise, load the test set.</li>\n<li><strong>transform</strong> (<code>Callable</code>, optional): Transform to apply to each image.</li>\n<li><strong>target_transform</strong> (<code>Callable</code>, optional): Transform to apply to each label.</li>\n<li><strong>download</strong> (<code>bool</code>): If True, download the dataset if it's not already available.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"n\">pathlib</span><span class=\"o\">.</span><span class=\"n\">Path</span>,</span><span class=\"param\">\t<span class=\"n\">train</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">target_transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.raw.facescrub.FaceScrub10.base_folder", "modulename": "clarena.stl_datasets.raw.facescrub", "qualname": "FaceScrub10.base_folder", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;facescrub_10&#x27;"}, {"fullname": "clarena.stl_datasets.raw.facescrub.FaceScrub10.url", "modulename": "clarena.stl_datasets.raw.facescrub", "qualname": "FaceScrub10.url", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;https://github.com/nkundiushuti/facescrub_subset/blob/master/data/facescrub_10.zip?raw=true&#x27;"}, {"fullname": "clarena.stl_datasets.raw.facescrub.FaceScrub10.filename", "modulename": "clarena.stl_datasets.raw.facescrub", "qualname": "FaceScrub10.filename", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;facescrub_10.zip&#x27;"}, {"fullname": "clarena.stl_datasets.raw.facescrub.FaceScrub10.train_file", "modulename": "clarena.stl_datasets.raw.facescrub", "qualname": "FaceScrub10.train_file", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;facescrub_train_10.pkl&#x27;"}, {"fullname": "clarena.stl_datasets.raw.facescrub.FaceScrub10.test_file", "modulename": "clarena.stl_datasets.raw.facescrub", "qualname": "FaceScrub10.test_file", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;facescrub_test_10.pkl&#x27;"}, {"fullname": "clarena.stl_datasets.raw.facescrub.FaceScrub10.root", "modulename": "clarena.stl_datasets.raw.facescrub", "qualname": "FaceScrub10.root", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "clarena.stl_datasets.raw.facescrub.FaceScrub10.train", "modulename": "clarena.stl_datasets.raw.facescrub", "qualname": "FaceScrub10.train", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "clarena.stl_datasets.raw.facescrub.FaceScrub20", "modulename": "clarena.stl_datasets.raw.facescrub", "qualname": "FaceScrub20", "kind": "class", "doc": "<p>FaceScrub-20 dataset. <a href=\"https://github.com/nkundiushuti/facescrub_subset/\">FaceScrub-20</a> is a 20-class subset of the official <a href=\"http://megaface.cs.washington.edu/participate/challenge.html\">Megaface FaceScrub challenge</a>, cropped and resized to 32x32.</p>\n", "bases": "typing.Generic[+_T_co]"}, {"fullname": "clarena.stl_datasets.raw.facescrub.FaceScrub20.__init__", "modulename": "clarena.stl_datasets.raw.facescrub", "qualname": "FaceScrub20.__init__", "kind": "function", "doc": "<p>Initialize the FaceScrub20 dataset.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code> | <code>Path</code>): Root directory of the dataset.</li>\n<li><strong>train</strong> (<code>bool</code>): If True, load the training set. Otherwise, load the test set.</li>\n<li><strong>transform</strong> (<code>Callable</code>, optional): Transform to apply to each image.</li>\n<li><strong>target_transform</strong> (<code>Callable</code>, optional): Transform to apply to each label.</li>\n<li><strong>download</strong> (<code>bool</code>): If True, download the dataset if it's not already available.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"n\">pathlib</span><span class=\"o\">.</span><span class=\"n\">Path</span>,</span><span class=\"param\">\t<span class=\"n\">train</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">target_transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.raw.facescrub.FaceScrub20.base_folder", "modulename": "clarena.stl_datasets.raw.facescrub", "qualname": "FaceScrub20.base_folder", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;facescrub_20&#x27;"}, {"fullname": "clarena.stl_datasets.raw.facescrub.FaceScrub20.url", "modulename": "clarena.stl_datasets.raw.facescrub", "qualname": "FaceScrub20.url", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;https://github.com/nkundiushuti/facescrub_subset/blob/master/data/facescrub_20.zip?raw=true&#x27;"}, {"fullname": "clarena.stl_datasets.raw.facescrub.FaceScrub20.filename", "modulename": "clarena.stl_datasets.raw.facescrub", "qualname": "FaceScrub20.filename", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;facescrub_20.zip&#x27;"}, {"fullname": "clarena.stl_datasets.raw.facescrub.FaceScrub20.train_file", "modulename": "clarena.stl_datasets.raw.facescrub", "qualname": "FaceScrub20.train_file", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;facescrub_train_20.pkl&#x27;"}, {"fullname": "clarena.stl_datasets.raw.facescrub.FaceScrub20.test_file", "modulename": "clarena.stl_datasets.raw.facescrub", "qualname": "FaceScrub20.test_file", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;facescrub_test_20.pkl&#x27;"}, {"fullname": "clarena.stl_datasets.raw.facescrub.FaceScrub20.root", "modulename": "clarena.stl_datasets.raw.facescrub", "qualname": "FaceScrub20.root", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "clarena.stl_datasets.raw.facescrub.FaceScrub20.train", "modulename": "clarena.stl_datasets.raw.facescrub", "qualname": "FaceScrub20.train", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "clarena.stl_datasets.raw.facescrub.FaceScrub50", "modulename": "clarena.stl_datasets.raw.facescrub", "qualname": "FaceScrub50", "kind": "class", "doc": "<p>FaceScrub-50 dataset. <a href=\"https://github.com/nkundiushuti/facescrub_subset/\">FaceScrub-50</a> is a 50-class subset of the official <a href=\"http://megaface.cs.washington.edu/participate/challenge.html\">Megaface FaceScrub challenge</a>, cropped and resized to 32x32.</p>\n", "bases": "typing.Generic[+_T_co]"}, {"fullname": "clarena.stl_datasets.raw.facescrub.FaceScrub50.__init__", "modulename": "clarena.stl_datasets.raw.facescrub", "qualname": "FaceScrub50.__init__", "kind": "function", "doc": "<p>Initialize the FaceScrub50 dataset.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code> | <code>Path</code>): Root directory of the dataset.</li>\n<li><strong>train</strong> (<code>bool</code>): If True, load the training set. Otherwise, load the test set.</li>\n<li><strong>transform</strong> (<code>Callable</code>, optional): Transform to apply to each image.</li>\n<li><strong>target_transform</strong> (<code>Callable</code>, optional): Transform to apply to each label.</li>\n<li><strong>download</strong> (<code>bool</code>): If True, download the dataset if it's not already available.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"n\">pathlib</span><span class=\"o\">.</span><span class=\"n\">Path</span>,</span><span class=\"param\">\t<span class=\"n\">train</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">target_transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.raw.facescrub.FaceScrub50.base_folder", "modulename": "clarena.stl_datasets.raw.facescrub", "qualname": "FaceScrub50.base_folder", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;facescrub_50&#x27;"}, {"fullname": "clarena.stl_datasets.raw.facescrub.FaceScrub50.url", "modulename": "clarena.stl_datasets.raw.facescrub", "qualname": "FaceScrub50.url", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;https://github.com/nkundiushuti/facescrub_subset/blob/master/data/facescrub_50.zip?raw=true&#x27;"}, {"fullname": "clarena.stl_datasets.raw.facescrub.FaceScrub50.filename", "modulename": "clarena.stl_datasets.raw.facescrub", "qualname": "FaceScrub50.filename", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;facescrub_50.zip&#x27;"}, {"fullname": "clarena.stl_datasets.raw.facescrub.FaceScrub50.train_file", "modulename": "clarena.stl_datasets.raw.facescrub", "qualname": "FaceScrub50.train_file", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;facescrub_train_50.pkl&#x27;"}, {"fullname": "clarena.stl_datasets.raw.facescrub.FaceScrub50.test_file", "modulename": "clarena.stl_datasets.raw.facescrub", "qualname": "FaceScrub50.test_file", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;facescrub_test_50.pkl&#x27;"}, {"fullname": "clarena.stl_datasets.raw.facescrub.FaceScrub50.root", "modulename": "clarena.stl_datasets.raw.facescrub", "qualname": "FaceScrub50.root", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "clarena.stl_datasets.raw.facescrub.FaceScrub50.train", "modulename": "clarena.stl_datasets.raw.facescrub", "qualname": "FaceScrub50.train", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "clarena.stl_datasets.raw.facescrub.FaceScrub100", "modulename": "clarena.stl_datasets.raw.facescrub", "qualname": "FaceScrub100", "kind": "class", "doc": "<p>FaceScrub-100 dataset. <a href=\"https://github.com/nkundiushuti/facescrub_subset/\">FaceScrub-100</a> is a 100-class subset of the official <a href=\"http://megaface.cs.washington.edu/participate/challenge.html\">Megaface FaceScrub challenge</a>, cropped and resized to 32x32.</p>\n\n<p>The dataset contains images of 100 different individuals with separate train/test splits.</p>\n", "bases": "typing.Generic[+_T_co]"}, {"fullname": "clarena.stl_datasets.raw.facescrub.FaceScrub100.__init__", "modulename": "clarena.stl_datasets.raw.facescrub", "qualname": "FaceScrub100.__init__", "kind": "function", "doc": "<p>Initialize the FaceScrub100 dataset.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code> | <code>Path</code>): Root directory of the dataset.</li>\n<li><strong>train</strong> (<code>bool</code>): If True, load the training split; otherwise, load the test split.</li>\n<li><strong>transform</strong> (<code>Callable</code>, optional): Function to apply to each image.</li>\n<li><strong>target_transform</strong> (<code>Callable</code>, optional): Function to apply to each label.</li>\n<li><strong>download</strong> (<code>bool</code>): If True, download the dataset if it's not found in root.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"n\">pathlib</span><span class=\"o\">.</span><span class=\"n\">Path</span>,</span><span class=\"param\">\t<span class=\"n\">train</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">target_transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.raw.facescrub.FaceScrub100.base_folder", "modulename": "clarena.stl_datasets.raw.facescrub", "qualname": "FaceScrub100.base_folder", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;facescrub_100&#x27;"}, {"fullname": "clarena.stl_datasets.raw.facescrub.FaceScrub100.url", "modulename": "clarena.stl_datasets.raw.facescrub", "qualname": "FaceScrub100.url", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;https://github.com/nkundiushuti/facescrub_subset/blob/master/data/facescrub_100.zip?raw=true&#x27;"}, {"fullname": "clarena.stl_datasets.raw.facescrub.FaceScrub100.filename", "modulename": "clarena.stl_datasets.raw.facescrub", "qualname": "FaceScrub100.filename", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;facescrub_100.zip&#x27;"}, {"fullname": "clarena.stl_datasets.raw.facescrub.FaceScrub100.train_file", "modulename": "clarena.stl_datasets.raw.facescrub", "qualname": "FaceScrub100.train_file", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;facescrub_train_100.pkl&#x27;"}, {"fullname": "clarena.stl_datasets.raw.facescrub.FaceScrub100.test_file", "modulename": "clarena.stl_datasets.raw.facescrub", "qualname": "FaceScrub100.test_file", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;facescrub_test_100.pkl&#x27;"}, {"fullname": "clarena.stl_datasets.raw.facescrub.FaceScrub100.root", "modulename": "clarena.stl_datasets.raw.facescrub", "qualname": "FaceScrub100.root", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "clarena.stl_datasets.raw.facescrub.FaceScrub100.train", "modulename": "clarena.stl_datasets.raw.facescrub", "qualname": "FaceScrub100.train", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "clarena.stl_datasets.raw.facescrub.FaceScrubFromHAT", "modulename": "clarena.stl_datasets.raw.facescrub", "qualname": "FaceScrubFromHAT", "kind": "class", "doc": "<p>Subset of the FaceScrub cropped from the official Megaface challenge page: <a href=\"http://megaface.cs.washington.edu/participate/challenge.html\">http://megaface.cs.washington.edu/participate/challenge.html</a>, resized to 38x38</p>\n\n<p>Args:\n    root (string): Root directory of dataset where directory <code>Traffic signs</code> exists.\n    split (string): One of {'train', 'test'}.\n    transform (callable, optional): A function/transform that  takes in an PIL image\n        and returns a transformed version. E.g, <code>transforms.RandomCrop</code>\n    target_transform (callable, optional): A function/transform that takes in the\n        target and transforms it.\n    download (bool, optional): If true, downloads the dataset from the internet and puts it in root directory.\n        If dataset is already downloaded, it is not downloaded again.</p>\n", "bases": "typing.Generic[+_T_co]"}, {"fullname": "clarena.stl_datasets.raw.facescrub.FaceScrubFromHAT.__init__", "modulename": "clarena.stl_datasets.raw.facescrub", "qualname": "FaceScrubFromHAT.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span>,</span><span class=\"param\">\t<span class=\"n\">train</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">transform</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">target_transform</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"o\">=</span><span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.raw.facescrub.FaceScrubFromHAT.root", "modulename": "clarena.stl_datasets.raw.facescrub", "qualname": "FaceScrubFromHAT.root", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "clarena.stl_datasets.raw.facescrub.FaceScrubFromHAT.transform", "modulename": "clarena.stl_datasets.raw.facescrub", "qualname": "FaceScrubFromHAT.transform", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "clarena.stl_datasets.raw.facescrub.FaceScrubFromHAT.target_transform", "modulename": "clarena.stl_datasets.raw.facescrub", "qualname": "FaceScrubFromHAT.target_transform", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "clarena.stl_datasets.raw.facescrub.FaceScrubFromHAT.filename", "modulename": "clarena.stl_datasets.raw.facescrub", "qualname": "FaceScrubFromHAT.filename", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "clarena.stl_datasets.raw.facescrub.FaceScrubFromHAT.url", "modulename": "clarena.stl_datasets.raw.facescrub", "qualname": "FaceScrubFromHAT.url", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "clarena.stl_datasets.raw.facescrub.FaceScrubFromHAT.download", "modulename": "clarena.stl_datasets.raw.facescrub", "qualname": "FaceScrubFromHAT.download", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.raw.fgvc_aircraft", "modulename": "clarena.stl_datasets.raw.fgvc_aircraft", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets.original</code> for the original FGVC-Aircraft dataset.</p>\n"}, {"fullname": "clarena.stl_datasets.raw.fgvc_aircraft.FGVCAircraftVariant", "modulename": "clarena.stl_datasets.raw.fgvc_aircraft", "qualname": "FGVCAircraftVariant", "kind": "class", "doc": "<p>FGVC-Aircraft dataset annotated by variant. The <a href=\"https://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/\">original FGVC-Aircraft dataset</a> is a collection of aircraft images. It consists of 10,000 images.</p>\n", "bases": "typing.Generic[+_T_co]"}, {"fullname": "clarena.stl_datasets.raw.fgvc_aircraft.FGVCAircraftVariant.__init__", "modulename": "clarena.stl_datasets.raw.fgvc_aircraft", "qualname": "FGVCAircraftVariant.__init__", "kind": "function", "doc": "<p>Initialize the FGVC-Aircraft dataset annotated by variant.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): Root directory of the FGVC-Aircraft dataset.</li>\n<li><strong>split</strong> (<code>str</code>): The dataset split, supports 'train', 'val', 'trainval', 'test'.</li>\n<li><strong>transform</strong> (<code>callable</code> | <code>None</code>): A function/transform that takes in an PIL image and returns a transformed version. E.g, <code>transforms.RandomCrop</code></li>\n<li><strong>target_transform</strong> (<code>callable</code> | <code>None</code>): A function/transform that takes in the target and transforms it.</li>\n<li><strong>download</strong> (<code>bool</code>): If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"n\">pathlib</span><span class=\"o\">.</span><span class=\"n\">Path</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;trainval&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">target_transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.raw.fgvc_aircraft.FGVCAircraftFamily", "modulename": "clarena.stl_datasets.raw.fgvc_aircraft", "qualname": "FGVCAircraftFamily", "kind": "class", "doc": "<p>FGVC-Aircraft dataset annotated by family. The <a href=\"https://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/\">original FGVC-Aircraft dataset</a> is a collection of aircraft images. It consists of 10,000 images.</p>\n", "bases": "typing.Generic[+_T_co]"}, {"fullname": "clarena.stl_datasets.raw.fgvc_aircraft.FGVCAircraftFamily.__init__", "modulename": "clarena.stl_datasets.raw.fgvc_aircraft", "qualname": "FGVCAircraftFamily.__init__", "kind": "function", "doc": "<p>Initialize the FGVC-Aircraft dataset annotated by family.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): Root directory of the FGVC-Aircraft dataset.</li>\n<li><strong>split</strong> (<code>str</code>): The dataset split, supports 'train', 'val', 'trainval', 'test'.</li>\n<li><strong>transform</strong> (<code>callable</code> | <code>None</code>): A function/transform that takes in an PIL image and returns a transformed version. E.g, <code>transforms.RandomCrop</code></li>\n<li><strong>target_transform</strong> (<code>callable</code> | <code>None</code>): A function/transform that takes in the target and transforms it.</li>\n<li><strong>download</strong> (<code>bool</code>): If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"n\">pathlib</span><span class=\"o\">.</span><span class=\"n\">Path</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;trainval&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">target_transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.raw.fgvc_aircraft.FGVCAircraftManufacturer", "modulename": "clarena.stl_datasets.raw.fgvc_aircraft", "qualname": "FGVCAircraftManufacturer", "kind": "class", "doc": "<p>FGVC-Aircraft dataset annotated by manufacturer. The <a href=\"https://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/\">original FGVC-Aircraft dataset</a> is a collection of aircraft images. It consists of 10,000 images.</p>\n", "bases": "typing.Generic[+_T_co]"}, {"fullname": "clarena.stl_datasets.raw.fgvc_aircraft.FGVCAircraftManufacturer.__init__", "modulename": "clarena.stl_datasets.raw.fgvc_aircraft", "qualname": "FGVCAircraftManufacturer.__init__", "kind": "function", "doc": "<p>Initialize the FGVC-Aircraft dataset annotated by manufacturer.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): Root directory of the FGVC-Aircraft dataset.</li>\n<li><strong>split</strong> (<code>str</code>): The dataset split, supports 'train', 'val', 'trainval', 'test'.</li>\n<li><strong>transform</strong> (<code>callable</code> | <code>None</code>): A function/transform that takes in an PIL image and returns a transformed version. E.g, <code>transforms.RandomCrop</code></li>\n<li><strong>target_transform</strong> (<code>callable</code> | <code>None</code>): A function/transform that takes in the target and transforms it.</li>\n<li><strong>download</strong> (<code>bool</code>): If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"n\">pathlib</span><span class=\"o\">.</span><span class=\"n\">Path</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;trainval&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">target_transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.raw.kannada_mnist", "modulename": "clarena.stl_datasets.raw.kannada_mnist", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets.original</code> for the original Kannada-MNIST dataset.</p>\n"}, {"fullname": "clarena.stl_datasets.raw.kannada_mnist.KannadaMNIST", "modulename": "clarena.stl_datasets.raw.kannada_mnist", "qualname": "KannadaMNIST", "kind": "class", "doc": "<p>Kannada-MNIST dataset. The <a href=\"https://github.com/vinayprabhu/Kannada_MNIST\">Kannada-MNIST dataset</a> is a collection of handwritten Kannada digits (0-9). It consists of 70,000 28x28 grayscale images in 10 classes.</p>\n", "bases": "typing.Generic[+_T_co]"}, {"fullname": "clarena.stl_datasets.raw.kannada_mnist.KannadaMNIST.__init__", "modulename": "clarena.stl_datasets.raw.kannada_mnist", "qualname": "KannadaMNIST.__init__", "kind": "function", "doc": "<p>Initialize the Kannada MNIST dataset.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code> | <code>Path</code>): Root directory of the dataset.</li>\n<li><strong>train</strong> (<code>bool</code>): Whether to load training data or test data.</li>\n<li><strong>transform</strong> (<code>callable</code> | <code>None</code>): Transform to apply to images.</li>\n<li><strong>target_transform</strong> (<code>callable</code> | <code>None</code>): Transform to apply to labels.</li>\n<li><strong>download</strong> (<code>bool</code>): Not implemented. Dataset must be manually downloaded and extracted.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"n\">pathlib</span><span class=\"o\">.</span><span class=\"n\">Path</span>,</span><span class=\"param\">\t<span class=\"n\">train</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">target_transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.raw.kannada_mnist.KannadaMNIST.base_folder", "modulename": "clarena.stl_datasets.raw.kannada_mnist", "qualname": "KannadaMNIST.base_folder", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;Kannada_MNIST&#x27;"}, {"fullname": "clarena.stl_datasets.raw.kannada_mnist.KannadaMNIST.train_images_file", "modulename": "clarena.stl_datasets.raw.kannada_mnist", "qualname": "KannadaMNIST.train_images_file", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;X_kannada_MNIST_train-idx3-ubyte.gz&#x27;"}, {"fullname": "clarena.stl_datasets.raw.kannada_mnist.KannadaMNIST.train_labels_file", "modulename": "clarena.stl_datasets.raw.kannada_mnist", "qualname": "KannadaMNIST.train_labels_file", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;y_kannada_MNIST_train-idx1-ubyte.gz&#x27;"}, {"fullname": "clarena.stl_datasets.raw.kannada_mnist.KannadaMNIST.test_images_file", "modulename": "clarena.stl_datasets.raw.kannada_mnist", "qualname": "KannadaMNIST.test_images_file", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;X_kannada_MNIST_test-idx3-ubyte.gz&#x27;"}, {"fullname": "clarena.stl_datasets.raw.kannada_mnist.KannadaMNIST.test_labels_file", "modulename": "clarena.stl_datasets.raw.kannada_mnist", "qualname": "KannadaMNIST.test_labels_file", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;y_kannada_MNIST_test-idx1-ubyte.gz&#x27;"}, {"fullname": "clarena.stl_datasets.raw.kannada_mnist.KannadaMNIST.train", "modulename": "clarena.stl_datasets.raw.kannada_mnist", "qualname": "KannadaMNIST.train", "kind": "variable", "doc": "<p>Flag indicating whether training data is loaded.</p>\n"}, {"fullname": "clarena.stl_datasets.raw.kannada_mnist.KannadaMNIST.images", "modulename": "clarena.stl_datasets.raw.kannada_mnist", "qualname": "KannadaMNIST.images", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "clarena.stl_datasets.raw.kannada_mnist.KannadaMNIST.labels", "modulename": "clarena.stl_datasets.raw.kannada_mnist", "qualname": "KannadaMNIST.labels", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "clarena.stl_datasets.raw.kannada_mnist.KannadaMNIST.class_names", "modulename": "clarena.stl_datasets.raw.kannada_mnist", "qualname": "KannadaMNIST.class_names", "kind": "variable", "doc": "<p>Digit classes 0 to 9.</p>\n", "annotation": ": list[str]"}, {"fullname": "clarena.stl_datasets.raw.linnaeus5", "modulename": "clarena.stl_datasets.raw.linnaeus5", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets.original</code> for the original Linnaeus 5 dataset.</p>\n"}, {"fullname": "clarena.stl_datasets.raw.linnaeus5.Linnaeus5", "modulename": "clarena.stl_datasets.raw.linnaeus5", "qualname": "Linnaeus5", "kind": "class", "doc": "<p>Linnaeus 5 dataset. The <a href=\"https://chaladze.com/l5/\">original Linnaeus 5 dataset</a>\ncontains images from 5 classes at various resolutions, with train/test splits.</p>\n", "bases": "typing.Generic[+_T_co]"}, {"fullname": "clarena.stl_datasets.raw.linnaeus5.Linnaeus5.__init__", "modulename": "clarena.stl_datasets.raw.linnaeus5", "qualname": "Linnaeus5.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"n\">pathlib</span><span class=\"o\">.</span><span class=\"n\">Path</span>,</span><span class=\"param\">\t<span class=\"n\">resolution</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;256&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">train</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">target_transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.raw.linnaeus5.Linnaeus5.base_url", "modulename": "clarena.stl_datasets.raw.linnaeus5", "qualname": "Linnaeus5.base_url", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;https://chaladze.com/l5/img/&#x27;"}, {"fullname": "clarena.stl_datasets.raw.linnaeus5.Linnaeus5.versions", "modulename": "clarena.stl_datasets.raw.linnaeus5", "qualname": "Linnaeus5.versions", "kind": "variable", "doc": "<p></p>\n", "annotation": ": dict[str, str]", "default_value": "{&#x27;256&#x27;: &#x27;Linnaeus%205%20256X256.rar&#x27;, &#x27;128&#x27;: &#x27;Linnaeus%205%20128X128.rar&#x27;, &#x27;64&#x27;: &#x27;Linnaeus%205%2064X64.rar&#x27;, &#x27;32&#x27;: &#x27;Linnaeus%205%2032X32.rar&#x27;}"}, {"fullname": "clarena.stl_datasets.raw.linnaeus5.Linnaeus5.resolution", "modulename": "clarena.stl_datasets.raw.linnaeus5", "qualname": "Linnaeus5.resolution", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "clarena.stl_datasets.raw.linnaeus5.Linnaeus5.train", "modulename": "clarena.stl_datasets.raw.linnaeus5", "qualname": "Linnaeus5.train", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "clarena.stl_datasets.raw.linnaeus5.Linnaeus5.class_names", "modulename": "clarena.stl_datasets.raw.linnaeus5", "qualname": "Linnaeus5.class_names", "kind": "variable", "doc": "<p></p>\n", "annotation": ": list[str]"}, {"fullname": "clarena.stl_datasets.raw.linnaeus5.Linnaeus5.filename", "modulename": "clarena.stl_datasets.raw.linnaeus5", "qualname": "Linnaeus5.filename", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "clarena.stl_datasets.raw.linnaeus5.Linnaeus5.base_folder", "modulename": "clarena.stl_datasets.raw.linnaeus5", "qualname": "Linnaeus5.base_folder", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "clarena.stl_datasets.raw.linnaeus5.Linnaeus5.url", "modulename": "clarena.stl_datasets.raw.linnaeus5", "qualname": "Linnaeus5.url", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "clarena.stl_datasets.raw.linnaeus5.Linnaeus5.split_folder", "modulename": "clarena.stl_datasets.raw.linnaeus5", "qualname": "Linnaeus5.split_folder", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "clarena.stl_datasets.raw.linnaeus5.Linnaeus5.data", "modulename": "clarena.stl_datasets.raw.linnaeus5", "qualname": "Linnaeus5.data", "kind": "variable", "doc": "<p></p>\n", "annotation": ": pandas.core.frame.DataFrame"}, {"fullname": "clarena.stl_datasets.raw.linnaeus5.Linnaeus5.loader", "modulename": "clarena.stl_datasets.raw.linnaeus5", "qualname": "Linnaeus5.loader", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "clarena.stl_datasets.raw.linnaeus5.Linnaeus5.download", "modulename": "clarena.stl_datasets.raw.linnaeus5", "qualname": "Linnaeus5.download", "kind": "function", "doc": "<p>Download and extract the dataset.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.raw.linnaeus5.Linnaeus5_32", "modulename": "clarena.stl_datasets.raw.linnaeus5", "qualname": "Linnaeus5_32", "kind": "class", "doc": "<p>Linnaeus 5 dataset with 32x32 resolution. This is a subclass of Linnaeus5 that sets the resolution to 32x32.</p>\n", "bases": "typing.Generic[+_T_co]"}, {"fullname": "clarena.stl_datasets.raw.linnaeus5.Linnaeus5_32.__init__", "modulename": "clarena.stl_datasets.raw.linnaeus5", "qualname": "Linnaeus5_32.__init__", "kind": "function", "doc": "<p>Initialize the Linnaeus 5 dataset with 32x32 resolution.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code> | <code>Path</code>): Root directory of the dataset.</li>\n<li><strong>train</strong> (<code>bool</code>): If True, uses the train set. If False, uses the test set.</li>\n<li><strong>transform</strong> (<code>callable</code> | <code>None</code>): Image transform pipeline.</li>\n<li><strong>target_transform</strong> (<code>callable</code> | <code>None</code>): Label transform.</li>\n<li><strong>download</strong> (<code>bool</code>): If True, downloads the dataset if not found.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"n\">pathlib</span><span class=\"o\">.</span><span class=\"n\">Path</span>,</span><span class=\"param\">\t<span class=\"n\">train</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">target_transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.raw.linnaeus5.Linnaeus5_64", "modulename": "clarena.stl_datasets.raw.linnaeus5", "qualname": "Linnaeus5_64", "kind": "class", "doc": "<p>Linnaeus 5 dataset with 64x64 resolution. This is a subclass of Linnaeus5 that sets the resolution to 64x64.</p>\n", "bases": "typing.Generic[+_T_co]"}, {"fullname": "clarena.stl_datasets.raw.linnaeus5.Linnaeus5_64.__init__", "modulename": "clarena.stl_datasets.raw.linnaeus5", "qualname": "Linnaeus5_64.__init__", "kind": "function", "doc": "<p>Initialize the Linnaeus 5 dataset with 64x64 resolution.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code> | <code>Path</code>): Root directory of the dataset.</li>\n<li><strong>train</strong> (<code>bool</code>): If True, uses the train set. If False, uses the test set.</li>\n<li><strong>transform</strong> (<code>callable</code> | <code>None</code>): Image transform pipeline.</li>\n<li><strong>target_transform</strong> (<code>callable</code> | <code>None</code>): Label transform.</li>\n<li><strong>download</strong> (<code>bool</code>): If True, downloads the dataset if not found.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"n\">pathlib</span><span class=\"o\">.</span><span class=\"n\">Path</span>,</span><span class=\"param\">\t<span class=\"n\">train</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">target_transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.raw.linnaeus5.Linnaeus5_128", "modulename": "clarena.stl_datasets.raw.linnaeus5", "qualname": "Linnaeus5_128", "kind": "class", "doc": "<p>Linnaeus 5 dataset with 128x128 resolution. This is a subclass of Linnaeus5 that sets the resolution to 128x128.</p>\n", "bases": "typing.Generic[+_T_co]"}, {"fullname": "clarena.stl_datasets.raw.linnaeus5.Linnaeus5_128.__init__", "modulename": "clarena.stl_datasets.raw.linnaeus5", "qualname": "Linnaeus5_128.__init__", "kind": "function", "doc": "<p>Initialize the Linnaeus 5 dataset with 128x128 resolution.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code> | <code>Path</code>): Root directory of the dataset.</li>\n<li><strong>train</strong> (<code>bool</code>): If True, uses the train set. If False, uses the test set.</li>\n<li><strong>transform</strong> (<code>callable</code> | <code>None</code>): Image transform pipeline.</li>\n<li><strong>target_transform</strong> (<code>callable</code> | <code>None</code>): Label transform.</li>\n<li><strong>download</strong> (<code>bool</code>): If True, downloads the dataset if not found.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"n\">pathlib</span><span class=\"o\">.</span><span class=\"n\">Path</span>,</span><span class=\"param\">\t<span class=\"n\">train</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">target_transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.raw.linnaeus5.Linnaeus5_256", "modulename": "clarena.stl_datasets.raw.linnaeus5", "qualname": "Linnaeus5_256", "kind": "class", "doc": "<p>Linnaeus 5 dataset with 256x256 resolution. This is a subclass of Linnaeus5 that sets the resolution to 256x256.</p>\n", "bases": "typing.Generic[+_T_co]"}, {"fullname": "clarena.stl_datasets.raw.linnaeus5.Linnaeus5_256.__init__", "modulename": "clarena.stl_datasets.raw.linnaeus5", "qualname": "Linnaeus5_256.__init__", "kind": "function", "doc": "<p>Initialize the Linnaeus 5 dataset with 256x256 resolution.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code> | <code>Path</code>): Root directory of the dataset.</li>\n<li><strong>train</strong> (<code>bool</code>): If True, uses the train set. If False, uses the test set.</li>\n<li><strong>transform</strong> (<code>callable</code> | <code>None</code>): Image transform pipeline.</li>\n<li><strong>target_transform</strong> (<code>callable</code> | <code>None</code>): Label transform.</li>\n<li><strong>download</strong> (<code>bool</code>): If True, downloads the dataset if not found.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"n\">pathlib</span><span class=\"o\">.</span><span class=\"n\">Path</span>,</span><span class=\"param\">\t<span class=\"n\">train</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">target_transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.raw.notmnist", "modulename": "clarena.stl_datasets.raw.notmnist", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets.original</code> for the original NotMNIST dataset.</p>\n"}, {"fullname": "clarena.stl_datasets.raw.notmnist.NotMNIST", "modulename": "clarena.stl_datasets.raw.notmnist", "qualname": "NotMNIST", "kind": "class", "doc": "<p>NotMNIST dataset. The <a href=\"https://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html\">original NotMNIST dataset</a> is a collection of letters (including A-J). It consists 28x28 grayscale images in 10 classes. The larger dataset contains 500,000 images, while the smaller dataset contains around 19,000 images. This class loads the larger dataset as training and the smaller dataset as test set.</p>\n", "bases": "typing.Generic[+_T_co]"}, {"fullname": "clarena.stl_datasets.raw.notmnist.NotMNIST.__init__", "modulename": "clarena.stl_datasets.raw.notmnist", "qualname": "NotMNIST.__init__", "kind": "function", "doc": "<p>Initialize the NotMNIST dataset.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code> | <code>Path</code>): Root directory of the dataset.</li>\n<li><strong>train</strong> (<code>bool</code> | <code>None</code>): If True, creates dataset from training set, otherwise creates from test set.</li>\n<li><strong>transform</strong> (<code>callable</code> | <code>None</code>): A function/transform that  takes in an PIL image and returns a transformed version. E.g, <code>transforms.RandomCrop</code></li>\n<li><strong>target_transform</strong> (<code>callable</code> | <code>None</code>): A function/transform that takes in the target and transforms it.</li>\n<li><strong>download</strong> (<code>bool</code>): If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"n\">pathlib</span><span class=\"o\">.</span><span class=\"n\">Path</span>,</span><span class=\"param\">\t<span class=\"n\">train</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">target_transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.raw.notmnist.NotMNIST.small_base_folder", "modulename": "clarena.stl_datasets.raw.notmnist", "qualname": "NotMNIST.small_base_folder", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;notMNIST_small&#x27;"}, {"fullname": "clarena.stl_datasets.raw.notmnist.NotMNIST.large_base_folder", "modulename": "clarena.stl_datasets.raw.notmnist", "qualname": "NotMNIST.large_base_folder", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;notMNIST_large&#x27;"}, {"fullname": "clarena.stl_datasets.raw.notmnist.NotMNIST.small_url", "modulename": "clarena.stl_datasets.raw.notmnist", "qualname": "NotMNIST.small_url", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;http://yaroslavvb.com/upload/notMNIST/notMNIST_small.tar.gz&#x27;"}, {"fullname": "clarena.stl_datasets.raw.notmnist.NotMNIST.large_url", "modulename": "clarena.stl_datasets.raw.notmnist", "qualname": "NotMNIST.large_url", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;http://yaroslavvb.com/upload/notMNIST/notMNIST_large.tar.gz&#x27;"}, {"fullname": "clarena.stl_datasets.raw.notmnist.NotMNIST.small_filename", "modulename": "clarena.stl_datasets.raw.notmnist", "qualname": "NotMNIST.small_filename", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;notMNIST_small.tar.gz&#x27;"}, {"fullname": "clarena.stl_datasets.raw.notmnist.NotMNIST.large_filename", "modulename": "clarena.stl_datasets.raw.notmnist", "qualname": "NotMNIST.large_filename", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;notMNIST_large.tar.gz&#x27;"}, {"fullname": "clarena.stl_datasets.raw.notmnist.NotMNIST.small_tgz_md5", "modulename": "clarena.stl_datasets.raw.notmnist", "qualname": "NotMNIST.small_tgz_md5", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;c9890a473a9769fda4bdf314aaf500dd&#x27;"}, {"fullname": "clarena.stl_datasets.raw.notmnist.NotMNIST.large_tgz_md5", "modulename": "clarena.stl_datasets.raw.notmnist", "qualname": "NotMNIST.large_tgz_md5", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;70a95b805ecfb6592c48e196df7c1499&#x27;"}, {"fullname": "clarena.stl_datasets.raw.notmnist.NotMNIST.data", "modulename": "clarena.stl_datasets.raw.notmnist", "qualname": "NotMNIST.data", "kind": "variable", "doc": "<p></p>\n", "annotation": ": pandas.core.frame.DataFrame"}, {"fullname": "clarena.stl_datasets.raw.notmnist.NotMNIST.class_names", "modulename": "clarena.stl_datasets.raw.notmnist", "qualname": "NotMNIST.class_names", "kind": "variable", "doc": "<p></p>\n", "annotation": ": list[str]"}, {"fullname": "clarena.stl_datasets.raw.notmnist.NotMNIST.loader", "modulename": "clarena.stl_datasets.raw.notmnist", "qualname": "NotMNIST.loader", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "clarena.stl_datasets.raw.notmnist.NotMNIST.train", "modulename": "clarena.stl_datasets.raw.notmnist", "qualname": "NotMNIST.train", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool"}, {"fullname": "clarena.stl_datasets.raw.notmnist.NotMNIST.url", "modulename": "clarena.stl_datasets.raw.notmnist", "qualname": "NotMNIST.url", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "clarena.stl_datasets.raw.notmnist.NotMNIST.base_folder", "modulename": "clarena.stl_datasets.raw.notmnist", "qualname": "NotMNIST.base_folder", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "clarena.stl_datasets.raw.notmnist.NotMNIST.filename", "modulename": "clarena.stl_datasets.raw.notmnist", "qualname": "NotMNIST.filename", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "clarena.stl_datasets.raw.notmnist.NotMNIST.tgz_md5", "modulename": "clarena.stl_datasets.raw.notmnist", "qualname": "NotMNIST.tgz_md5", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "clarena.stl_datasets.raw.notmnist.NotMNIST.download", "modulename": "clarena.stl_datasets.raw.notmnist", "qualname": "NotMNIST.download", "kind": "function", "doc": "<p>Download the NotMNIST data if it doesn't exist already.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.raw.notmnist.NotMNISTFromHAT", "modulename": "clarena.stl_datasets.raw.notmnist", "qualname": "NotMNISTFromHAT", "kind": "class", "doc": "<p>The notMNIST dataset is a image recognition dataset of font glypyhs for the letters A through J useful with simple neural networks. It is quite similar to the classic MNIST dataset of handwritten digits 0 through 9.</p>\n\n<p>Args:\n    root (string): Root directory of dataset where directory <code>Traffic signs</code> exists.\n    split (string): One of {'train', 'test'}.\n    transform (callable, optional): A function/transform that  takes in an PIL image\n        and returns a transformed version. E.g, <code>transforms.RandomCrop</code>\n    target_transform (callable, optional): A function/transform that takes in the\n        target and transforms it.\n    download (bool, optional): If true, downloads the dataset from the internet and puts it in root directory.\n        If dataset is already downloaded, it is not downloaded again.</p>\n", "bases": "typing.Generic[+_T_co]"}, {"fullname": "clarena.stl_datasets.raw.notmnist.NotMNISTFromHAT.__init__", "modulename": "clarena.stl_datasets.raw.notmnist", "qualname": "NotMNISTFromHAT.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span>,</span><span class=\"param\">\t<span class=\"n\">train</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">transform</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">target_transform</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"o\">=</span><span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.raw.notmnist.NotMNISTFromHAT.root", "modulename": "clarena.stl_datasets.raw.notmnist", "qualname": "NotMNISTFromHAT.root", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "clarena.stl_datasets.raw.notmnist.NotMNISTFromHAT.transform", "modulename": "clarena.stl_datasets.raw.notmnist", "qualname": "NotMNISTFromHAT.transform", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "clarena.stl_datasets.raw.notmnist.NotMNISTFromHAT.target_transform", "modulename": "clarena.stl_datasets.raw.notmnist", "qualname": "NotMNISTFromHAT.target_transform", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "clarena.stl_datasets.raw.notmnist.NotMNISTFromHAT.filename", "modulename": "clarena.stl_datasets.raw.notmnist", "qualname": "NotMNISTFromHAT.filename", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "clarena.stl_datasets.raw.notmnist.NotMNISTFromHAT.url", "modulename": "clarena.stl_datasets.raw.notmnist", "qualname": "NotMNISTFromHAT.url", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "clarena.stl_datasets.raw.notmnist.NotMNISTFromHAT.download", "modulename": "clarena.stl_datasets.raw.notmnist", "qualname": "NotMNISTFromHAT.download", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.raw.oxford_iiit_pet", "modulename": "clarena.stl_datasets.raw.oxford_iiit_pet", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets.original</code> for the original Oxford-IIIT Pet dataset.</p>\n"}, {"fullname": "clarena.stl_datasets.raw.oxford_iiit_pet.OxfordIIITPet37", "modulename": "clarena.stl_datasets.raw.oxford_iiit_pet", "qualname": "OxfordIIITPet37", "kind": "class", "doc": "<p>Oxford-IIIT Pet dataset with 37 breed classes. The <a href=\"http://www.robots.ox.ac.uk/~vgg/data/pets/\">original Oxford IIIT Pet dataset</a> is a collection of pet images. It consists of 37 breeds of pets with 200 images per breed.</p>\n", "bases": "typing.Generic[+_T_co]"}, {"fullname": "clarena.stl_datasets.raw.oxford_iiit_pet.OxfordIIITPet37.__init__", "modulename": "clarena.stl_datasets.raw.oxford_iiit_pet", "qualname": "OxfordIIITPet37.__init__", "kind": "function", "doc": "<p>Initialize the Oxford-IIIT Pet dataset with 37 breed classes.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): Root directory of the dataset.</li>\n<li><strong>split</strong> (<code>str</code>): The dataset split to use. Can be 'trainval', or 'test'. Default is 'trainval'.</li>\n<li><strong>transform</strong> (<code>callable</code> | <code>None</code>): A function/transform that takes in an PIL image and returns a transformed version. E.g, <code>transforms.RandomCrop</code></li>\n<li><strong>target_transform</strong> (<code>callable</code> | <code>None</code>): A function/transform that takes in the target and transforms it.</li>\n<li><strong>download</strong> (<code>bool</code>): If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"n\">pathlib</span><span class=\"o\">.</span><span class=\"n\">Path</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;trainval&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">target_transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.raw.oxford_iiit_pet.OxfordIIITPet2", "modulename": "clarena.stl_datasets.raw.oxford_iiit_pet", "qualname": "OxfordIIITPet2", "kind": "class", "doc": "<p>Oxford-IIIT Pet dataset with 2 classes (cat, dog). The <a href=\"http://www.robots.ox.ac.uk/~vgg/data/pets/\">original Oxford IIIT Pet dataset</a> is a collection of pet images. It consists of 37 breeds of pets with 200 images per breed.</p>\n", "bases": "typing.Generic[+_T_co]"}, {"fullname": "clarena.stl_datasets.raw.oxford_iiit_pet.OxfordIIITPet2.__init__", "modulename": "clarena.stl_datasets.raw.oxford_iiit_pet", "qualname": "OxfordIIITPet2.__init__", "kind": "function", "doc": "<p>Initialize the Oxford-IIIT Pet dataset with 37 breed classes.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): Root directory of the dataset.</li>\n<li><strong>split</strong> (<code>str</code>): The dataset split to use. Can be 'trainval', or 'test'. Default is 'trainval'.</li>\n<li><strong>transform</strong> (<code>callable</code> | <code>None</code>): A function/transform that takes in an PIL image and returns a transformed version. E.g, <code>transforms.RandomCrop</code></li>\n<li><strong>target_transform</strong> (<code>callable</code> | <code>None</code>): A function/transform that takes in the target and transforms it.</li>\n<li><strong>download</strong> (<code>bool</code>): If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"n\">pathlib</span><span class=\"o\">.</span><span class=\"n\">Path</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;trainval&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">target_transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.raw.sign_language_mnist", "modulename": "clarena.stl_datasets.raw.sign_language_mnist", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets.original</code> for the original Sign Language MNIST dataset.</p>\n"}, {"fullname": "clarena.stl_datasets.raw.sign_language_mnist.SignLanguageMNIST", "modulename": "clarena.stl_datasets.raw.sign_language_mnist", "qualname": "SignLanguageMNIST", "kind": "class", "doc": "<p>Sign Language MNIST dataset. The <a href=\"https://www.kaggle.com/datamunge/sign-language-mnist\">Sign Language MNIST dataset</a> is a collection of hand gesture images representing ASL letters (A-Y, excluding J). It consists of 34,627 28x28 grayscale images in 24 classes.</p>\n", "bases": "typing.Generic[+_T_co]"}, {"fullname": "clarena.stl_datasets.raw.sign_language_mnist.SignLanguageMNIST.__init__", "modulename": "clarena.stl_datasets.raw.sign_language_mnist", "qualname": "SignLanguageMNIST.__init__", "kind": "function", "doc": "<p>Initialize the Sign Language MNIST dataset.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code> | <code>Path</code>): Root directory of the dataset.</li>\n<li><strong>train</strong> (<code>bool</code>): If True, creates dataset from training set, otherwise from test set.</li>\n<li><strong>transform</strong> (<code>callable</code> | <code>None</code>): A function/transform that takes in a PIL image and returns a transformed version.</li>\n<li><strong>target_transform</strong> (<code>callable</code> | <code>None</code>): A function/transform that takes in the target and transforms it.</li>\n<li><strong>download</strong> (<code>bool</code>): Placeholder flag. Download is not supported. Dataset must be manually placed in the root directory.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"n\">pathlib</span><span class=\"o\">.</span><span class=\"n\">Path</span>,</span><span class=\"param\">\t<span class=\"n\">train</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">target_transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.raw.sign_language_mnist.SignLanguageMNIST.base_folder", "modulename": "clarena.stl_datasets.raw.sign_language_mnist", "qualname": "SignLanguageMNIST.base_folder", "kind": "variable", "doc": "<p>The folder name where the main dataset (images) are stored.</p>\n", "annotation": ": str", "default_value": "&#x27;archive&#x27;"}, {"fullname": "clarena.stl_datasets.raw.sign_language_mnist.SignLanguageMNIST.train_csv", "modulename": "clarena.stl_datasets.raw.sign_language_mnist", "qualname": "SignLanguageMNIST.train_csv", "kind": "variable", "doc": "<p>The CSV file name for the training dataset.</p>\n", "annotation": ": str", "default_value": "&#x27;sign_mnist_train.csv&#x27;"}, {"fullname": "clarena.stl_datasets.raw.sign_language_mnist.SignLanguageMNIST.test_csv", "modulename": "clarena.stl_datasets.raw.sign_language_mnist", "qualname": "SignLanguageMNIST.test_csv", "kind": "variable", "doc": "<p>The CSV file name for the test dataset.</p>\n", "annotation": ": str", "default_value": "&#x27;sign_mnist_test.csv&#x27;"}, {"fullname": "clarena.stl_datasets.raw.sign_language_mnist.SignLanguageMNIST.train", "modulename": "clarena.stl_datasets.raw.sign_language_mnist", "qualname": "SignLanguageMNIST.train", "kind": "variable", "doc": "<p>Store the flag to indicate whether to load training or test dataset.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.stl_datasets.raw.sign_language_mnist.SignLanguageMNIST.csv_file", "modulename": "clarena.stl_datasets.raw.sign_language_mnist", "qualname": "SignLanguageMNIST.csv_file", "kind": "variable", "doc": "<p>Store the filename of the CSV data.</p>\n", "annotation": ": str"}, {"fullname": "clarena.stl_datasets.raw.sign_language_mnist.SignLanguageMNIST.class_names", "modulename": "clarena.stl_datasets.raw.sign_language_mnist", "qualname": "SignLanguageMNIST.class_names", "kind": "variable", "doc": "<p>Store the class names of the dataset (A-Y, excluding J).</p>\n", "annotation": ": list[str]"}, {"fullname": "clarena.stl_datasets.raw.sign_language_mnist.SignLanguageMNIST.data", "modulename": "clarena.stl_datasets.raw.sign_language_mnist", "qualname": "SignLanguageMNIST.data", "kind": "variable", "doc": "<p>Store the full dataset loaded from CSV (image pixels and labels).</p>\n", "annotation": ": pandas.core.frame.DataFrame"}, {"fullname": "clarena.stl_datasets.raw.traffic_signs", "modulename": "clarena.stl_datasets.raw.traffic_signs", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets.original</code> for the original Traffic Signs dataset.</p>\n"}, {"fullname": "clarena.stl_datasets.raw.traffic_signs.TrafficSignsFromHAT", "modulename": "clarena.stl_datasets.raw.traffic_signs", "qualname": "TrafficSignsFromHAT", "kind": "class", "doc": "<p><a href=\"http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset\">German Traffic Signs </a> Dataset.</p>\n\n<p>Args:\n    root (string): Root directory of dataset where directory <code>Traffic signs</code> exists.\n    split (string): One of {'train', 'test'}.\n    transform (callable, optional): A function/transform that  takes in an PIL image\n        and returns a transformed version. E.g, <code>transforms.RandomCrop</code>\n    target_transform (callable, optional): A function/transform that takes in the\n        target and transforms it.\n    download (bool, optional): If true, downloads the dataset from the internet and puts it in root directory.\n        If dataset is already downloaded, it is not downloaded again.</p>\n", "bases": "typing.Generic[+_T_co]"}, {"fullname": "clarena.stl_datasets.raw.traffic_signs.TrafficSignsFromHAT.__init__", "modulename": "clarena.stl_datasets.raw.traffic_signs", "qualname": "TrafficSignsFromHAT.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span>,</span><span class=\"param\">\t<span class=\"n\">train</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">transform</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">target_transform</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"o\">=</span><span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.raw.traffic_signs.TrafficSignsFromHAT.root", "modulename": "clarena.stl_datasets.raw.traffic_signs", "qualname": "TrafficSignsFromHAT.root", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "clarena.stl_datasets.raw.traffic_signs.TrafficSignsFromHAT.transform", "modulename": "clarena.stl_datasets.raw.traffic_signs", "qualname": "TrafficSignsFromHAT.transform", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "clarena.stl_datasets.raw.traffic_signs.TrafficSignsFromHAT.target_transform", "modulename": "clarena.stl_datasets.raw.traffic_signs", "qualname": "TrafficSignsFromHAT.target_transform", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "clarena.stl_datasets.raw.traffic_signs.TrafficSignsFromHAT.filename", "modulename": "clarena.stl_datasets.raw.traffic_signs", "qualname": "TrafficSignsFromHAT.filename", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "clarena.stl_datasets.raw.traffic_signs.TrafficSignsFromHAT.url", "modulename": "clarena.stl_datasets.raw.traffic_signs", "qualname": "TrafficSignsFromHAT.url", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "clarena.stl_datasets.raw.traffic_signs.TrafficSignsFromHAT.data", "modulename": "clarena.stl_datasets.raw.traffic_signs", "qualname": "TrafficSignsFromHAT.data", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "clarena.stl_datasets.raw.traffic_signs.TrafficSignsFromHAT.download", "modulename": "clarena.stl_datasets.raw.traffic_signs", "qualname": "TrafficSignsFromHAT.download", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "clarena.unlearning_algorithms", "modulename": "clarena.unlearning_algorithms", "kind": "module", "doc": "<h1 id=\"continual-unlearning-algorithms\">Continual Unlearning Algorithms</h1>\n\n<p>This submodule provides the <strong>continual unlearning algorithms</strong> in CLArena.</p>\n\n<p>The algorithms are implemented as subclasses of <code>UnlearningAlgorithm</code>.</p>\n\n<p>Please note that this is an API documantation. Please refer to the main documentation pages for more information about the backbone networks and how to configure and implement them:</p>\n\n<ul>\n<li><a href=\"\"><strong>Configure Unlearning Algorithm</strong></a></li>\n<li><a href=\"\"><strong>Implement Your Unlearning Algorithm Class</strong></a></li>\n</ul>\n"}, {"fullname": "clarena.unlearning_algorithms.CULAlgorithm", "modulename": "clarena.unlearning_algorithms", "qualname": "CULAlgorithm", "kind": "class", "doc": "<p>The base class of unlearning algorithms.</p>\n"}, {"fullname": "clarena.unlearning_algorithms.CULAlgorithm.__init__", "modulename": "clarena.unlearning_algorithms", "qualname": "CULAlgorithm.__init__", "kind": "function", "doc": "<p>Initialize the unlearning algorithm with the continual learning model.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>model</strong> (<code>CLAlgorithm</code>): the continual learning model (<code>CLAlgorithm</code> object which already contains the backbone and heads).</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span>)</span>"}, {"fullname": "clarena.unlearning_algorithms.CULAlgorithm.model", "modulename": "clarena.unlearning_algorithms", "qualname": "CULAlgorithm.model", "kind": "variable", "doc": "<p>Store the continual learning model.</p>\n", "annotation": ": clarena.cl_algorithms.base.CLAlgorithm"}, {"fullname": "clarena.unlearning_algorithms.CULAlgorithm.task_id", "modulename": "clarena.unlearning_algorithms", "qualname": "CULAlgorithm.task_id", "kind": "variable", "doc": "<p>Task ID counter indicating which task is being processed. Self updated during the task loop. Starting from 1.</p>\n", "annotation": ": int"}, {"fullname": "clarena.unlearning_algorithms.CULAlgorithm.unlearning_task_ids", "modulename": "clarena.unlearning_algorithms", "qualname": "CULAlgorithm.unlearning_task_ids", "kind": "variable", "doc": "<p>The list of task IDs to be unlearned after <code>self.task_id</code>. If none of the tasks need to be unlearned, it will be an empty list.</p>\n", "annotation": ": list[int]"}, {"fullname": "clarena.unlearning_algorithms.CULAlgorithm.unlearned_task_ids", "modulename": "clarena.unlearning_algorithms", "qualname": "CULAlgorithm.unlearned_task_ids", "kind": "variable", "doc": "<p>Store the list of task IDs that have been unlearned in the experiment.</p>\n", "annotation": ": set[int]"}, {"fullname": "clarena.unlearning_algorithms.CULAlgorithm.if_permanent_t", "modulename": "clarena.unlearning_algorithms", "qualname": "CULAlgorithm.if_permanent_t", "kind": "variable", "doc": "<p>Whether the task is permanent or not. If <code>True</code>, the task will not be unlearned i.e. not shown in future unlearning requests.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.unlearning_algorithms.CULAlgorithm.setup_task_id", "modulename": "clarena.unlearning_algorithms", "qualname": "CULAlgorithm.setup_task_id", "kind": "function", "doc": "<p>Set up which task the CUL experiment is on. This must be done before <code>unlearn()</code> method is called.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID to be set up.</li>\n<li><strong>unlearning_requests</strong> (<code>dict[int, list[int]]</code>): the entire unlearning requests. Keys are IDs of the tasks that request unlearning after their learning, and values are the list of the previous tasks to be unlearned.</li>\n<li><strong>if_permanent</strong> (<code>bool</code>): whether the task is permanent or not. If <code>True</code>, the task will not be unlearned i.e. not shown in future unlearning requests.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">unlearning_requests</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">if_permanent</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.unlearning_algorithms.CULAlgorithm.setup_test_task_id", "modulename": "clarena.unlearning_algorithms", "qualname": "CULAlgorithm.setup_test_task_id", "kind": "function", "doc": "<p>Set up before testing <code>self.task_id</code>. This must be done after <code>unlearn()</code> method is called.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.unlearning_algorithms.CULAlgorithm.unlearn", "modulename": "clarena.unlearning_algorithms", "qualname": "CULAlgorithm.unlearn", "kind": "function", "doc": "<p>Unlearn the requested unlearning tasks in current task <code>self.task_id</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.unlearning_algorithms.IndependentUnlearn", "modulename": "clarena.unlearning_algorithms", "qualname": "IndependentUnlearn", "kind": "class", "doc": "<p>The base class of the unlearning algorithm of independent learning.</p>\n", "bases": "clarena.unlearning_algorithms.base.CULAlgorithm"}, {"fullname": "clarena.unlearning_algorithms.IndependentUnlearn.__init__", "modulename": "clarena.unlearning_algorithms", "qualname": "IndependentUnlearn.__init__", "kind": "function", "doc": "<p>Initialize the unlearning algorithm with the continual learning model.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>model</strong> (<code>Independent</code>): the continual learning model (<code>CLAlgorithm</code> object which already contains the backbone and heads). It must be <code>Independent</code> algorithm.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">independent</span><span class=\"o\">.</span><span class=\"n\">Independent</span></span>)</span>"}, {"fullname": "clarena.unlearning_algorithms.IndependentUnlearn.unlearn", "modulename": "clarena.unlearning_algorithms", "qualname": "IndependentUnlearn.unlearn", "kind": "function", "doc": "<p>Unlearn the requested unlearning tasks in current task <code>self.task_id</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.unlearning_algorithms.AmnesiacHATUnlearn", "modulename": "clarena.unlearning_algorithms", "qualname": "AmnesiacHATUnlearn", "kind": "class", "doc": "<p>The base class of the AmnesiacHAT unlearning algorithm.</p>\n", "bases": "clarena.unlearning_algorithms.base.CULAlgorithm"}, {"fullname": "clarena.unlearning_algorithms.AmnesiacHATUnlearn.__init__", "modulename": "clarena.unlearning_algorithms", "qualname": "AmnesiacHATUnlearn.__init__", "kind": "function", "doc": "<p>Initialize the unlearning algorithm with the continual learning model.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>model</strong> (<code>Independent</code>): the continual learning model (<code>CLAlgorithm</code> object which already contains the backbone and heads). It must be <code>HAT</code> algorithm.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">hat</span><span class=\"o\">.</span><span class=\"n\">HAT</span></span>)</span>"}, {"fullname": "clarena.unlearning_algorithms.AmnesiacHATUnlearn.delete_update", "modulename": "clarena.unlearning_algorithms", "qualname": "AmnesiacHATUnlearn.delete_update", "kind": "function", "doc": "<p>Delete the update of the specified unlearning task.\n<strong>Args:</strong></p>\n\n<ul>\n<li><strong>unlearning_task_id</strong> (<code>str</code>): the ID of the unlearning task to delete the update.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">unlearning_task_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.unlearning_algorithms.AmnesiacHATUnlearn.compensate_layer_if_first_task", "modulename": "clarena.unlearning_algorithms", "qualname": "AmnesiacHATUnlearn.compensate_layer_if_first_task", "kind": "function", "doc": "<p>Compensate if the first task.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">unlearning_task_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">if_first_task_layer</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">bool</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">next_masked_task_layer</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">bool</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.unlearning_algorithms.AmnesiacHATUnlearn.unlearn", "modulename": "clarena.unlearning_algorithms", "qualname": "AmnesiacHATUnlearn.unlearn", "kind": "function", "doc": "<p>Unlearn the requested unlearning tasks in current task <code>self.task_id</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.utils", "modulename": "clarena.utils", "kind": "module", "doc": "<h1 id=\"utilities\">Utilities</h1>\n\n<p>This submodule provides utilities that are used in CLArena, which includes:</p>\n\n<ul>\n<li><strong>Cfg</strong>: configuration file for CLArena.</li>\n<li><strong>Transforms</strong>: for data transforms.</li>\n<li><strong>Misc</strong>: miscellaneous functions.</li>\n</ul>\n"}, {"fullname": "clarena.utils.cfg", "modulename": "clarena.utils.cfg", "kind": "module", "doc": "<p>The submodule in <code>utils</code> with tools related to configs.</p>\n"}, {"fullname": "clarena.utils.cfg.preprocess_config", "modulename": "clarena.utils.cfg", "qualname": "preprocess_config", "kind": "function", "doc": "<p>Preprocess the configuration before constructing experiment, which include:</p>\n\n<ol>\n<li>Convert the <code>DictConfig</code> to a Rich <code>Tree</code>.</li>\n<li>Print the Rich <code>Tree</code>.</li>\n<li>Save the Rich <code>Tree</code> to a file.</li>\n</ol>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>cfg</strong> (<code>DictConfig</code>): the config dict to preprocess.</li>\n<li><strong>expr_type</strong> (<code>str</code>): the type of the experiment, should be one of the following:\n<ol>\n<li>'clmain_train': continual learning main experiment.</li>\n<li>'clmain_eval': evaluating trained continual learning main experiment.</li>\n<li>'clrefjl_train': joint learning as a reference experiment of continual learning.</li>\n<li>'clrefil_train': independent learning as a reference experiment of continual learning.</li>\n<li>'clrefrandom_train': random stratified model as a reference experiment of continual learning.</li>\n<li>'cl_eval': full evaluating trained continual learning experiment.</li>\n<li>'culmain_train': continual unlearning main experiment.</li>\n<li>'culref_train': the reference experiment of continual unlearning.</li>\n<li>'culreffull_train': the reference full experiment of continual unlearning.</li>\n<li>'cul_eval': full evaluating trained continual unlearning experiment.</li>\n<li>'mtl_train': multi-task learning experiment.</li>\n<li>'mtl_eval': evaluating trained multi-task learning experiment.</li>\n<li>'stl_train': single-task learning experiment.</li>\n<li>'stl_eval': evaluating trained single-task learning experiment.</li>\n</ol></li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>cfg</strong> (<code>DictConfig</code>): the preprocessed config dict.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span>, </span><span class=\"param\"><span class=\"n\">expr_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.utils.cfg.cfg_to_tree", "modulename": "clarena.utils.cfg", "qualname": "cfg_to_tree", "kind": "function", "doc": "<p>Convert the configuration to a Rich <code>Tree</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>cfg</strong> (<code>DictConfig</code>): the target config dict to be converted.</li>\n<li><strong>config_tree_cfg</strong> (<code>DictConfig</code>): the configuration for conversion of config tree.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>tree</strong> (<code>Tree</code>): the Rich <code>Tree</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span>,</span><span class=\"param\">\t<span class=\"n\">config_tree_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"n\">rich</span><span class=\"o\">.</span><span class=\"n\">tree</span><span class=\"o\">.</span><span class=\"n\">Tree</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.utils.cfg.save_tree_to_file", "modulename": "clarena.utils.cfg", "qualname": "save_tree_to_file", "kind": "function", "doc": "<p>Save Rich <code>Tree</code> to a file.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>tree</strong> (<code>dict</code>): the Rich <code>Tree</code> to save.</li>\n<li><strong>save_path</strong> (<code>str</code>): the path to save the tree.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">tree</span><span class=\"p\">:</span> <span class=\"nb\">dict</span>, </span><span class=\"param\"><span class=\"n\">save_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.utils.cfg.construct_unlearning_ref_config", "modulename": "clarena.utils.cfg", "qualname": "construct_unlearning_ref_config", "kind": "function", "doc": "<p>Construct the config for reference experiment to evaluate the unlearning performance, for the continual unlearning experiment whose config is given.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>cul_cfg</strong> (<code>DictConfig</code>): the config dict of the continual unlearning experiment to be evaluated.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>ulref_cfg</strong> (<code>DictConfig</code>): the  constructed unlearning reference config.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">cul_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.utils.eval", "modulename": "clarena.utils.eval", "kind": "module", "doc": "<p>The submodule in <code>utils</code> for evaluation utilities.</p>\n"}, {"fullname": "clarena.utils.eval.CULEvaluation", "modulename": "clarena.utils.eval", "qualname": "CULEvaluation", "kind": "class", "doc": "<p>Full evaluation module for continual unlearning.</p>\n", "bases": "lightning.pytorch.core.module.LightningModule"}, {"fullname": "clarena.utils.eval.CULEvaluation.__init__", "modulename": "clarena.utils.eval", "qualname": "CULEvaluation.__init__", "kind": "function", "doc": "<p>Initialize the evaluation module for continual unlearning.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>main_model</strong> (<code>CULAlgorithm</code>): the main model to evaluate.</li>\n<li><strong>ref_model</strong> (<code>CLAlgorithm</code>): the reference model to evaluate against.</li>\n<li><strong>full_model</strong> (<code>CLAlgorithm</code>): the full model that has been trained on all tasks.</li>\n<li><strong>dd_eval_task_ids</strong> (<code>list[int]</code>): the list of task IDs to evaluate the DD on.</li>\n<li><strong>ad_eval_task_ids</strong> (<code>list[int]</code>): the list of task IDs to evaluate the accuracy difference on.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">main_model</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">unlearning_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CULAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">ref_model</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">full_model</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">dd_eval_task_ids</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">ad_eval_task_ids</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span></span>)</span>"}, {"fullname": "clarena.utils.eval.CULEvaluation.criterion", "modulename": "clarena.utils.eval", "qualname": "CULEvaluation.criterion", "kind": "variable", "doc": "<p>The loss function bewteen the output logits and the target labels. Default is cross-entropy loss.</p>\n"}, {"fullname": "clarena.utils.eval.CULEvaluation.main_model", "modulename": "clarena.utils.eval", "qualname": "CULEvaluation.main_model", "kind": "variable", "doc": "<p>Store the main model for evaluation.</p>\n"}, {"fullname": "clarena.utils.eval.CULEvaluation.ref_model", "modulename": "clarena.utils.eval", "qualname": "CULEvaluation.ref_model", "kind": "variable", "doc": "<p>Store the reference model for evaluation.</p>\n"}, {"fullname": "clarena.utils.eval.CULEvaluation.full_model", "modulename": "clarena.utils.eval", "qualname": "CULEvaluation.full_model", "kind": "variable", "doc": "<p>Store the full model for evaluation.</p>\n"}, {"fullname": "clarena.utils.eval.CULEvaluation.dd_eval_task_ids", "modulename": "clarena.utils.eval", "qualname": "CULEvaluation.dd_eval_task_ids", "kind": "variable", "doc": "<p>Store the task IDs to evaluate the DD on.</p>\n", "annotation": ": list[int]"}, {"fullname": "clarena.utils.eval.CULEvaluation.ad_eval_task_ids", "modulename": "clarena.utils.eval", "qualname": "CULEvaluation.ad_eval_task_ids", "kind": "variable", "doc": "<p>Store the task IDs to evaluate the accuracy difference on.</p>\n", "annotation": ": list[int]"}, {"fullname": "clarena.utils.eval.CULEvaluation.task_id", "modulename": "clarena.utils.eval", "qualname": "CULEvaluation.task_id", "kind": "variable", "doc": "<p>Task ID counter indicating which task is being processed. Self updated during the task loop. Valid from 1 to <code>cl_dataset.num_tasks</code>.</p>\n", "annotation": ": int"}, {"fullname": "clarena.utils.eval.CULEvaluation.processed_task_ids", "modulename": "clarena.utils.eval", "qualname": "CULEvaluation.processed_task_ids", "kind": "variable", "doc": "<p>Task IDs that have been processed in the experiment.</p>\n", "annotation": ": list[int]"}, {"fullname": "clarena.utils.eval.CULEvaluation.setup_task_id", "modulename": "clarena.utils.eval", "qualname": "CULEvaluation.setup_task_id", "kind": "function", "doc": "<p>Set up which task the CUL evaluation is on. This must be done before <code>forward()</code> method is called.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.utils.eval.CULEvaluation.get_test_task_id_from_dataloader_idx", "modulename": "clarena.utils.eval", "qualname": "CULEvaluation.get_test_task_id_from_dataloader_idx", "kind": "function", "doc": "<p>Get the test task ID from the dataloader index.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>dataloader_idx</strong> (<code>int</code>): the dataloader index.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_task_id</strong> (<code>str</code>): the test task ID.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">dataloader_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">int</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.utils.eval.CULEvaluation.test_step", "modulename": "clarena.utils.eval", "qualname": "CULEvaluation.test_step", "kind": "function", "doc": "<p>Test step for current task <code>self.task_id</code>, which tests for all seen tasks indexed by <code>dataloader_idx</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of test data.</li>\n<li><strong>dataloader_idx</strong> (<code>int</code>): the task ID of seen tasks to be tested. A default value of 0 is given otherwise the LightningModule will raise a <code>RuntimeError</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and other metrics from this test step. Keys (<code>str</code>) are the metrics names, and values (<code>Tensor</code>) are the metrics.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">dataloader_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.utils.metrics", "modulename": "clarena.utils.metrics", "kind": "module", "doc": "<p>The submodule in <code>utils</code> for custom torchmetrics.</p>\n"}, {"fullname": "clarena.utils.metrics.MeanMetricBatch", "modulename": "clarena.utils.metrics", "qualname": "MeanMetricBatch", "kind": "class", "doc": "<p>A TorchMetrics metric to calculate the mean of metrics across data batches.</p>\n\n<p>This is used for accumulated metrics in deep learning. See <a href=\"https://pengxiang-wang.com/posts/continual-learning-metrics.html#nte-accumulate\">here</a> for more details.</p>\n", "bases": "torchmetrics.aggregation.BaseAggregator"}, {"fullname": "clarena.utils.metrics.MeanMetricBatch.__init__", "modulename": "clarena.utils.metrics", "qualname": "MeanMetricBatch.__init__", "kind": "function", "doc": "<p>Initialize the metric. Add state variables.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">nan_strategy</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;error&#39;</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span>)</span>"}, {"fullname": "clarena.utils.metrics.MeanMetricBatch.sum", "modulename": "clarena.utils.metrics", "qualname": "MeanMetricBatch.sum", "kind": "variable", "doc": "<p>State variable created by <code>super().__init__()</code> to store the sum of the metric values till this batch.</p>\n", "annotation": ": torch.Tensor"}, {"fullname": "clarena.utils.metrics.MeanMetricBatch.num", "modulename": "clarena.utils.metrics", "qualname": "MeanMetricBatch.num", "kind": "variable", "doc": "<p>State variable created by <code>add_state()</code> to store the number of the data till this batch.</p>\n", "annotation": ": torch.Tensor"}, {"fullname": "clarena.utils.metrics.MeanMetricBatch.update", "modulename": "clarena.utils.metrics", "qualname": "MeanMetricBatch.update", "kind": "function", "doc": "<p>Update and accumulate the sum of metric value and num of the data till this batch from the batch.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>val</strong> (<code>torch.Tensor</code>): the metric value of the batch to update the sum.</li>\n<li><strong>batch_size</strong> (<code>int</code>): the value to update the num, which is the batch size.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">value</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.utils.metrics.MeanMetricBatch.compute", "modulename": "clarena.utils.metrics", "qualname": "MeanMetricBatch.compute", "kind": "function", "doc": "<p>Compute this mean metric value till this batch.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>mean</strong> (<code>Tensor</code>): the calculated mean result.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.utils.metrics.HATNetworkCapacityMetric", "modulename": "clarena.utils.metrics", "qualname": "HATNetworkCapacityMetric", "kind": "class", "doc": "<p>A torchmetrics metric to calculate the network capacity of HAT (Hard Attention to the Task) algorithm.</p>\n\n<p>Network capacity is defined as the average adjustment rate over all parameters. See chapter 4.1 in <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>.</p>\n", "bases": "torchmetrics.aggregation.BaseAggregator"}, {"fullname": "clarena.utils.metrics.HATNetworkCapacityMetric.__init__", "modulename": "clarena.utils.metrics", "qualname": "HATNetworkCapacityMetric.__init__", "kind": "function", "doc": "<p>Initialise the HAT network capacity metric. Add state variables.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">nan_strategy</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;error&#39;</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span>)</span>"}, {"fullname": "clarena.utils.metrics.HATNetworkCapacityMetric.sum_adjustment_rate", "modulename": "clarena.utils.metrics", "qualname": "HATNetworkCapacityMetric.sum_adjustment_rate", "kind": "variable", "doc": "<p>State variable created by <code>add_state()</code> to store the sum of the adjustment rate values till this layer.</p>\n", "annotation": ": torch.Tensor"}, {"fullname": "clarena.utils.metrics.HATNetworkCapacityMetric.num_params", "modulename": "clarena.utils.metrics", "qualname": "HATNetworkCapacityMetric.num_params", "kind": "variable", "doc": "<p>State variable created by <code>add_state()</code> to store the number of the parameters till this layer.</p>\n", "annotation": ": torch.Tensor"}, {"fullname": "clarena.utils.metrics.HATNetworkCapacityMetric.update", "modulename": "clarena.utils.metrics", "qualname": "HATNetworkCapacityMetric.update", "kind": "function", "doc": "<p>Update and accumulate the sum of adjustment rate values till this layer from the layer.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>adjustment_rate_weight_layer</strong> (<code>Tensor</code>): the adjustment rate values of the weight matrix of the layer.</li>\n<li><strong>adjustment_rate_bias_layer</strong> (<code>Tensor</code>): the adjustment rate values of the bias vector of the layer.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">adjustment_rate_weight_layer</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">adjustment_rate_bias_layer</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.utils.metrics.HATNetworkCapacityMetric.compute", "modulename": "clarena.utils.metrics", "qualname": "HATNetworkCapacityMetric.compute", "kind": "function", "doc": "<p>Compute this HAT network capacity till this layer.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>network_capacity</strong> (<code>Tensor</code>): the calculated network capacity result.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.utils.misc", "modulename": "clarena.utils.misc", "kind": "module", "doc": "<p>The submodule in <code>utils</code> of miscellaneous utils.</p>\n"}, {"fullname": "clarena.utils.misc.str_to_class", "modulename": "clarena.utils.misc", "qualname": "str_to_class", "kind": "function", "doc": "<p>Convert a string to a class.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>class_path</strong> (<code>str</code>): the string of the class path, e.g. <code>torchvision.datasets.MNIST</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>cls</strong> (<code>type</code>): the class object.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">class_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">type</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.utils.transforms", "modulename": "clarena.utils.transforms", "kind": "module", "doc": "<p>The submodule in <code>utils</code> for transforming tensors.</p>\n"}, {"fullname": "clarena.utils.transforms.ClassMapping", "modulename": "clarena.utils.transforms", "qualname": "ClassMapping", "kind": "class", "doc": "<p>Class mapping to dataset labels. Used as a PyTorch target Transform.</p>\n"}, {"fullname": "clarena.utils.transforms.ClassMapping.__init__", "modulename": "clarena.utils.transforms", "qualname": "ClassMapping.__init__", "kind": "function", "doc": "<p>Initialize the class mapping transform object from the class map.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>cl_class_map</strong> (<code>dict[str | int, int]</code>): the class map.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">class_map</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span></span>)</span>"}, {"fullname": "clarena.utils.transforms.ClassMapping.class_map", "modulename": "clarena.utils.transforms", "qualname": "ClassMapping.class_map", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "clarena.utils.transforms.Permute", "modulename": "clarena.utils.transforms", "qualname": "Permute", "kind": "class", "doc": "<p>Permutation operation to image. Used to construct permuted CL dataset.</p>\n\n<p>Used as a PyTorch Dataset Transform.</p>\n"}, {"fullname": "clarena.utils.transforms.Permute.__init__", "modulename": "clarena.utils.transforms", "qualname": "Permute.__init__", "kind": "function", "doc": "<p>Initialize the Permute transform object. The permutation order is constructed in the initialization to save runtime.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>num_channels</strong> (<code>int</code>): the number of channels in the image.</li>\n<li><strong>img_size</strong> (<code>torch.Size</code>): the size of the image to be permuted.</li>\n<li><strong>mode</strong> (<code>str</code>): the mode of permutation, shouble be one of the following:\n<ul>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ul></li>\n<li><strong>seed</strong> (<code>int</code> or <code>None</code>): seed for permutation operation. If None, the permutation will use a default seed from PyTorch generator.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">num_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">img_size</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Size</span>,</span><span class=\"param\">\t<span class=\"n\">mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">seed</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.utils.transforms.Permute.mode", "modulename": "clarena.utils.transforms", "qualname": "Permute.mode", "kind": "variable", "doc": "<p>Store the mode of permutation.</p>\n"}, {"fullname": "clarena.utils.transforms.Permute.permute", "modulename": "clarena.utils.transforms", "qualname": "Permute.permute", "kind": "variable", "doc": "<p>The permutation order, a <code>Tensor</code> permuted from [1,2, ..., <code>num_pixels</code>] with the given seed. It is the core element of permutation operation.</p>\n", "annotation": ": torch.Tensor"}, {"fullname": "clarena.utils.transforms.min_max_normalize", "modulename": "clarena.utils.transforms", "qualname": "min_max_normalize", "kind": "function", "doc": "<p>Normalize the tensor using min-max normalization.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>tensor</strong> (<code>Tensor</code>): the input tensor to normalize.</li>\n<li><strong>dim</strong> (<code>int</code> | <code>None</code>): the dimension to normalize along. If <code>None</code>, normalize the whole tensor.</li>\n<li><strong>epsilon</strong> (<code>float</code>): the epsilon value to avoid division by zero.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>tensor</strong> (<code>Tensor</code>): the normalized tensor.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">tensor</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">epsilon</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1e-08</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.utils.transforms.js_div", "modulename": "clarena.utils.transforms", "qualname": "js_div", "kind": "function", "doc": "<p>Jensen-Shannon divergence between two probability distributions.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">target</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">size_average</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">reduce</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}];

    // mirrored in build-search-index.js (part 1)
    // Also split on html tags. this is a cheap heuristic, but good enough.
    elasticlunr.tokenizer.setSeperator(/[\s\-.;&_'"=,()]+|<[^>]*>/);

    let searchIndex;
    if (docs._isPrebuiltIndex) {
        console.info("using precompiled search index");
        searchIndex = elasticlunr.Index.load(docs);
    } else {
        console.time("building search index");
        // mirrored in build-search-index.js (part 2)
        searchIndex = elasticlunr(function () {
            this.pipeline.remove(elasticlunr.stemmer);
            this.pipeline.remove(elasticlunr.stopWordFilter);
            this.addField("qualname");
            this.addField("fullname");
            this.addField("annotation");
            this.addField("default_value");
            this.addField("signature");
            this.addField("bases");
            this.addField("doc");
            this.setRef("fullname");
        });
        for (let doc of docs) {
            searchIndex.addDoc(doc);
        }
        console.timeEnd("building search index");
    }

    return (term) => searchIndex.search(term, {
        fields: {
            qualname: {boost: 4},
            fullname: {boost: 2},
            annotation: {boost: 2},
            default_value: {boost: 2},
            signature: {boost: 2},
            bases: {boost: 2},
            doc: {boost: 1},
        },
        expand: true
    });
})();