<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="pdoc 15.0.1"/>
    <title>clarena.cl_algorithms.hat API documentation</title>

    <style>/*! * Bootstrap Reboot v5.0.0 (https://getbootstrap.com/) * Copyright 2011-2021 The Bootstrap Authors * Copyright 2011-2021 Twitter, Inc. * Licensed under MIT (https://github.com/twbs/bootstrap/blob/main/LICENSE) * Forked from Normalize.css, licensed MIT (https://github.com/necolas/normalize.css/blob/master/LICENSE.md) */*,::after,::before{box-sizing:border-box}@media (prefers-reduced-motion:no-preference){:root{scroll-behavior:smooth}}body{margin:0;font-family:system-ui,-apple-system,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans","Liberation Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";font-size:1rem;font-weight:400;line-height:1.5;color:#212529;background-color:#fff;-webkit-text-size-adjust:100%;-webkit-tap-highlight-color:transparent}hr{margin:1rem 0;color:inherit;background-color:currentColor;border:0;opacity:.25}hr:not([size]){height:1px}h1,h2,h3,h4,h5,h6{margin-top:0;margin-bottom:.5rem;font-weight:500;line-height:1.2}h1{font-size:calc(1.375rem + 1.5vw)}@media (min-width:1200px){h1{font-size:2.5rem}}h2{font-size:calc(1.325rem + .9vw)}@media (min-width:1200px){h2{font-size:2rem}}h3{font-size:calc(1.3rem + .6vw)}@media (min-width:1200px){h3{font-size:1.75rem}}h4{font-size:calc(1.275rem + .3vw)}@media (min-width:1200px){h4{font-size:1.5rem}}h5{font-size:1.25rem}h6{font-size:1rem}p{margin-top:0;margin-bottom:1rem}abbr[data-bs-original-title],abbr[title]{-webkit-text-decoration:underline dotted;text-decoration:underline dotted;cursor:help;-webkit-text-decoration-skip-ink:none;text-decoration-skip-ink:none}address{margin-bottom:1rem;font-style:normal;line-height:inherit}ol,ul{padding-left:2rem}dl,ol,ul{margin-top:0;margin-bottom:1rem}ol ol,ol ul,ul ol,ul ul{margin-bottom:0}dt{font-weight:700}dd{margin-bottom:.5rem;margin-left:0}blockquote{margin:0 0 1rem}b,strong{font-weight:bolder}small{font-size:.875em}mark{padding:.2em;background-color:#fcf8e3}sub,sup{position:relative;font-size:.75em;line-height:0;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}a{color:#0d6efd;text-decoration:underline}a:hover{color:#0a58ca}a:not([href]):not([class]),a:not([href]):not([class]):hover{color:inherit;text-decoration:none}code,kbd,pre,samp{font-family:SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;font-size:1em;direction:ltr;unicode-bidi:bidi-override}pre{display:block;margin-top:0;margin-bottom:1rem;overflow:auto;font-size:.875em}pre code{font-size:inherit;color:inherit;word-break:normal}code{font-size:.875em;color:#d63384;word-wrap:break-word}a>code{color:inherit}kbd{padding:.2rem .4rem;font-size:.875em;color:#fff;background-color:#212529;border-radius:.2rem}kbd kbd{padding:0;font-size:1em;font-weight:700}figure{margin:0 0 1rem}img,svg{vertical-align:middle}table{caption-side:bottom;border-collapse:collapse}caption{padding-top:.5rem;padding-bottom:.5rem;color:#6c757d;text-align:left}th{text-align:inherit;text-align:-webkit-match-parent}tbody,td,tfoot,th,thead,tr{border-color:inherit;border-style:solid;border-width:0}label{display:inline-block}button{border-radius:0}button:focus:not(:focus-visible){outline:0}button,input,optgroup,select,textarea{margin:0;font-family:inherit;font-size:inherit;line-height:inherit}button,select{text-transform:none}[role=button]{cursor:pointer}select{word-wrap:normal}select:disabled{opacity:1}[list]::-webkit-calendar-picker-indicator{display:none}[type=button],[type=reset],[type=submit],button{-webkit-appearance:button}[type=button]:not(:disabled),[type=reset]:not(:disabled),[type=submit]:not(:disabled),button:not(:disabled){cursor:pointer}::-moz-focus-inner{padding:0;border-style:none}textarea{resize:vertical}fieldset{min-width:0;padding:0;margin:0;border:0}legend{float:left;width:100%;padding:0;margin-bottom:.5rem;font-size:calc(1.275rem + .3vw);line-height:inherit}@media (min-width:1200px){legend{font-size:1.5rem}}legend+*{clear:left}::-webkit-datetime-edit-day-field,::-webkit-datetime-edit-fields-wrapper,::-webkit-datetime-edit-hour-field,::-webkit-datetime-edit-minute,::-webkit-datetime-edit-month-field,::-webkit-datetime-edit-text,::-webkit-datetime-edit-year-field{padding:0}::-webkit-inner-spin-button{height:auto}[type=search]{outline-offset:-2px;-webkit-appearance:textfield}::-webkit-search-decoration{-webkit-appearance:none}::-webkit-color-swatch-wrapper{padding:0}::file-selector-button{font:inherit}::-webkit-file-upload-button{font:inherit;-webkit-appearance:button}output{display:inline-block}iframe{border:0}summary{display:list-item;cursor:pointer}progress{vertical-align:baseline}[hidden]{display:none!important}</style>
    <style>/*! syntax-highlighting.css */pre{line-height:125%;}span.linenos{color:inherit; background-color:transparent; padding-left:5px; padding-right:20px;}.pdoc-code .hll{background-color:#ffffcc}.pdoc-code{background:#f8f8f8;}.pdoc-code .c{color:#3D7B7B; font-style:italic}.pdoc-code .err{border:1px solid #FF0000}.pdoc-code .k{color:#008000; font-weight:bold}.pdoc-code .o{color:#666666}.pdoc-code .ch{color:#3D7B7B; font-style:italic}.pdoc-code .cm{color:#3D7B7B; font-style:italic}.pdoc-code .cp{color:#9C6500}.pdoc-code .cpf{color:#3D7B7B; font-style:italic}.pdoc-code .c1{color:#3D7B7B; font-style:italic}.pdoc-code .cs{color:#3D7B7B; font-style:italic}.pdoc-code .gd{color:#A00000}.pdoc-code .ge{font-style:italic}.pdoc-code .gr{color:#E40000}.pdoc-code .gh{color:#000080; font-weight:bold}.pdoc-code .gi{color:#008400}.pdoc-code .go{color:#717171}.pdoc-code .gp{color:#000080; font-weight:bold}.pdoc-code .gs{font-weight:bold}.pdoc-code .gu{color:#800080; font-weight:bold}.pdoc-code .gt{color:#0044DD}.pdoc-code .kc{color:#008000; font-weight:bold}.pdoc-code .kd{color:#008000; font-weight:bold}.pdoc-code .kn{color:#008000; font-weight:bold}.pdoc-code .kp{color:#008000}.pdoc-code .kr{color:#008000; font-weight:bold}.pdoc-code .kt{color:#B00040}.pdoc-code .m{color:#666666}.pdoc-code .s{color:#BA2121}.pdoc-code .na{color:#687822}.pdoc-code .nb{color:#008000}.pdoc-code .nc{color:#0000FF; font-weight:bold}.pdoc-code .no{color:#880000}.pdoc-code .nd{color:#AA22FF}.pdoc-code .ni{color:#717171; font-weight:bold}.pdoc-code .ne{color:#CB3F38; font-weight:bold}.pdoc-code .nf{color:#0000FF}.pdoc-code .nl{color:#767600}.pdoc-code .nn{color:#0000FF; font-weight:bold}.pdoc-code .nt{color:#008000; font-weight:bold}.pdoc-code .nv{color:#19177C}.pdoc-code .ow{color:#AA22FF; font-weight:bold}.pdoc-code .w{color:#bbbbbb}.pdoc-code .mb{color:#666666}.pdoc-code .mf{color:#666666}.pdoc-code .mh{color:#666666}.pdoc-code .mi{color:#666666}.pdoc-code .mo{color:#666666}.pdoc-code .sa{color:#BA2121}.pdoc-code .sb{color:#BA2121}.pdoc-code .sc{color:#BA2121}.pdoc-code .dl{color:#BA2121}.pdoc-code .sd{color:#BA2121; font-style:italic}.pdoc-code .s2{color:#BA2121}.pdoc-code .se{color:#AA5D1F; font-weight:bold}.pdoc-code .sh{color:#BA2121}.pdoc-code .si{color:#A45A77; font-weight:bold}.pdoc-code .sx{color:#008000}.pdoc-code .sr{color:#A45A77}.pdoc-code .s1{color:#BA2121}.pdoc-code .ss{color:#19177C}.pdoc-code .bp{color:#008000}.pdoc-code .fm{color:#0000FF}.pdoc-code .vc{color:#19177C}.pdoc-code .vg{color:#19177C}.pdoc-code .vi{color:#19177C}.pdoc-code .vm{color:#19177C}.pdoc-code .il{color:#666666}</style>
    <style>/*! theme.css */:root{--pdoc-background:#fff;}.pdoc{--text:#212529;--muted:#6c757d;--link:#3660a5;--link-hover:#1659c5;--code:#f8f8f8;--active:#fff598;--accent:#eee;--accent2:#c1c1c1;--nav-hover:rgba(255, 255, 255, 0.5);--name:#0066BB;--def:#008800;--annotation:#007020;}</style>
    <style>/*! layout.css */html, body{width:100%;height:100%;}html, main{scroll-behavior:smooth;}body{background-color:var(--pdoc-background);}@media (max-width:769px){#navtoggle{cursor:pointer;position:absolute;width:50px;height:40px;top:1rem;right:1rem;border-color:var(--text);color:var(--text);display:flex;opacity:0.8;z-index:999;}#navtoggle:hover{opacity:1;}#togglestate + div{display:none;}#togglestate:checked + div{display:inherit;}main, header{padding:2rem 3vw;}header + main{margin-top:-3rem;}.git-button{display:none !important;}nav input[type="search"]{max-width:77%;}nav input[type="search"]:first-child{margin-top:-6px;}nav input[type="search"]:valid ~ *{display:none !important;}}@media (min-width:770px){:root{--sidebar-width:clamp(12.5rem, 28vw, 22rem);}nav{position:fixed;overflow:auto;height:100vh;width:var(--sidebar-width);}main, header{padding:3rem 2rem 3rem calc(var(--sidebar-width) + 3rem);width:calc(54rem + var(--sidebar-width));max-width:100%;}header + main{margin-top:-4rem;}#navtoggle{display:none;}}#togglestate{position:absolute;height:0;opacity:0;}nav.pdoc{--pad:clamp(0.5rem, 2vw, 1.75rem);--indent:1.5rem;background-color:var(--accent);border-right:1px solid var(--accent2);box-shadow:0 0 20px rgba(50, 50, 50, .2) inset;padding:0 0 0 var(--pad);overflow-wrap:anywhere;scrollbar-width:thin; scrollbar-color:var(--accent2) transparent; z-index:1}nav.pdoc::-webkit-scrollbar{width:.4rem; }nav.pdoc::-webkit-scrollbar-thumb{background-color:var(--accent2); }nav.pdoc > div{padding:var(--pad) 0;}nav.pdoc .module-list-button{display:inline-flex;align-items:center;color:var(--text);border-color:var(--muted);margin-bottom:1rem;}nav.pdoc .module-list-button:hover{border-color:var(--text);}nav.pdoc input[type=search]{display:block;outline-offset:0;width:calc(100% - var(--pad));}nav.pdoc .logo{max-width:calc(100% - var(--pad));max-height:35vh;display:block;margin:0 auto 1rem;transform:translate(calc(-.5 * var(--pad)), 0);}nav.pdoc ul{list-style:none;padding-left:0;}nav.pdoc > div > ul{margin-left:calc(0px - var(--pad));}nav.pdoc li a{padding:.2rem 0 .2rem calc(var(--pad) + var(--indent));}nav.pdoc > div > ul > li > a{padding-left:var(--pad);}nav.pdoc li{transition:all 100ms;}nav.pdoc li:hover{background-color:var(--nav-hover);}nav.pdoc a, nav.pdoc a:hover{color:var(--text);}nav.pdoc a{display:block;}nav.pdoc > h2:first-of-type{margin-top:1.5rem;}nav.pdoc .class:before{content:"class ";color:var(--muted);}nav.pdoc .function:after{content:"()";color:var(--muted);}nav.pdoc footer:before{content:"";display:block;width:calc(100% - var(--pad));border-top:solid var(--accent2) 1px;margin-top:1.5rem;padding-top:.5rem;}nav.pdoc footer{font-size:small;}</style>
    <style>/*! content.css */.pdoc{color:var(--text);box-sizing:border-box;line-height:1.5;background:none;}.pdoc .pdoc-button{cursor:pointer;display:inline-block;border:solid black 1px;border-radius:2px;font-size:.75rem;padding:calc(0.5em - 1px) 1em;transition:100ms all;}.pdoc .alert{padding:1rem 1rem 1rem calc(1.5rem + 24px);border:1px solid transparent;border-radius:.25rem;background-repeat:no-repeat;background-position:.75rem center;margin-bottom:1rem;}.pdoc .alert > em{display:none;}.pdoc .alert > *:last-child{margin-bottom:0;}.pdoc .alert.note {color:#084298;background-color:#cfe2ff;border-color:#b6d4fe;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23084298%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8%2016A8%208%200%201%200%208%200a8%208%200%200%200%200%2016zm.93-9.412-1%204.705c-.07.34.029.533.304.533.194%200%20.487-.07.686-.246l-.088.416c-.287.346-.92.598-1.465.598-.703%200-1.002-.422-.808-1.319l.738-3.468c.064-.293.006-.399-.287-.47l-.451-.081.082-.381%202.29-.287zM8%205.5a1%201%200%201%201%200-2%201%201%200%200%201%200%202z%22/%3E%3C/svg%3E");}.pdoc .alert.warning{color:#664d03;background-color:#fff3cd;border-color:#ffecb5;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23664d03%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8.982%201.566a1.13%201.13%200%200%200-1.96%200L.165%2013.233c-.457.778.091%201.767.98%201.767h13.713c.889%200%201.438-.99.98-1.767L8.982%201.566zM8%205c.535%200%20.954.462.9.995l-.35%203.507a.552.552%200%200%201-1.1%200L7.1%205.995A.905.905%200%200%201%208%205zm.002%206a1%201%200%201%201%200%202%201%201%200%200%201%200-2z%22/%3E%3C/svg%3E");}.pdoc .alert.danger{color:#842029;background-color:#f8d7da;border-color:#f5c2c7;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23842029%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M5.52.359A.5.5%200%200%201%206%200h4a.5.5%200%200%201%20.474.658L8.694%206H12.5a.5.5%200%200%201%20.395.807l-7%209a.5.5%200%200%201-.873-.454L6.823%209.5H3.5a.5.5%200%200%201-.48-.641l2.5-8.5z%22/%3E%3C/svg%3E");}.pdoc .visually-hidden{position:absolute !important;width:1px !important;height:1px !important;padding:0 !important;margin:-1px !important;overflow:hidden !important;clip:rect(0, 0, 0, 0) !important;white-space:nowrap !important;border:0 !important;}.pdoc h1, .pdoc h2, .pdoc h3{font-weight:300;margin:.3em 0;padding:.2em 0;}.pdoc > section:not(.module-info) h1{font-size:1.5rem;font-weight:500;}.pdoc > section:not(.module-info) h2{font-size:1.4rem;font-weight:500;}.pdoc > section:not(.module-info) h3{font-size:1.3rem;font-weight:500;}.pdoc > section:not(.module-info) h4{font-size:1.2rem;}.pdoc > section:not(.module-info) h5{font-size:1.1rem;}.pdoc a{text-decoration:none;color:var(--link);}.pdoc a:hover{color:var(--link-hover);}.pdoc blockquote{margin-left:2rem;}.pdoc pre{border-top:1px solid var(--accent2);border-bottom:1px solid var(--accent2);margin-top:0;margin-bottom:1em;padding:.5rem 0 .5rem .5rem;overflow-x:auto;background-color:var(--code);}.pdoc code{color:var(--text);padding:.2em .4em;margin:0;font-size:85%;background-color:var(--accent);border-radius:6px;}.pdoc a > code{color:inherit;}.pdoc pre > code{display:inline-block;font-size:inherit;background:none;border:none;padding:0;}.pdoc > section:not(.module-info){margin-bottom:1.5rem;}.pdoc .modulename{margin-top:0;font-weight:bold;}.pdoc .modulename a{color:var(--link);transition:100ms all;}.pdoc .git-button{float:right;border:solid var(--link) 1px;}.pdoc .git-button:hover{background-color:var(--link);color:var(--pdoc-background);}.view-source-toggle-state,.view-source-toggle-state ~ .pdoc-code{display:none;}.view-source-toggle-state:checked ~ .pdoc-code{display:block;}.view-source-button{display:inline-block;float:right;font-size:.75rem;line-height:1.5rem;color:var(--muted);padding:0 .4rem 0 1.3rem;cursor:pointer;text-indent:-2px;}.view-source-button > span{visibility:hidden;}.module-info .view-source-button{float:none;display:flex;justify-content:flex-end;margin:-1.2rem .4rem -.2rem 0;}.view-source-button::before{position:absolute;content:"View Source";display:list-item;list-style-type:disclosure-closed;}.view-source-toggle-state:checked ~ .attr .view-source-button::before,.view-source-toggle-state:checked ~ .view-source-button::before{list-style-type:disclosure-open;}.pdoc .docstring{margin-bottom:1.5rem;}.pdoc section:not(.module-info) .docstring{margin-left:clamp(0rem, 5vw - 2rem, 1rem);}.pdoc .docstring .pdoc-code{margin-left:1em;margin-right:1em;}.pdoc h1:target,.pdoc h2:target,.pdoc h3:target,.pdoc h4:target,.pdoc h5:target,.pdoc h6:target,.pdoc .pdoc-code > pre > span:target{background-color:var(--active);box-shadow:-1rem 0 0 0 var(--active);}.pdoc .pdoc-code > pre > span:target{display:block;}.pdoc div:target > .attr,.pdoc section:target > .attr,.pdoc dd:target > a{background-color:var(--active);}.pdoc *{scroll-margin:2rem;}.pdoc .pdoc-code .linenos{user-select:none;}.pdoc .attr:hover{filter:contrast(0.95);}.pdoc section, .pdoc .classattr{position:relative;}.pdoc .headerlink{--width:clamp(1rem, 3vw, 2rem);position:absolute;top:0;left:calc(0rem - var(--width));transition:all 100ms ease-in-out;opacity:0;}.pdoc .headerlink::before{content:"#";display:block;text-align:center;width:var(--width);height:2.3rem;line-height:2.3rem;font-size:1.5rem;}.pdoc .attr:hover ~ .headerlink,.pdoc *:target > .headerlink,.pdoc .headerlink:hover{opacity:1;}.pdoc .attr{display:block;margin:.5rem 0 .5rem;padding:.4rem .4rem .4rem 1rem;background-color:var(--accent);overflow-x:auto;}.pdoc .classattr{margin-left:2rem;}.pdoc .decorator-deprecated{color:#842029;}.pdoc .decorator-deprecated ~ span{filter:grayscale(1) opacity(0.8);}.pdoc .name{color:var(--name);font-weight:bold;}.pdoc .def{color:var(--def);font-weight:bold;}.pdoc .signature{background-color:transparent;}.pdoc .param, .pdoc .return-annotation{white-space:pre;}.pdoc .signature.multiline .param{display:block;}.pdoc .signature.condensed .param{display:inline-block;}.pdoc .annotation{color:var(--annotation);}.pdoc .view-value-toggle-state,.pdoc .view-value-toggle-state ~ .default_value{display:none;}.pdoc .view-value-toggle-state:checked ~ .default_value{display:inherit;}.pdoc .view-value-button{font-size:.5rem;vertical-align:middle;border-style:dashed;margin-top:-0.1rem;}.pdoc .view-value-button:hover{background:white;}.pdoc .view-value-button::before{content:"show";text-align:center;width:2.2em;display:inline-block;}.pdoc .view-value-toggle-state:checked ~ .view-value-button::before{content:"hide";}.pdoc .inherited{margin-left:2rem;}.pdoc .inherited dt{font-weight:700;}.pdoc .inherited dt, .pdoc .inherited dd{display:inline;margin-left:0;margin-bottom:.5rem;}.pdoc .inherited dd:not(:last-child):after{content:", ";}.pdoc .inherited .class:before{content:"class ";}.pdoc .inherited .function a:after{content:"()";}.pdoc .search-result .docstring{overflow:auto;max-height:25vh;}.pdoc .search-result.focused > .attr{background-color:var(--active);}.pdoc .attribution{margin-top:2rem;display:block;opacity:0.5;transition:all 200ms;filter:grayscale(100%);}.pdoc .attribution:hover{opacity:1;filter:grayscale(0%);}.pdoc .attribution img{margin-left:5px;height:35px;vertical-align:middle;width:70px;transition:all 200ms;}.pdoc table{display:block;width:max-content;max-width:100%;overflow:auto;margin-bottom:1rem;}.pdoc table th{font-weight:600;}.pdoc table th, .pdoc table td{padding:6px 13px;border:1px solid var(--accent2);}</style>
    <style>/*! custom.css */</style><script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']]
        }
    };
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>
    /* Re-invoke MathJax when DOM content changes, for example during search. */
    document.addEventListener("DOMContentLoaded", () => {
        new MutationObserver(() => MathJax.typeset()).observe(
            document.querySelector("main.pdoc").parentNode,
            {childList: true}
        );
    })
</script>
<style>
    mjx-container {
        overflow-x: auto;
        overflow-y: hidden;
    }
</style></head>
<body>
    <nav class="pdoc">
        <label id="navtoggle" for="togglestate" class="pdoc-button"><svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 30 30'><path stroke-linecap='round' stroke="currentColor" stroke-miterlimit='10' stroke-width='2' d='M4 7h22M4 15h22M4 23h22'/></svg></label>
        <input id="togglestate" type="checkbox" aria-hidden="true" tabindex="-1">
        <div>            <a class="pdoc-button module-list-button" href="../cl_algorithms.html">
<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-box-arrow-in-left" viewBox="0 0 16 16">
  <path fill-rule="evenodd" d="M10 3.5a.5.5 0 0 0-.5-.5h-8a.5.5 0 0 0-.5.5v9a.5.5 0 0 0 .5.5h8a.5.5 0 0 0 .5-.5v-2a.5.5 0 0 1 1 0v2A1.5 1.5 0 0 1 9.5 14h-8A1.5 1.5 0 0 1 0 12.5v-9A1.5 1.5 0 0 1 1.5 2h8A1.5 1.5 0 0 1 11 3.5v2a.5.5 0 0 1-1 0v-2z"/>
  <path fill-rule="evenodd" d="M4.146 8.354a.5.5 0 0 1 0-.708l3-3a.5.5 0 1 1 .708.708L5.707 7.5H14.5a.5.5 0 0 1 0 1H5.707l2.147 2.146a.5.5 0 0 1-.708.708l-3-3z"/>
</svg>                &nbsp;clarena.cl_algorithms</a>


            <input type="search" placeholder="Search..." role="searchbox" aria-label="search"
                   pattern=".+" required>



            <h2>API Documentation</h2>
                <ul class="memberlist">
            <li>
                    <a class="class" href="#HAT">HAT</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#HAT.__init__">HAT</a>
                        </li>
                        <li>
                                <a class="variable" href="#HAT.adjustment_mode">adjustment_mode</a>
                        </li>
                        <li>
                                <a class="variable" href="#HAT.s_max">s_max</a>
                        </li>
                        <li>
                                <a class="variable" href="#HAT.clamp_threshold">clamp_threshold</a>
                        </li>
                        <li>
                                <a class="variable" href="#HAT.mask_sparsity_reg_factor">mask_sparsity_reg_factor</a>
                        </li>
                        <li>
                                <a class="variable" href="#HAT.mask_sparsity_reg_mode">mask_sparsity_reg_mode</a>
                        </li>
                        <li>
                                <a class="variable" href="#HAT.mark_sparsity_reg">mark_sparsity_reg</a>
                        </li>
                        <li>
                                <a class="variable" href="#HAT.task_embedding_init_mode">task_embedding_init_mode</a>
                        </li>
                        <li>
                                <a class="variable" href="#HAT.alpha">alpha</a>
                        </li>
                        <li>
                                <a class="variable" href="#HAT.cumulative_mask_for_previous_tasks">cumulative_mask_for_previous_tasks</a>
                        </li>
                        <li>
                                <a class="variable" href="#HAT.automatic_optimization">automatic_optimization</a>
                        </li>
                        <li>
                                <a class="function" href="#HAT.sanity_check">sanity_check</a>
                        </li>
                        <li>
                                <a class="function" href="#HAT.on_train_start">on_train_start</a>
                        </li>
                        <li>
                                <a class="function" href="#HAT.clip_grad_by_adjustment">clip_grad_by_adjustment</a>
                        </li>
                        <li>
                                <a class="function" href="#HAT.compensate_task_embedding_gradients">compensate_task_embedding_gradients</a>
                        </li>
                        <li>
                                <a class="function" href="#HAT.forward">forward</a>
                        </li>
                        <li>
                                <a class="function" href="#HAT.training_step">training_step</a>
                        </li>
                        <li>
                                <a class="function" href="#HAT.on_train_end">on_train_end</a>
                        </li>
                        <li>
                                <a class="function" href="#HAT.validation_step">validation_step</a>
                        </li>
                        <li>
                                <a class="function" href="#HAT.test_step">test_step</a>
                        </li>
                </ul>

            </li>
    </ul>



        <a class="attribution" title="pdoc: Python API documentation generator" href="https://pdoc.dev" target="_blank">
            built with <span class="visually-hidden">pdoc</span><img
                alt="pdoc logo"
                src="data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20role%3D%22img%22%20aria-label%3D%22pdoc%20logo%22%20width%3D%22300%22%20height%3D%22150%22%20viewBox%3D%22-1%200%2060%2030%22%3E%3Ctitle%3Epdoc%3C/title%3E%3Cpath%20d%3D%22M29.621%2021.293c-.011-.273-.214-.475-.511-.481a.5.5%200%200%200-.489.503l-.044%201.393c-.097.551-.695%201.215-1.566%201.704-.577.428-1.306.486-2.193.182-1.426-.617-2.467-1.654-3.304-2.487l-.173-.172a3.43%203.43%200%200%200-.365-.306.49.49%200%200%200-.286-.196c-1.718-1.06-4.931-1.47-7.353.191l-.219.15c-1.707%201.187-3.413%202.131-4.328%201.03-.02-.027-.49-.685-.141-1.763.233-.721.546-2.408.772-4.076.042-.09.067-.187.046-.288.166-1.347.277-2.625.241-3.351%201.378-1.008%202.271-2.586%202.271-4.362%200-.976-.272-1.935-.788-2.774-.057-.094-.122-.18-.184-.268.033-.167.052-.339.052-.516%200-1.477-1.202-2.679-2.679-2.679-.791%200-1.496.352-1.987.9a6.3%206.3%200%200%200-1.001.029c-.492-.564-1.207-.929-2.012-.929-1.477%200-2.679%201.202-2.679%202.679A2.65%202.65%200%200%200%20.97%206.554c-.383.747-.595%201.572-.595%202.41%200%202.311%201.507%204.29%203.635%205.107-.037.699-.147%202.27-.423%203.294l-.137.461c-.622%202.042-2.515%208.257%201.727%2010.643%201.614.908%203.06%201.248%204.317%201.248%202.665%200%204.492-1.524%205.322-2.401%201.476-1.559%202.886-1.854%206.491.82%201.877%201.393%203.514%201.753%204.861%201.068%202.223-1.713%202.811-3.867%203.399-6.374.077-.846.056-1.469.054-1.537zm-4.835%204.313c-.054.305-.156.586-.242.629-.034-.007-.131-.022-.307-.157-.145-.111-.314-.478-.456-.908.221.121.432.25.675.355.115.039.219.051.33.081zm-2.251-1.238c-.05.33-.158.648-.252.694-.022.001-.125-.018-.307-.157-.217-.166-.488-.906-.639-1.573.358.344.754.693%201.198%201.036zm-3.887-2.337c-.006-.116-.018-.231-.041-.342.635.145%201.189.368%201.599.625.097.231.166.481.174.642-.03.049-.055.101-.067.158-.046.013-.128.026-.298.004-.278-.037-.901-.57-1.367-1.087zm-1.127-.497c.116.306.176.625.12.71-.019.014-.117.045-.345.016-.206-.027-.604-.332-.986-.695.41-.051.816-.056%201.211-.031zm-4.535%201.535c.209.22.379.47.358.598-.006.041-.088.138-.351.234-.144.055-.539-.063-.979-.259a11.66%2011.66%200%200%200%20.972-.573zm.983-.664c.359-.237.738-.418%201.126-.554.25.237.479.548.457.694-.006.042-.087.138-.351.235-.174.064-.694-.105-1.232-.375zm-3.381%201.794c-.022.145-.061.29-.149.401-.133.166-.358.248-.69.251h-.002c-.133%200-.306-.26-.45-.621.417.091.854.07%201.291-.031zm-2.066-8.077a4.78%204.78%200%200%201-.775-.584c.172-.115.505-.254.88-.378l-.105.962zm-.331%202.302a10.32%2010.32%200%200%201-.828-.502c.202-.143.576-.328.984-.49l-.156.992zm-.45%202.157l-.701-.403c.214-.115.536-.249.891-.376a11.57%2011.57%200%200%201-.19.779zm-.181%201.716c.064.398.194.702.298.893-.194-.051-.435-.162-.736-.398.061-.119.224-.3.438-.495zM8.87%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zm-.735-.389a1.15%201.15%200%200%200-.314.783%201.16%201.16%200%200%200%201.162%201.162c.457%200%20.842-.27%201.032-.653.026.117.042.238.042.362a1.68%201.68%200%200%201-1.679%201.679%201.68%201.68%200%200%201-1.679-1.679c0-.843.626-1.535%201.436-1.654zM5.059%205.406A1.68%201.68%200%200%201%203.38%207.085a1.68%201.68%200%200%201-1.679-1.679c0-.037.009-.072.011-.109.21.3.541.508.935.508a1.16%201.16%200%200%200%201.162-1.162%201.14%201.14%200%200%200-.474-.912c.015%200%20.03-.005.045-.005.926.001%201.679.754%201.679%201.68zM3.198%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zM1.375%208.964c0-.52.103-1.035.288-1.52.466.394%201.06.64%201.717.64%201.144%200%202.116-.725%202.499-1.738.383%201.012%201.355%201.738%202.499%201.738.867%200%201.631-.421%202.121-1.062.307.605.478%201.267.478%201.942%200%202.486-2.153%204.51-4.801%204.51s-4.801-2.023-4.801-4.51zm24.342%2019.349c-.985.498-2.267.168-3.813-.979-3.073-2.281-5.453-3.199-7.813-.705-1.315%201.391-4.163%203.365-8.423.97-3.174-1.786-2.239-6.266-1.261-9.479l.146-.492c.276-1.02.395-2.457.444-3.268a6.11%206.11%200%200%200%201.18.115%206.01%206.01%200%200%200%202.536-.562l-.006.175c-.802.215-1.848.612-2.021%201.25-.079.295.021.601.274.837.219.203.415.364.598.501-.667.304-1.243.698-1.311%201.179-.02.144-.022.507.393.787.213.144.395.26.564.365-1.285.521-1.361.96-1.381%201.126-.018.142-.011.496.427.746l.854.489c-.473.389-.971.914-.999%201.429-.018.278.095.532.316.713.675.556%201.231.721%201.653.721.059%200%20.104-.014.158-.02.207.707.641%201.64%201.513%201.64h.013c.8-.008%201.236-.345%201.462-.626.173-.216.268-.457.325-.692.424.195.93.374%201.372.374.151%200%20.294-.021.423-.068.732-.27.944-.704.993-1.021.009-.061.003-.119.002-.179.266.086.538.147.789.147.15%200%20.294-.021.423-.069.542-.2.797-.489.914-.754.237.147.478.258.704.288.106.014.205.021.296.021.356%200%20.595-.101.767-.229.438.435%201.094.992%201.656%201.067.106.014.205.021.296.021a1.56%201.56%200%200%200%20.323-.035c.17.575.453%201.289.866%201.605.358.273.665.362.914.362a.99.99%200%200%200%20.421-.093%201.03%201.03%200%200%200%20.245-.164c.168.428.39.846.68%201.068.358.273.665.362.913.362a.99.99%200%200%200%20.421-.093c.317-.148.512-.448.639-.762.251.157.495.257.726.257.127%200%20.25-.024.37-.071.427-.17.706-.617.841-1.314.022-.015.047-.022.068-.038.067-.051.133-.104.196-.159-.443%201.486-1.107%202.761-2.086%203.257zM8.66%209.925a.5.5%200%201%200-1%200c0%20.653-.818%201.205-1.787%201.205s-1.787-.552-1.787-1.205a.5.5%200%201%200-1%200c0%201.216%201.25%202.205%202.787%202.205s2.787-.989%202.787-2.205zm4.4%2015.965l-.208.097c-2.661%201.258-4.708%201.436-6.086.527-1.542-1.017-1.88-3.19-1.844-4.198a.4.4%200%200%200-.385-.414c-.242-.029-.406.164-.414.385-.046%201.249.367%203.686%202.202%204.896.708.467%201.547.7%202.51.7%201.248%200%202.706-.392%204.362-1.174l.185-.086a.4.4%200%200%200%20.205-.527c-.089-.204-.326-.291-.527-.206zM9.547%202.292c.093.077.205.114.317.114a.5.5%200%200%200%20.318-.886L8.817.397a.5.5%200%200%200-.703.068.5.5%200%200%200%20.069.703l1.364%201.124zm-7.661-.065c.086%200%20.173-.022.253-.068l1.523-.893a.5.5%200%200%200-.506-.863l-1.523.892a.5.5%200%200%200-.179.685c.094.158.261.247.432.247z%22%20transform%3D%22matrix%28-1%200%200%201%2058%200%29%22%20fill%3D%22%233bb300%22/%3E%3Cpath%20d%3D%22M.3%2021.86V10.18q0-.46.02-.68.04-.22.18-.5.28-.54%201.34-.54%201.06%200%201.42.28.38.26.44.78.76-1.04%202.38-1.04%201.64%200%203.1%201.54%201.46%201.54%201.46%203.58%200%202.04-1.46%203.58-1.44%201.54-3.08%201.54-1.64%200-2.38-.92v4.04q0%20.46-.04.68-.02.22-.18.5-.14.3-.5.42-.36.12-.98.12-.62%200-1-.12-.36-.12-.52-.4-.14-.28-.18-.5-.02-.22-.02-.68zm3.96-9.42q-.46.54-.46%201.18%200%20.64.46%201.18.48.52%201.2.52.74%200%201.24-.52.52-.52.52-1.18%200-.66-.48-1.18-.48-.54-1.26-.54-.76%200-1.22.54zm14.741-8.36q.16-.3.54-.42.38-.12%201-.12.64%200%201.02.12.38.12.52.42.16.3.18.54.04.22.04.68v11.94q0%20.46-.04.7-.02.22-.18.5-.3.54-1.7.54-1.38%200-1.54-.98-.84.96-2.34.96-1.8%200-3.28-1.56-1.48-1.58-1.48-3.66%200-2.1%201.48-3.68%201.5-1.58%203.28-1.58%201.48%200%202.3%201v-4.2q0-.46.02-.68.04-.24.18-.52zm-3.24%2010.86q.52.54%201.26.54.74%200%201.22-.54.5-.54.5-1.18%200-.66-.48-1.22-.46-.56-1.26-.56-.8%200-1.28.56-.48.54-.48%201.2%200%20.66.52%201.2zm7.833-1.2q0-2.4%201.68-3.96%201.68-1.56%203.84-1.56%202.16%200%203.82%201.56%201.66%201.54%201.66%203.94%200%201.66-.86%202.96-.86%201.28-2.1%201.9-1.22.6-2.54.6-1.32%200-2.56-.64-1.24-.66-2.1-1.92-.84-1.28-.84-2.88zm4.18%201.44q.64.48%201.3.48.66%200%201.32-.5.66-.5.66-1.48%200-.98-.62-1.46-.62-.48-1.34-.48-.72%200-1.34.5-.62.5-.62%201.48%200%20.96.64%201.46zm11.412-1.44q0%20.84.56%201.32.56.46%201.18.46.64%200%201.18-.36.56-.38.9-.38.6%200%201.46%201.06.46.58.46%201.04%200%20.76-1.1%201.42-1.14.8-2.8.8-1.86%200-3.58-1.34-.82-.64-1.34-1.7-.52-1.08-.52-2.36%200-1.3.52-2.34.52-1.06%201.34-1.7%201.66-1.32%203.54-1.32.76%200%201.48.22.72.2%201.06.4l.32.2q.36.24.56.38.52.4.52.92%200%20.5-.42%201.14-.72%201.1-1.38%201.1-.38%200-1.08-.44-.36-.34-1.04-.34-.66%200-1.24.48-.58.48-.58%201.34z%22%20fill%3D%22green%22/%3E%3C/svg%3E"/>
        </a>
</div>
    </nav>
    <main class="pdoc">
            <section class="module-info">
                    <h1 class="modulename">
<a href="./../../clarena.html">clarena</a><wbr>.<a href="./../cl_algorithms.html">cl_algorithms</a><wbr>.hat    </h1>

                        <div class="docstring"><p>The submodule in <code>cl_algorithms</code> for <a href="http://proceedings.mlr.press/v80/serra18a">HAT (Hard Attention to the Task) algorithm</a>.</p>
</div>

                        <input id="mod-hat-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">

                        <label class="view-source-button" for="mod-hat-view-source"><span>View Source</span></label>

                        <div class="pdoc-code codehilite"><pre><span></span><span id="L-1"><a href="#L-1"><span class="linenos">  1</span></a><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-2"><a href="#L-2"><span class="linenos">  2</span></a><span class="sd">The submodule in `cl_algorithms` for [HAT (Hard Attention to the Task) algorithm](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="L-3"><a href="#L-3"><span class="linenos">  3</span></a><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-4"><a href="#L-4"><span class="linenos">  4</span></a>
</span><span id="L-5"><a href="#L-5"><span class="linenos">  5</span></a><span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;HAT&quot;</span><span class="p">]</span>
</span><span id="L-6"><a href="#L-6"><span class="linenos">  6</span></a>
</span><span id="L-7"><a href="#L-7"><span class="linenos">  7</span></a><span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
</span><span id="L-8"><a href="#L-8"><span class="linenos">  8</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Any</span>
</span><span id="L-9"><a href="#L-9"><span class="linenos">  9</span></a>
</span><span id="L-10"><a href="#L-10"><span class="linenos"> 10</span></a><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span id="L-11"><a href="#L-11"><span class="linenos"> 11</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tensor</span>
</span><span id="L-12"><a href="#L-12"><span class="linenos"> 12</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">DataLoader</span>
</span><span id="L-13"><a href="#L-13"><span class="linenos"> 13</span></a>
</span><span id="L-14"><a href="#L-14"><span class="linenos"> 14</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">clarena.backbones</span><span class="w"> </span><span class="kn">import</span> <span class="n">HATMaskBackbone</span>
</span><span id="L-15"><a href="#L-15"><span class="linenos"> 15</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">clarena.cl_algorithms</span><span class="w"> </span><span class="kn">import</span> <span class="n">CLAlgorithm</span>
</span><span id="L-16"><a href="#L-16"><span class="linenos"> 16</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">clarena.cl_algorithms.regularizers</span><span class="w"> </span><span class="kn">import</span> <span class="n">HATMaskSparsityReg</span>
</span><span id="L-17"><a href="#L-17"><span class="linenos"> 17</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">clarena.heads</span><span class="w"> </span><span class="kn">import</span> <span class="n">HeadsTIL</span>
</span><span id="L-18"><a href="#L-18"><span class="linenos"> 18</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">clarena.utils.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">HATNetworkCapacityMetric</span>
</span><span id="L-19"><a href="#L-19"><span class="linenos"> 19</span></a>
</span><span id="L-20"><a href="#L-20"><span class="linenos"> 20</span></a><span class="c1"># always get logger for built-in logging in each module</span>
</span><span id="L-21"><a href="#L-21"><span class="linenos"> 21</span></a><span class="n">pylogger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>
</span><span id="L-22"><a href="#L-22"><span class="linenos"> 22</span></a>
</span><span id="L-23"><a href="#L-23"><span class="linenos"> 23</span></a>
</span><span id="L-24"><a href="#L-24"><span class="linenos"> 24</span></a><span class="k">class</span><span class="w"> </span><span class="nc">HAT</span><span class="p">(</span><span class="n">CLAlgorithm</span><span class="p">):</span>
</span><span id="L-25"><a href="#L-25"><span class="linenos"> 25</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;[HAT (Hard Attention to the Task)](http://proceedings.mlr.press/v80/serra18a) algorithm.</span>
</span><span id="L-26"><a href="#L-26"><span class="linenos"> 26</span></a>
</span><span id="L-27"><a href="#L-27"><span class="linenos"> 27</span></a><span class="sd">    An architecture-based continual learning approach that uses learnable hard attention masks to select task-specific parameters.</span>
</span><span id="L-28"><a href="#L-28"><span class="linenos"> 28</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-29"><a href="#L-29"><span class="linenos"> 29</span></a>
</span><span id="L-30"><a href="#L-30"><span class="linenos"> 30</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-31"><a href="#L-31"><span class="linenos"> 31</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-32"><a href="#L-32"><span class="linenos"> 32</span></a>        <span class="n">backbone</span><span class="p">:</span> <span class="n">HATMaskBackbone</span><span class="p">,</span>
</span><span id="L-33"><a href="#L-33"><span class="linenos"> 33</span></a>        <span class="n">heads</span><span class="p">:</span> <span class="n">HeadsTIL</span><span class="p">,</span>
</span><span id="L-34"><a href="#L-34"><span class="linenos"> 34</span></a>        <span class="n">adjustment_mode</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-35"><a href="#L-35"><span class="linenos"> 35</span></a>        <span class="n">s_max</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="L-36"><a href="#L-36"><span class="linenos"> 36</span></a>        <span class="n">clamp_threshold</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="L-37"><a href="#L-37"><span class="linenos"> 37</span></a>        <span class="n">mask_sparsity_reg_factor</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="L-38"><a href="#L-38"><span class="linenos"> 38</span></a>        <span class="n">mask_sparsity_reg_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;original&quot;</span><span class="p">,</span>
</span><span id="L-39"><a href="#L-39"><span class="linenos"> 39</span></a>        <span class="n">task_embedding_init_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;N01&quot;</span><span class="p">,</span>
</span><span id="L-40"><a href="#L-40"><span class="linenos"> 40</span></a>        <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-41"><a href="#L-41"><span class="linenos"> 41</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-42"><a href="#L-42"><span class="linenos"> 42</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialize the HAT algorithm with the network.</span>
</span><span id="L-43"><a href="#L-43"><span class="linenos"> 43</span></a>
</span><span id="L-44"><a href="#L-44"><span class="linenos"> 44</span></a><span class="sd">        **Args:**</span>
</span><span id="L-45"><a href="#L-45"><span class="linenos"> 45</span></a><span class="sd">        - **backbone** (`HATMaskBackbone`): must be a backbone network with the HAT mask mechanism.</span>
</span><span id="L-46"><a href="#L-46"><span class="linenos"> 46</span></a><span class="sd">        - **heads** (`HeadsTIL`): output heads. HAT only supports TIL (Task-Incremental Learning).</span>
</span><span id="L-47"><a href="#L-47"><span class="linenos"> 47</span></a><span class="sd">        - **adjustment_mode** (`str`): the strategy of adjustment (i.e., the mode of gradient clipping), must be one of:</span>
</span><span id="L-48"><a href="#L-48"><span class="linenos"> 48</span></a><span class="sd">            1. &#39;hat&#39;: set gradients of parameters linking to masked units to zero. This is how HAT fixes the part of the network for previous tasks completely. See Eq. (2) in Sec. 2.3 &quot;Network Training&quot; in the [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="L-49"><a href="#L-49"><span class="linenos"> 49</span></a><span class="sd">            2. &#39;hat_random&#39;: set gradients of parameters linking to masked units to random 0â€“1 values. See &quot;Baselines&quot; in Sec. 4.1 in the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="L-50"><a href="#L-50"><span class="linenos"> 50</span></a><span class="sd">            3. &#39;hat_const_alpha&#39;: set gradients of parameters linking to masked units to a constant value `alpha`. See &quot;Baselines&quot; in Sec. 4.1 in the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="L-51"><a href="#L-51"><span class="linenos"> 51</span></a><span class="sd">            4. &#39;hat_const_1&#39;: set gradients of parameters linking to masked units to a constant value of 1 (i.e., no gradient constraint). See &quot;Baselines&quot; in Sec. 4.1 in the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="L-52"><a href="#L-52"><span class="linenos"> 52</span></a><span class="sd">        - **s_max** (`float`): hyperparameter, the maximum scaling factor in the gate function. See Sec. 2.4 &quot;Hard Attention Training&quot; in the [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="L-53"><a href="#L-53"><span class="linenos"> 53</span></a><span class="sd">        - **clamp_threshold** (`float`): the threshold for task embedding gradient compensation. See Sec. 2.5 &quot;Embedding Gradient Compensation&quot; in the [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="L-54"><a href="#L-54"><span class="linenos"> 54</span></a><span class="sd">        - **mask_sparsity_reg_factor** (`float`): hyperparameter, the regularization factor for mask sparsity.</span>
</span><span id="L-55"><a href="#L-55"><span class="linenos"> 55</span></a><span class="sd">        - **mask_sparsity_reg_mode** (`str`): the mode of mask sparsity regularization, must be one of:</span>
</span><span id="L-56"><a href="#L-56"><span class="linenos"> 56</span></a><span class="sd">            1. &#39;original&#39; (default): the original mask sparsity regularization in the HAT paper.</span>
</span><span id="L-57"><a href="#L-57"><span class="linenos"> 57</span></a><span class="sd">            2. &#39;cross&#39;: the cross version of mask sparsity regularization.</span>
</span><span id="L-58"><a href="#L-58"><span class="linenos"> 58</span></a><span class="sd">        - **task_embedding_init_mode** (`str`): the initialization mode for task embeddings, must be one of:</span>
</span><span id="L-59"><a href="#L-59"><span class="linenos"> 59</span></a><span class="sd">            1. &#39;N01&#39; (default): standard normal distribution $N(0, 1)$.</span>
</span><span id="L-60"><a href="#L-60"><span class="linenos"> 60</span></a><span class="sd">            2. &#39;U-11&#39;: uniform distribution $U(-1, 1)$.</span>
</span><span id="L-61"><a href="#L-61"><span class="linenos"> 61</span></a><span class="sd">            3. &#39;U01&#39;: uniform distribution $U(0, 1)$.</span>
</span><span id="L-62"><a href="#L-62"><span class="linenos"> 62</span></a><span class="sd">            4. &#39;U-10&#39;: uniform distribution $U(-1, 0)$.</span>
</span><span id="L-63"><a href="#L-63"><span class="linenos"> 63</span></a><span class="sd">            5. &#39;last&#39;: inherit the task embedding from the last task.</span>
</span><span id="L-64"><a href="#L-64"><span class="linenos"> 64</span></a><span class="sd">        - **alpha** (`float` | `None`): the `alpha` in the &#39;HAT-const-alpha&#39; mode. Applies only when `adjustment_mode` is &#39;hat_const_alpha&#39;.</span>
</span><span id="L-65"><a href="#L-65"><span class="linenos"> 65</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-66"><a href="#L-66"><span class="linenos"> 66</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">backbone</span><span class="o">=</span><span class="n">backbone</span><span class="p">,</span> <span class="n">heads</span><span class="o">=</span><span class="n">heads</span><span class="p">)</span>
</span><span id="L-67"><a href="#L-67"><span class="linenos"> 67</span></a>
</span><span id="L-68"><a href="#L-68"><span class="linenos"> 68</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">adjustment_mode</span>
</span><span id="L-69"><a href="#L-69"><span class="linenos"> 69</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The adjustment mode for gradient clipping.&quot;&quot;&quot;</span>
</span><span id="L-70"><a href="#L-70"><span class="linenos"> 70</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">s_max</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">s_max</span>
</span><span id="L-71"><a href="#L-71"><span class="linenos"> 71</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The hyperparameter s_max.&quot;&quot;&quot;</span>
</span><span id="L-72"><a href="#L-72"><span class="linenos"> 72</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">clamp_threshold</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">clamp_threshold</span>
</span><span id="L-73"><a href="#L-73"><span class="linenos"> 73</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The clamp threshold for task embedding gradient compensation.&quot;&quot;&quot;</span>
</span><span id="L-74"><a href="#L-74"><span class="linenos"> 74</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">mask_sparsity_reg_factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">mask_sparsity_reg_factor</span>
</span><span id="L-75"><a href="#L-75"><span class="linenos"> 75</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The mask sparsity regularization factor.&quot;&quot;&quot;</span>
</span><span id="L-76"><a href="#L-76"><span class="linenos"> 76</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">mask_sparsity_reg_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">mask_sparsity_reg_mode</span>
</span><span id="L-77"><a href="#L-77"><span class="linenos"> 77</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The mask sparsity regularization mode.&quot;&quot;&quot;</span>
</span><span id="L-78"><a href="#L-78"><span class="linenos"> 78</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">mark_sparsity_reg</span><span class="p">:</span> <span class="n">HATMaskSparsityReg</span> <span class="o">=</span> <span class="n">HATMaskSparsityReg</span><span class="p">(</span>
</span><span id="L-79"><a href="#L-79"><span class="linenos"> 79</span></a>            <span class="n">factor</span><span class="o">=</span><span class="n">mask_sparsity_reg_factor</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">mask_sparsity_reg_mode</span>
</span><span id="L-80"><a href="#L-80"><span class="linenos"> 80</span></a>        <span class="p">)</span>
</span><span id="L-81"><a href="#L-81"><span class="linenos"> 81</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The mask sparsity regularizer.&quot;&quot;&quot;</span>
</span><span id="L-82"><a href="#L-82"><span class="linenos"> 82</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_init_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">task_embedding_init_mode</span>
</span><span id="L-83"><a href="#L-83"><span class="linenos"> 83</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the task embedding initialization mode.&quot;&quot;&quot;</span>
</span><span id="L-84"><a href="#L-84"><span class="linenos"> 84</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">alpha</span>
</span><span id="L-85"><a href="#L-85"><span class="linenos"> 85</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The hyperparameter alpha for `hat_const_alpha`.&quot;&quot;&quot;</span>
</span><span id="L-86"><a href="#L-86"><span class="linenos"> 86</span></a>        <span class="c1"># self.epsilon: float | None = None</span>
</span><span id="L-87"><a href="#L-87"><span class="linenos"> 87</span></a>        <span class="c1"># r&quot;&quot;&quot;HAT doesn&#39;t use epsilon for `hat_const_alpha`. It is kept for consistency with `epsilon` in `clip_grad_by_adjustment()` in `HATMaskBackbone`.&quot;&quot;&quot;</span>
</span><span id="L-88"><a href="#L-88"><span class="linenos"> 88</span></a>
</span><span id="L-89"><a href="#L-89"><span class="linenos"> 89</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">cumulative_mask_for_previous_tasks</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="L-90"><a href="#L-90"><span class="linenos"> 90</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The cumulative binary attention mask $\mathrm{M}^{&lt;t}$ of previous tasks $1,\cdots, t-1$, gated from the task embedding ($t$ is `self.task_id`). It is a dict where keys are layer names and values are the binary mask tensors for the layers. The mask tensor has size (number of units, ). &quot;&quot;&quot;</span>
</span><span id="L-91"><a href="#L-91"><span class="linenos"> 91</span></a>
</span><span id="L-92"><a href="#L-92"><span class="linenos"> 92</span></a>        <span class="c1"># set manual optimization</span>
</span><span id="L-93"><a href="#L-93"><span class="linenos"> 93</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">automatic_optimization</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="L-94"><a href="#L-94"><span class="linenos"> 94</span></a>
</span><span id="L-95"><a href="#L-95"><span class="linenos"> 95</span></a>        <span class="n">HAT</span><span class="o">.</span><span class="n">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</span><span id="L-96"><a href="#L-96"><span class="linenos"> 96</span></a>
</span><span id="L-97"><a href="#L-97"><span class="linenos"> 97</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-98"><a href="#L-98"><span class="linenos"> 98</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Check the sanity of the arguments.&quot;&quot;&quot;</span>
</span><span id="L-99"><a href="#L-99"><span class="linenos"> 99</span></a>
</span><span id="L-100"><a href="#L-100"><span class="linenos">100</span></a>        <span class="c1"># check the backbone and heads</span>
</span><span id="L-101"><a href="#L-101"><span class="linenos">101</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="p">,</span> <span class="n">HATMaskBackbone</span><span class="p">):</span>
</span><span id="L-102"><a href="#L-102"><span class="linenos">102</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The backbone should be an instance of `HATMaskBackbone`.&quot;</span><span class="p">)</span>
</span><span id="L-103"><a href="#L-103"><span class="linenos">103</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">HeadsTIL</span><span class="p">):</span>
</span><span id="L-104"><a href="#L-104"><span class="linenos">104</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The heads should be an instance of `HeadsTIL`.&quot;</span><span class="p">)</span>
</span><span id="L-105"><a href="#L-105"><span class="linenos">105</span></a>
</span><span id="L-106"><a href="#L-106"><span class="linenos">106</span></a>        <span class="c1"># check marker sparsity regularization mode</span>
</span><span id="L-107"><a href="#L-107"><span class="linenos">107</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask_sparsity_reg_mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;original&quot;</span><span class="p">,</span> <span class="s2">&quot;cross&quot;</span><span class="p">]:</span>
</span><span id="L-108"><a href="#L-108"><span class="linenos">108</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="L-109"><a href="#L-109"><span class="linenos">109</span></a>                <span class="s2">&quot;The mask_sparsity_reg_mode should be one of &#39;original&#39;, &#39;cross&#39;.&quot;</span>
</span><span id="L-110"><a href="#L-110"><span class="linenos">110</span></a>            <span class="p">)</span>
</span><span id="L-111"><a href="#L-111"><span class="linenos">111</span></a>
</span><span id="L-112"><a href="#L-112"><span class="linenos">112</span></a>        <span class="c1"># check task embedding initialization mode</span>
</span><span id="L-113"><a href="#L-113"><span class="linenos">113</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_init_mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span>
</span><span id="L-114"><a href="#L-114"><span class="linenos">114</span></a>            <span class="s2">&quot;N01&quot;</span><span class="p">,</span>
</span><span id="L-115"><a href="#L-115"><span class="linenos">115</span></a>            <span class="s2">&quot;U01&quot;</span><span class="p">,</span>
</span><span id="L-116"><a href="#L-116"><span class="linenos">116</span></a>            <span class="s2">&quot;U-10&quot;</span><span class="p">,</span>
</span><span id="L-117"><a href="#L-117"><span class="linenos">117</span></a>            <span class="s2">&quot;masked&quot;</span><span class="p">,</span>
</span><span id="L-118"><a href="#L-118"><span class="linenos">118</span></a>            <span class="s2">&quot;unmasked&quot;</span><span class="p">,</span>
</span><span id="L-119"><a href="#L-119"><span class="linenos">119</span></a>        <span class="p">]:</span>
</span><span id="L-120"><a href="#L-120"><span class="linenos">120</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="L-121"><a href="#L-121"><span class="linenos">121</span></a>                <span class="s2">&quot;The task_embedding_init_mode should be one of &#39;N01&#39;, &#39;U01&#39;, &#39;U-10&#39;, &#39;masked&#39;, &#39;unmasked&#39;.&quot;</span>
</span><span id="L-122"><a href="#L-122"><span class="linenos">122</span></a>            <span class="p">)</span>
</span><span id="L-123"><a href="#L-123"><span class="linenos">123</span></a>
</span><span id="L-124"><a href="#L-124"><span class="linenos">124</span></a>        <span class="c1"># check adjustment mode `hat_const_alpha`</span>
</span><span id="L-125"><a href="#L-125"><span class="linenos">125</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;hat_const_alpha&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-126"><a href="#L-126"><span class="linenos">126</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="L-127"><a href="#L-127"><span class="linenos">127</span></a>                <span class="s2">&quot;Alpha should be given when the adjustment_mode is &#39;hat_const_alpha&#39;.&quot;</span>
</span><span id="L-128"><a href="#L-128"><span class="linenos">128</span></a>            <span class="p">)</span>
</span><span id="L-129"><a href="#L-129"><span class="linenos">129</span></a>
</span><span id="L-130"><a href="#L-130"><span class="linenos">130</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_start</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-131"><a href="#L-131"><span class="linenos">131</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialize the task embedding before training the next task and initialize the cumulative mask at the beginning of the first task.&quot;&quot;&quot;</span>
</span><span id="L-132"><a href="#L-132"><span class="linenos">132</span></a>
</span><span id="L-133"><a href="#L-133"><span class="linenos">133</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">initialize_task_embedding</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_init_mode</span><span class="p">)</span>
</span><span id="L-134"><a href="#L-134"><span class="linenos">134</span></a>
</span><span id="L-135"><a href="#L-135"><span class="linenos">135</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">initialize_independent_bn</span><span class="p">()</span>
</span><span id="L-136"><a href="#L-136"><span class="linenos">136</span></a>
</span><span id="L-137"><a href="#L-137"><span class="linenos">137</span></a>        <span class="c1"># initialize the cumulative mask for the first task at the beginning of the first task. This should not be called in `__init__()` because `self.device` is not available at that time.</span>
</span><span id="L-138"><a href="#L-138"><span class="linenos">138</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="L-139"><a href="#L-139"><span class="linenos">139</span></a>            <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="L-140"><a href="#L-140"><span class="linenos">140</span></a>                <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span>
</span><span id="L-141"><a href="#L-141"><span class="linenos">141</span></a>                    <span class="n">layer_name</span>
</span><span id="L-142"><a href="#L-142"><span class="linenos">142</span></a>                <span class="p">)</span>  <span class="c1"># get the layer by its name</span>
</span><span id="L-143"><a href="#L-143"><span class="linenos">143</span></a>                <span class="n">num_units</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="L-144"><a href="#L-144"><span class="linenos">144</span></a>
</span><span id="L-145"><a href="#L-145"><span class="linenos">145</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">cumulative_mask_for_previous_tasks</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
</span><span id="L-146"><a href="#L-146"><span class="linenos">146</span></a>                    <span class="n">num_units</span>
</span><span id="L-147"><a href="#L-147"><span class="linenos">147</span></a>                <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
</span><span id="L-148"><a href="#L-148"><span class="linenos">148</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
</span><span id="L-149"><a href="#L-149"><span class="linenos">149</span></a>                <span class="p">)</span>  <span class="c1"># the cumulative mask $\mathrm{M}^{&lt;t}$ is initialized as a zeros mask ($t = 1$). See Eq. (2) in Sec. 3 in the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9), or Eq. (5) in Sec. 2.6 &quot;Promoting Low Capacity Usage&quot; in the [HAT paper](http://proceedings.mlr.press/v80/serra18a)</span>
</span><span id="L-150"><a href="#L-150"><span class="linenos">150</span></a>
</span><span id="L-151"><a href="#L-151"><span class="linenos">151</span></a>                <span class="c1"># self.neuron_first_task[layer_name] = [None] * num_units</span>
</span><span id="L-152"><a href="#L-152"><span class="linenos">152</span></a>
</span><span id="L-153"><a href="#L-153"><span class="linenos">153</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">clip_grad_by_adjustment</span><span class="p">(</span>
</span><span id="L-154"><a href="#L-154"><span class="linenos">154</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-155"><a href="#L-155"><span class="linenos">155</span></a>        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
</span><span id="L-156"><a href="#L-156"><span class="linenos">156</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="n">Tensor</span><span class="p">]:</span>
</span><span id="L-157"><a href="#L-157"><span class="linenos">157</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Clip the gradients by the adjustment rate. See Eq. (2) in Sec. 2.3 &quot;Network Training&quot; in the [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="L-158"><a href="#L-158"><span class="linenos">158</span></a>
</span><span id="L-159"><a href="#L-159"><span class="linenos">159</span></a><span class="sd">        Note that because the task embedding fully covers every layer in the backbone network, no parameters are left out of this system.</span>
</span><span id="L-160"><a href="#L-160"><span class="linenos">160</span></a><span class="sd">        This applies not only to parameters between layers with task embeddings, but also to those before the first layer. We design it separately in the code.</span>
</span><span id="L-161"><a href="#L-161"><span class="linenos">161</span></a>
</span><span id="L-162"><a href="#L-162"><span class="linenos">162</span></a><span class="sd">        Network capacity is measured alongside this method. Network capacity is defined as the average adjustment rate over all parameters.</span>
</span><span id="L-163"><a href="#L-163"><span class="linenos">163</span></a><span class="sd">        See Sec. 4.1 in the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="L-164"><a href="#L-164"><span class="linenos">164</span></a>
</span><span id="L-165"><a href="#L-165"><span class="linenos">165</span></a><span class="sd">        **Returns:**</span>
</span><span id="L-166"><a href="#L-166"><span class="linenos">166</span></a><span class="sd">        - **adjustment_rate_weight** (`dict[str, Tensor]`): the adjustment rate for weights. Keys (`str`) are layer names and values (`Tensor`) are the adjustment rate tensors.</span>
</span><span id="L-167"><a href="#L-167"><span class="linenos">167</span></a><span class="sd">        - **adjustment_rate_bias** (`dict[str, Tensor]`): the adjustment rate for biases. Keys (`str`) are layer name and values (`Tensor`) are the adjustment rate tensors.</span>
</span><span id="L-168"><a href="#L-168"><span class="linenos">168</span></a><span class="sd">        - **capacity** (`Tensor`): the calculated network capacity.</span>
</span><span id="L-169"><a href="#L-169"><span class="linenos">169</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-170"><a href="#L-170"><span class="linenos">170</span></a>
</span><span id="L-171"><a href="#L-171"><span class="linenos">171</span></a>        <span class="c1"># initialize network capacity metric</span>
</span><span id="L-172"><a href="#L-172"><span class="linenos">172</span></a>        <span class="n">capacity</span> <span class="o">=</span> <span class="n">HATNetworkCapacityMetric</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="L-173"><a href="#L-173"><span class="linenos">173</span></a>        <span class="n">adjustment_rate_weight</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="L-174"><a href="#L-174"><span class="linenos">174</span></a>        <span class="n">adjustment_rate_bias</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="L-175"><a href="#L-175"><span class="linenos">175</span></a>
</span><span id="L-176"><a href="#L-176"><span class="linenos">176</span></a>        <span class="c1"># calculate the adjustment rate for gradients of the parameters, both weights and biases (if they exist)</span>
</span><span id="L-177"><a href="#L-177"><span class="linenos">177</span></a>        <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="L-178"><a href="#L-178"><span class="linenos">178</span></a>
</span><span id="L-179"><a href="#L-179"><span class="linenos">179</span></a>            <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span>
</span><span id="L-180"><a href="#L-180"><span class="linenos">180</span></a>                <span class="n">layer_name</span>
</span><span id="L-181"><a href="#L-181"><span class="linenos">181</span></a>            <span class="p">)</span>  <span class="c1"># get the layer by its name</span>
</span><span id="L-182"><a href="#L-182"><span class="linenos">182</span></a>
</span><span id="L-183"><a href="#L-183"><span class="linenos">183</span></a>            <span class="c1"># placeholder for the adjustment rate to avoid the error of using it before assignment</span>
</span><span id="L-184"><a href="#L-184"><span class="linenos">184</span></a>            <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="L-185"><a href="#L-185"><span class="linenos">185</span></a>            <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="L-186"><a href="#L-186"><span class="linenos">186</span></a>
</span><span id="L-187"><a href="#L-187"><span class="linenos">187</span></a>            <span class="n">weight_mask</span><span class="p">,</span> <span class="n">bias_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_measure_parameter_wise</span><span class="p">(</span>
</span><span id="L-188"><a href="#L-188"><span class="linenos">188</span></a>                <span class="n">unit_wise_measure</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">cumulative_mask_for_previous_tasks</span><span class="p">,</span>
</span><span id="L-189"><a href="#L-189"><span class="linenos">189</span></a>                <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="L-190"><a href="#L-190"><span class="linenos">190</span></a>                <span class="n">aggregation_mode</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span>
</span><span id="L-191"><a href="#L-191"><span class="linenos">191</span></a>            <span class="p">)</span>
</span><span id="L-192"><a href="#L-192"><span class="linenos">192</span></a>
</span><span id="L-193"><a href="#L-193"><span class="linenos">193</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;hat&quot;</span><span class="p">:</span>
</span><span id="L-194"><a href="#L-194"><span class="linenos">194</span></a>                <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">weight_mask</span>
</span><span id="L-195"><a href="#L-195"><span class="linenos">195</span></a>                <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">bias_mask</span>
</span><span id="L-196"><a href="#L-196"><span class="linenos">196</span></a>
</span><span id="L-197"><a href="#L-197"><span class="linenos">197</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;hat_random&quot;</span><span class="p">:</span>
</span><span id="L-198"><a href="#L-198"><span class="linenos">198</span></a>                <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand_like</span><span class="p">(</span>
</span><span id="L-199"><a href="#L-199"><span class="linenos">199</span></a>                    <span class="n">weight_mask</span>
</span><span id="L-200"><a href="#L-200"><span class="linenos">200</span></a>                <span class="p">)</span> <span class="o">*</span> <span class="n">weight_mask</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">weight_mask</span><span class="p">)</span>
</span><span id="L-201"><a href="#L-201"><span class="linenos">201</span></a>                <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand_like</span><span class="p">(</span><span class="n">bias_mask</span><span class="p">)</span> <span class="o">*</span> <span class="n">bias_mask</span> <span class="o">+</span> <span class="p">(</span>
</span><span id="L-202"><a href="#L-202"><span class="linenos">202</span></a>                    <span class="mi">1</span> <span class="o">-</span> <span class="n">bias_mask</span>
</span><span id="L-203"><a href="#L-203"><span class="linenos">203</span></a>                <span class="p">)</span>
</span><span id="L-204"><a href="#L-204"><span class="linenos">204</span></a>
</span><span id="L-205"><a href="#L-205"><span class="linenos">205</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;hat_const_alpha&quot;</span><span class="p">:</span>
</span><span id="L-206"><a href="#L-206"><span class="linenos">206</span></a>                <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span>
</span><span id="L-207"><a href="#L-207"><span class="linenos">207</span></a>                    <span class="n">weight_mask</span>
</span><span id="L-208"><a href="#L-208"><span class="linenos">208</span></a>                <span class="p">)</span> <span class="o">*</span> <span class="n">weight_mask</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">weight_mask</span><span class="p">)</span>
</span><span id="L-209"><a href="#L-209"><span class="linenos">209</span></a>                <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span>
</span><span id="L-210"><a href="#L-210"><span class="linenos">210</span></a>                    <span class="n">bias_mask</span>
</span><span id="L-211"><a href="#L-211"><span class="linenos">211</span></a>                <span class="p">)</span> <span class="o">*</span> <span class="n">bias_mask</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">bias_mask</span><span class="p">)</span>
</span><span id="L-212"><a href="#L-212"><span class="linenos">212</span></a>
</span><span id="L-213"><a href="#L-213"><span class="linenos">213</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;hat_const_1&quot;</span><span class="p">:</span>
</span><span id="L-214"><a href="#L-214"><span class="linenos">214</span></a>                <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span>
</span><span id="L-215"><a href="#L-215"><span class="linenos">215</span></a>                    <span class="n">weight_mask</span>
</span><span id="L-216"><a href="#L-216"><span class="linenos">216</span></a>                <span class="p">)</span> <span class="o">*</span> <span class="n">weight_mask</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">weight_mask</span><span class="p">)</span>
</span><span id="L-217"><a href="#L-217"><span class="linenos">217</span></a>                <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">bias_mask</span><span class="p">)</span> <span class="o">*</span> <span class="n">bias_mask</span> <span class="o">+</span> <span class="p">(</span>
</span><span id="L-218"><a href="#L-218"><span class="linenos">218</span></a>                    <span class="mi">1</span> <span class="o">-</span> <span class="n">bias_mask</span>
</span><span id="L-219"><a href="#L-219"><span class="linenos">219</span></a>                <span class="p">)</span>
</span><span id="L-220"><a href="#L-220"><span class="linenos">220</span></a>
</span><span id="L-221"><a href="#L-221"><span class="linenos">221</span></a>            <span class="c1"># apply the adjustment rate to the gradients</span>
</span><span id="L-222"><a href="#L-222"><span class="linenos">222</span></a>            <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">adjustment_rate_weight_layer</span>
</span><span id="L-223"><a href="#L-223"><span class="linenos">223</span></a>            <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-224"><a href="#L-224"><span class="linenos">224</span></a>                <span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">adjustment_rate_bias_layer</span>
</span><span id="L-225"><a href="#L-225"><span class="linenos">225</span></a>
</span><span id="L-226"><a href="#L-226"><span class="linenos">226</span></a>            <span class="c1"># store the adjustment rate for logging</span>
</span><span id="L-227"><a href="#L-227"><span class="linenos">227</span></a>            <span class="n">adjustment_rate_weight</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">adjustment_rate_weight_layer</span>
</span><span id="L-228"><a href="#L-228"><span class="linenos">228</span></a>            <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-229"><a href="#L-229"><span class="linenos">229</span></a>                <span class="n">adjustment_rate_bias</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">adjustment_rate_bias_layer</span>
</span><span id="L-230"><a href="#L-230"><span class="linenos">230</span></a>
</span><span id="L-231"><a href="#L-231"><span class="linenos">231</span></a>            <span class="c1"># update network capacity metric</span>
</span><span id="L-232"><a href="#L-232"><span class="linenos">232</span></a>            <span class="n">capacity</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">adjustment_rate_weight_layer</span><span class="p">,</span> <span class="n">adjustment_rate_bias_layer</span><span class="p">)</span>
</span><span id="L-233"><a href="#L-233"><span class="linenos">233</span></a>
</span><span id="L-234"><a href="#L-234"><span class="linenos">234</span></a>        <span class="k">return</span> <span class="n">adjustment_rate_weight</span><span class="p">,</span> <span class="n">adjustment_rate_bias</span><span class="p">,</span> <span class="n">capacity</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
</span><span id="L-235"><a href="#L-235"><span class="linenos">235</span></a>
</span><span id="L-236"><a href="#L-236"><span class="linenos">236</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">compensate_task_embedding_gradients</span><span class="p">(</span>
</span><span id="L-237"><a href="#L-237"><span class="linenos">237</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-238"><a href="#L-238"><span class="linenos">238</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-239"><a href="#L-239"><span class="linenos">239</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-240"><a href="#L-240"><span class="linenos">240</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-241"><a href="#L-241"><span class="linenos">241</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Compensate the gradients of task embeddings during training. See Sec. 2.5 &quot;Embedding Gradient Compensation&quot; in the [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="L-242"><a href="#L-242"><span class="linenos">242</span></a>
</span><span id="L-243"><a href="#L-243"><span class="linenos">243</span></a><span class="sd">        **Args:**</span>
</span><span id="L-244"><a href="#L-244"><span class="linenos">244</span></a><span class="sd">        - **batch_idx** (`int`): the current training batch index.</span>
</span><span id="L-245"><a href="#L-245"><span class="linenos">245</span></a><span class="sd">        - **num_batches** (`int`): the total number of training batches.</span>
</span><span id="L-246"><a href="#L-246"><span class="linenos">246</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-247"><a href="#L-247"><span class="linenos">247</span></a>
</span><span id="L-248"><a href="#L-248"><span class="linenos">248</span></a>        <span class="k">for</span> <span class="n">te</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
</span><span id="L-249"><a href="#L-249"><span class="linenos">249</span></a>            <span class="n">anneal_scalar</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_max</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">s_max</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_max</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span>
</span><span id="L-250"><a href="#L-250"><span class="linenos">250</span></a>                <span class="n">batch_idx</span> <span class="o">-</span> <span class="mi">1</span>
</span><span id="L-251"><a href="#L-251"><span class="linenos">251</span></a>            <span class="p">)</span> <span class="o">/</span> <span class="p">(</span>
</span><span id="L-252"><a href="#L-252"><span class="linenos">252</span></a>                <span class="n">num_batches</span> <span class="o">-</span> <span class="mi">1</span>
</span><span id="L-253"><a href="#L-253"><span class="linenos">253</span></a>            <span class="p">)</span>  <span class="c1"># see Eq. (3) in Sec. 2.4 &quot;Hard Attention Training&quot; in the [HAT paper](http://proceedings.mlr.press/v80/serra18a)</span>
</span><span id="L-254"><a href="#L-254"><span class="linenos">254</span></a>
</span><span id="L-255"><a href="#L-255"><span class="linenos">255</span></a>            <span class="n">num</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-256"><a href="#L-256"><span class="linenos">256</span></a>                <span class="n">torch</span><span class="o">.</span><span class="n">cosh</span><span class="p">(</span>
</span><span id="L-257"><a href="#L-257"><span class="linenos">257</span></a>                    <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span>
</span><span id="L-258"><a href="#L-258"><span class="linenos">258</span></a>                        <span class="n">anneal_scalar</span> <span class="o">*</span> <span class="n">te</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">,</span>
</span><span id="L-259"><a href="#L-259"><span class="linenos">259</span></a>                        <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">clamp_threshold</span><span class="p">,</span>
</span><span id="L-260"><a href="#L-260"><span class="linenos">260</span></a>                        <span class="bp">self</span><span class="o">.</span><span class="n">clamp_threshold</span><span class="p">,</span>
</span><span id="L-261"><a href="#L-261"><span class="linenos">261</span></a>                    <span class="p">)</span>
</span><span id="L-262"><a href="#L-262"><span class="linenos">262</span></a>                <span class="p">)</span>
</span><span id="L-263"><a href="#L-263"><span class="linenos">263</span></a>                <span class="o">+</span> <span class="mi">1</span>
</span><span id="L-264"><a href="#L-264"><span class="linenos">264</span></a>            <span class="p">)</span>
</span><span id="L-265"><a href="#L-265"><span class="linenos">265</span></a>
</span><span id="L-266"><a href="#L-266"><span class="linenos">266</span></a>            <span class="n">den</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cosh</span><span class="p">(</span><span class="n">te</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="L-267"><a href="#L-267"><span class="linenos">267</span></a>
</span><span id="L-268"><a href="#L-268"><span class="linenos">268</span></a>            <span class="n">compensation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_max</span> <span class="o">/</span> <span class="n">anneal_scalar</span> <span class="o">*</span> <span class="n">num</span> <span class="o">/</span> <span class="n">den</span>
</span><span id="L-269"><a href="#L-269"><span class="linenos">269</span></a>
</span><span id="L-270"><a href="#L-270"><span class="linenos">270</span></a>            <span class="n">te</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">compensation</span>
</span><span id="L-271"><a href="#L-271"><span class="linenos">271</span></a>
</span><span id="L-272"><a href="#L-272"><span class="linenos">272</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="L-273"><a href="#L-273"><span class="linenos">273</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-274"><a href="#L-274"><span class="linenos">274</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-275"><a href="#L-275"><span class="linenos">275</span></a>        <span class="n">stage</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-276"><a href="#L-276"><span class="linenos">276</span></a>        <span class="n">task_id</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-277"><a href="#L-277"><span class="linenos">277</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-278"><a href="#L-278"><span class="linenos">278</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-279"><a href="#L-279"><span class="linenos">279</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
</span><span id="L-280"><a href="#L-280"><span class="linenos">280</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The forward pass for data from task `task_id`. Note that it is nothing to do with `forward()` method in `nn.Module`.</span>
</span><span id="L-281"><a href="#L-281"><span class="linenos">281</span></a>
</span><span id="L-282"><a href="#L-282"><span class="linenos">282</span></a><span class="sd">        **Args:**</span>
</span><span id="L-283"><a href="#L-283"><span class="linenos">283</span></a><span class="sd">        - **input** (`Tensor`): The input tensor from data.</span>
</span><span id="L-284"><a href="#L-284"><span class="linenos">284</span></a><span class="sd">        - **stage** (`str`): the stage of the forward pass, should be one of the following:</span>
</span><span id="L-285"><a href="#L-285"><span class="linenos">285</span></a><span class="sd">            1. &#39;train&#39;: training stage.</span>
</span><span id="L-286"><a href="#L-286"><span class="linenos">286</span></a><span class="sd">            2. &#39;validation&#39;: validation stage.</span>
</span><span id="L-287"><a href="#L-287"><span class="linenos">287</span></a><span class="sd">            3. &#39;test&#39;: testing stage.</span>
</span><span id="L-288"><a href="#L-288"><span class="linenos">288</span></a><span class="sd">        - **task_id** (`int`| `None`): the task ID where the data are from. If the stage is &#39;train&#39; or &#39;validation&#39;, it should be the current task `self.task_id`. If stage is &#39;test&#39;, it could be from any seen task. In TIL, the task IDs of test data are provided thus this argument can be used. HAT algorithm works only for TIL.</span>
</span><span id="L-289"><a href="#L-289"><span class="linenos">289</span></a><span class="sd">        - **batch_idx** (`int` | `None`): the current batch index. Applies only to training stage. For other stages, it is default `None`.</span>
</span><span id="L-290"><a href="#L-290"><span class="linenos">290</span></a><span class="sd">        - **num_batches** (`int` | `None`): the total number of batches. Applies only to training stage. For other stages, it is default `None`.</span>
</span><span id="L-291"><a href="#L-291"><span class="linenos">291</span></a>
</span><span id="L-292"><a href="#L-292"><span class="linenos">292</span></a><span class="sd">        **Returns:**</span>
</span><span id="L-293"><a href="#L-293"><span class="linenos">293</span></a><span class="sd">        - **logits** (`Tensor`): the output logits tensor.</span>
</span><span id="L-294"><a href="#L-294"><span class="linenos">294</span></a><span class="sd">        - **mask** (`dict[str, Tensor]`): the mask for the current task. Key (`str`) is layer name, value (`Tensor`) is the mask tensor. The mask tensor has size (number of units, ).</span>
</span><span id="L-295"><a href="#L-295"><span class="linenos">295</span></a><span class="sd">        - **activations** (`dict[str, Tensor]`): the hidden features (after activation) in each weighted layer. Key (`str`) is the weighted layer name, value (`Tensor`) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes. Although HAT algorithm does not need this, it is still provided for API consistence for other HAT-based algorithms inherited this `forward()` method of `HAT` class.</span>
</span><span id="L-296"><a href="#L-296"><span class="linenos">296</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-297"><a href="#L-297"><span class="linenos">297</span></a>        <span class="n">feature</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="p">(</span>
</span><span id="L-298"><a href="#L-298"><span class="linenos">298</span></a>            <span class="nb">input</span><span class="p">,</span>
</span><span id="L-299"><a href="#L-299"><span class="linenos">299</span></a>            <span class="n">stage</span><span class="o">=</span><span class="n">stage</span><span class="p">,</span>
</span><span id="L-300"><a href="#L-300"><span class="linenos">300</span></a>            <span class="n">s_max</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">s_max</span> <span class="k">if</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;train&quot;</span> <span class="ow">or</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;validation&quot;</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-301"><a href="#L-301"><span class="linenos">301</span></a>            <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span> <span class="k">if</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;train&quot;</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-302"><a href="#L-302"><span class="linenos">302</span></a>            <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span> <span class="k">if</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;train&quot;</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-303"><a href="#L-303"><span class="linenos">303</span></a>            <span class="n">test_task_id</span><span class="o">=</span><span class="n">task_id</span> <span class="k">if</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;test&quot;</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-304"><a href="#L-304"><span class="linenos">304</span></a>        <span class="p">)</span>
</span><span id="L-305"><a href="#L-305"><span class="linenos">305</span></a>        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">heads</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="n">task_id</span><span class="p">)</span>
</span><span id="L-306"><a href="#L-306"><span class="linenos">306</span></a>
</span><span id="L-307"><a href="#L-307"><span class="linenos">307</span></a>        <span class="k">return</span> <span class="p">(</span>
</span><span id="L-308"><a href="#L-308"><span class="linenos">308</span></a>            <span class="n">logits</span>
</span><span id="L-309"><a href="#L-309"><span class="linenos">309</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">if_forward_func_return_logits_only</span>
</span><span id="L-310"><a href="#L-310"><span class="linenos">310</span></a>            <span class="k">else</span> <span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">activations</span><span class="p">)</span>
</span><span id="L-311"><a href="#L-311"><span class="linenos">311</span></a>        <span class="p">)</span>
</span><span id="L-312"><a href="#L-312"><span class="linenos">312</span></a>
</span><span id="L-313"><a href="#L-313"><span class="linenos">313</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
</span><span id="L-314"><a href="#L-314"><span class="linenos">314</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Training step for current task `self.task_id`.</span>
</span><span id="L-315"><a href="#L-315"><span class="linenos">315</span></a>
</span><span id="L-316"><a href="#L-316"><span class="linenos">316</span></a><span class="sd">        **Args:**</span>
</span><span id="L-317"><a href="#L-317"><span class="linenos">317</span></a><span class="sd">        - **batch** (`Any`): a batch of training data.</span>
</span><span id="L-318"><a href="#L-318"><span class="linenos">318</span></a><span class="sd">        - **batch_idx** (`int`): the index of the batch. Used for calculating annealed scalar in HAT. See Sec. 2.4 &quot;Hard Attention Training&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="L-319"><a href="#L-319"><span class="linenos">319</span></a>
</span><span id="L-320"><a href="#L-320"><span class="linenos">320</span></a><span class="sd">        **Returns:**</span>
</span><span id="L-321"><a href="#L-321"><span class="linenos">321</span></a><span class="sd">        - **outputs** (`dict[str, Tensor]`): a dictionary containing loss and other metrics from this training step. Keys (`str`) are metric names, and values (`Tensor`) are the metrics. Must include the key &#39;loss&#39; (total loss) in the case of automatic optimization, according to PyTorch Lightning. For HAT, it includes &#39;mask&#39; and &#39;capacity&#39; for logging.</span>
</span><span id="L-322"><a href="#L-322"><span class="linenos">322</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-323"><a href="#L-323"><span class="linenos">323</span></a>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
</span><span id="L-324"><a href="#L-324"><span class="linenos">324</span></a>
</span><span id="L-325"><a href="#L-325"><span class="linenos">325</span></a>        <span class="c1"># zero the gradients before forward pass in manual optimization mode</span>
</span><span id="L-326"><a href="#L-326"><span class="linenos">326</span></a>        <span class="n">opt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizers</span><span class="p">()</span>
</span><span id="L-327"><a href="#L-327"><span class="linenos">327</span></a>        <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</span><span id="L-328"><a href="#L-328"><span class="linenos">328</span></a>
</span><span id="L-329"><a href="#L-329"><span class="linenos">329</span></a>        <span class="c1"># classification loss</span>
</span><span id="L-330"><a href="#L-330"><span class="linenos">330</span></a>        <span class="n">num_batches</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">num_training_batches</span>
</span><span id="L-331"><a href="#L-331"><span class="linenos">331</span></a>        <span class="n">logits</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
</span><span id="L-332"><a href="#L-332"><span class="linenos">332</span></a>            <span class="n">x</span><span class="p">,</span>
</span><span id="L-333"><a href="#L-333"><span class="linenos">333</span></a>            <span class="n">stage</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">,</span>
</span><span id="L-334"><a href="#L-334"><span class="linenos">334</span></a>            <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="L-335"><a href="#L-335"><span class="linenos">335</span></a>            <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="L-336"><a href="#L-336"><span class="linenos">336</span></a>            <span class="n">task_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">,</span>
</span><span id="L-337"><a href="#L-337"><span class="linenos">337</span></a>        <span class="p">)</span>
</span><span id="L-338"><a href="#L-338"><span class="linenos">338</span></a>        <span class="n">loss_cls</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span><span id="L-339"><a href="#L-339"><span class="linenos">339</span></a>
</span><span id="L-340"><a href="#L-340"><span class="linenos">340</span></a>        <span class="c1"># regularization loss. See Sec. 2.6 &quot;Promoting Low Capacity Usage&quot; in the [HAT paper](http://proceedings.mlr.press/v80/serra18a)</span>
</span><span id="L-341"><a href="#L-341"><span class="linenos">341</span></a>        <span class="n">loss_reg</span><span class="p">,</span> <span class="n">network_sparsity</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mark_sparsity_reg</span><span class="p">(</span>
</span><span id="L-342"><a href="#L-342"><span class="linenos">342</span></a>            <span class="n">mask</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cumulative_mask_for_previous_tasks</span>
</span><span id="L-343"><a href="#L-343"><span class="linenos">343</span></a>        <span class="p">)</span>
</span><span id="L-344"><a href="#L-344"><span class="linenos">344</span></a>
</span><span id="L-345"><a href="#L-345"><span class="linenos">345</span></a>        <span class="c1"># total loss. See Eq. (4) in Sec. 2.6 &quot;Promoting Low Capacity Usage&quot; in the [HAT paper](http://proceedings.mlr.press/v80/serra18a)</span>
</span><span id="L-346"><a href="#L-346"><span class="linenos">346</span></a>        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_cls</span> <span class="o">+</span> <span class="n">loss_reg</span>
</span><span id="L-347"><a href="#L-347"><span class="linenos">347</span></a>
</span><span id="L-348"><a href="#L-348"><span class="linenos">348</span></a>        <span class="c1"># backward step (manually)</span>
</span><span id="L-349"><a href="#L-349"><span class="linenos">349</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">manual_backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>  <span class="c1"># calculate the gradients</span>
</span><span id="L-350"><a href="#L-350"><span class="linenos">350</span></a>        <span class="c1"># HAT hard-clips gradients using the cumulative masks. See Eq. (2) in Sec. 2.3 &quot;Network Training&quot; in the HAT paper.</span>
</span><span id="L-351"><a href="#L-351"><span class="linenos">351</span></a>        <span class="c1"># Network capacity is computed along with this process (defined as the average adjustment rate over all parameters; see Sec. 4.1 in the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9)).</span>
</span><span id="L-352"><a href="#L-352"><span class="linenos">352</span></a>
</span><span id="L-353"><a href="#L-353"><span class="linenos">353</span></a>        <span class="n">adjustment_rate_weight</span><span class="p">,</span> <span class="n">adjustment_rate_bias</span><span class="p">,</span> <span class="n">capacity</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-354"><a href="#L-354"><span class="linenos">354</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">clip_grad_by_adjustment</span><span class="p">(</span>
</span><span id="L-355"><a href="#L-355"><span class="linenos">355</span></a>                <span class="n">network_sparsity</span><span class="o">=</span><span class="n">network_sparsity</span><span class="p">,</span>  <span class="c1"># passed for compatibility with AdaHAT, which inherits this method</span>
</span><span id="L-356"><a href="#L-356"><span class="linenos">356</span></a>            <span class="p">)</span>
</span><span id="L-357"><a href="#L-357"><span class="linenos">357</span></a>        <span class="p">)</span>
</span><span id="L-358"><a href="#L-358"><span class="linenos">358</span></a>        <span class="c1"># compensate the gradients of task embedding. See Sec. 2.5 &quot;Embedding Gradient Compensation&quot; in the [HAT paper](http://proceedings.mlr.press/v80/serra18a)</span>
</span><span id="L-359"><a href="#L-359"><span class="linenos">359</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">compensate_task_embedding_gradients</span><span class="p">(</span>
</span><span id="L-360"><a href="#L-360"><span class="linenos">360</span></a>            <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="L-361"><a href="#L-361"><span class="linenos">361</span></a>            <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="L-362"><a href="#L-362"><span class="linenos">362</span></a>        <span class="p">)</span>
</span><span id="L-363"><a href="#L-363"><span class="linenos">363</span></a>        <span class="c1"># update parameters with the modified gradients</span>
</span><span id="L-364"><a href="#L-364"><span class="linenos">364</span></a>        <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span><span id="L-365"><a href="#L-365"><span class="linenos">365</span></a>
</span><span id="L-366"><a href="#L-366"><span class="linenos">366</span></a>        <span class="c1"># accuracy of the batch</span>
</span><span id="L-367"><a href="#L-367"><span class="linenos">367</span></a>        <span class="n">acc</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</span><span id="L-368"><a href="#L-368"><span class="linenos">368</span></a>
</span><span id="L-369"><a href="#L-369"><span class="linenos">369</span></a>        <span class="k">return</span> <span class="p">{</span>
</span><span id="L-370"><a href="#L-370"><span class="linenos">370</span></a>            <span class="s2">&quot;loss&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span>  <span class="c1"># return loss is essential for training step, or backpropagation will fail</span>
</span><span id="L-371"><a href="#L-371"><span class="linenos">371</span></a>            <span class="s2">&quot;loss_cls&quot;</span><span class="p">:</span> <span class="n">loss_cls</span><span class="p">,</span>
</span><span id="L-372"><a href="#L-372"><span class="linenos">372</span></a>            <span class="s2">&quot;loss_reg&quot;</span><span class="p">:</span> <span class="n">loss_reg</span><span class="p">,</span>
</span><span id="L-373"><a href="#L-373"><span class="linenos">373</span></a>            <span class="s2">&quot;acc&quot;</span><span class="p">:</span> <span class="n">acc</span><span class="p">,</span>
</span><span id="L-374"><a href="#L-374"><span class="linenos">374</span></a>            <span class="s2">&quot;activations&quot;</span><span class="p">:</span> <span class="n">activations</span><span class="p">,</span>
</span><span id="L-375"><a href="#L-375"><span class="linenos">375</span></a>            <span class="s2">&quot;logits&quot;</span><span class="p">:</span> <span class="n">logits</span><span class="p">,</span>
</span><span id="L-376"><a href="#L-376"><span class="linenos">376</span></a>            <span class="s2">&quot;mask&quot;</span><span class="p">:</span> <span class="n">mask</span><span class="p">,</span>  <span class="c1"># return other metrics for lightning loggers callback to handle at `on_train_batch_end()`</span>
</span><span id="L-377"><a href="#L-377"><span class="linenos">377</span></a>            <span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span>  <span class="c1"># return the input batch for Captum to use</span>
</span><span id="L-378"><a href="#L-378"><span class="linenos">378</span></a>            <span class="s2">&quot;target&quot;</span><span class="p">:</span> <span class="n">y</span><span class="p">,</span>  <span class="c1"># return the target batch for Captum to use</span>
</span><span id="L-379"><a href="#L-379"><span class="linenos">379</span></a>            <span class="s2">&quot;adjustment_rate_weight&quot;</span><span class="p">:</span> <span class="n">adjustment_rate_weight</span><span class="p">,</span>  <span class="c1"># return the adjustment rate for weights and biases for logging</span>
</span><span id="L-380"><a href="#L-380"><span class="linenos">380</span></a>            <span class="s2">&quot;adjustment_rate_bias&quot;</span><span class="p">:</span> <span class="n">adjustment_rate_bias</span><span class="p">,</span>
</span><span id="L-381"><a href="#L-381"><span class="linenos">381</span></a>            <span class="s2">&quot;capacity&quot;</span><span class="p">:</span> <span class="n">capacity</span><span class="p">,</span>  <span class="c1"># return the network capacity for logging</span>
</span><span id="L-382"><a href="#L-382"><span class="linenos">382</span></a>        <span class="p">}</span>
</span><span id="L-383"><a href="#L-383"><span class="linenos">383</span></a>
</span><span id="L-384"><a href="#L-384"><span class="linenos">384</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_end</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-385"><a href="#L-385"><span class="linenos">385</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the mask and update the cumulative mask after training the task.&quot;&quot;&quot;</span>
</span><span id="L-386"><a href="#L-386"><span class="linenos">386</span></a>
</span><span id="L-387"><a href="#L-387"><span class="linenos">387</span></a>        <span class="c1"># store the mask for the current task</span>
</span><span id="L-388"><a href="#L-388"><span class="linenos">388</span></a>        <span class="n">mask_t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">store_mask</span><span class="p">()</span>
</span><span id="L-389"><a href="#L-389"><span class="linenos">389</span></a>
</span><span id="L-390"><a href="#L-390"><span class="linenos">390</span></a>        <span class="c1"># store the batch normalization if necessary</span>
</span><span id="L-391"><a href="#L-391"><span class="linenos">391</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">store_bn</span><span class="p">()</span>
</span><span id="L-392"><a href="#L-392"><span class="linenos">392</span></a>
</span><span id="L-393"><a href="#L-393"><span class="linenos">393</span></a>        <span class="c1"># update the cumulative mask. See the first Eq. in Sec 2.3 &quot;Network Training&quot; in the [HAT paper](http://proceedings.mlr.press/v80/serra18a)</span>
</span><span id="L-394"><a href="#L-394"><span class="linenos">394</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">cumulative_mask_for_previous_tasks</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="L-395"><a href="#L-395"><span class="linenos">395</span></a>            <span class="n">layer_name</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span>
</span><span id="L-396"><a href="#L-396"><span class="linenos">396</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">cumulative_mask_for_previous_tasks</span><span class="p">[</span><span class="n">layer_name</span><span class="p">],</span> <span class="n">mask_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="L-397"><a href="#L-397"><span class="linenos">397</span></a>            <span class="p">)</span>
</span><span id="L-398"><a href="#L-398"><span class="linenos">398</span></a>            <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span>
</span><span id="L-399"><a href="#L-399"><span class="linenos">399</span></a>        <span class="p">}</span>
</span><span id="L-400"><a href="#L-400"><span class="linenos">400</span></a>
</span><span id="L-401"><a href="#L-401"><span class="linenos">401</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
</span><span id="L-402"><a href="#L-402"><span class="linenos">402</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Validation step for current task `self.task_id`.</span>
</span><span id="L-403"><a href="#L-403"><span class="linenos">403</span></a>
</span><span id="L-404"><a href="#L-404"><span class="linenos">404</span></a><span class="sd">        **Args:**</span>
</span><span id="L-405"><a href="#L-405"><span class="linenos">405</span></a><span class="sd">        - **batch** (`Any`): a batch of validation data.</span>
</span><span id="L-406"><a href="#L-406"><span class="linenos">406</span></a>
</span><span id="L-407"><a href="#L-407"><span class="linenos">407</span></a><span class="sd">        **Returns:**</span>
</span><span id="L-408"><a href="#L-408"><span class="linenos">408</span></a><span class="sd">        - **outputs** (`dict[str, Tensor]`): a dictionary contains loss and other metrics from this validation step. Keys (`str`) are the metrics names, and values (`Tensor`) are the metrics.</span>
</span><span id="L-409"><a href="#L-409"><span class="linenos">409</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-410"><a href="#L-410"><span class="linenos">410</span></a>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
</span><span id="L-411"><a href="#L-411"><span class="linenos">411</span></a>        <span class="n">logits</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">stage</span><span class="o">=</span><span class="s2">&quot;validation&quot;</span><span class="p">,</span> <span class="n">task_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">)</span>
</span><span id="L-412"><a href="#L-412"><span class="linenos">412</span></a>        <span class="n">loss_cls</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span><span id="L-413"><a href="#L-413"><span class="linenos">413</span></a>        <span class="n">acc</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</span><span id="L-414"><a href="#L-414"><span class="linenos">414</span></a>
</span><span id="L-415"><a href="#L-415"><span class="linenos">415</span></a>        <span class="k">return</span> <span class="p">{</span>
</span><span id="L-416"><a href="#L-416"><span class="linenos">416</span></a>            <span class="s2">&quot;loss_cls&quot;</span><span class="p">:</span> <span class="n">loss_cls</span><span class="p">,</span>
</span><span id="L-417"><a href="#L-417"><span class="linenos">417</span></a>            <span class="s2">&quot;acc&quot;</span><span class="p">:</span> <span class="n">acc</span><span class="p">,</span>  <span class="c1"># Return metrics for lightning loggers callback to handle at `on_validation_batch_end()`</span>
</span><span id="L-418"><a href="#L-418"><span class="linenos">418</span></a>        <span class="p">}</span>
</span><span id="L-419"><a href="#L-419"><span class="linenos">419</span></a>
</span><span id="L-420"><a href="#L-420"><span class="linenos">420</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">test_step</span><span class="p">(</span>
</span><span id="L-421"><a href="#L-421"><span class="linenos">421</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="L-422"><a href="#L-422"><span class="linenos">422</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
</span><span id="L-423"><a href="#L-423"><span class="linenos">423</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Test step for current task `self.task_id`, which tests for all seen tasks indexed by `dataloader_idx`.</span>
</span><span id="L-424"><a href="#L-424"><span class="linenos">424</span></a>
</span><span id="L-425"><a href="#L-425"><span class="linenos">425</span></a><span class="sd">        **Args:**</span>
</span><span id="L-426"><a href="#L-426"><span class="linenos">426</span></a><span class="sd">        - **batch** (`Any`): a batch of test data.</span>
</span><span id="L-427"><a href="#L-427"><span class="linenos">427</span></a><span class="sd">        - **dataloader_idx** (`int`): the task ID of seen tasks to be tested. A default value of 0 is given otherwise the LightningModule will raise a `RuntimeError`.</span>
</span><span id="L-428"><a href="#L-428"><span class="linenos">428</span></a>
</span><span id="L-429"><a href="#L-429"><span class="linenos">429</span></a><span class="sd">        **Returns:**</span>
</span><span id="L-430"><a href="#L-430"><span class="linenos">430</span></a><span class="sd">        - **outputs** (`dict[str, Tensor]`): a dictionary contains loss and other metrics from this test step. Keys (`str`) are the metrics names, and values (`Tensor`) are the metrics.</span>
</span><span id="L-431"><a href="#L-431"><span class="linenos">431</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-432"><a href="#L-432"><span class="linenos">432</span></a>        <span class="n">test_task_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_test_task_id_from_dataloader_idx</span><span class="p">(</span><span class="n">dataloader_idx</span><span class="p">)</span>
</span><span id="L-433"><a href="#L-433"><span class="linenos">433</span></a>
</span><span id="L-434"><a href="#L-434"><span class="linenos">434</span></a>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
</span><span id="L-435"><a href="#L-435"><span class="linenos">435</span></a>        <span class="n">logits</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
</span><span id="L-436"><a href="#L-436"><span class="linenos">436</span></a>            <span class="n">x</span><span class="p">,</span>
</span><span id="L-437"><a href="#L-437"><span class="linenos">437</span></a>            <span class="n">stage</span><span class="o">=</span><span class="s2">&quot;test&quot;</span><span class="p">,</span>
</span><span id="L-438"><a href="#L-438"><span class="linenos">438</span></a>            <span class="n">task_id</span><span class="o">=</span><span class="n">test_task_id</span><span class="p">,</span>
</span><span id="L-439"><a href="#L-439"><span class="linenos">439</span></a>        <span class="p">)</span>  <span class="c1"># use the corresponding head and mask to test (instead of the current task `self.task_id`)</span>
</span><span id="L-440"><a href="#L-440"><span class="linenos">440</span></a>        <span class="n">loss_cls</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span><span id="L-441"><a href="#L-441"><span class="linenos">441</span></a>        <span class="n">acc</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</span><span id="L-442"><a href="#L-442"><span class="linenos">442</span></a>
</span><span id="L-443"><a href="#L-443"><span class="linenos">443</span></a>        <span class="k">return</span> <span class="p">{</span>
</span><span id="L-444"><a href="#L-444"><span class="linenos">444</span></a>            <span class="s2">&quot;loss_cls&quot;</span><span class="p">:</span> <span class="n">loss_cls</span><span class="p">,</span>
</span><span id="L-445"><a href="#L-445"><span class="linenos">445</span></a>            <span class="s2">&quot;acc&quot;</span><span class="p">:</span> <span class="n">acc</span><span class="p">,</span>  <span class="c1"># Return metrics for lightning loggers callback to handle at `on_test_batch_end()`</span>
</span><span id="L-446"><a href="#L-446"><span class="linenos">446</span></a>        <span class="p">}</span>
</span></pre></div>


            </section>
                <section id="HAT">
                            <input id="HAT-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">HAT</span><wbr>(<span class="base">clarena.cl_algorithms.base.CLAlgorithm</span>):

                <label class="view-source-button" for="HAT-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HAT"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HAT-25"><a href="#HAT-25"><span class="linenos"> 25</span></a><span class="k">class</span><span class="w"> </span><span class="nc">HAT</span><span class="p">(</span><span class="n">CLAlgorithm</span><span class="p">):</span>
</span><span id="HAT-26"><a href="#HAT-26"><span class="linenos"> 26</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;[HAT (Hard Attention to the Task)](http://proceedings.mlr.press/v80/serra18a) algorithm.</span>
</span><span id="HAT-27"><a href="#HAT-27"><span class="linenos"> 27</span></a>
</span><span id="HAT-28"><a href="#HAT-28"><span class="linenos"> 28</span></a><span class="sd">    An architecture-based continual learning approach that uses learnable hard attention masks to select task-specific parameters.</span>
</span><span id="HAT-29"><a href="#HAT-29"><span class="linenos"> 29</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="HAT-30"><a href="#HAT-30"><span class="linenos"> 30</span></a>
</span><span id="HAT-31"><a href="#HAT-31"><span class="linenos"> 31</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="HAT-32"><a href="#HAT-32"><span class="linenos"> 32</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="HAT-33"><a href="#HAT-33"><span class="linenos"> 33</span></a>        <span class="n">backbone</span><span class="p">:</span> <span class="n">HATMaskBackbone</span><span class="p">,</span>
</span><span id="HAT-34"><a href="#HAT-34"><span class="linenos"> 34</span></a>        <span class="n">heads</span><span class="p">:</span> <span class="n">HeadsTIL</span><span class="p">,</span>
</span><span id="HAT-35"><a href="#HAT-35"><span class="linenos"> 35</span></a>        <span class="n">adjustment_mode</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="HAT-36"><a href="#HAT-36"><span class="linenos"> 36</span></a>        <span class="n">s_max</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="HAT-37"><a href="#HAT-37"><span class="linenos"> 37</span></a>        <span class="n">clamp_threshold</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="HAT-38"><a href="#HAT-38"><span class="linenos"> 38</span></a>        <span class="n">mask_sparsity_reg_factor</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="HAT-39"><a href="#HAT-39"><span class="linenos"> 39</span></a>        <span class="n">mask_sparsity_reg_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;original&quot;</span><span class="p">,</span>
</span><span id="HAT-40"><a href="#HAT-40"><span class="linenos"> 40</span></a>        <span class="n">task_embedding_init_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;N01&quot;</span><span class="p">,</span>
</span><span id="HAT-41"><a href="#HAT-41"><span class="linenos"> 41</span></a>        <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HAT-42"><a href="#HAT-42"><span class="linenos"> 42</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HAT-43"><a href="#HAT-43"><span class="linenos"> 43</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialize the HAT algorithm with the network.</span>
</span><span id="HAT-44"><a href="#HAT-44"><span class="linenos"> 44</span></a>
</span><span id="HAT-45"><a href="#HAT-45"><span class="linenos"> 45</span></a><span class="sd">        **Args:**</span>
</span><span id="HAT-46"><a href="#HAT-46"><span class="linenos"> 46</span></a><span class="sd">        - **backbone** (`HATMaskBackbone`): must be a backbone network with the HAT mask mechanism.</span>
</span><span id="HAT-47"><a href="#HAT-47"><span class="linenos"> 47</span></a><span class="sd">        - **heads** (`HeadsTIL`): output heads. HAT only supports TIL (Task-Incremental Learning).</span>
</span><span id="HAT-48"><a href="#HAT-48"><span class="linenos"> 48</span></a><span class="sd">        - **adjustment_mode** (`str`): the strategy of adjustment (i.e., the mode of gradient clipping), must be one of:</span>
</span><span id="HAT-49"><a href="#HAT-49"><span class="linenos"> 49</span></a><span class="sd">            1. &#39;hat&#39;: set gradients of parameters linking to masked units to zero. This is how HAT fixes the part of the network for previous tasks completely. See Eq. (2) in Sec. 2.3 &quot;Network Training&quot; in the [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="HAT-50"><a href="#HAT-50"><span class="linenos"> 50</span></a><span class="sd">            2. &#39;hat_random&#39;: set gradients of parameters linking to masked units to random 0â€“1 values. See &quot;Baselines&quot; in Sec. 4.1 in the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="HAT-51"><a href="#HAT-51"><span class="linenos"> 51</span></a><span class="sd">            3. &#39;hat_const_alpha&#39;: set gradients of parameters linking to masked units to a constant value `alpha`. See &quot;Baselines&quot; in Sec. 4.1 in the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="HAT-52"><a href="#HAT-52"><span class="linenos"> 52</span></a><span class="sd">            4. &#39;hat_const_1&#39;: set gradients of parameters linking to masked units to a constant value of 1 (i.e., no gradient constraint). See &quot;Baselines&quot; in Sec. 4.1 in the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="HAT-53"><a href="#HAT-53"><span class="linenos"> 53</span></a><span class="sd">        - **s_max** (`float`): hyperparameter, the maximum scaling factor in the gate function. See Sec. 2.4 &quot;Hard Attention Training&quot; in the [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="HAT-54"><a href="#HAT-54"><span class="linenos"> 54</span></a><span class="sd">        - **clamp_threshold** (`float`): the threshold for task embedding gradient compensation. See Sec. 2.5 &quot;Embedding Gradient Compensation&quot; in the [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="HAT-55"><a href="#HAT-55"><span class="linenos"> 55</span></a><span class="sd">        - **mask_sparsity_reg_factor** (`float`): hyperparameter, the regularization factor for mask sparsity.</span>
</span><span id="HAT-56"><a href="#HAT-56"><span class="linenos"> 56</span></a><span class="sd">        - **mask_sparsity_reg_mode** (`str`): the mode of mask sparsity regularization, must be one of:</span>
</span><span id="HAT-57"><a href="#HAT-57"><span class="linenos"> 57</span></a><span class="sd">            1. &#39;original&#39; (default): the original mask sparsity regularization in the HAT paper.</span>
</span><span id="HAT-58"><a href="#HAT-58"><span class="linenos"> 58</span></a><span class="sd">            2. &#39;cross&#39;: the cross version of mask sparsity regularization.</span>
</span><span id="HAT-59"><a href="#HAT-59"><span class="linenos"> 59</span></a><span class="sd">        - **task_embedding_init_mode** (`str`): the initialization mode for task embeddings, must be one of:</span>
</span><span id="HAT-60"><a href="#HAT-60"><span class="linenos"> 60</span></a><span class="sd">            1. &#39;N01&#39; (default): standard normal distribution $N(0, 1)$.</span>
</span><span id="HAT-61"><a href="#HAT-61"><span class="linenos"> 61</span></a><span class="sd">            2. &#39;U-11&#39;: uniform distribution $U(-1, 1)$.</span>
</span><span id="HAT-62"><a href="#HAT-62"><span class="linenos"> 62</span></a><span class="sd">            3. &#39;U01&#39;: uniform distribution $U(0, 1)$.</span>
</span><span id="HAT-63"><a href="#HAT-63"><span class="linenos"> 63</span></a><span class="sd">            4. &#39;U-10&#39;: uniform distribution $U(-1, 0)$.</span>
</span><span id="HAT-64"><a href="#HAT-64"><span class="linenos"> 64</span></a><span class="sd">            5. &#39;last&#39;: inherit the task embedding from the last task.</span>
</span><span id="HAT-65"><a href="#HAT-65"><span class="linenos"> 65</span></a><span class="sd">        - **alpha** (`float` | `None`): the `alpha` in the &#39;HAT-const-alpha&#39; mode. Applies only when `adjustment_mode` is &#39;hat_const_alpha&#39;.</span>
</span><span id="HAT-66"><a href="#HAT-66"><span class="linenos"> 66</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HAT-67"><a href="#HAT-67"><span class="linenos"> 67</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">backbone</span><span class="o">=</span><span class="n">backbone</span><span class="p">,</span> <span class="n">heads</span><span class="o">=</span><span class="n">heads</span><span class="p">)</span>
</span><span id="HAT-68"><a href="#HAT-68"><span class="linenos"> 68</span></a>
</span><span id="HAT-69"><a href="#HAT-69"><span class="linenos"> 69</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">adjustment_mode</span>
</span><span id="HAT-70"><a href="#HAT-70"><span class="linenos"> 70</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The adjustment mode for gradient clipping.&quot;&quot;&quot;</span>
</span><span id="HAT-71"><a href="#HAT-71"><span class="linenos"> 71</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">s_max</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">s_max</span>
</span><span id="HAT-72"><a href="#HAT-72"><span class="linenos"> 72</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The hyperparameter s_max.&quot;&quot;&quot;</span>
</span><span id="HAT-73"><a href="#HAT-73"><span class="linenos"> 73</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">clamp_threshold</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">clamp_threshold</span>
</span><span id="HAT-74"><a href="#HAT-74"><span class="linenos"> 74</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The clamp threshold for task embedding gradient compensation.&quot;&quot;&quot;</span>
</span><span id="HAT-75"><a href="#HAT-75"><span class="linenos"> 75</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">mask_sparsity_reg_factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">mask_sparsity_reg_factor</span>
</span><span id="HAT-76"><a href="#HAT-76"><span class="linenos"> 76</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The mask sparsity regularization factor.&quot;&quot;&quot;</span>
</span><span id="HAT-77"><a href="#HAT-77"><span class="linenos"> 77</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">mask_sparsity_reg_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">mask_sparsity_reg_mode</span>
</span><span id="HAT-78"><a href="#HAT-78"><span class="linenos"> 78</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The mask sparsity regularization mode.&quot;&quot;&quot;</span>
</span><span id="HAT-79"><a href="#HAT-79"><span class="linenos"> 79</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">mark_sparsity_reg</span><span class="p">:</span> <span class="n">HATMaskSparsityReg</span> <span class="o">=</span> <span class="n">HATMaskSparsityReg</span><span class="p">(</span>
</span><span id="HAT-80"><a href="#HAT-80"><span class="linenos"> 80</span></a>            <span class="n">factor</span><span class="o">=</span><span class="n">mask_sparsity_reg_factor</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">mask_sparsity_reg_mode</span>
</span><span id="HAT-81"><a href="#HAT-81"><span class="linenos"> 81</span></a>        <span class="p">)</span>
</span><span id="HAT-82"><a href="#HAT-82"><span class="linenos"> 82</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The mask sparsity regularizer.&quot;&quot;&quot;</span>
</span><span id="HAT-83"><a href="#HAT-83"><span class="linenos"> 83</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_init_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">task_embedding_init_mode</span>
</span><span id="HAT-84"><a href="#HAT-84"><span class="linenos"> 84</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the task embedding initialization mode.&quot;&quot;&quot;</span>
</span><span id="HAT-85"><a href="#HAT-85"><span class="linenos"> 85</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">alpha</span>
</span><span id="HAT-86"><a href="#HAT-86"><span class="linenos"> 86</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The hyperparameter alpha for `hat_const_alpha`.&quot;&quot;&quot;</span>
</span><span id="HAT-87"><a href="#HAT-87"><span class="linenos"> 87</span></a>        <span class="c1"># self.epsilon: float | None = None</span>
</span><span id="HAT-88"><a href="#HAT-88"><span class="linenos"> 88</span></a>        <span class="c1"># r&quot;&quot;&quot;HAT doesn&#39;t use epsilon for `hat_const_alpha`. It is kept for consistency with `epsilon` in `clip_grad_by_adjustment()` in `HATMaskBackbone`.&quot;&quot;&quot;</span>
</span><span id="HAT-89"><a href="#HAT-89"><span class="linenos"> 89</span></a>
</span><span id="HAT-90"><a href="#HAT-90"><span class="linenos"> 90</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">cumulative_mask_for_previous_tasks</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="HAT-91"><a href="#HAT-91"><span class="linenos"> 91</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The cumulative binary attention mask $\mathrm{M}^{&lt;t}$ of previous tasks $1,\cdots, t-1$, gated from the task embedding ($t$ is `self.task_id`). It is a dict where keys are layer names and values are the binary mask tensors for the layers. The mask tensor has size (number of units, ). &quot;&quot;&quot;</span>
</span><span id="HAT-92"><a href="#HAT-92"><span class="linenos"> 92</span></a>
</span><span id="HAT-93"><a href="#HAT-93"><span class="linenos"> 93</span></a>        <span class="c1"># set manual optimization</span>
</span><span id="HAT-94"><a href="#HAT-94"><span class="linenos"> 94</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">automatic_optimization</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="HAT-95"><a href="#HAT-95"><span class="linenos"> 95</span></a>
</span><span id="HAT-96"><a href="#HAT-96"><span class="linenos"> 96</span></a>        <span class="n">HAT</span><span class="o">.</span><span class="n">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</span><span id="HAT-97"><a href="#HAT-97"><span class="linenos"> 97</span></a>
</span><span id="HAT-98"><a href="#HAT-98"><span class="linenos"> 98</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HAT-99"><a href="#HAT-99"><span class="linenos"> 99</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Check the sanity of the arguments.&quot;&quot;&quot;</span>
</span><span id="HAT-100"><a href="#HAT-100"><span class="linenos">100</span></a>
</span><span id="HAT-101"><a href="#HAT-101"><span class="linenos">101</span></a>        <span class="c1"># check the backbone and heads</span>
</span><span id="HAT-102"><a href="#HAT-102"><span class="linenos">102</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="p">,</span> <span class="n">HATMaskBackbone</span><span class="p">):</span>
</span><span id="HAT-103"><a href="#HAT-103"><span class="linenos">103</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The backbone should be an instance of `HATMaskBackbone`.&quot;</span><span class="p">)</span>
</span><span id="HAT-104"><a href="#HAT-104"><span class="linenos">104</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">HeadsTIL</span><span class="p">):</span>
</span><span id="HAT-105"><a href="#HAT-105"><span class="linenos">105</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The heads should be an instance of `HeadsTIL`.&quot;</span><span class="p">)</span>
</span><span id="HAT-106"><a href="#HAT-106"><span class="linenos">106</span></a>
</span><span id="HAT-107"><a href="#HAT-107"><span class="linenos">107</span></a>        <span class="c1"># check marker sparsity regularization mode</span>
</span><span id="HAT-108"><a href="#HAT-108"><span class="linenos">108</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask_sparsity_reg_mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;original&quot;</span><span class="p">,</span> <span class="s2">&quot;cross&quot;</span><span class="p">]:</span>
</span><span id="HAT-109"><a href="#HAT-109"><span class="linenos">109</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="HAT-110"><a href="#HAT-110"><span class="linenos">110</span></a>                <span class="s2">&quot;The mask_sparsity_reg_mode should be one of &#39;original&#39;, &#39;cross&#39;.&quot;</span>
</span><span id="HAT-111"><a href="#HAT-111"><span class="linenos">111</span></a>            <span class="p">)</span>
</span><span id="HAT-112"><a href="#HAT-112"><span class="linenos">112</span></a>
</span><span id="HAT-113"><a href="#HAT-113"><span class="linenos">113</span></a>        <span class="c1"># check task embedding initialization mode</span>
</span><span id="HAT-114"><a href="#HAT-114"><span class="linenos">114</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_init_mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span>
</span><span id="HAT-115"><a href="#HAT-115"><span class="linenos">115</span></a>            <span class="s2">&quot;N01&quot;</span><span class="p">,</span>
</span><span id="HAT-116"><a href="#HAT-116"><span class="linenos">116</span></a>            <span class="s2">&quot;U01&quot;</span><span class="p">,</span>
</span><span id="HAT-117"><a href="#HAT-117"><span class="linenos">117</span></a>            <span class="s2">&quot;U-10&quot;</span><span class="p">,</span>
</span><span id="HAT-118"><a href="#HAT-118"><span class="linenos">118</span></a>            <span class="s2">&quot;masked&quot;</span><span class="p">,</span>
</span><span id="HAT-119"><a href="#HAT-119"><span class="linenos">119</span></a>            <span class="s2">&quot;unmasked&quot;</span><span class="p">,</span>
</span><span id="HAT-120"><a href="#HAT-120"><span class="linenos">120</span></a>        <span class="p">]:</span>
</span><span id="HAT-121"><a href="#HAT-121"><span class="linenos">121</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="HAT-122"><a href="#HAT-122"><span class="linenos">122</span></a>                <span class="s2">&quot;The task_embedding_init_mode should be one of &#39;N01&#39;, &#39;U01&#39;, &#39;U-10&#39;, &#39;masked&#39;, &#39;unmasked&#39;.&quot;</span>
</span><span id="HAT-123"><a href="#HAT-123"><span class="linenos">123</span></a>            <span class="p">)</span>
</span><span id="HAT-124"><a href="#HAT-124"><span class="linenos">124</span></a>
</span><span id="HAT-125"><a href="#HAT-125"><span class="linenos">125</span></a>        <span class="c1"># check adjustment mode `hat_const_alpha`</span>
</span><span id="HAT-126"><a href="#HAT-126"><span class="linenos">126</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;hat_const_alpha&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HAT-127"><a href="#HAT-127"><span class="linenos">127</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="HAT-128"><a href="#HAT-128"><span class="linenos">128</span></a>                <span class="s2">&quot;Alpha should be given when the adjustment_mode is &#39;hat_const_alpha&#39;.&quot;</span>
</span><span id="HAT-129"><a href="#HAT-129"><span class="linenos">129</span></a>            <span class="p">)</span>
</span><span id="HAT-130"><a href="#HAT-130"><span class="linenos">130</span></a>
</span><span id="HAT-131"><a href="#HAT-131"><span class="linenos">131</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_start</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HAT-132"><a href="#HAT-132"><span class="linenos">132</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialize the task embedding before training the next task and initialize the cumulative mask at the beginning of the first task.&quot;&quot;&quot;</span>
</span><span id="HAT-133"><a href="#HAT-133"><span class="linenos">133</span></a>
</span><span id="HAT-134"><a href="#HAT-134"><span class="linenos">134</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">initialize_task_embedding</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_init_mode</span><span class="p">)</span>
</span><span id="HAT-135"><a href="#HAT-135"><span class="linenos">135</span></a>
</span><span id="HAT-136"><a href="#HAT-136"><span class="linenos">136</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">initialize_independent_bn</span><span class="p">()</span>
</span><span id="HAT-137"><a href="#HAT-137"><span class="linenos">137</span></a>
</span><span id="HAT-138"><a href="#HAT-138"><span class="linenos">138</span></a>        <span class="c1"># initialize the cumulative mask for the first task at the beginning of the first task. This should not be called in `__init__()` because `self.device` is not available at that time.</span>
</span><span id="HAT-139"><a href="#HAT-139"><span class="linenos">139</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="HAT-140"><a href="#HAT-140"><span class="linenos">140</span></a>            <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="HAT-141"><a href="#HAT-141"><span class="linenos">141</span></a>                <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span>
</span><span id="HAT-142"><a href="#HAT-142"><span class="linenos">142</span></a>                    <span class="n">layer_name</span>
</span><span id="HAT-143"><a href="#HAT-143"><span class="linenos">143</span></a>                <span class="p">)</span>  <span class="c1"># get the layer by its name</span>
</span><span id="HAT-144"><a href="#HAT-144"><span class="linenos">144</span></a>                <span class="n">num_units</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="HAT-145"><a href="#HAT-145"><span class="linenos">145</span></a>
</span><span id="HAT-146"><a href="#HAT-146"><span class="linenos">146</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">cumulative_mask_for_previous_tasks</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
</span><span id="HAT-147"><a href="#HAT-147"><span class="linenos">147</span></a>                    <span class="n">num_units</span>
</span><span id="HAT-148"><a href="#HAT-148"><span class="linenos">148</span></a>                <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
</span><span id="HAT-149"><a href="#HAT-149"><span class="linenos">149</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
</span><span id="HAT-150"><a href="#HAT-150"><span class="linenos">150</span></a>                <span class="p">)</span>  <span class="c1"># the cumulative mask $\mathrm{M}^{&lt;t}$ is initialized as a zeros mask ($t = 1$). See Eq. (2) in Sec. 3 in the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9), or Eq. (5) in Sec. 2.6 &quot;Promoting Low Capacity Usage&quot; in the [HAT paper](http://proceedings.mlr.press/v80/serra18a)</span>
</span><span id="HAT-151"><a href="#HAT-151"><span class="linenos">151</span></a>
</span><span id="HAT-152"><a href="#HAT-152"><span class="linenos">152</span></a>                <span class="c1"># self.neuron_first_task[layer_name] = [None] * num_units</span>
</span><span id="HAT-153"><a href="#HAT-153"><span class="linenos">153</span></a>
</span><span id="HAT-154"><a href="#HAT-154"><span class="linenos">154</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">clip_grad_by_adjustment</span><span class="p">(</span>
</span><span id="HAT-155"><a href="#HAT-155"><span class="linenos">155</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="HAT-156"><a href="#HAT-156"><span class="linenos">156</span></a>        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
</span><span id="HAT-157"><a href="#HAT-157"><span class="linenos">157</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="n">Tensor</span><span class="p">]:</span>
</span><span id="HAT-158"><a href="#HAT-158"><span class="linenos">158</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Clip the gradients by the adjustment rate. See Eq. (2) in Sec. 2.3 &quot;Network Training&quot; in the [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="HAT-159"><a href="#HAT-159"><span class="linenos">159</span></a>
</span><span id="HAT-160"><a href="#HAT-160"><span class="linenos">160</span></a><span class="sd">        Note that because the task embedding fully covers every layer in the backbone network, no parameters are left out of this system.</span>
</span><span id="HAT-161"><a href="#HAT-161"><span class="linenos">161</span></a><span class="sd">        This applies not only to parameters between layers with task embeddings, but also to those before the first layer. We design it separately in the code.</span>
</span><span id="HAT-162"><a href="#HAT-162"><span class="linenos">162</span></a>
</span><span id="HAT-163"><a href="#HAT-163"><span class="linenos">163</span></a><span class="sd">        Network capacity is measured alongside this method. Network capacity is defined as the average adjustment rate over all parameters.</span>
</span><span id="HAT-164"><a href="#HAT-164"><span class="linenos">164</span></a><span class="sd">        See Sec. 4.1 in the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="HAT-165"><a href="#HAT-165"><span class="linenos">165</span></a>
</span><span id="HAT-166"><a href="#HAT-166"><span class="linenos">166</span></a><span class="sd">        **Returns:**</span>
</span><span id="HAT-167"><a href="#HAT-167"><span class="linenos">167</span></a><span class="sd">        - **adjustment_rate_weight** (`dict[str, Tensor]`): the adjustment rate for weights. Keys (`str`) are layer names and values (`Tensor`) are the adjustment rate tensors.</span>
</span><span id="HAT-168"><a href="#HAT-168"><span class="linenos">168</span></a><span class="sd">        - **adjustment_rate_bias** (`dict[str, Tensor]`): the adjustment rate for biases. Keys (`str`) are layer name and values (`Tensor`) are the adjustment rate tensors.</span>
</span><span id="HAT-169"><a href="#HAT-169"><span class="linenos">169</span></a><span class="sd">        - **capacity** (`Tensor`): the calculated network capacity.</span>
</span><span id="HAT-170"><a href="#HAT-170"><span class="linenos">170</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HAT-171"><a href="#HAT-171"><span class="linenos">171</span></a>
</span><span id="HAT-172"><a href="#HAT-172"><span class="linenos">172</span></a>        <span class="c1"># initialize network capacity metric</span>
</span><span id="HAT-173"><a href="#HAT-173"><span class="linenos">173</span></a>        <span class="n">capacity</span> <span class="o">=</span> <span class="n">HATNetworkCapacityMetric</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="HAT-174"><a href="#HAT-174"><span class="linenos">174</span></a>        <span class="n">adjustment_rate_weight</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="HAT-175"><a href="#HAT-175"><span class="linenos">175</span></a>        <span class="n">adjustment_rate_bias</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="HAT-176"><a href="#HAT-176"><span class="linenos">176</span></a>
</span><span id="HAT-177"><a href="#HAT-177"><span class="linenos">177</span></a>        <span class="c1"># calculate the adjustment rate for gradients of the parameters, both weights and biases (if they exist)</span>
</span><span id="HAT-178"><a href="#HAT-178"><span class="linenos">178</span></a>        <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="HAT-179"><a href="#HAT-179"><span class="linenos">179</span></a>
</span><span id="HAT-180"><a href="#HAT-180"><span class="linenos">180</span></a>            <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span>
</span><span id="HAT-181"><a href="#HAT-181"><span class="linenos">181</span></a>                <span class="n">layer_name</span>
</span><span id="HAT-182"><a href="#HAT-182"><span class="linenos">182</span></a>            <span class="p">)</span>  <span class="c1"># get the layer by its name</span>
</span><span id="HAT-183"><a href="#HAT-183"><span class="linenos">183</span></a>
</span><span id="HAT-184"><a href="#HAT-184"><span class="linenos">184</span></a>            <span class="c1"># placeholder for the adjustment rate to avoid the error of using it before assignment</span>
</span><span id="HAT-185"><a href="#HAT-185"><span class="linenos">185</span></a>            <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="HAT-186"><a href="#HAT-186"><span class="linenos">186</span></a>            <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="HAT-187"><a href="#HAT-187"><span class="linenos">187</span></a>
</span><span id="HAT-188"><a href="#HAT-188"><span class="linenos">188</span></a>            <span class="n">weight_mask</span><span class="p">,</span> <span class="n">bias_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_measure_parameter_wise</span><span class="p">(</span>
</span><span id="HAT-189"><a href="#HAT-189"><span class="linenos">189</span></a>                <span class="n">unit_wise_measure</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">cumulative_mask_for_previous_tasks</span><span class="p">,</span>
</span><span id="HAT-190"><a href="#HAT-190"><span class="linenos">190</span></a>                <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="HAT-191"><a href="#HAT-191"><span class="linenos">191</span></a>                <span class="n">aggregation_mode</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span>
</span><span id="HAT-192"><a href="#HAT-192"><span class="linenos">192</span></a>            <span class="p">)</span>
</span><span id="HAT-193"><a href="#HAT-193"><span class="linenos">193</span></a>
</span><span id="HAT-194"><a href="#HAT-194"><span class="linenos">194</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;hat&quot;</span><span class="p">:</span>
</span><span id="HAT-195"><a href="#HAT-195"><span class="linenos">195</span></a>                <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">weight_mask</span>
</span><span id="HAT-196"><a href="#HAT-196"><span class="linenos">196</span></a>                <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">bias_mask</span>
</span><span id="HAT-197"><a href="#HAT-197"><span class="linenos">197</span></a>
</span><span id="HAT-198"><a href="#HAT-198"><span class="linenos">198</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;hat_random&quot;</span><span class="p">:</span>
</span><span id="HAT-199"><a href="#HAT-199"><span class="linenos">199</span></a>                <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand_like</span><span class="p">(</span>
</span><span id="HAT-200"><a href="#HAT-200"><span class="linenos">200</span></a>                    <span class="n">weight_mask</span>
</span><span id="HAT-201"><a href="#HAT-201"><span class="linenos">201</span></a>                <span class="p">)</span> <span class="o">*</span> <span class="n">weight_mask</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">weight_mask</span><span class="p">)</span>
</span><span id="HAT-202"><a href="#HAT-202"><span class="linenos">202</span></a>                <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand_like</span><span class="p">(</span><span class="n">bias_mask</span><span class="p">)</span> <span class="o">*</span> <span class="n">bias_mask</span> <span class="o">+</span> <span class="p">(</span>
</span><span id="HAT-203"><a href="#HAT-203"><span class="linenos">203</span></a>                    <span class="mi">1</span> <span class="o">-</span> <span class="n">bias_mask</span>
</span><span id="HAT-204"><a href="#HAT-204"><span class="linenos">204</span></a>                <span class="p">)</span>
</span><span id="HAT-205"><a href="#HAT-205"><span class="linenos">205</span></a>
</span><span id="HAT-206"><a href="#HAT-206"><span class="linenos">206</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;hat_const_alpha&quot;</span><span class="p">:</span>
</span><span id="HAT-207"><a href="#HAT-207"><span class="linenos">207</span></a>                <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span>
</span><span id="HAT-208"><a href="#HAT-208"><span class="linenos">208</span></a>                    <span class="n">weight_mask</span>
</span><span id="HAT-209"><a href="#HAT-209"><span class="linenos">209</span></a>                <span class="p">)</span> <span class="o">*</span> <span class="n">weight_mask</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">weight_mask</span><span class="p">)</span>
</span><span id="HAT-210"><a href="#HAT-210"><span class="linenos">210</span></a>                <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span>
</span><span id="HAT-211"><a href="#HAT-211"><span class="linenos">211</span></a>                    <span class="n">bias_mask</span>
</span><span id="HAT-212"><a href="#HAT-212"><span class="linenos">212</span></a>                <span class="p">)</span> <span class="o">*</span> <span class="n">bias_mask</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">bias_mask</span><span class="p">)</span>
</span><span id="HAT-213"><a href="#HAT-213"><span class="linenos">213</span></a>
</span><span id="HAT-214"><a href="#HAT-214"><span class="linenos">214</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;hat_const_1&quot;</span><span class="p">:</span>
</span><span id="HAT-215"><a href="#HAT-215"><span class="linenos">215</span></a>                <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span>
</span><span id="HAT-216"><a href="#HAT-216"><span class="linenos">216</span></a>                    <span class="n">weight_mask</span>
</span><span id="HAT-217"><a href="#HAT-217"><span class="linenos">217</span></a>                <span class="p">)</span> <span class="o">*</span> <span class="n">weight_mask</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">weight_mask</span><span class="p">)</span>
</span><span id="HAT-218"><a href="#HAT-218"><span class="linenos">218</span></a>                <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">bias_mask</span><span class="p">)</span> <span class="o">*</span> <span class="n">bias_mask</span> <span class="o">+</span> <span class="p">(</span>
</span><span id="HAT-219"><a href="#HAT-219"><span class="linenos">219</span></a>                    <span class="mi">1</span> <span class="o">-</span> <span class="n">bias_mask</span>
</span><span id="HAT-220"><a href="#HAT-220"><span class="linenos">220</span></a>                <span class="p">)</span>
</span><span id="HAT-221"><a href="#HAT-221"><span class="linenos">221</span></a>
</span><span id="HAT-222"><a href="#HAT-222"><span class="linenos">222</span></a>            <span class="c1"># apply the adjustment rate to the gradients</span>
</span><span id="HAT-223"><a href="#HAT-223"><span class="linenos">223</span></a>            <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">adjustment_rate_weight_layer</span>
</span><span id="HAT-224"><a href="#HAT-224"><span class="linenos">224</span></a>            <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HAT-225"><a href="#HAT-225"><span class="linenos">225</span></a>                <span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">adjustment_rate_bias_layer</span>
</span><span id="HAT-226"><a href="#HAT-226"><span class="linenos">226</span></a>
</span><span id="HAT-227"><a href="#HAT-227"><span class="linenos">227</span></a>            <span class="c1"># store the adjustment rate for logging</span>
</span><span id="HAT-228"><a href="#HAT-228"><span class="linenos">228</span></a>            <span class="n">adjustment_rate_weight</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">adjustment_rate_weight_layer</span>
</span><span id="HAT-229"><a href="#HAT-229"><span class="linenos">229</span></a>            <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HAT-230"><a href="#HAT-230"><span class="linenos">230</span></a>                <span class="n">adjustment_rate_bias</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">adjustment_rate_bias_layer</span>
</span><span id="HAT-231"><a href="#HAT-231"><span class="linenos">231</span></a>
</span><span id="HAT-232"><a href="#HAT-232"><span class="linenos">232</span></a>            <span class="c1"># update network capacity metric</span>
</span><span id="HAT-233"><a href="#HAT-233"><span class="linenos">233</span></a>            <span class="n">capacity</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">adjustment_rate_weight_layer</span><span class="p">,</span> <span class="n">adjustment_rate_bias_layer</span><span class="p">)</span>
</span><span id="HAT-234"><a href="#HAT-234"><span class="linenos">234</span></a>
</span><span id="HAT-235"><a href="#HAT-235"><span class="linenos">235</span></a>        <span class="k">return</span> <span class="n">adjustment_rate_weight</span><span class="p">,</span> <span class="n">adjustment_rate_bias</span><span class="p">,</span> <span class="n">capacity</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
</span><span id="HAT-236"><a href="#HAT-236"><span class="linenos">236</span></a>
</span><span id="HAT-237"><a href="#HAT-237"><span class="linenos">237</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">compensate_task_embedding_gradients</span><span class="p">(</span>
</span><span id="HAT-238"><a href="#HAT-238"><span class="linenos">238</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="HAT-239"><a href="#HAT-239"><span class="linenos">239</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HAT-240"><a href="#HAT-240"><span class="linenos">240</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HAT-241"><a href="#HAT-241"><span class="linenos">241</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HAT-242"><a href="#HAT-242"><span class="linenos">242</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Compensate the gradients of task embeddings during training. See Sec. 2.5 &quot;Embedding Gradient Compensation&quot; in the [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="HAT-243"><a href="#HAT-243"><span class="linenos">243</span></a>
</span><span id="HAT-244"><a href="#HAT-244"><span class="linenos">244</span></a><span class="sd">        **Args:**</span>
</span><span id="HAT-245"><a href="#HAT-245"><span class="linenos">245</span></a><span class="sd">        - **batch_idx** (`int`): the current training batch index.</span>
</span><span id="HAT-246"><a href="#HAT-246"><span class="linenos">246</span></a><span class="sd">        - **num_batches** (`int`): the total number of training batches.</span>
</span><span id="HAT-247"><a href="#HAT-247"><span class="linenos">247</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HAT-248"><a href="#HAT-248"><span class="linenos">248</span></a>
</span><span id="HAT-249"><a href="#HAT-249"><span class="linenos">249</span></a>        <span class="k">for</span> <span class="n">te</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
</span><span id="HAT-250"><a href="#HAT-250"><span class="linenos">250</span></a>            <span class="n">anneal_scalar</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_max</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">s_max</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_max</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span>
</span><span id="HAT-251"><a href="#HAT-251"><span class="linenos">251</span></a>                <span class="n">batch_idx</span> <span class="o">-</span> <span class="mi">1</span>
</span><span id="HAT-252"><a href="#HAT-252"><span class="linenos">252</span></a>            <span class="p">)</span> <span class="o">/</span> <span class="p">(</span>
</span><span id="HAT-253"><a href="#HAT-253"><span class="linenos">253</span></a>                <span class="n">num_batches</span> <span class="o">-</span> <span class="mi">1</span>
</span><span id="HAT-254"><a href="#HAT-254"><span class="linenos">254</span></a>            <span class="p">)</span>  <span class="c1"># see Eq. (3) in Sec. 2.4 &quot;Hard Attention Training&quot; in the [HAT paper](http://proceedings.mlr.press/v80/serra18a)</span>
</span><span id="HAT-255"><a href="#HAT-255"><span class="linenos">255</span></a>
</span><span id="HAT-256"><a href="#HAT-256"><span class="linenos">256</span></a>            <span class="n">num</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="HAT-257"><a href="#HAT-257"><span class="linenos">257</span></a>                <span class="n">torch</span><span class="o">.</span><span class="n">cosh</span><span class="p">(</span>
</span><span id="HAT-258"><a href="#HAT-258"><span class="linenos">258</span></a>                    <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span>
</span><span id="HAT-259"><a href="#HAT-259"><span class="linenos">259</span></a>                        <span class="n">anneal_scalar</span> <span class="o">*</span> <span class="n">te</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">,</span>
</span><span id="HAT-260"><a href="#HAT-260"><span class="linenos">260</span></a>                        <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">clamp_threshold</span><span class="p">,</span>
</span><span id="HAT-261"><a href="#HAT-261"><span class="linenos">261</span></a>                        <span class="bp">self</span><span class="o">.</span><span class="n">clamp_threshold</span><span class="p">,</span>
</span><span id="HAT-262"><a href="#HAT-262"><span class="linenos">262</span></a>                    <span class="p">)</span>
</span><span id="HAT-263"><a href="#HAT-263"><span class="linenos">263</span></a>                <span class="p">)</span>
</span><span id="HAT-264"><a href="#HAT-264"><span class="linenos">264</span></a>                <span class="o">+</span> <span class="mi">1</span>
</span><span id="HAT-265"><a href="#HAT-265"><span class="linenos">265</span></a>            <span class="p">)</span>
</span><span id="HAT-266"><a href="#HAT-266"><span class="linenos">266</span></a>
</span><span id="HAT-267"><a href="#HAT-267"><span class="linenos">267</span></a>            <span class="n">den</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cosh</span><span class="p">(</span><span class="n">te</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="HAT-268"><a href="#HAT-268"><span class="linenos">268</span></a>
</span><span id="HAT-269"><a href="#HAT-269"><span class="linenos">269</span></a>            <span class="n">compensation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_max</span> <span class="o">/</span> <span class="n">anneal_scalar</span> <span class="o">*</span> <span class="n">num</span> <span class="o">/</span> <span class="n">den</span>
</span><span id="HAT-270"><a href="#HAT-270"><span class="linenos">270</span></a>
</span><span id="HAT-271"><a href="#HAT-271"><span class="linenos">271</span></a>            <span class="n">te</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">compensation</span>
</span><span id="HAT-272"><a href="#HAT-272"><span class="linenos">272</span></a>
</span><span id="HAT-273"><a href="#HAT-273"><span class="linenos">273</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="HAT-274"><a href="#HAT-274"><span class="linenos">274</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="HAT-275"><a href="#HAT-275"><span class="linenos">275</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="HAT-276"><a href="#HAT-276"><span class="linenos">276</span></a>        <span class="n">stage</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="HAT-277"><a href="#HAT-277"><span class="linenos">277</span></a>        <span class="n">task_id</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HAT-278"><a href="#HAT-278"><span class="linenos">278</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HAT-279"><a href="#HAT-279"><span class="linenos">279</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HAT-280"><a href="#HAT-280"><span class="linenos">280</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
</span><span id="HAT-281"><a href="#HAT-281"><span class="linenos">281</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The forward pass for data from task `task_id`. Note that it is nothing to do with `forward()` method in `nn.Module`.</span>
</span><span id="HAT-282"><a href="#HAT-282"><span class="linenos">282</span></a>
</span><span id="HAT-283"><a href="#HAT-283"><span class="linenos">283</span></a><span class="sd">        **Args:**</span>
</span><span id="HAT-284"><a href="#HAT-284"><span class="linenos">284</span></a><span class="sd">        - **input** (`Tensor`): The input tensor from data.</span>
</span><span id="HAT-285"><a href="#HAT-285"><span class="linenos">285</span></a><span class="sd">        - **stage** (`str`): the stage of the forward pass, should be one of the following:</span>
</span><span id="HAT-286"><a href="#HAT-286"><span class="linenos">286</span></a><span class="sd">            1. &#39;train&#39;: training stage.</span>
</span><span id="HAT-287"><a href="#HAT-287"><span class="linenos">287</span></a><span class="sd">            2. &#39;validation&#39;: validation stage.</span>
</span><span id="HAT-288"><a href="#HAT-288"><span class="linenos">288</span></a><span class="sd">            3. &#39;test&#39;: testing stage.</span>
</span><span id="HAT-289"><a href="#HAT-289"><span class="linenos">289</span></a><span class="sd">        - **task_id** (`int`| `None`): the task ID where the data are from. If the stage is &#39;train&#39; or &#39;validation&#39;, it should be the current task `self.task_id`. If stage is &#39;test&#39;, it could be from any seen task. In TIL, the task IDs of test data are provided thus this argument can be used. HAT algorithm works only for TIL.</span>
</span><span id="HAT-290"><a href="#HAT-290"><span class="linenos">290</span></a><span class="sd">        - **batch_idx** (`int` | `None`): the current batch index. Applies only to training stage. For other stages, it is default `None`.</span>
</span><span id="HAT-291"><a href="#HAT-291"><span class="linenos">291</span></a><span class="sd">        - **num_batches** (`int` | `None`): the total number of batches. Applies only to training stage. For other stages, it is default `None`.</span>
</span><span id="HAT-292"><a href="#HAT-292"><span class="linenos">292</span></a>
</span><span id="HAT-293"><a href="#HAT-293"><span class="linenos">293</span></a><span class="sd">        **Returns:**</span>
</span><span id="HAT-294"><a href="#HAT-294"><span class="linenos">294</span></a><span class="sd">        - **logits** (`Tensor`): the output logits tensor.</span>
</span><span id="HAT-295"><a href="#HAT-295"><span class="linenos">295</span></a><span class="sd">        - **mask** (`dict[str, Tensor]`): the mask for the current task. Key (`str`) is layer name, value (`Tensor`) is the mask tensor. The mask tensor has size (number of units, ).</span>
</span><span id="HAT-296"><a href="#HAT-296"><span class="linenos">296</span></a><span class="sd">        - **activations** (`dict[str, Tensor]`): the hidden features (after activation) in each weighted layer. Key (`str`) is the weighted layer name, value (`Tensor`) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes. Although HAT algorithm does not need this, it is still provided for API consistence for other HAT-based algorithms inherited this `forward()` method of `HAT` class.</span>
</span><span id="HAT-297"><a href="#HAT-297"><span class="linenos">297</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HAT-298"><a href="#HAT-298"><span class="linenos">298</span></a>        <span class="n">feature</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="p">(</span>
</span><span id="HAT-299"><a href="#HAT-299"><span class="linenos">299</span></a>            <span class="nb">input</span><span class="p">,</span>
</span><span id="HAT-300"><a href="#HAT-300"><span class="linenos">300</span></a>            <span class="n">stage</span><span class="o">=</span><span class="n">stage</span><span class="p">,</span>
</span><span id="HAT-301"><a href="#HAT-301"><span class="linenos">301</span></a>            <span class="n">s_max</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">s_max</span> <span class="k">if</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;train&quot;</span> <span class="ow">or</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;validation&quot;</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HAT-302"><a href="#HAT-302"><span class="linenos">302</span></a>            <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span> <span class="k">if</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;train&quot;</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HAT-303"><a href="#HAT-303"><span class="linenos">303</span></a>            <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span> <span class="k">if</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;train&quot;</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HAT-304"><a href="#HAT-304"><span class="linenos">304</span></a>            <span class="n">test_task_id</span><span class="o">=</span><span class="n">task_id</span> <span class="k">if</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;test&quot;</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HAT-305"><a href="#HAT-305"><span class="linenos">305</span></a>        <span class="p">)</span>
</span><span id="HAT-306"><a href="#HAT-306"><span class="linenos">306</span></a>        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">heads</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="n">task_id</span><span class="p">)</span>
</span><span id="HAT-307"><a href="#HAT-307"><span class="linenos">307</span></a>
</span><span id="HAT-308"><a href="#HAT-308"><span class="linenos">308</span></a>        <span class="k">return</span> <span class="p">(</span>
</span><span id="HAT-309"><a href="#HAT-309"><span class="linenos">309</span></a>            <span class="n">logits</span>
</span><span id="HAT-310"><a href="#HAT-310"><span class="linenos">310</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">if_forward_func_return_logits_only</span>
</span><span id="HAT-311"><a href="#HAT-311"><span class="linenos">311</span></a>            <span class="k">else</span> <span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">activations</span><span class="p">)</span>
</span><span id="HAT-312"><a href="#HAT-312"><span class="linenos">312</span></a>        <span class="p">)</span>
</span><span id="HAT-313"><a href="#HAT-313"><span class="linenos">313</span></a>
</span><span id="HAT-314"><a href="#HAT-314"><span class="linenos">314</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
</span><span id="HAT-315"><a href="#HAT-315"><span class="linenos">315</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Training step for current task `self.task_id`.</span>
</span><span id="HAT-316"><a href="#HAT-316"><span class="linenos">316</span></a>
</span><span id="HAT-317"><a href="#HAT-317"><span class="linenos">317</span></a><span class="sd">        **Args:**</span>
</span><span id="HAT-318"><a href="#HAT-318"><span class="linenos">318</span></a><span class="sd">        - **batch** (`Any`): a batch of training data.</span>
</span><span id="HAT-319"><a href="#HAT-319"><span class="linenos">319</span></a><span class="sd">        - **batch_idx** (`int`): the index of the batch. Used for calculating annealed scalar in HAT. See Sec. 2.4 &quot;Hard Attention Training&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="HAT-320"><a href="#HAT-320"><span class="linenos">320</span></a>
</span><span id="HAT-321"><a href="#HAT-321"><span class="linenos">321</span></a><span class="sd">        **Returns:**</span>
</span><span id="HAT-322"><a href="#HAT-322"><span class="linenos">322</span></a><span class="sd">        - **outputs** (`dict[str, Tensor]`): a dictionary containing loss and other metrics from this training step. Keys (`str`) are metric names, and values (`Tensor`) are the metrics. Must include the key &#39;loss&#39; (total loss) in the case of automatic optimization, according to PyTorch Lightning. For HAT, it includes &#39;mask&#39; and &#39;capacity&#39; for logging.</span>
</span><span id="HAT-323"><a href="#HAT-323"><span class="linenos">323</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HAT-324"><a href="#HAT-324"><span class="linenos">324</span></a>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
</span><span id="HAT-325"><a href="#HAT-325"><span class="linenos">325</span></a>
</span><span id="HAT-326"><a href="#HAT-326"><span class="linenos">326</span></a>        <span class="c1"># zero the gradients before forward pass in manual optimization mode</span>
</span><span id="HAT-327"><a href="#HAT-327"><span class="linenos">327</span></a>        <span class="n">opt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizers</span><span class="p">()</span>
</span><span id="HAT-328"><a href="#HAT-328"><span class="linenos">328</span></a>        <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</span><span id="HAT-329"><a href="#HAT-329"><span class="linenos">329</span></a>
</span><span id="HAT-330"><a href="#HAT-330"><span class="linenos">330</span></a>        <span class="c1"># classification loss</span>
</span><span id="HAT-331"><a href="#HAT-331"><span class="linenos">331</span></a>        <span class="n">num_batches</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">num_training_batches</span>
</span><span id="HAT-332"><a href="#HAT-332"><span class="linenos">332</span></a>        <span class="n">logits</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
</span><span id="HAT-333"><a href="#HAT-333"><span class="linenos">333</span></a>            <span class="n">x</span><span class="p">,</span>
</span><span id="HAT-334"><a href="#HAT-334"><span class="linenos">334</span></a>            <span class="n">stage</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">,</span>
</span><span id="HAT-335"><a href="#HAT-335"><span class="linenos">335</span></a>            <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="HAT-336"><a href="#HAT-336"><span class="linenos">336</span></a>            <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="HAT-337"><a href="#HAT-337"><span class="linenos">337</span></a>            <span class="n">task_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">,</span>
</span><span id="HAT-338"><a href="#HAT-338"><span class="linenos">338</span></a>        <span class="p">)</span>
</span><span id="HAT-339"><a href="#HAT-339"><span class="linenos">339</span></a>        <span class="n">loss_cls</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span><span id="HAT-340"><a href="#HAT-340"><span class="linenos">340</span></a>
</span><span id="HAT-341"><a href="#HAT-341"><span class="linenos">341</span></a>        <span class="c1"># regularization loss. See Sec. 2.6 &quot;Promoting Low Capacity Usage&quot; in the [HAT paper](http://proceedings.mlr.press/v80/serra18a)</span>
</span><span id="HAT-342"><a href="#HAT-342"><span class="linenos">342</span></a>        <span class="n">loss_reg</span><span class="p">,</span> <span class="n">network_sparsity</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mark_sparsity_reg</span><span class="p">(</span>
</span><span id="HAT-343"><a href="#HAT-343"><span class="linenos">343</span></a>            <span class="n">mask</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cumulative_mask_for_previous_tasks</span>
</span><span id="HAT-344"><a href="#HAT-344"><span class="linenos">344</span></a>        <span class="p">)</span>
</span><span id="HAT-345"><a href="#HAT-345"><span class="linenos">345</span></a>
</span><span id="HAT-346"><a href="#HAT-346"><span class="linenos">346</span></a>        <span class="c1"># total loss. See Eq. (4) in Sec. 2.6 &quot;Promoting Low Capacity Usage&quot; in the [HAT paper](http://proceedings.mlr.press/v80/serra18a)</span>
</span><span id="HAT-347"><a href="#HAT-347"><span class="linenos">347</span></a>        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_cls</span> <span class="o">+</span> <span class="n">loss_reg</span>
</span><span id="HAT-348"><a href="#HAT-348"><span class="linenos">348</span></a>
</span><span id="HAT-349"><a href="#HAT-349"><span class="linenos">349</span></a>        <span class="c1"># backward step (manually)</span>
</span><span id="HAT-350"><a href="#HAT-350"><span class="linenos">350</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">manual_backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>  <span class="c1"># calculate the gradients</span>
</span><span id="HAT-351"><a href="#HAT-351"><span class="linenos">351</span></a>        <span class="c1"># HAT hard-clips gradients using the cumulative masks. See Eq. (2) in Sec. 2.3 &quot;Network Training&quot; in the HAT paper.</span>
</span><span id="HAT-352"><a href="#HAT-352"><span class="linenos">352</span></a>        <span class="c1"># Network capacity is computed along with this process (defined as the average adjustment rate over all parameters; see Sec. 4.1 in the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9)).</span>
</span><span id="HAT-353"><a href="#HAT-353"><span class="linenos">353</span></a>
</span><span id="HAT-354"><a href="#HAT-354"><span class="linenos">354</span></a>        <span class="n">adjustment_rate_weight</span><span class="p">,</span> <span class="n">adjustment_rate_bias</span><span class="p">,</span> <span class="n">capacity</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="HAT-355"><a href="#HAT-355"><span class="linenos">355</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">clip_grad_by_adjustment</span><span class="p">(</span>
</span><span id="HAT-356"><a href="#HAT-356"><span class="linenos">356</span></a>                <span class="n">network_sparsity</span><span class="o">=</span><span class="n">network_sparsity</span><span class="p">,</span>  <span class="c1"># passed for compatibility with AdaHAT, which inherits this method</span>
</span><span id="HAT-357"><a href="#HAT-357"><span class="linenos">357</span></a>            <span class="p">)</span>
</span><span id="HAT-358"><a href="#HAT-358"><span class="linenos">358</span></a>        <span class="p">)</span>
</span><span id="HAT-359"><a href="#HAT-359"><span class="linenos">359</span></a>        <span class="c1"># compensate the gradients of task embedding. See Sec. 2.5 &quot;Embedding Gradient Compensation&quot; in the [HAT paper](http://proceedings.mlr.press/v80/serra18a)</span>
</span><span id="HAT-360"><a href="#HAT-360"><span class="linenos">360</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">compensate_task_embedding_gradients</span><span class="p">(</span>
</span><span id="HAT-361"><a href="#HAT-361"><span class="linenos">361</span></a>            <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="HAT-362"><a href="#HAT-362"><span class="linenos">362</span></a>            <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="HAT-363"><a href="#HAT-363"><span class="linenos">363</span></a>        <span class="p">)</span>
</span><span id="HAT-364"><a href="#HAT-364"><span class="linenos">364</span></a>        <span class="c1"># update parameters with the modified gradients</span>
</span><span id="HAT-365"><a href="#HAT-365"><span class="linenos">365</span></a>        <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span><span id="HAT-366"><a href="#HAT-366"><span class="linenos">366</span></a>
</span><span id="HAT-367"><a href="#HAT-367"><span class="linenos">367</span></a>        <span class="c1"># accuracy of the batch</span>
</span><span id="HAT-368"><a href="#HAT-368"><span class="linenos">368</span></a>        <span class="n">acc</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</span><span id="HAT-369"><a href="#HAT-369"><span class="linenos">369</span></a>
</span><span id="HAT-370"><a href="#HAT-370"><span class="linenos">370</span></a>        <span class="k">return</span> <span class="p">{</span>
</span><span id="HAT-371"><a href="#HAT-371"><span class="linenos">371</span></a>            <span class="s2">&quot;loss&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span>  <span class="c1"># return loss is essential for training step, or backpropagation will fail</span>
</span><span id="HAT-372"><a href="#HAT-372"><span class="linenos">372</span></a>            <span class="s2">&quot;loss_cls&quot;</span><span class="p">:</span> <span class="n">loss_cls</span><span class="p">,</span>
</span><span id="HAT-373"><a href="#HAT-373"><span class="linenos">373</span></a>            <span class="s2">&quot;loss_reg&quot;</span><span class="p">:</span> <span class="n">loss_reg</span><span class="p">,</span>
</span><span id="HAT-374"><a href="#HAT-374"><span class="linenos">374</span></a>            <span class="s2">&quot;acc&quot;</span><span class="p">:</span> <span class="n">acc</span><span class="p">,</span>
</span><span id="HAT-375"><a href="#HAT-375"><span class="linenos">375</span></a>            <span class="s2">&quot;activations&quot;</span><span class="p">:</span> <span class="n">activations</span><span class="p">,</span>
</span><span id="HAT-376"><a href="#HAT-376"><span class="linenos">376</span></a>            <span class="s2">&quot;logits&quot;</span><span class="p">:</span> <span class="n">logits</span><span class="p">,</span>
</span><span id="HAT-377"><a href="#HAT-377"><span class="linenos">377</span></a>            <span class="s2">&quot;mask&quot;</span><span class="p">:</span> <span class="n">mask</span><span class="p">,</span>  <span class="c1"># return other metrics for lightning loggers callback to handle at `on_train_batch_end()`</span>
</span><span id="HAT-378"><a href="#HAT-378"><span class="linenos">378</span></a>            <span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span>  <span class="c1"># return the input batch for Captum to use</span>
</span><span id="HAT-379"><a href="#HAT-379"><span class="linenos">379</span></a>            <span class="s2">&quot;target&quot;</span><span class="p">:</span> <span class="n">y</span><span class="p">,</span>  <span class="c1"># return the target batch for Captum to use</span>
</span><span id="HAT-380"><a href="#HAT-380"><span class="linenos">380</span></a>            <span class="s2">&quot;adjustment_rate_weight&quot;</span><span class="p">:</span> <span class="n">adjustment_rate_weight</span><span class="p">,</span>  <span class="c1"># return the adjustment rate for weights and biases for logging</span>
</span><span id="HAT-381"><a href="#HAT-381"><span class="linenos">381</span></a>            <span class="s2">&quot;adjustment_rate_bias&quot;</span><span class="p">:</span> <span class="n">adjustment_rate_bias</span><span class="p">,</span>
</span><span id="HAT-382"><a href="#HAT-382"><span class="linenos">382</span></a>            <span class="s2">&quot;capacity&quot;</span><span class="p">:</span> <span class="n">capacity</span><span class="p">,</span>  <span class="c1"># return the network capacity for logging</span>
</span><span id="HAT-383"><a href="#HAT-383"><span class="linenos">383</span></a>        <span class="p">}</span>
</span><span id="HAT-384"><a href="#HAT-384"><span class="linenos">384</span></a>
</span><span id="HAT-385"><a href="#HAT-385"><span class="linenos">385</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_end</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HAT-386"><a href="#HAT-386"><span class="linenos">386</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the mask and update the cumulative mask after training the task.&quot;&quot;&quot;</span>
</span><span id="HAT-387"><a href="#HAT-387"><span class="linenos">387</span></a>
</span><span id="HAT-388"><a href="#HAT-388"><span class="linenos">388</span></a>        <span class="c1"># store the mask for the current task</span>
</span><span id="HAT-389"><a href="#HAT-389"><span class="linenos">389</span></a>        <span class="n">mask_t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">store_mask</span><span class="p">()</span>
</span><span id="HAT-390"><a href="#HAT-390"><span class="linenos">390</span></a>
</span><span id="HAT-391"><a href="#HAT-391"><span class="linenos">391</span></a>        <span class="c1"># store the batch normalization if necessary</span>
</span><span id="HAT-392"><a href="#HAT-392"><span class="linenos">392</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">store_bn</span><span class="p">()</span>
</span><span id="HAT-393"><a href="#HAT-393"><span class="linenos">393</span></a>
</span><span id="HAT-394"><a href="#HAT-394"><span class="linenos">394</span></a>        <span class="c1"># update the cumulative mask. See the first Eq. in Sec 2.3 &quot;Network Training&quot; in the [HAT paper](http://proceedings.mlr.press/v80/serra18a)</span>
</span><span id="HAT-395"><a href="#HAT-395"><span class="linenos">395</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">cumulative_mask_for_previous_tasks</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="HAT-396"><a href="#HAT-396"><span class="linenos">396</span></a>            <span class="n">layer_name</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span>
</span><span id="HAT-397"><a href="#HAT-397"><span class="linenos">397</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">cumulative_mask_for_previous_tasks</span><span class="p">[</span><span class="n">layer_name</span><span class="p">],</span> <span class="n">mask_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="HAT-398"><a href="#HAT-398"><span class="linenos">398</span></a>            <span class="p">)</span>
</span><span id="HAT-399"><a href="#HAT-399"><span class="linenos">399</span></a>            <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span>
</span><span id="HAT-400"><a href="#HAT-400"><span class="linenos">400</span></a>        <span class="p">}</span>
</span><span id="HAT-401"><a href="#HAT-401"><span class="linenos">401</span></a>
</span><span id="HAT-402"><a href="#HAT-402"><span class="linenos">402</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
</span><span id="HAT-403"><a href="#HAT-403"><span class="linenos">403</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Validation step for current task `self.task_id`.</span>
</span><span id="HAT-404"><a href="#HAT-404"><span class="linenos">404</span></a>
</span><span id="HAT-405"><a href="#HAT-405"><span class="linenos">405</span></a><span class="sd">        **Args:**</span>
</span><span id="HAT-406"><a href="#HAT-406"><span class="linenos">406</span></a><span class="sd">        - **batch** (`Any`): a batch of validation data.</span>
</span><span id="HAT-407"><a href="#HAT-407"><span class="linenos">407</span></a>
</span><span id="HAT-408"><a href="#HAT-408"><span class="linenos">408</span></a><span class="sd">        **Returns:**</span>
</span><span id="HAT-409"><a href="#HAT-409"><span class="linenos">409</span></a><span class="sd">        - **outputs** (`dict[str, Tensor]`): a dictionary contains loss and other metrics from this validation step. Keys (`str`) are the metrics names, and values (`Tensor`) are the metrics.</span>
</span><span id="HAT-410"><a href="#HAT-410"><span class="linenos">410</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HAT-411"><a href="#HAT-411"><span class="linenos">411</span></a>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
</span><span id="HAT-412"><a href="#HAT-412"><span class="linenos">412</span></a>        <span class="n">logits</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">stage</span><span class="o">=</span><span class="s2">&quot;validation&quot;</span><span class="p">,</span> <span class="n">task_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">)</span>
</span><span id="HAT-413"><a href="#HAT-413"><span class="linenos">413</span></a>        <span class="n">loss_cls</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span><span id="HAT-414"><a href="#HAT-414"><span class="linenos">414</span></a>        <span class="n">acc</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</span><span id="HAT-415"><a href="#HAT-415"><span class="linenos">415</span></a>
</span><span id="HAT-416"><a href="#HAT-416"><span class="linenos">416</span></a>        <span class="k">return</span> <span class="p">{</span>
</span><span id="HAT-417"><a href="#HAT-417"><span class="linenos">417</span></a>            <span class="s2">&quot;loss_cls&quot;</span><span class="p">:</span> <span class="n">loss_cls</span><span class="p">,</span>
</span><span id="HAT-418"><a href="#HAT-418"><span class="linenos">418</span></a>            <span class="s2">&quot;acc&quot;</span><span class="p">:</span> <span class="n">acc</span><span class="p">,</span>  <span class="c1"># Return metrics for lightning loggers callback to handle at `on_validation_batch_end()`</span>
</span><span id="HAT-419"><a href="#HAT-419"><span class="linenos">419</span></a>        <span class="p">}</span>
</span><span id="HAT-420"><a href="#HAT-420"><span class="linenos">420</span></a>
</span><span id="HAT-421"><a href="#HAT-421"><span class="linenos">421</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">test_step</span><span class="p">(</span>
</span><span id="HAT-422"><a href="#HAT-422"><span class="linenos">422</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="HAT-423"><a href="#HAT-423"><span class="linenos">423</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
</span><span id="HAT-424"><a href="#HAT-424"><span class="linenos">424</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Test step for current task `self.task_id`, which tests for all seen tasks indexed by `dataloader_idx`.</span>
</span><span id="HAT-425"><a href="#HAT-425"><span class="linenos">425</span></a>
</span><span id="HAT-426"><a href="#HAT-426"><span class="linenos">426</span></a><span class="sd">        **Args:**</span>
</span><span id="HAT-427"><a href="#HAT-427"><span class="linenos">427</span></a><span class="sd">        - **batch** (`Any`): a batch of test data.</span>
</span><span id="HAT-428"><a href="#HAT-428"><span class="linenos">428</span></a><span class="sd">        - **dataloader_idx** (`int`): the task ID of seen tasks to be tested. A default value of 0 is given otherwise the LightningModule will raise a `RuntimeError`.</span>
</span><span id="HAT-429"><a href="#HAT-429"><span class="linenos">429</span></a>
</span><span id="HAT-430"><a href="#HAT-430"><span class="linenos">430</span></a><span class="sd">        **Returns:**</span>
</span><span id="HAT-431"><a href="#HAT-431"><span class="linenos">431</span></a><span class="sd">        - **outputs** (`dict[str, Tensor]`): a dictionary contains loss and other metrics from this test step. Keys (`str`) are the metrics names, and values (`Tensor`) are the metrics.</span>
</span><span id="HAT-432"><a href="#HAT-432"><span class="linenos">432</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HAT-433"><a href="#HAT-433"><span class="linenos">433</span></a>        <span class="n">test_task_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_test_task_id_from_dataloader_idx</span><span class="p">(</span><span class="n">dataloader_idx</span><span class="p">)</span>
</span><span id="HAT-434"><a href="#HAT-434"><span class="linenos">434</span></a>
</span><span id="HAT-435"><a href="#HAT-435"><span class="linenos">435</span></a>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
</span><span id="HAT-436"><a href="#HAT-436"><span class="linenos">436</span></a>        <span class="n">logits</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
</span><span id="HAT-437"><a href="#HAT-437"><span class="linenos">437</span></a>            <span class="n">x</span><span class="p">,</span>
</span><span id="HAT-438"><a href="#HAT-438"><span class="linenos">438</span></a>            <span class="n">stage</span><span class="o">=</span><span class="s2">&quot;test&quot;</span><span class="p">,</span>
</span><span id="HAT-439"><a href="#HAT-439"><span class="linenos">439</span></a>            <span class="n">task_id</span><span class="o">=</span><span class="n">test_task_id</span><span class="p">,</span>
</span><span id="HAT-440"><a href="#HAT-440"><span class="linenos">440</span></a>        <span class="p">)</span>  <span class="c1"># use the corresponding head and mask to test (instead of the current task `self.task_id`)</span>
</span><span id="HAT-441"><a href="#HAT-441"><span class="linenos">441</span></a>        <span class="n">loss_cls</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span><span id="HAT-442"><a href="#HAT-442"><span class="linenos">442</span></a>        <span class="n">acc</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</span><span id="HAT-443"><a href="#HAT-443"><span class="linenos">443</span></a>
</span><span id="HAT-444"><a href="#HAT-444"><span class="linenos">444</span></a>        <span class="k">return</span> <span class="p">{</span>
</span><span id="HAT-445"><a href="#HAT-445"><span class="linenos">445</span></a>            <span class="s2">&quot;loss_cls&quot;</span><span class="p">:</span> <span class="n">loss_cls</span><span class="p">,</span>
</span><span id="HAT-446"><a href="#HAT-446"><span class="linenos">446</span></a>            <span class="s2">&quot;acc&quot;</span><span class="p">:</span> <span class="n">acc</span><span class="p">,</span>  <span class="c1"># Return metrics for lightning loggers callback to handle at `on_test_batch_end()`</span>
</span><span id="HAT-447"><a href="#HAT-447"><span class="linenos">447</span></a>        <span class="p">}</span>
</span></pre></div>


            <div class="docstring"><p><a href="http://proceedings.mlr.press/v80/serra18a">HAT (Hard Attention to the Task)</a> algorithm.</p>

<p>An architecture-based continual learning approach that uses learnable hard attention masks to select task-specific parameters.</p>
</div>


                            <div id="HAT.__init__" class="classattr">
                                        <input id="HAT.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">HAT</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">backbone</span><span class="p">:</span> <span class="n"><a href="../backbones.html#HATMaskBackbone">clarena.backbones.HATMaskBackbone</a></span>,</span><span class="param">	<span class="n">heads</span><span class="p">:</span> <span class="n"><a href="../heads.html#HeadsTIL">clarena.heads.HeadsTIL</a></span>,</span><span class="param">	<span class="n">adjustment_mode</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">s_max</span><span class="p">:</span> <span class="nb">float</span>,</span><span class="param">	<span class="n">clamp_threshold</span><span class="p">:</span> <span class="nb">float</span>,</span><span class="param">	<span class="n">mask_sparsity_reg_factor</span><span class="p">:</span> <span class="nb">float</span>,</span><span class="param">	<span class="n">mask_sparsity_reg_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;original&#39;</span>,</span><span class="param">	<span class="n">task_embedding_init_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;N01&#39;</span>,</span><span class="param">	<span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span></span>)</span>

                <label class="view-source-button" for="HAT.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HAT.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HAT.__init__-31"><a href="#HAT.__init__-31"><span class="linenos">31</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="HAT.__init__-32"><a href="#HAT.__init__-32"><span class="linenos">32</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="HAT.__init__-33"><a href="#HAT.__init__-33"><span class="linenos">33</span></a>        <span class="n">backbone</span><span class="p">:</span> <span class="n">HATMaskBackbone</span><span class="p">,</span>
</span><span id="HAT.__init__-34"><a href="#HAT.__init__-34"><span class="linenos">34</span></a>        <span class="n">heads</span><span class="p">:</span> <span class="n">HeadsTIL</span><span class="p">,</span>
</span><span id="HAT.__init__-35"><a href="#HAT.__init__-35"><span class="linenos">35</span></a>        <span class="n">adjustment_mode</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="HAT.__init__-36"><a href="#HAT.__init__-36"><span class="linenos">36</span></a>        <span class="n">s_max</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="HAT.__init__-37"><a href="#HAT.__init__-37"><span class="linenos">37</span></a>        <span class="n">clamp_threshold</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="HAT.__init__-38"><a href="#HAT.__init__-38"><span class="linenos">38</span></a>        <span class="n">mask_sparsity_reg_factor</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="HAT.__init__-39"><a href="#HAT.__init__-39"><span class="linenos">39</span></a>        <span class="n">mask_sparsity_reg_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;original&quot;</span><span class="p">,</span>
</span><span id="HAT.__init__-40"><a href="#HAT.__init__-40"><span class="linenos">40</span></a>        <span class="n">task_embedding_init_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;N01&quot;</span><span class="p">,</span>
</span><span id="HAT.__init__-41"><a href="#HAT.__init__-41"><span class="linenos">41</span></a>        <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HAT.__init__-42"><a href="#HAT.__init__-42"><span class="linenos">42</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HAT.__init__-43"><a href="#HAT.__init__-43"><span class="linenos">43</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialize the HAT algorithm with the network.</span>
</span><span id="HAT.__init__-44"><a href="#HAT.__init__-44"><span class="linenos">44</span></a>
</span><span id="HAT.__init__-45"><a href="#HAT.__init__-45"><span class="linenos">45</span></a><span class="sd">        **Args:**</span>
</span><span id="HAT.__init__-46"><a href="#HAT.__init__-46"><span class="linenos">46</span></a><span class="sd">        - **backbone** (`HATMaskBackbone`): must be a backbone network with the HAT mask mechanism.</span>
</span><span id="HAT.__init__-47"><a href="#HAT.__init__-47"><span class="linenos">47</span></a><span class="sd">        - **heads** (`HeadsTIL`): output heads. HAT only supports TIL (Task-Incremental Learning).</span>
</span><span id="HAT.__init__-48"><a href="#HAT.__init__-48"><span class="linenos">48</span></a><span class="sd">        - **adjustment_mode** (`str`): the strategy of adjustment (i.e., the mode of gradient clipping), must be one of:</span>
</span><span id="HAT.__init__-49"><a href="#HAT.__init__-49"><span class="linenos">49</span></a><span class="sd">            1. &#39;hat&#39;: set gradients of parameters linking to masked units to zero. This is how HAT fixes the part of the network for previous tasks completely. See Eq. (2) in Sec. 2.3 &quot;Network Training&quot; in the [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="HAT.__init__-50"><a href="#HAT.__init__-50"><span class="linenos">50</span></a><span class="sd">            2. &#39;hat_random&#39;: set gradients of parameters linking to masked units to random 0â€“1 values. See &quot;Baselines&quot; in Sec. 4.1 in the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="HAT.__init__-51"><a href="#HAT.__init__-51"><span class="linenos">51</span></a><span class="sd">            3. &#39;hat_const_alpha&#39;: set gradients of parameters linking to masked units to a constant value `alpha`. See &quot;Baselines&quot; in Sec. 4.1 in the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="HAT.__init__-52"><a href="#HAT.__init__-52"><span class="linenos">52</span></a><span class="sd">            4. &#39;hat_const_1&#39;: set gradients of parameters linking to masked units to a constant value of 1 (i.e., no gradient constraint). See &quot;Baselines&quot; in Sec. 4.1 in the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="HAT.__init__-53"><a href="#HAT.__init__-53"><span class="linenos">53</span></a><span class="sd">        - **s_max** (`float`): hyperparameter, the maximum scaling factor in the gate function. See Sec. 2.4 &quot;Hard Attention Training&quot; in the [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="HAT.__init__-54"><a href="#HAT.__init__-54"><span class="linenos">54</span></a><span class="sd">        - **clamp_threshold** (`float`): the threshold for task embedding gradient compensation. See Sec. 2.5 &quot;Embedding Gradient Compensation&quot; in the [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="HAT.__init__-55"><a href="#HAT.__init__-55"><span class="linenos">55</span></a><span class="sd">        - **mask_sparsity_reg_factor** (`float`): hyperparameter, the regularization factor for mask sparsity.</span>
</span><span id="HAT.__init__-56"><a href="#HAT.__init__-56"><span class="linenos">56</span></a><span class="sd">        - **mask_sparsity_reg_mode** (`str`): the mode of mask sparsity regularization, must be one of:</span>
</span><span id="HAT.__init__-57"><a href="#HAT.__init__-57"><span class="linenos">57</span></a><span class="sd">            1. &#39;original&#39; (default): the original mask sparsity regularization in the HAT paper.</span>
</span><span id="HAT.__init__-58"><a href="#HAT.__init__-58"><span class="linenos">58</span></a><span class="sd">            2. &#39;cross&#39;: the cross version of mask sparsity regularization.</span>
</span><span id="HAT.__init__-59"><a href="#HAT.__init__-59"><span class="linenos">59</span></a><span class="sd">        - **task_embedding_init_mode** (`str`): the initialization mode for task embeddings, must be one of:</span>
</span><span id="HAT.__init__-60"><a href="#HAT.__init__-60"><span class="linenos">60</span></a><span class="sd">            1. &#39;N01&#39; (default): standard normal distribution $N(0, 1)$.</span>
</span><span id="HAT.__init__-61"><a href="#HAT.__init__-61"><span class="linenos">61</span></a><span class="sd">            2. &#39;U-11&#39;: uniform distribution $U(-1, 1)$.</span>
</span><span id="HAT.__init__-62"><a href="#HAT.__init__-62"><span class="linenos">62</span></a><span class="sd">            3. &#39;U01&#39;: uniform distribution $U(0, 1)$.</span>
</span><span id="HAT.__init__-63"><a href="#HAT.__init__-63"><span class="linenos">63</span></a><span class="sd">            4. &#39;U-10&#39;: uniform distribution $U(-1, 0)$.</span>
</span><span id="HAT.__init__-64"><a href="#HAT.__init__-64"><span class="linenos">64</span></a><span class="sd">            5. &#39;last&#39;: inherit the task embedding from the last task.</span>
</span><span id="HAT.__init__-65"><a href="#HAT.__init__-65"><span class="linenos">65</span></a><span class="sd">        - **alpha** (`float` | `None`): the `alpha` in the &#39;HAT-const-alpha&#39; mode. Applies only when `adjustment_mode` is &#39;hat_const_alpha&#39;.</span>
</span><span id="HAT.__init__-66"><a href="#HAT.__init__-66"><span class="linenos">66</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HAT.__init__-67"><a href="#HAT.__init__-67"><span class="linenos">67</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">backbone</span><span class="o">=</span><span class="n">backbone</span><span class="p">,</span> <span class="n">heads</span><span class="o">=</span><span class="n">heads</span><span class="p">)</span>
</span><span id="HAT.__init__-68"><a href="#HAT.__init__-68"><span class="linenos">68</span></a>
</span><span id="HAT.__init__-69"><a href="#HAT.__init__-69"><span class="linenos">69</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">adjustment_mode</span>
</span><span id="HAT.__init__-70"><a href="#HAT.__init__-70"><span class="linenos">70</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The adjustment mode for gradient clipping.&quot;&quot;&quot;</span>
</span><span id="HAT.__init__-71"><a href="#HAT.__init__-71"><span class="linenos">71</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">s_max</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">s_max</span>
</span><span id="HAT.__init__-72"><a href="#HAT.__init__-72"><span class="linenos">72</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The hyperparameter s_max.&quot;&quot;&quot;</span>
</span><span id="HAT.__init__-73"><a href="#HAT.__init__-73"><span class="linenos">73</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">clamp_threshold</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">clamp_threshold</span>
</span><span id="HAT.__init__-74"><a href="#HAT.__init__-74"><span class="linenos">74</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The clamp threshold for task embedding gradient compensation.&quot;&quot;&quot;</span>
</span><span id="HAT.__init__-75"><a href="#HAT.__init__-75"><span class="linenos">75</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">mask_sparsity_reg_factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">mask_sparsity_reg_factor</span>
</span><span id="HAT.__init__-76"><a href="#HAT.__init__-76"><span class="linenos">76</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The mask sparsity regularization factor.&quot;&quot;&quot;</span>
</span><span id="HAT.__init__-77"><a href="#HAT.__init__-77"><span class="linenos">77</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">mask_sparsity_reg_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">mask_sparsity_reg_mode</span>
</span><span id="HAT.__init__-78"><a href="#HAT.__init__-78"><span class="linenos">78</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The mask sparsity regularization mode.&quot;&quot;&quot;</span>
</span><span id="HAT.__init__-79"><a href="#HAT.__init__-79"><span class="linenos">79</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">mark_sparsity_reg</span><span class="p">:</span> <span class="n">HATMaskSparsityReg</span> <span class="o">=</span> <span class="n">HATMaskSparsityReg</span><span class="p">(</span>
</span><span id="HAT.__init__-80"><a href="#HAT.__init__-80"><span class="linenos">80</span></a>            <span class="n">factor</span><span class="o">=</span><span class="n">mask_sparsity_reg_factor</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">mask_sparsity_reg_mode</span>
</span><span id="HAT.__init__-81"><a href="#HAT.__init__-81"><span class="linenos">81</span></a>        <span class="p">)</span>
</span><span id="HAT.__init__-82"><a href="#HAT.__init__-82"><span class="linenos">82</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The mask sparsity regularizer.&quot;&quot;&quot;</span>
</span><span id="HAT.__init__-83"><a href="#HAT.__init__-83"><span class="linenos">83</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_init_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">task_embedding_init_mode</span>
</span><span id="HAT.__init__-84"><a href="#HAT.__init__-84"><span class="linenos">84</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the task embedding initialization mode.&quot;&quot;&quot;</span>
</span><span id="HAT.__init__-85"><a href="#HAT.__init__-85"><span class="linenos">85</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">alpha</span>
</span><span id="HAT.__init__-86"><a href="#HAT.__init__-86"><span class="linenos">86</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The hyperparameter alpha for `hat_const_alpha`.&quot;&quot;&quot;</span>
</span><span id="HAT.__init__-87"><a href="#HAT.__init__-87"><span class="linenos">87</span></a>        <span class="c1"># self.epsilon: float | None = None</span>
</span><span id="HAT.__init__-88"><a href="#HAT.__init__-88"><span class="linenos">88</span></a>        <span class="c1"># r&quot;&quot;&quot;HAT doesn&#39;t use epsilon for `hat_const_alpha`. It is kept for consistency with `epsilon` in `clip_grad_by_adjustment()` in `HATMaskBackbone`.&quot;&quot;&quot;</span>
</span><span id="HAT.__init__-89"><a href="#HAT.__init__-89"><span class="linenos">89</span></a>
</span><span id="HAT.__init__-90"><a href="#HAT.__init__-90"><span class="linenos">90</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">cumulative_mask_for_previous_tasks</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="HAT.__init__-91"><a href="#HAT.__init__-91"><span class="linenos">91</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The cumulative binary attention mask $\mathrm{M}^{&lt;t}$ of previous tasks $1,\cdots, t-1$, gated from the task embedding ($t$ is `self.task_id`). It is a dict where keys are layer names and values are the binary mask tensors for the layers. The mask tensor has size (number of units, ). &quot;&quot;&quot;</span>
</span><span id="HAT.__init__-92"><a href="#HAT.__init__-92"><span class="linenos">92</span></a>
</span><span id="HAT.__init__-93"><a href="#HAT.__init__-93"><span class="linenos">93</span></a>        <span class="c1"># set manual optimization</span>
</span><span id="HAT.__init__-94"><a href="#HAT.__init__-94"><span class="linenos">94</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">automatic_optimization</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="HAT.__init__-95"><a href="#HAT.__init__-95"><span class="linenos">95</span></a>
</span><span id="HAT.__init__-96"><a href="#HAT.__init__-96"><span class="linenos">96</span></a>        <span class="n">HAT</span><span class="o">.</span><span class="n">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Initialize the HAT algorithm with the network.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>backbone</strong> (<code>HATMaskBackbone</code>): must be a backbone network with the HAT mask mechanism.</li>
<li><strong>heads</strong> (<code>HeadsTIL</code>): output heads. HAT only supports TIL (Task-Incremental Learning).</li>
<li><strong>adjustment_mode</strong> (<code>str</code>): the strategy of adjustment (i.e., the mode of gradient clipping), must be one of:
<ol>
<li>'hat': set gradients of parameters linking to masked units to zero. This is how HAT fixes the part of the network for previous tasks completely. See Eq. (2) in Sec. 2.3 "Network Training" in the <a href="http://proceedings.mlr.press/v80/serra18a">HAT paper</a>.</li>
<li>'hat_random': set gradients of parameters linking to masked units to random 0â€“1 values. See "Baselines" in Sec. 4.1 in the <a href="https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9">AdaHAT paper</a>.</li>
<li>'hat_const_alpha': set gradients of parameters linking to masked units to a constant value <code><a href="#HAT.alpha">alpha</a></code>. See "Baselines" in Sec. 4.1 in the <a href="https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9">AdaHAT paper</a>.</li>
<li>'hat_const_1': set gradients of parameters linking to masked units to a constant value of 1 (i.e., no gradient constraint). See "Baselines" in Sec. 4.1 in the <a href="https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9">AdaHAT paper</a>.</li>
</ol></li>
<li><strong>s_max</strong> (<code><a href="#HAT.float">float</a></code>): hyperparameter, the maximum scaling factor in the gate function. See Sec. 2.4 "Hard Attention Training" in the <a href="http://proceedings.mlr.press/v80/serra18a">HAT paper</a>.</li>
<li><strong>clamp_threshold</strong> (<code><a href="#HAT.float">float</a></code>): the threshold for task embedding gradient compensation. See Sec. 2.5 "Embedding Gradient Compensation" in the <a href="http://proceedings.mlr.press/v80/serra18a">HAT paper</a>.</li>
<li><strong>mask_sparsity_reg_factor</strong> (<code><a href="#HAT.float">float</a></code>): hyperparameter, the regularization factor for mask sparsity.</li>
<li><strong>mask_sparsity_reg_mode</strong> (<code>str</code>): the mode of mask sparsity regularization, must be one of:
<ol>
<li>'original' (default): the original mask sparsity regularization in the HAT paper.</li>
<li>'cross': the cross version of mask sparsity regularization.</li>
</ol></li>
<li><strong>task_embedding_init_mode</strong> (<code>str</code>): the initialization mode for task embeddings, must be one of:
<ol>
<li>'N01' (default): standard normal distribution $N(0, 1)$.</li>
<li>'U-11': uniform distribution $U(-1, 1)$.</li>
<li>'U01': uniform distribution $U(0, 1)$.</li>
<li>'U-10': uniform distribution $U(-1, 0)$.</li>
<li>'last': inherit the task embedding from the last task.</li>
</ol></li>
<li><strong>alpha</strong> (<code><a href="#HAT.float">float</a></code> | <code>None</code>): the <code><a href="#HAT.alpha">alpha</a></code> in the 'HAT-const-alpha' mode. Applies only when <code><a href="#HAT.adjustment_mode">adjustment_mode</a></code> is 'hat_const_alpha'.</li>
</ul>
</div>


                            </div>
                            <div id="HAT.adjustment_mode" class="classattr">
                                <div class="attr variable">
            <span class="name">adjustment_mode</span><span class="annotation">: str</span>

        
    </div>
    <a class="headerlink" href="#HAT.adjustment_mode"></a>
    
            <div class="docstring"><p>The adjustment mode for gradient clipping.</p>
</div>


                            </div>
                            <div id="HAT.s_max" class="classattr">
                                <div class="attr variable">
            <span class="name">s_max</span><span class="annotation">: float</span>

        
    </div>
    <a class="headerlink" href="#HAT.s_max"></a>
    
            <div class="docstring"><p>The hyperparameter s_max.</p>
</div>


                            </div>
                            <div id="HAT.clamp_threshold" class="classattr">
                                <div class="attr variable">
            <span class="name">clamp_threshold</span><span class="annotation">: float</span>

        
    </div>
    <a class="headerlink" href="#HAT.clamp_threshold"></a>
    
            <div class="docstring"><p>The clamp threshold for task embedding gradient compensation.</p>
</div>


                            </div>
                            <div id="HAT.mask_sparsity_reg_factor" class="classattr">
                                <div class="attr variable">
            <span class="name">mask_sparsity_reg_factor</span><span class="annotation">: float</span>

        
    </div>
    <a class="headerlink" href="#HAT.mask_sparsity_reg_factor"></a>
    
            <div class="docstring"><p>The mask sparsity regularization factor.</p>
</div>


                            </div>
                            <div id="HAT.mask_sparsity_reg_mode" class="classattr">
                                <div class="attr variable">
            <span class="name">mask_sparsity_reg_mode</span><span class="annotation">: str</span>

        
    </div>
    <a class="headerlink" href="#HAT.mask_sparsity_reg_mode"></a>
    
            <div class="docstring"><p>The mask sparsity regularization mode.</p>
</div>


                            </div>
                            <div id="HAT.mark_sparsity_reg" class="classattr">
                                <div class="attr variable">
            <span class="name">mark_sparsity_reg</span><span class="annotation">: <a href="regularizers/hat_mask_sparsity.html#HATMaskSparsityReg">clarena.cl_algorithms.regularizers.hat_mask_sparsity.HATMaskSparsityReg</a></span>

        
    </div>
    <a class="headerlink" href="#HAT.mark_sparsity_reg"></a>
    
            <div class="docstring"><p>The mask sparsity regularizer.</p>
</div>


                            </div>
                            <div id="HAT.task_embedding_init_mode" class="classattr">
                                <div class="attr variable">
            <span class="name">task_embedding_init_mode</span><span class="annotation">: str</span>

        
    </div>
    <a class="headerlink" href="#HAT.task_embedding_init_mode"></a>
    
            <div class="docstring"><p>Store the task embedding initialization mode.</p>
</div>


                            </div>
                            <div id="HAT.alpha" class="classattr">
                                <div class="attr variable">
            <span class="name">alpha</span><span class="annotation">: float | None</span>

        
    </div>
    <a class="headerlink" href="#HAT.alpha"></a>
    
            <div class="docstring"><p>The hyperparameter alpha for <code>hat_const_alpha</code>.</p>
</div>


                            </div>
                            <div id="HAT.cumulative_mask_for_previous_tasks" class="classattr">
                                <div class="attr variable">
            <span class="name">cumulative_mask_for_previous_tasks</span><span class="annotation">: dict[str, torch.Tensor]</span>

        
    </div>
    <a class="headerlink" href="#HAT.cumulative_mask_for_previous_tasks"></a>
    
            <div class="docstring"><p>The cumulative binary attention mask $\mathrm{M}^{<t}$ of previous tasks $1,\cdots, t-1$, gated from the task embedding ($t$ is <code>self.task_id</code>). It is a dict where keys are layer names and values are the binary mask tensors for the layers. The mask tensor has size (number of units, ).</p>
</div>


                            </div>
                            <div id="HAT.automatic_optimization" class="classattr">
                                        <input id="HAT.automatic_optimization-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr variable">
            <span class="name">automatic_optimization</span><span class="annotation">: bool</span>

                <label class="view-source-button" for="HAT.automatic_optimization-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HAT.automatic_optimization"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HAT.automatic_optimization-290"><a href="#HAT.automatic_optimization-290"><span class="linenos">290</span></a>    <span class="nd">@property</span>
</span><span id="HAT.automatic_optimization-291"><a href="#HAT.automatic_optimization-291"><span class="linenos">291</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">automatic_optimization</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
</span><span id="HAT.automatic_optimization-292"><a href="#HAT.automatic_optimization-292"><span class="linenos">292</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``.&quot;&quot;&quot;</span>
</span><span id="HAT.automatic_optimization-293"><a href="#HAT.automatic_optimization-293"><span class="linenos">293</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_automatic_optimization</span>
</span></pre></div>


            <div class="docstring"><p>If set to <code>False</code> you are responsible for calling <code>.backward()</code>, <code>.step()</code>, <code>.zero_grad()</code>.</p>
</div>


                            </div>
                            <div id="HAT.sanity_check" class="classattr">
                                        <input id="HAT.sanity_check-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">sanity_check</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span></span><span class="return-annotation">) -> <span class="kc">None</span>:</span></span>

                <label class="view-source-button" for="HAT.sanity_check-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HAT.sanity_check"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HAT.sanity_check-98"><a href="#HAT.sanity_check-98"><span class="linenos"> 98</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HAT.sanity_check-99"><a href="#HAT.sanity_check-99"><span class="linenos"> 99</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Check the sanity of the arguments.&quot;&quot;&quot;</span>
</span><span id="HAT.sanity_check-100"><a href="#HAT.sanity_check-100"><span class="linenos">100</span></a>
</span><span id="HAT.sanity_check-101"><a href="#HAT.sanity_check-101"><span class="linenos">101</span></a>        <span class="c1"># check the backbone and heads</span>
</span><span id="HAT.sanity_check-102"><a href="#HAT.sanity_check-102"><span class="linenos">102</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="p">,</span> <span class="n">HATMaskBackbone</span><span class="p">):</span>
</span><span id="HAT.sanity_check-103"><a href="#HAT.sanity_check-103"><span class="linenos">103</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The backbone should be an instance of `HATMaskBackbone`.&quot;</span><span class="p">)</span>
</span><span id="HAT.sanity_check-104"><a href="#HAT.sanity_check-104"><span class="linenos">104</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">HeadsTIL</span><span class="p">):</span>
</span><span id="HAT.sanity_check-105"><a href="#HAT.sanity_check-105"><span class="linenos">105</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The heads should be an instance of `HeadsTIL`.&quot;</span><span class="p">)</span>
</span><span id="HAT.sanity_check-106"><a href="#HAT.sanity_check-106"><span class="linenos">106</span></a>
</span><span id="HAT.sanity_check-107"><a href="#HAT.sanity_check-107"><span class="linenos">107</span></a>        <span class="c1"># check marker sparsity regularization mode</span>
</span><span id="HAT.sanity_check-108"><a href="#HAT.sanity_check-108"><span class="linenos">108</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask_sparsity_reg_mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;original&quot;</span><span class="p">,</span> <span class="s2">&quot;cross&quot;</span><span class="p">]:</span>
</span><span id="HAT.sanity_check-109"><a href="#HAT.sanity_check-109"><span class="linenos">109</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="HAT.sanity_check-110"><a href="#HAT.sanity_check-110"><span class="linenos">110</span></a>                <span class="s2">&quot;The mask_sparsity_reg_mode should be one of &#39;original&#39;, &#39;cross&#39;.&quot;</span>
</span><span id="HAT.sanity_check-111"><a href="#HAT.sanity_check-111"><span class="linenos">111</span></a>            <span class="p">)</span>
</span><span id="HAT.sanity_check-112"><a href="#HAT.sanity_check-112"><span class="linenos">112</span></a>
</span><span id="HAT.sanity_check-113"><a href="#HAT.sanity_check-113"><span class="linenos">113</span></a>        <span class="c1"># check task embedding initialization mode</span>
</span><span id="HAT.sanity_check-114"><a href="#HAT.sanity_check-114"><span class="linenos">114</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_init_mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span>
</span><span id="HAT.sanity_check-115"><a href="#HAT.sanity_check-115"><span class="linenos">115</span></a>            <span class="s2">&quot;N01&quot;</span><span class="p">,</span>
</span><span id="HAT.sanity_check-116"><a href="#HAT.sanity_check-116"><span class="linenos">116</span></a>            <span class="s2">&quot;U01&quot;</span><span class="p">,</span>
</span><span id="HAT.sanity_check-117"><a href="#HAT.sanity_check-117"><span class="linenos">117</span></a>            <span class="s2">&quot;U-10&quot;</span><span class="p">,</span>
</span><span id="HAT.sanity_check-118"><a href="#HAT.sanity_check-118"><span class="linenos">118</span></a>            <span class="s2">&quot;masked&quot;</span><span class="p">,</span>
</span><span id="HAT.sanity_check-119"><a href="#HAT.sanity_check-119"><span class="linenos">119</span></a>            <span class="s2">&quot;unmasked&quot;</span><span class="p">,</span>
</span><span id="HAT.sanity_check-120"><a href="#HAT.sanity_check-120"><span class="linenos">120</span></a>        <span class="p">]:</span>
</span><span id="HAT.sanity_check-121"><a href="#HAT.sanity_check-121"><span class="linenos">121</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="HAT.sanity_check-122"><a href="#HAT.sanity_check-122"><span class="linenos">122</span></a>                <span class="s2">&quot;The task_embedding_init_mode should be one of &#39;N01&#39;, &#39;U01&#39;, &#39;U-10&#39;, &#39;masked&#39;, &#39;unmasked&#39;.&quot;</span>
</span><span id="HAT.sanity_check-123"><a href="#HAT.sanity_check-123"><span class="linenos">123</span></a>            <span class="p">)</span>
</span><span id="HAT.sanity_check-124"><a href="#HAT.sanity_check-124"><span class="linenos">124</span></a>
</span><span id="HAT.sanity_check-125"><a href="#HAT.sanity_check-125"><span class="linenos">125</span></a>        <span class="c1"># check adjustment mode `hat_const_alpha`</span>
</span><span id="HAT.sanity_check-126"><a href="#HAT.sanity_check-126"><span class="linenos">126</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;hat_const_alpha&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HAT.sanity_check-127"><a href="#HAT.sanity_check-127"><span class="linenos">127</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="HAT.sanity_check-128"><a href="#HAT.sanity_check-128"><span class="linenos">128</span></a>                <span class="s2">&quot;Alpha should be given when the adjustment_mode is &#39;hat_const_alpha&#39;.&quot;</span>
</span><span id="HAT.sanity_check-129"><a href="#HAT.sanity_check-129"><span class="linenos">129</span></a>            <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Check the sanity of the arguments.</p>
</div>


                            </div>
                            <div id="HAT.on_train_start" class="classattr">
                                        <input id="HAT.on_train_start-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">on_train_start</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span></span><span class="return-annotation">) -> <span class="kc">None</span>:</span></span>

                <label class="view-source-button" for="HAT.on_train_start-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HAT.on_train_start"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HAT.on_train_start-131"><a href="#HAT.on_train_start-131"><span class="linenos">131</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_start</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HAT.on_train_start-132"><a href="#HAT.on_train_start-132"><span class="linenos">132</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialize the task embedding before training the next task and initialize the cumulative mask at the beginning of the first task.&quot;&quot;&quot;</span>
</span><span id="HAT.on_train_start-133"><a href="#HAT.on_train_start-133"><span class="linenos">133</span></a>
</span><span id="HAT.on_train_start-134"><a href="#HAT.on_train_start-134"><span class="linenos">134</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">initialize_task_embedding</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_init_mode</span><span class="p">)</span>
</span><span id="HAT.on_train_start-135"><a href="#HAT.on_train_start-135"><span class="linenos">135</span></a>
</span><span id="HAT.on_train_start-136"><a href="#HAT.on_train_start-136"><span class="linenos">136</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">initialize_independent_bn</span><span class="p">()</span>
</span><span id="HAT.on_train_start-137"><a href="#HAT.on_train_start-137"><span class="linenos">137</span></a>
</span><span id="HAT.on_train_start-138"><a href="#HAT.on_train_start-138"><span class="linenos">138</span></a>        <span class="c1"># initialize the cumulative mask for the first task at the beginning of the first task. This should not be called in `__init__()` because `self.device` is not available at that time.</span>
</span><span id="HAT.on_train_start-139"><a href="#HAT.on_train_start-139"><span class="linenos">139</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="HAT.on_train_start-140"><a href="#HAT.on_train_start-140"><span class="linenos">140</span></a>            <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="HAT.on_train_start-141"><a href="#HAT.on_train_start-141"><span class="linenos">141</span></a>                <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span>
</span><span id="HAT.on_train_start-142"><a href="#HAT.on_train_start-142"><span class="linenos">142</span></a>                    <span class="n">layer_name</span>
</span><span id="HAT.on_train_start-143"><a href="#HAT.on_train_start-143"><span class="linenos">143</span></a>                <span class="p">)</span>  <span class="c1"># get the layer by its name</span>
</span><span id="HAT.on_train_start-144"><a href="#HAT.on_train_start-144"><span class="linenos">144</span></a>                <span class="n">num_units</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="HAT.on_train_start-145"><a href="#HAT.on_train_start-145"><span class="linenos">145</span></a>
</span><span id="HAT.on_train_start-146"><a href="#HAT.on_train_start-146"><span class="linenos">146</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">cumulative_mask_for_previous_tasks</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
</span><span id="HAT.on_train_start-147"><a href="#HAT.on_train_start-147"><span class="linenos">147</span></a>                    <span class="n">num_units</span>
</span><span id="HAT.on_train_start-148"><a href="#HAT.on_train_start-148"><span class="linenos">148</span></a>                <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
</span><span id="HAT.on_train_start-149"><a href="#HAT.on_train_start-149"><span class="linenos">149</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
</span><span id="HAT.on_train_start-150"><a href="#HAT.on_train_start-150"><span class="linenos">150</span></a>                <span class="p">)</span>  <span class="c1"># the cumulative mask $\mathrm{M}^{&lt;t}$ is initialized as a zeros mask ($t = 1$). See Eq. (2) in Sec. 3 in the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9), or Eq. (5) in Sec. 2.6 &quot;Promoting Low Capacity Usage&quot; in the [HAT paper](http://proceedings.mlr.press/v80/serra18a)</span>
</span><span id="HAT.on_train_start-151"><a href="#HAT.on_train_start-151"><span class="linenos">151</span></a>
</span><span id="HAT.on_train_start-152"><a href="#HAT.on_train_start-152"><span class="linenos">152</span></a>                <span class="c1"># self.neuron_first_task[layer_name] = [None] * num_units</span>
</span></pre></div>


            <div class="docstring"><p>Initialize the task embedding before training the next task and initialize the cumulative mask at the beginning of the first task.</p>
</div>


                            </div>
                            <div id="HAT.clip_grad_by_adjustment" class="classattr">
                                        <input id="HAT.clip_grad_by_adjustment-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">clip_grad_by_adjustment</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="o">**</span><span class="n">kwargs</span></span><span class="return-annotation">) -> <span class="nb">tuple</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>:</span></span>

                <label class="view-source-button" for="HAT.clip_grad_by_adjustment-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HAT.clip_grad_by_adjustment"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HAT.clip_grad_by_adjustment-154"><a href="#HAT.clip_grad_by_adjustment-154"><span class="linenos">154</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">clip_grad_by_adjustment</span><span class="p">(</span>
</span><span id="HAT.clip_grad_by_adjustment-155"><a href="#HAT.clip_grad_by_adjustment-155"><span class="linenos">155</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="HAT.clip_grad_by_adjustment-156"><a href="#HAT.clip_grad_by_adjustment-156"><span class="linenos">156</span></a>        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
</span><span id="HAT.clip_grad_by_adjustment-157"><a href="#HAT.clip_grad_by_adjustment-157"><span class="linenos">157</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="n">Tensor</span><span class="p">]:</span>
</span><span id="HAT.clip_grad_by_adjustment-158"><a href="#HAT.clip_grad_by_adjustment-158"><span class="linenos">158</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Clip the gradients by the adjustment rate. See Eq. (2) in Sec. 2.3 &quot;Network Training&quot; in the [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="HAT.clip_grad_by_adjustment-159"><a href="#HAT.clip_grad_by_adjustment-159"><span class="linenos">159</span></a>
</span><span id="HAT.clip_grad_by_adjustment-160"><a href="#HAT.clip_grad_by_adjustment-160"><span class="linenos">160</span></a><span class="sd">        Note that because the task embedding fully covers every layer in the backbone network, no parameters are left out of this system.</span>
</span><span id="HAT.clip_grad_by_adjustment-161"><a href="#HAT.clip_grad_by_adjustment-161"><span class="linenos">161</span></a><span class="sd">        This applies not only to parameters between layers with task embeddings, but also to those before the first layer. We design it separately in the code.</span>
</span><span id="HAT.clip_grad_by_adjustment-162"><a href="#HAT.clip_grad_by_adjustment-162"><span class="linenos">162</span></a>
</span><span id="HAT.clip_grad_by_adjustment-163"><a href="#HAT.clip_grad_by_adjustment-163"><span class="linenos">163</span></a><span class="sd">        Network capacity is measured alongside this method. Network capacity is defined as the average adjustment rate over all parameters.</span>
</span><span id="HAT.clip_grad_by_adjustment-164"><a href="#HAT.clip_grad_by_adjustment-164"><span class="linenos">164</span></a><span class="sd">        See Sec. 4.1 in the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="HAT.clip_grad_by_adjustment-165"><a href="#HAT.clip_grad_by_adjustment-165"><span class="linenos">165</span></a>
</span><span id="HAT.clip_grad_by_adjustment-166"><a href="#HAT.clip_grad_by_adjustment-166"><span class="linenos">166</span></a><span class="sd">        **Returns:**</span>
</span><span id="HAT.clip_grad_by_adjustment-167"><a href="#HAT.clip_grad_by_adjustment-167"><span class="linenos">167</span></a><span class="sd">        - **adjustment_rate_weight** (`dict[str, Tensor]`): the adjustment rate for weights. Keys (`str`) are layer names and values (`Tensor`) are the adjustment rate tensors.</span>
</span><span id="HAT.clip_grad_by_adjustment-168"><a href="#HAT.clip_grad_by_adjustment-168"><span class="linenos">168</span></a><span class="sd">        - **adjustment_rate_bias** (`dict[str, Tensor]`): the adjustment rate for biases. Keys (`str`) are layer name and values (`Tensor`) are the adjustment rate tensors.</span>
</span><span id="HAT.clip_grad_by_adjustment-169"><a href="#HAT.clip_grad_by_adjustment-169"><span class="linenos">169</span></a><span class="sd">        - **capacity** (`Tensor`): the calculated network capacity.</span>
</span><span id="HAT.clip_grad_by_adjustment-170"><a href="#HAT.clip_grad_by_adjustment-170"><span class="linenos">170</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HAT.clip_grad_by_adjustment-171"><a href="#HAT.clip_grad_by_adjustment-171"><span class="linenos">171</span></a>
</span><span id="HAT.clip_grad_by_adjustment-172"><a href="#HAT.clip_grad_by_adjustment-172"><span class="linenos">172</span></a>        <span class="c1"># initialize network capacity metric</span>
</span><span id="HAT.clip_grad_by_adjustment-173"><a href="#HAT.clip_grad_by_adjustment-173"><span class="linenos">173</span></a>        <span class="n">capacity</span> <span class="o">=</span> <span class="n">HATNetworkCapacityMetric</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="HAT.clip_grad_by_adjustment-174"><a href="#HAT.clip_grad_by_adjustment-174"><span class="linenos">174</span></a>        <span class="n">adjustment_rate_weight</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="HAT.clip_grad_by_adjustment-175"><a href="#HAT.clip_grad_by_adjustment-175"><span class="linenos">175</span></a>        <span class="n">adjustment_rate_bias</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="HAT.clip_grad_by_adjustment-176"><a href="#HAT.clip_grad_by_adjustment-176"><span class="linenos">176</span></a>
</span><span id="HAT.clip_grad_by_adjustment-177"><a href="#HAT.clip_grad_by_adjustment-177"><span class="linenos">177</span></a>        <span class="c1"># calculate the adjustment rate for gradients of the parameters, both weights and biases (if they exist)</span>
</span><span id="HAT.clip_grad_by_adjustment-178"><a href="#HAT.clip_grad_by_adjustment-178"><span class="linenos">178</span></a>        <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="HAT.clip_grad_by_adjustment-179"><a href="#HAT.clip_grad_by_adjustment-179"><span class="linenos">179</span></a>
</span><span id="HAT.clip_grad_by_adjustment-180"><a href="#HAT.clip_grad_by_adjustment-180"><span class="linenos">180</span></a>            <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span>
</span><span id="HAT.clip_grad_by_adjustment-181"><a href="#HAT.clip_grad_by_adjustment-181"><span class="linenos">181</span></a>                <span class="n">layer_name</span>
</span><span id="HAT.clip_grad_by_adjustment-182"><a href="#HAT.clip_grad_by_adjustment-182"><span class="linenos">182</span></a>            <span class="p">)</span>  <span class="c1"># get the layer by its name</span>
</span><span id="HAT.clip_grad_by_adjustment-183"><a href="#HAT.clip_grad_by_adjustment-183"><span class="linenos">183</span></a>
</span><span id="HAT.clip_grad_by_adjustment-184"><a href="#HAT.clip_grad_by_adjustment-184"><span class="linenos">184</span></a>            <span class="c1"># placeholder for the adjustment rate to avoid the error of using it before assignment</span>
</span><span id="HAT.clip_grad_by_adjustment-185"><a href="#HAT.clip_grad_by_adjustment-185"><span class="linenos">185</span></a>            <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="HAT.clip_grad_by_adjustment-186"><a href="#HAT.clip_grad_by_adjustment-186"><span class="linenos">186</span></a>            <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="HAT.clip_grad_by_adjustment-187"><a href="#HAT.clip_grad_by_adjustment-187"><span class="linenos">187</span></a>
</span><span id="HAT.clip_grad_by_adjustment-188"><a href="#HAT.clip_grad_by_adjustment-188"><span class="linenos">188</span></a>            <span class="n">weight_mask</span><span class="p">,</span> <span class="n">bias_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_measure_parameter_wise</span><span class="p">(</span>
</span><span id="HAT.clip_grad_by_adjustment-189"><a href="#HAT.clip_grad_by_adjustment-189"><span class="linenos">189</span></a>                <span class="n">unit_wise_measure</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">cumulative_mask_for_previous_tasks</span><span class="p">,</span>
</span><span id="HAT.clip_grad_by_adjustment-190"><a href="#HAT.clip_grad_by_adjustment-190"><span class="linenos">190</span></a>                <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="HAT.clip_grad_by_adjustment-191"><a href="#HAT.clip_grad_by_adjustment-191"><span class="linenos">191</span></a>                <span class="n">aggregation_mode</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span>
</span><span id="HAT.clip_grad_by_adjustment-192"><a href="#HAT.clip_grad_by_adjustment-192"><span class="linenos">192</span></a>            <span class="p">)</span>
</span><span id="HAT.clip_grad_by_adjustment-193"><a href="#HAT.clip_grad_by_adjustment-193"><span class="linenos">193</span></a>
</span><span id="HAT.clip_grad_by_adjustment-194"><a href="#HAT.clip_grad_by_adjustment-194"><span class="linenos">194</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;hat&quot;</span><span class="p">:</span>
</span><span id="HAT.clip_grad_by_adjustment-195"><a href="#HAT.clip_grad_by_adjustment-195"><span class="linenos">195</span></a>                <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">weight_mask</span>
</span><span id="HAT.clip_grad_by_adjustment-196"><a href="#HAT.clip_grad_by_adjustment-196"><span class="linenos">196</span></a>                <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">bias_mask</span>
</span><span id="HAT.clip_grad_by_adjustment-197"><a href="#HAT.clip_grad_by_adjustment-197"><span class="linenos">197</span></a>
</span><span id="HAT.clip_grad_by_adjustment-198"><a href="#HAT.clip_grad_by_adjustment-198"><span class="linenos">198</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;hat_random&quot;</span><span class="p">:</span>
</span><span id="HAT.clip_grad_by_adjustment-199"><a href="#HAT.clip_grad_by_adjustment-199"><span class="linenos">199</span></a>                <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand_like</span><span class="p">(</span>
</span><span id="HAT.clip_grad_by_adjustment-200"><a href="#HAT.clip_grad_by_adjustment-200"><span class="linenos">200</span></a>                    <span class="n">weight_mask</span>
</span><span id="HAT.clip_grad_by_adjustment-201"><a href="#HAT.clip_grad_by_adjustment-201"><span class="linenos">201</span></a>                <span class="p">)</span> <span class="o">*</span> <span class="n">weight_mask</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">weight_mask</span><span class="p">)</span>
</span><span id="HAT.clip_grad_by_adjustment-202"><a href="#HAT.clip_grad_by_adjustment-202"><span class="linenos">202</span></a>                <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand_like</span><span class="p">(</span><span class="n">bias_mask</span><span class="p">)</span> <span class="o">*</span> <span class="n">bias_mask</span> <span class="o">+</span> <span class="p">(</span>
</span><span id="HAT.clip_grad_by_adjustment-203"><a href="#HAT.clip_grad_by_adjustment-203"><span class="linenos">203</span></a>                    <span class="mi">1</span> <span class="o">-</span> <span class="n">bias_mask</span>
</span><span id="HAT.clip_grad_by_adjustment-204"><a href="#HAT.clip_grad_by_adjustment-204"><span class="linenos">204</span></a>                <span class="p">)</span>
</span><span id="HAT.clip_grad_by_adjustment-205"><a href="#HAT.clip_grad_by_adjustment-205"><span class="linenos">205</span></a>
</span><span id="HAT.clip_grad_by_adjustment-206"><a href="#HAT.clip_grad_by_adjustment-206"><span class="linenos">206</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;hat_const_alpha&quot;</span><span class="p">:</span>
</span><span id="HAT.clip_grad_by_adjustment-207"><a href="#HAT.clip_grad_by_adjustment-207"><span class="linenos">207</span></a>                <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span>
</span><span id="HAT.clip_grad_by_adjustment-208"><a href="#HAT.clip_grad_by_adjustment-208"><span class="linenos">208</span></a>                    <span class="n">weight_mask</span>
</span><span id="HAT.clip_grad_by_adjustment-209"><a href="#HAT.clip_grad_by_adjustment-209"><span class="linenos">209</span></a>                <span class="p">)</span> <span class="o">*</span> <span class="n">weight_mask</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">weight_mask</span><span class="p">)</span>
</span><span id="HAT.clip_grad_by_adjustment-210"><a href="#HAT.clip_grad_by_adjustment-210"><span class="linenos">210</span></a>                <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span>
</span><span id="HAT.clip_grad_by_adjustment-211"><a href="#HAT.clip_grad_by_adjustment-211"><span class="linenos">211</span></a>                    <span class="n">bias_mask</span>
</span><span id="HAT.clip_grad_by_adjustment-212"><a href="#HAT.clip_grad_by_adjustment-212"><span class="linenos">212</span></a>                <span class="p">)</span> <span class="o">*</span> <span class="n">bias_mask</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">bias_mask</span><span class="p">)</span>
</span><span id="HAT.clip_grad_by_adjustment-213"><a href="#HAT.clip_grad_by_adjustment-213"><span class="linenos">213</span></a>
</span><span id="HAT.clip_grad_by_adjustment-214"><a href="#HAT.clip_grad_by_adjustment-214"><span class="linenos">214</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;hat_const_1&quot;</span><span class="p">:</span>
</span><span id="HAT.clip_grad_by_adjustment-215"><a href="#HAT.clip_grad_by_adjustment-215"><span class="linenos">215</span></a>                <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span>
</span><span id="HAT.clip_grad_by_adjustment-216"><a href="#HAT.clip_grad_by_adjustment-216"><span class="linenos">216</span></a>                    <span class="n">weight_mask</span>
</span><span id="HAT.clip_grad_by_adjustment-217"><a href="#HAT.clip_grad_by_adjustment-217"><span class="linenos">217</span></a>                <span class="p">)</span> <span class="o">*</span> <span class="n">weight_mask</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">weight_mask</span><span class="p">)</span>
</span><span id="HAT.clip_grad_by_adjustment-218"><a href="#HAT.clip_grad_by_adjustment-218"><span class="linenos">218</span></a>                <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">bias_mask</span><span class="p">)</span> <span class="o">*</span> <span class="n">bias_mask</span> <span class="o">+</span> <span class="p">(</span>
</span><span id="HAT.clip_grad_by_adjustment-219"><a href="#HAT.clip_grad_by_adjustment-219"><span class="linenos">219</span></a>                    <span class="mi">1</span> <span class="o">-</span> <span class="n">bias_mask</span>
</span><span id="HAT.clip_grad_by_adjustment-220"><a href="#HAT.clip_grad_by_adjustment-220"><span class="linenos">220</span></a>                <span class="p">)</span>
</span><span id="HAT.clip_grad_by_adjustment-221"><a href="#HAT.clip_grad_by_adjustment-221"><span class="linenos">221</span></a>
</span><span id="HAT.clip_grad_by_adjustment-222"><a href="#HAT.clip_grad_by_adjustment-222"><span class="linenos">222</span></a>            <span class="c1"># apply the adjustment rate to the gradients</span>
</span><span id="HAT.clip_grad_by_adjustment-223"><a href="#HAT.clip_grad_by_adjustment-223"><span class="linenos">223</span></a>            <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">adjustment_rate_weight_layer</span>
</span><span id="HAT.clip_grad_by_adjustment-224"><a href="#HAT.clip_grad_by_adjustment-224"><span class="linenos">224</span></a>            <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HAT.clip_grad_by_adjustment-225"><a href="#HAT.clip_grad_by_adjustment-225"><span class="linenos">225</span></a>                <span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">adjustment_rate_bias_layer</span>
</span><span id="HAT.clip_grad_by_adjustment-226"><a href="#HAT.clip_grad_by_adjustment-226"><span class="linenos">226</span></a>
</span><span id="HAT.clip_grad_by_adjustment-227"><a href="#HAT.clip_grad_by_adjustment-227"><span class="linenos">227</span></a>            <span class="c1"># store the adjustment rate for logging</span>
</span><span id="HAT.clip_grad_by_adjustment-228"><a href="#HAT.clip_grad_by_adjustment-228"><span class="linenos">228</span></a>            <span class="n">adjustment_rate_weight</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">adjustment_rate_weight_layer</span>
</span><span id="HAT.clip_grad_by_adjustment-229"><a href="#HAT.clip_grad_by_adjustment-229"><span class="linenos">229</span></a>            <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HAT.clip_grad_by_adjustment-230"><a href="#HAT.clip_grad_by_adjustment-230"><span class="linenos">230</span></a>                <span class="n">adjustment_rate_bias</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">adjustment_rate_bias_layer</span>
</span><span id="HAT.clip_grad_by_adjustment-231"><a href="#HAT.clip_grad_by_adjustment-231"><span class="linenos">231</span></a>
</span><span id="HAT.clip_grad_by_adjustment-232"><a href="#HAT.clip_grad_by_adjustment-232"><span class="linenos">232</span></a>            <span class="c1"># update network capacity metric</span>
</span><span id="HAT.clip_grad_by_adjustment-233"><a href="#HAT.clip_grad_by_adjustment-233"><span class="linenos">233</span></a>            <span class="n">capacity</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">adjustment_rate_weight_layer</span><span class="p">,</span> <span class="n">adjustment_rate_bias_layer</span><span class="p">)</span>
</span><span id="HAT.clip_grad_by_adjustment-234"><a href="#HAT.clip_grad_by_adjustment-234"><span class="linenos">234</span></a>
</span><span id="HAT.clip_grad_by_adjustment-235"><a href="#HAT.clip_grad_by_adjustment-235"><span class="linenos">235</span></a>        <span class="k">return</span> <span class="n">adjustment_rate_weight</span><span class="p">,</span> <span class="n">adjustment_rate_bias</span><span class="p">,</span> <span class="n">capacity</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
</span></pre></div>


            <div class="docstring"><p>Clip the gradients by the adjustment rate. See Eq. (2) in Sec. 2.3 "Network Training" in the <a href="http://proceedings.mlr.press/v80/serra18a">HAT paper</a>.</p>

<p>Note that because the task embedding fully covers every layer in the backbone network, no parameters are left out of this system.
This applies not only to parameters between layers with task embeddings, but also to those before the first layer. We design it separately in the code.</p>

<p>Network capacity is measured alongside this method. Network capacity is defined as the average adjustment rate over all parameters.
See Sec. 4.1 in the <a href="https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9">AdaHAT paper</a>.</p>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>adjustment_rate_weight</strong> (<code>dict[str, Tensor]</code>): the adjustment rate for weights. Keys (<code>str</code>) are layer names and values (<code>Tensor</code>) are the adjustment rate tensors.</li>
<li><strong>adjustment_rate_bias</strong> (<code>dict[str, Tensor]</code>): the adjustment rate for biases. Keys (<code>str</code>) are layer name and values (<code>Tensor</code>) are the adjustment rate tensors.</li>
<li><strong>capacity</strong> (<code>Tensor</code>): the calculated network capacity.</li>
</ul>
</div>


                            </div>
                            <div id="HAT.compensate_task_embedding_gradients" class="classattr">
                                        <input id="HAT.compensate_task_embedding_gradients-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">compensate_task_embedding_gradients</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span>, </span><span class="param"><span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span></span><span class="return-annotation">) -> <span class="kc">None</span>:</span></span>

                <label class="view-source-button" for="HAT.compensate_task_embedding_gradients-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HAT.compensate_task_embedding_gradients"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HAT.compensate_task_embedding_gradients-237"><a href="#HAT.compensate_task_embedding_gradients-237"><span class="linenos">237</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">compensate_task_embedding_gradients</span><span class="p">(</span>
</span><span id="HAT.compensate_task_embedding_gradients-238"><a href="#HAT.compensate_task_embedding_gradients-238"><span class="linenos">238</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="HAT.compensate_task_embedding_gradients-239"><a href="#HAT.compensate_task_embedding_gradients-239"><span class="linenos">239</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HAT.compensate_task_embedding_gradients-240"><a href="#HAT.compensate_task_embedding_gradients-240"><span class="linenos">240</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HAT.compensate_task_embedding_gradients-241"><a href="#HAT.compensate_task_embedding_gradients-241"><span class="linenos">241</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HAT.compensate_task_embedding_gradients-242"><a href="#HAT.compensate_task_embedding_gradients-242"><span class="linenos">242</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Compensate the gradients of task embeddings during training. See Sec. 2.5 &quot;Embedding Gradient Compensation&quot; in the [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="HAT.compensate_task_embedding_gradients-243"><a href="#HAT.compensate_task_embedding_gradients-243"><span class="linenos">243</span></a>
</span><span id="HAT.compensate_task_embedding_gradients-244"><a href="#HAT.compensate_task_embedding_gradients-244"><span class="linenos">244</span></a><span class="sd">        **Args:**</span>
</span><span id="HAT.compensate_task_embedding_gradients-245"><a href="#HAT.compensate_task_embedding_gradients-245"><span class="linenos">245</span></a><span class="sd">        - **batch_idx** (`int`): the current training batch index.</span>
</span><span id="HAT.compensate_task_embedding_gradients-246"><a href="#HAT.compensate_task_embedding_gradients-246"><span class="linenos">246</span></a><span class="sd">        - **num_batches** (`int`): the total number of training batches.</span>
</span><span id="HAT.compensate_task_embedding_gradients-247"><a href="#HAT.compensate_task_embedding_gradients-247"><span class="linenos">247</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HAT.compensate_task_embedding_gradients-248"><a href="#HAT.compensate_task_embedding_gradients-248"><span class="linenos">248</span></a>
</span><span id="HAT.compensate_task_embedding_gradients-249"><a href="#HAT.compensate_task_embedding_gradients-249"><span class="linenos">249</span></a>        <span class="k">for</span> <span class="n">te</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
</span><span id="HAT.compensate_task_embedding_gradients-250"><a href="#HAT.compensate_task_embedding_gradients-250"><span class="linenos">250</span></a>            <span class="n">anneal_scalar</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_max</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">s_max</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_max</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span>
</span><span id="HAT.compensate_task_embedding_gradients-251"><a href="#HAT.compensate_task_embedding_gradients-251"><span class="linenos">251</span></a>                <span class="n">batch_idx</span> <span class="o">-</span> <span class="mi">1</span>
</span><span id="HAT.compensate_task_embedding_gradients-252"><a href="#HAT.compensate_task_embedding_gradients-252"><span class="linenos">252</span></a>            <span class="p">)</span> <span class="o">/</span> <span class="p">(</span>
</span><span id="HAT.compensate_task_embedding_gradients-253"><a href="#HAT.compensate_task_embedding_gradients-253"><span class="linenos">253</span></a>                <span class="n">num_batches</span> <span class="o">-</span> <span class="mi">1</span>
</span><span id="HAT.compensate_task_embedding_gradients-254"><a href="#HAT.compensate_task_embedding_gradients-254"><span class="linenos">254</span></a>            <span class="p">)</span>  <span class="c1"># see Eq. (3) in Sec. 2.4 &quot;Hard Attention Training&quot; in the [HAT paper](http://proceedings.mlr.press/v80/serra18a)</span>
</span><span id="HAT.compensate_task_embedding_gradients-255"><a href="#HAT.compensate_task_embedding_gradients-255"><span class="linenos">255</span></a>
</span><span id="HAT.compensate_task_embedding_gradients-256"><a href="#HAT.compensate_task_embedding_gradients-256"><span class="linenos">256</span></a>            <span class="n">num</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="HAT.compensate_task_embedding_gradients-257"><a href="#HAT.compensate_task_embedding_gradients-257"><span class="linenos">257</span></a>                <span class="n">torch</span><span class="o">.</span><span class="n">cosh</span><span class="p">(</span>
</span><span id="HAT.compensate_task_embedding_gradients-258"><a href="#HAT.compensate_task_embedding_gradients-258"><span class="linenos">258</span></a>                    <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span>
</span><span id="HAT.compensate_task_embedding_gradients-259"><a href="#HAT.compensate_task_embedding_gradients-259"><span class="linenos">259</span></a>                        <span class="n">anneal_scalar</span> <span class="o">*</span> <span class="n">te</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">,</span>
</span><span id="HAT.compensate_task_embedding_gradients-260"><a href="#HAT.compensate_task_embedding_gradients-260"><span class="linenos">260</span></a>                        <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">clamp_threshold</span><span class="p">,</span>
</span><span id="HAT.compensate_task_embedding_gradients-261"><a href="#HAT.compensate_task_embedding_gradients-261"><span class="linenos">261</span></a>                        <span class="bp">self</span><span class="o">.</span><span class="n">clamp_threshold</span><span class="p">,</span>
</span><span id="HAT.compensate_task_embedding_gradients-262"><a href="#HAT.compensate_task_embedding_gradients-262"><span class="linenos">262</span></a>                    <span class="p">)</span>
</span><span id="HAT.compensate_task_embedding_gradients-263"><a href="#HAT.compensate_task_embedding_gradients-263"><span class="linenos">263</span></a>                <span class="p">)</span>
</span><span id="HAT.compensate_task_embedding_gradients-264"><a href="#HAT.compensate_task_embedding_gradients-264"><span class="linenos">264</span></a>                <span class="o">+</span> <span class="mi">1</span>
</span><span id="HAT.compensate_task_embedding_gradients-265"><a href="#HAT.compensate_task_embedding_gradients-265"><span class="linenos">265</span></a>            <span class="p">)</span>
</span><span id="HAT.compensate_task_embedding_gradients-266"><a href="#HAT.compensate_task_embedding_gradients-266"><span class="linenos">266</span></a>
</span><span id="HAT.compensate_task_embedding_gradients-267"><a href="#HAT.compensate_task_embedding_gradients-267"><span class="linenos">267</span></a>            <span class="n">den</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cosh</span><span class="p">(</span><span class="n">te</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="HAT.compensate_task_embedding_gradients-268"><a href="#HAT.compensate_task_embedding_gradients-268"><span class="linenos">268</span></a>
</span><span id="HAT.compensate_task_embedding_gradients-269"><a href="#HAT.compensate_task_embedding_gradients-269"><span class="linenos">269</span></a>            <span class="n">compensation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_max</span> <span class="o">/</span> <span class="n">anneal_scalar</span> <span class="o">*</span> <span class="n">num</span> <span class="o">/</span> <span class="n">den</span>
</span><span id="HAT.compensate_task_embedding_gradients-270"><a href="#HAT.compensate_task_embedding_gradients-270"><span class="linenos">270</span></a>
</span><span id="HAT.compensate_task_embedding_gradients-271"><a href="#HAT.compensate_task_embedding_gradients-271"><span class="linenos">271</span></a>            <span class="n">te</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">compensation</span>
</span></pre></div>


            <div class="docstring"><p>Compensate the gradients of task embeddings during training. See Sec. 2.5 "Embedding Gradient Compensation" in the <a href="http://proceedings.mlr.press/v80/serra18a">HAT paper</a>.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>batch_idx</strong> (<code>int</code>): the current training batch index.</li>
<li><strong>num_batches</strong> (<code>int</code>): the total number of training batches.</li>
</ul>
</div>


                            </div>
                            <div id="HAT.forward" class="classattr">
                                        <input id="HAT.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">stage</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">task_id</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span></span><span class="return-annotation">) -> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span>:</span></span>

                <label class="view-source-button" for="HAT.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HAT.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HAT.forward-273"><a href="#HAT.forward-273"><span class="linenos">273</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="HAT.forward-274"><a href="#HAT.forward-274"><span class="linenos">274</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="HAT.forward-275"><a href="#HAT.forward-275"><span class="linenos">275</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="HAT.forward-276"><a href="#HAT.forward-276"><span class="linenos">276</span></a>        <span class="n">stage</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="HAT.forward-277"><a href="#HAT.forward-277"><span class="linenos">277</span></a>        <span class="n">task_id</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HAT.forward-278"><a href="#HAT.forward-278"><span class="linenos">278</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HAT.forward-279"><a href="#HAT.forward-279"><span class="linenos">279</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HAT.forward-280"><a href="#HAT.forward-280"><span class="linenos">280</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
</span><span id="HAT.forward-281"><a href="#HAT.forward-281"><span class="linenos">281</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The forward pass for data from task `task_id`. Note that it is nothing to do with `forward()` method in `nn.Module`.</span>
</span><span id="HAT.forward-282"><a href="#HAT.forward-282"><span class="linenos">282</span></a>
</span><span id="HAT.forward-283"><a href="#HAT.forward-283"><span class="linenos">283</span></a><span class="sd">        **Args:**</span>
</span><span id="HAT.forward-284"><a href="#HAT.forward-284"><span class="linenos">284</span></a><span class="sd">        - **input** (`Tensor`): The input tensor from data.</span>
</span><span id="HAT.forward-285"><a href="#HAT.forward-285"><span class="linenos">285</span></a><span class="sd">        - **stage** (`str`): the stage of the forward pass, should be one of the following:</span>
</span><span id="HAT.forward-286"><a href="#HAT.forward-286"><span class="linenos">286</span></a><span class="sd">            1. &#39;train&#39;: training stage.</span>
</span><span id="HAT.forward-287"><a href="#HAT.forward-287"><span class="linenos">287</span></a><span class="sd">            2. &#39;validation&#39;: validation stage.</span>
</span><span id="HAT.forward-288"><a href="#HAT.forward-288"><span class="linenos">288</span></a><span class="sd">            3. &#39;test&#39;: testing stage.</span>
</span><span id="HAT.forward-289"><a href="#HAT.forward-289"><span class="linenos">289</span></a><span class="sd">        - **task_id** (`int`| `None`): the task ID where the data are from. If the stage is &#39;train&#39; or &#39;validation&#39;, it should be the current task `self.task_id`. If stage is &#39;test&#39;, it could be from any seen task. In TIL, the task IDs of test data are provided thus this argument can be used. HAT algorithm works only for TIL.</span>
</span><span id="HAT.forward-290"><a href="#HAT.forward-290"><span class="linenos">290</span></a><span class="sd">        - **batch_idx** (`int` | `None`): the current batch index. Applies only to training stage. For other stages, it is default `None`.</span>
</span><span id="HAT.forward-291"><a href="#HAT.forward-291"><span class="linenos">291</span></a><span class="sd">        - **num_batches** (`int` | `None`): the total number of batches. Applies only to training stage. For other stages, it is default `None`.</span>
</span><span id="HAT.forward-292"><a href="#HAT.forward-292"><span class="linenos">292</span></a>
</span><span id="HAT.forward-293"><a href="#HAT.forward-293"><span class="linenos">293</span></a><span class="sd">        **Returns:**</span>
</span><span id="HAT.forward-294"><a href="#HAT.forward-294"><span class="linenos">294</span></a><span class="sd">        - **logits** (`Tensor`): the output logits tensor.</span>
</span><span id="HAT.forward-295"><a href="#HAT.forward-295"><span class="linenos">295</span></a><span class="sd">        - **mask** (`dict[str, Tensor]`): the mask for the current task. Key (`str`) is layer name, value (`Tensor`) is the mask tensor. The mask tensor has size (number of units, ).</span>
</span><span id="HAT.forward-296"><a href="#HAT.forward-296"><span class="linenos">296</span></a><span class="sd">        - **activations** (`dict[str, Tensor]`): the hidden features (after activation) in each weighted layer. Key (`str`) is the weighted layer name, value (`Tensor`) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes. Although HAT algorithm does not need this, it is still provided for API consistence for other HAT-based algorithms inherited this `forward()` method of `HAT` class.</span>
</span><span id="HAT.forward-297"><a href="#HAT.forward-297"><span class="linenos">297</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HAT.forward-298"><a href="#HAT.forward-298"><span class="linenos">298</span></a>        <span class="n">feature</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="p">(</span>
</span><span id="HAT.forward-299"><a href="#HAT.forward-299"><span class="linenos">299</span></a>            <span class="nb">input</span><span class="p">,</span>
</span><span id="HAT.forward-300"><a href="#HAT.forward-300"><span class="linenos">300</span></a>            <span class="n">stage</span><span class="o">=</span><span class="n">stage</span><span class="p">,</span>
</span><span id="HAT.forward-301"><a href="#HAT.forward-301"><span class="linenos">301</span></a>            <span class="n">s_max</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">s_max</span> <span class="k">if</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;train&quot;</span> <span class="ow">or</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;validation&quot;</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HAT.forward-302"><a href="#HAT.forward-302"><span class="linenos">302</span></a>            <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span> <span class="k">if</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;train&quot;</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HAT.forward-303"><a href="#HAT.forward-303"><span class="linenos">303</span></a>            <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span> <span class="k">if</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;train&quot;</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HAT.forward-304"><a href="#HAT.forward-304"><span class="linenos">304</span></a>            <span class="n">test_task_id</span><span class="o">=</span><span class="n">task_id</span> <span class="k">if</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;test&quot;</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HAT.forward-305"><a href="#HAT.forward-305"><span class="linenos">305</span></a>        <span class="p">)</span>
</span><span id="HAT.forward-306"><a href="#HAT.forward-306"><span class="linenos">306</span></a>        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">heads</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="n">task_id</span><span class="p">)</span>
</span><span id="HAT.forward-307"><a href="#HAT.forward-307"><span class="linenos">307</span></a>
</span><span id="HAT.forward-308"><a href="#HAT.forward-308"><span class="linenos">308</span></a>        <span class="k">return</span> <span class="p">(</span>
</span><span id="HAT.forward-309"><a href="#HAT.forward-309"><span class="linenos">309</span></a>            <span class="n">logits</span>
</span><span id="HAT.forward-310"><a href="#HAT.forward-310"><span class="linenos">310</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">if_forward_func_return_logits_only</span>
</span><span id="HAT.forward-311"><a href="#HAT.forward-311"><span class="linenos">311</span></a>            <span class="k">else</span> <span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">activations</span><span class="p">)</span>
</span><span id="HAT.forward-312"><a href="#HAT.forward-312"><span class="linenos">312</span></a>        <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>The forward pass for data from task <code><a href="#HAT.task_id">task_id</a></code>. Note that it is nothing to do with <code><a href="#HAT.forward">forward()</a></code> method in <code>nn.Module</code>.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>input</strong> (<code>Tensor</code>): The input tensor from data.</li>
<li><strong>stage</strong> (<code>str</code>): the stage of the forward pass, should be one of the following:
<ol>
<li>'train': training stage.</li>
<li>'validation': validation stage.</li>
<li>'test': testing stage.</li>
</ol></li>
<li><strong>task_id</strong> (<code>int</code>| <code>None</code>): the task ID where the data are from. If the stage is 'train' or 'validation', it should be the current task <code>self.task_id</code>. If stage is 'test', it could be from any seen task. In TIL, the task IDs of test data are provided thus this argument can be used. HAT algorithm works only for TIL.</li>
<li><strong>batch_idx</strong> (<code>int</code> | <code>None</code>): the current batch index. Applies only to training stage. For other stages, it is default <code>None</code>.</li>
<li><strong>num_batches</strong> (<code>int</code> | <code>None</code>): the total number of batches. Applies only to training stage. For other stages, it is default <code>None</code>.</li>
</ul>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>logits</strong> (<code>Tensor</code>): the output logits tensor.</li>
<li><strong>mask</strong> (<code>dict[str, Tensor]</code>): the mask for the current task. Key (<code>str</code>) is layer name, value (<code>Tensor</code>) is the mask tensor. The mask tensor has size (number of units, ).</li>
<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): the hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name, value (<code>Tensor</code>) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes. Although HAT algorithm does not need this, it is still provided for API consistence for other HAT-based algorithms inherited this <code><a href="#HAT.forward">forward()</a></code> method of <code><a href="#HAT">HAT</a></code> class.</li>
</ul>
</div>


                            </div>
                            <div id="HAT.training_step" class="classattr">
                                        <input id="HAT.training_step-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">training_step</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">batch</span><span class="p">:</span> <span class="n">Any</span>, </span><span class="param"><span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span></span><span class="return-annotation">) -> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>:</span></span>

                <label class="view-source-button" for="HAT.training_step-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HAT.training_step"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HAT.training_step-314"><a href="#HAT.training_step-314"><span class="linenos">314</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
</span><span id="HAT.training_step-315"><a href="#HAT.training_step-315"><span class="linenos">315</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Training step for current task `self.task_id`.</span>
</span><span id="HAT.training_step-316"><a href="#HAT.training_step-316"><span class="linenos">316</span></a>
</span><span id="HAT.training_step-317"><a href="#HAT.training_step-317"><span class="linenos">317</span></a><span class="sd">        **Args:**</span>
</span><span id="HAT.training_step-318"><a href="#HAT.training_step-318"><span class="linenos">318</span></a><span class="sd">        - **batch** (`Any`): a batch of training data.</span>
</span><span id="HAT.training_step-319"><a href="#HAT.training_step-319"><span class="linenos">319</span></a><span class="sd">        - **batch_idx** (`int`): the index of the batch. Used for calculating annealed scalar in HAT. See Sec. 2.4 &quot;Hard Attention Training&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="HAT.training_step-320"><a href="#HAT.training_step-320"><span class="linenos">320</span></a>
</span><span id="HAT.training_step-321"><a href="#HAT.training_step-321"><span class="linenos">321</span></a><span class="sd">        **Returns:**</span>
</span><span id="HAT.training_step-322"><a href="#HAT.training_step-322"><span class="linenos">322</span></a><span class="sd">        - **outputs** (`dict[str, Tensor]`): a dictionary containing loss and other metrics from this training step. Keys (`str`) are metric names, and values (`Tensor`) are the metrics. Must include the key &#39;loss&#39; (total loss) in the case of automatic optimization, according to PyTorch Lightning. For HAT, it includes &#39;mask&#39; and &#39;capacity&#39; for logging.</span>
</span><span id="HAT.training_step-323"><a href="#HAT.training_step-323"><span class="linenos">323</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HAT.training_step-324"><a href="#HAT.training_step-324"><span class="linenos">324</span></a>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
</span><span id="HAT.training_step-325"><a href="#HAT.training_step-325"><span class="linenos">325</span></a>
</span><span id="HAT.training_step-326"><a href="#HAT.training_step-326"><span class="linenos">326</span></a>        <span class="c1"># zero the gradients before forward pass in manual optimization mode</span>
</span><span id="HAT.training_step-327"><a href="#HAT.training_step-327"><span class="linenos">327</span></a>        <span class="n">opt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizers</span><span class="p">()</span>
</span><span id="HAT.training_step-328"><a href="#HAT.training_step-328"><span class="linenos">328</span></a>        <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</span><span id="HAT.training_step-329"><a href="#HAT.training_step-329"><span class="linenos">329</span></a>
</span><span id="HAT.training_step-330"><a href="#HAT.training_step-330"><span class="linenos">330</span></a>        <span class="c1"># classification loss</span>
</span><span id="HAT.training_step-331"><a href="#HAT.training_step-331"><span class="linenos">331</span></a>        <span class="n">num_batches</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">num_training_batches</span>
</span><span id="HAT.training_step-332"><a href="#HAT.training_step-332"><span class="linenos">332</span></a>        <span class="n">logits</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
</span><span id="HAT.training_step-333"><a href="#HAT.training_step-333"><span class="linenos">333</span></a>            <span class="n">x</span><span class="p">,</span>
</span><span id="HAT.training_step-334"><a href="#HAT.training_step-334"><span class="linenos">334</span></a>            <span class="n">stage</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">,</span>
</span><span id="HAT.training_step-335"><a href="#HAT.training_step-335"><span class="linenos">335</span></a>            <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="HAT.training_step-336"><a href="#HAT.training_step-336"><span class="linenos">336</span></a>            <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="HAT.training_step-337"><a href="#HAT.training_step-337"><span class="linenos">337</span></a>            <span class="n">task_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">,</span>
</span><span id="HAT.training_step-338"><a href="#HAT.training_step-338"><span class="linenos">338</span></a>        <span class="p">)</span>
</span><span id="HAT.training_step-339"><a href="#HAT.training_step-339"><span class="linenos">339</span></a>        <span class="n">loss_cls</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span><span id="HAT.training_step-340"><a href="#HAT.training_step-340"><span class="linenos">340</span></a>
</span><span id="HAT.training_step-341"><a href="#HAT.training_step-341"><span class="linenos">341</span></a>        <span class="c1"># regularization loss. See Sec. 2.6 &quot;Promoting Low Capacity Usage&quot; in the [HAT paper](http://proceedings.mlr.press/v80/serra18a)</span>
</span><span id="HAT.training_step-342"><a href="#HAT.training_step-342"><span class="linenos">342</span></a>        <span class="n">loss_reg</span><span class="p">,</span> <span class="n">network_sparsity</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mark_sparsity_reg</span><span class="p">(</span>
</span><span id="HAT.training_step-343"><a href="#HAT.training_step-343"><span class="linenos">343</span></a>            <span class="n">mask</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cumulative_mask_for_previous_tasks</span>
</span><span id="HAT.training_step-344"><a href="#HAT.training_step-344"><span class="linenos">344</span></a>        <span class="p">)</span>
</span><span id="HAT.training_step-345"><a href="#HAT.training_step-345"><span class="linenos">345</span></a>
</span><span id="HAT.training_step-346"><a href="#HAT.training_step-346"><span class="linenos">346</span></a>        <span class="c1"># total loss. See Eq. (4) in Sec. 2.6 &quot;Promoting Low Capacity Usage&quot; in the [HAT paper](http://proceedings.mlr.press/v80/serra18a)</span>
</span><span id="HAT.training_step-347"><a href="#HAT.training_step-347"><span class="linenos">347</span></a>        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_cls</span> <span class="o">+</span> <span class="n">loss_reg</span>
</span><span id="HAT.training_step-348"><a href="#HAT.training_step-348"><span class="linenos">348</span></a>
</span><span id="HAT.training_step-349"><a href="#HAT.training_step-349"><span class="linenos">349</span></a>        <span class="c1"># backward step (manually)</span>
</span><span id="HAT.training_step-350"><a href="#HAT.training_step-350"><span class="linenos">350</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">manual_backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>  <span class="c1"># calculate the gradients</span>
</span><span id="HAT.training_step-351"><a href="#HAT.training_step-351"><span class="linenos">351</span></a>        <span class="c1"># HAT hard-clips gradients using the cumulative masks. See Eq. (2) in Sec. 2.3 &quot;Network Training&quot; in the HAT paper.</span>
</span><span id="HAT.training_step-352"><a href="#HAT.training_step-352"><span class="linenos">352</span></a>        <span class="c1"># Network capacity is computed along with this process (defined as the average adjustment rate over all parameters; see Sec. 4.1 in the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9)).</span>
</span><span id="HAT.training_step-353"><a href="#HAT.training_step-353"><span class="linenos">353</span></a>
</span><span id="HAT.training_step-354"><a href="#HAT.training_step-354"><span class="linenos">354</span></a>        <span class="n">adjustment_rate_weight</span><span class="p">,</span> <span class="n">adjustment_rate_bias</span><span class="p">,</span> <span class="n">capacity</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="HAT.training_step-355"><a href="#HAT.training_step-355"><span class="linenos">355</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">clip_grad_by_adjustment</span><span class="p">(</span>
</span><span id="HAT.training_step-356"><a href="#HAT.training_step-356"><span class="linenos">356</span></a>                <span class="n">network_sparsity</span><span class="o">=</span><span class="n">network_sparsity</span><span class="p">,</span>  <span class="c1"># passed for compatibility with AdaHAT, which inherits this method</span>
</span><span id="HAT.training_step-357"><a href="#HAT.training_step-357"><span class="linenos">357</span></a>            <span class="p">)</span>
</span><span id="HAT.training_step-358"><a href="#HAT.training_step-358"><span class="linenos">358</span></a>        <span class="p">)</span>
</span><span id="HAT.training_step-359"><a href="#HAT.training_step-359"><span class="linenos">359</span></a>        <span class="c1"># compensate the gradients of task embedding. See Sec. 2.5 &quot;Embedding Gradient Compensation&quot; in the [HAT paper](http://proceedings.mlr.press/v80/serra18a)</span>
</span><span id="HAT.training_step-360"><a href="#HAT.training_step-360"><span class="linenos">360</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">compensate_task_embedding_gradients</span><span class="p">(</span>
</span><span id="HAT.training_step-361"><a href="#HAT.training_step-361"><span class="linenos">361</span></a>            <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="HAT.training_step-362"><a href="#HAT.training_step-362"><span class="linenos">362</span></a>            <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="HAT.training_step-363"><a href="#HAT.training_step-363"><span class="linenos">363</span></a>        <span class="p">)</span>
</span><span id="HAT.training_step-364"><a href="#HAT.training_step-364"><span class="linenos">364</span></a>        <span class="c1"># update parameters with the modified gradients</span>
</span><span id="HAT.training_step-365"><a href="#HAT.training_step-365"><span class="linenos">365</span></a>        <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span><span id="HAT.training_step-366"><a href="#HAT.training_step-366"><span class="linenos">366</span></a>
</span><span id="HAT.training_step-367"><a href="#HAT.training_step-367"><span class="linenos">367</span></a>        <span class="c1"># accuracy of the batch</span>
</span><span id="HAT.training_step-368"><a href="#HAT.training_step-368"><span class="linenos">368</span></a>        <span class="n">acc</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</span><span id="HAT.training_step-369"><a href="#HAT.training_step-369"><span class="linenos">369</span></a>
</span><span id="HAT.training_step-370"><a href="#HAT.training_step-370"><span class="linenos">370</span></a>        <span class="k">return</span> <span class="p">{</span>
</span><span id="HAT.training_step-371"><a href="#HAT.training_step-371"><span class="linenos">371</span></a>            <span class="s2">&quot;loss&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span>  <span class="c1"># return loss is essential for training step, or backpropagation will fail</span>
</span><span id="HAT.training_step-372"><a href="#HAT.training_step-372"><span class="linenos">372</span></a>            <span class="s2">&quot;loss_cls&quot;</span><span class="p">:</span> <span class="n">loss_cls</span><span class="p">,</span>
</span><span id="HAT.training_step-373"><a href="#HAT.training_step-373"><span class="linenos">373</span></a>            <span class="s2">&quot;loss_reg&quot;</span><span class="p">:</span> <span class="n">loss_reg</span><span class="p">,</span>
</span><span id="HAT.training_step-374"><a href="#HAT.training_step-374"><span class="linenos">374</span></a>            <span class="s2">&quot;acc&quot;</span><span class="p">:</span> <span class="n">acc</span><span class="p">,</span>
</span><span id="HAT.training_step-375"><a href="#HAT.training_step-375"><span class="linenos">375</span></a>            <span class="s2">&quot;activations&quot;</span><span class="p">:</span> <span class="n">activations</span><span class="p">,</span>
</span><span id="HAT.training_step-376"><a href="#HAT.training_step-376"><span class="linenos">376</span></a>            <span class="s2">&quot;logits&quot;</span><span class="p">:</span> <span class="n">logits</span><span class="p">,</span>
</span><span id="HAT.training_step-377"><a href="#HAT.training_step-377"><span class="linenos">377</span></a>            <span class="s2">&quot;mask&quot;</span><span class="p">:</span> <span class="n">mask</span><span class="p">,</span>  <span class="c1"># return other metrics for lightning loggers callback to handle at `on_train_batch_end()`</span>
</span><span id="HAT.training_step-378"><a href="#HAT.training_step-378"><span class="linenos">378</span></a>            <span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span>  <span class="c1"># return the input batch for Captum to use</span>
</span><span id="HAT.training_step-379"><a href="#HAT.training_step-379"><span class="linenos">379</span></a>            <span class="s2">&quot;target&quot;</span><span class="p">:</span> <span class="n">y</span><span class="p">,</span>  <span class="c1"># return the target batch for Captum to use</span>
</span><span id="HAT.training_step-380"><a href="#HAT.training_step-380"><span class="linenos">380</span></a>            <span class="s2">&quot;adjustment_rate_weight&quot;</span><span class="p">:</span> <span class="n">adjustment_rate_weight</span><span class="p">,</span>  <span class="c1"># return the adjustment rate for weights and biases for logging</span>
</span><span id="HAT.training_step-381"><a href="#HAT.training_step-381"><span class="linenos">381</span></a>            <span class="s2">&quot;adjustment_rate_bias&quot;</span><span class="p">:</span> <span class="n">adjustment_rate_bias</span><span class="p">,</span>
</span><span id="HAT.training_step-382"><a href="#HAT.training_step-382"><span class="linenos">382</span></a>            <span class="s2">&quot;capacity&quot;</span><span class="p">:</span> <span class="n">capacity</span><span class="p">,</span>  <span class="c1"># return the network capacity for logging</span>
</span><span id="HAT.training_step-383"><a href="#HAT.training_step-383"><span class="linenos">383</span></a>        <span class="p">}</span>
</span></pre></div>


            <div class="docstring"><p>Training step for current task <code>self.task_id</code>.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>batch</strong> (<code>Any</code>): a batch of training data.</li>
<li><strong>batch_idx</strong> (<code>int</code>): the index of the batch. Used for calculating annealed scalar in HAT. See Sec. 2.4 "Hard Attention Training" in <a href="http://proceedings.mlr.press/v80/serra18a">HAT paper</a>.</li>
</ul>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary containing loss and other metrics from this training step. Keys (<code>str</code>) are metric names, and values (<code>Tensor</code>) are the metrics. Must include the key 'loss' (total loss) in the case of automatic optimization, according to PyTorch Lightning. For HAT, it includes 'mask' and 'capacity' for logging.</li>
</ul>
</div>


                            </div>
                            <div id="HAT.on_train_end" class="classattr">
                                        <input id="HAT.on_train_end-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">on_train_end</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span></span><span class="return-annotation">) -> <span class="kc">None</span>:</span></span>

                <label class="view-source-button" for="HAT.on_train_end-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HAT.on_train_end"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HAT.on_train_end-385"><a href="#HAT.on_train_end-385"><span class="linenos">385</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_end</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HAT.on_train_end-386"><a href="#HAT.on_train_end-386"><span class="linenos">386</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the mask and update the cumulative mask after training the task.&quot;&quot;&quot;</span>
</span><span id="HAT.on_train_end-387"><a href="#HAT.on_train_end-387"><span class="linenos">387</span></a>
</span><span id="HAT.on_train_end-388"><a href="#HAT.on_train_end-388"><span class="linenos">388</span></a>        <span class="c1"># store the mask for the current task</span>
</span><span id="HAT.on_train_end-389"><a href="#HAT.on_train_end-389"><span class="linenos">389</span></a>        <span class="n">mask_t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">store_mask</span><span class="p">()</span>
</span><span id="HAT.on_train_end-390"><a href="#HAT.on_train_end-390"><span class="linenos">390</span></a>
</span><span id="HAT.on_train_end-391"><a href="#HAT.on_train_end-391"><span class="linenos">391</span></a>        <span class="c1"># store the batch normalization if necessary</span>
</span><span id="HAT.on_train_end-392"><a href="#HAT.on_train_end-392"><span class="linenos">392</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">store_bn</span><span class="p">()</span>
</span><span id="HAT.on_train_end-393"><a href="#HAT.on_train_end-393"><span class="linenos">393</span></a>
</span><span id="HAT.on_train_end-394"><a href="#HAT.on_train_end-394"><span class="linenos">394</span></a>        <span class="c1"># update the cumulative mask. See the first Eq. in Sec 2.3 &quot;Network Training&quot; in the [HAT paper](http://proceedings.mlr.press/v80/serra18a)</span>
</span><span id="HAT.on_train_end-395"><a href="#HAT.on_train_end-395"><span class="linenos">395</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">cumulative_mask_for_previous_tasks</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="HAT.on_train_end-396"><a href="#HAT.on_train_end-396"><span class="linenos">396</span></a>            <span class="n">layer_name</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span>
</span><span id="HAT.on_train_end-397"><a href="#HAT.on_train_end-397"><span class="linenos">397</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">cumulative_mask_for_previous_tasks</span><span class="p">[</span><span class="n">layer_name</span><span class="p">],</span> <span class="n">mask_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="HAT.on_train_end-398"><a href="#HAT.on_train_end-398"><span class="linenos">398</span></a>            <span class="p">)</span>
</span><span id="HAT.on_train_end-399"><a href="#HAT.on_train_end-399"><span class="linenos">399</span></a>            <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span>
</span><span id="HAT.on_train_end-400"><a href="#HAT.on_train_end-400"><span class="linenos">400</span></a>        <span class="p">}</span>
</span></pre></div>


            <div class="docstring"><p>Store the mask and update the cumulative mask after training the task.</p>
</div>


                            </div>
                            <div id="HAT.validation_step" class="classattr">
                                        <input id="HAT.validation_step-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">validation_step</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">batch</span><span class="p">:</span> <span class="n">Any</span></span><span class="return-annotation">) -> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>:</span></span>

                <label class="view-source-button" for="HAT.validation_step-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HAT.validation_step"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HAT.validation_step-402"><a href="#HAT.validation_step-402"><span class="linenos">402</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
</span><span id="HAT.validation_step-403"><a href="#HAT.validation_step-403"><span class="linenos">403</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Validation step for current task `self.task_id`.</span>
</span><span id="HAT.validation_step-404"><a href="#HAT.validation_step-404"><span class="linenos">404</span></a>
</span><span id="HAT.validation_step-405"><a href="#HAT.validation_step-405"><span class="linenos">405</span></a><span class="sd">        **Args:**</span>
</span><span id="HAT.validation_step-406"><a href="#HAT.validation_step-406"><span class="linenos">406</span></a><span class="sd">        - **batch** (`Any`): a batch of validation data.</span>
</span><span id="HAT.validation_step-407"><a href="#HAT.validation_step-407"><span class="linenos">407</span></a>
</span><span id="HAT.validation_step-408"><a href="#HAT.validation_step-408"><span class="linenos">408</span></a><span class="sd">        **Returns:**</span>
</span><span id="HAT.validation_step-409"><a href="#HAT.validation_step-409"><span class="linenos">409</span></a><span class="sd">        - **outputs** (`dict[str, Tensor]`): a dictionary contains loss and other metrics from this validation step. Keys (`str`) are the metrics names, and values (`Tensor`) are the metrics.</span>
</span><span id="HAT.validation_step-410"><a href="#HAT.validation_step-410"><span class="linenos">410</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HAT.validation_step-411"><a href="#HAT.validation_step-411"><span class="linenos">411</span></a>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
</span><span id="HAT.validation_step-412"><a href="#HAT.validation_step-412"><span class="linenos">412</span></a>        <span class="n">logits</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">stage</span><span class="o">=</span><span class="s2">&quot;validation&quot;</span><span class="p">,</span> <span class="n">task_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">)</span>
</span><span id="HAT.validation_step-413"><a href="#HAT.validation_step-413"><span class="linenos">413</span></a>        <span class="n">loss_cls</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span><span id="HAT.validation_step-414"><a href="#HAT.validation_step-414"><span class="linenos">414</span></a>        <span class="n">acc</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</span><span id="HAT.validation_step-415"><a href="#HAT.validation_step-415"><span class="linenos">415</span></a>
</span><span id="HAT.validation_step-416"><a href="#HAT.validation_step-416"><span class="linenos">416</span></a>        <span class="k">return</span> <span class="p">{</span>
</span><span id="HAT.validation_step-417"><a href="#HAT.validation_step-417"><span class="linenos">417</span></a>            <span class="s2">&quot;loss_cls&quot;</span><span class="p">:</span> <span class="n">loss_cls</span><span class="p">,</span>
</span><span id="HAT.validation_step-418"><a href="#HAT.validation_step-418"><span class="linenos">418</span></a>            <span class="s2">&quot;acc&quot;</span><span class="p">:</span> <span class="n">acc</span><span class="p">,</span>  <span class="c1"># Return metrics for lightning loggers callback to handle at `on_validation_batch_end()`</span>
</span><span id="HAT.validation_step-419"><a href="#HAT.validation_step-419"><span class="linenos">419</span></a>        <span class="p">}</span>
</span></pre></div>


            <div class="docstring"><p>Validation step for current task <code>self.task_id</code>.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>batch</strong> (<code>Any</code>): a batch of validation data.</li>
</ul>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and other metrics from this validation step. Keys (<code>str</code>) are the metrics names, and values (<code>Tensor</code>) are the metrics.</li>
</ul>
</div>


                            </div>
                            <div id="HAT.test_step" class="classattr">
                                        <input id="HAT.test_step-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">test_step</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">batch</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dataloader</span><span class="o">.</span><span class="n">DataLoader</span>,</span><span class="param">	<span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">dataloader_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span></span><span class="return-annotation">) -> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>:</span></span>

                <label class="view-source-button" for="HAT.test_step-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HAT.test_step"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HAT.test_step-421"><a href="#HAT.test_step-421"><span class="linenos">421</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">test_step</span><span class="p">(</span>
</span><span id="HAT.test_step-422"><a href="#HAT.test_step-422"><span class="linenos">422</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="HAT.test_step-423"><a href="#HAT.test_step-423"><span class="linenos">423</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
</span><span id="HAT.test_step-424"><a href="#HAT.test_step-424"><span class="linenos">424</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Test step for current task `self.task_id`, which tests for all seen tasks indexed by `dataloader_idx`.</span>
</span><span id="HAT.test_step-425"><a href="#HAT.test_step-425"><span class="linenos">425</span></a>
</span><span id="HAT.test_step-426"><a href="#HAT.test_step-426"><span class="linenos">426</span></a><span class="sd">        **Args:**</span>
</span><span id="HAT.test_step-427"><a href="#HAT.test_step-427"><span class="linenos">427</span></a><span class="sd">        - **batch** (`Any`): a batch of test data.</span>
</span><span id="HAT.test_step-428"><a href="#HAT.test_step-428"><span class="linenos">428</span></a><span class="sd">        - **dataloader_idx** (`int`): the task ID of seen tasks to be tested. A default value of 0 is given otherwise the LightningModule will raise a `RuntimeError`.</span>
</span><span id="HAT.test_step-429"><a href="#HAT.test_step-429"><span class="linenos">429</span></a>
</span><span id="HAT.test_step-430"><a href="#HAT.test_step-430"><span class="linenos">430</span></a><span class="sd">        **Returns:**</span>
</span><span id="HAT.test_step-431"><a href="#HAT.test_step-431"><span class="linenos">431</span></a><span class="sd">        - **outputs** (`dict[str, Tensor]`): a dictionary contains loss and other metrics from this test step. Keys (`str`) are the metrics names, and values (`Tensor`) are the metrics.</span>
</span><span id="HAT.test_step-432"><a href="#HAT.test_step-432"><span class="linenos">432</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HAT.test_step-433"><a href="#HAT.test_step-433"><span class="linenos">433</span></a>        <span class="n">test_task_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_test_task_id_from_dataloader_idx</span><span class="p">(</span><span class="n">dataloader_idx</span><span class="p">)</span>
</span><span id="HAT.test_step-434"><a href="#HAT.test_step-434"><span class="linenos">434</span></a>
</span><span id="HAT.test_step-435"><a href="#HAT.test_step-435"><span class="linenos">435</span></a>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
</span><span id="HAT.test_step-436"><a href="#HAT.test_step-436"><span class="linenos">436</span></a>        <span class="n">logits</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
</span><span id="HAT.test_step-437"><a href="#HAT.test_step-437"><span class="linenos">437</span></a>            <span class="n">x</span><span class="p">,</span>
</span><span id="HAT.test_step-438"><a href="#HAT.test_step-438"><span class="linenos">438</span></a>            <span class="n">stage</span><span class="o">=</span><span class="s2">&quot;test&quot;</span><span class="p">,</span>
</span><span id="HAT.test_step-439"><a href="#HAT.test_step-439"><span class="linenos">439</span></a>            <span class="n">task_id</span><span class="o">=</span><span class="n">test_task_id</span><span class="p">,</span>
</span><span id="HAT.test_step-440"><a href="#HAT.test_step-440"><span class="linenos">440</span></a>        <span class="p">)</span>  <span class="c1"># use the corresponding head and mask to test (instead of the current task `self.task_id`)</span>
</span><span id="HAT.test_step-441"><a href="#HAT.test_step-441"><span class="linenos">441</span></a>        <span class="n">loss_cls</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span><span id="HAT.test_step-442"><a href="#HAT.test_step-442"><span class="linenos">442</span></a>        <span class="n">acc</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</span><span id="HAT.test_step-443"><a href="#HAT.test_step-443"><span class="linenos">443</span></a>
</span><span id="HAT.test_step-444"><a href="#HAT.test_step-444"><span class="linenos">444</span></a>        <span class="k">return</span> <span class="p">{</span>
</span><span id="HAT.test_step-445"><a href="#HAT.test_step-445"><span class="linenos">445</span></a>            <span class="s2">&quot;loss_cls&quot;</span><span class="p">:</span> <span class="n">loss_cls</span><span class="p">,</span>
</span><span id="HAT.test_step-446"><a href="#HAT.test_step-446"><span class="linenos">446</span></a>            <span class="s2">&quot;acc&quot;</span><span class="p">:</span> <span class="n">acc</span><span class="p">,</span>  <span class="c1"># Return metrics for lightning loggers callback to handle at `on_test_batch_end()`</span>
</span><span id="HAT.test_step-447"><a href="#HAT.test_step-447"><span class="linenos">447</span></a>        <span class="p">}</span>
</span></pre></div>


            <div class="docstring"><p>Test step for current task <code>self.task_id</code>, which tests for all seen tasks indexed by <code>dataloader_idx</code>.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>batch</strong> (<code>Any</code>): a batch of test data.</li>
<li><strong>dataloader_idx</strong> (<code>int</code>): the task ID of seen tasks to be tested. A default value of 0 is given otherwise the LightningModule will raise a <code>RuntimeError</code>.</li>
</ul>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and other metrics from this test step. Keys (<code>str</code>) are the metrics names, and values (<code>Tensor</code>) are the metrics.</li>
</ul>
</div>


                            </div>
                </section>
    </main>
<script>
    function escapeHTML(html) {
        return document.createElement('div').appendChild(document.createTextNode(html)).parentNode.innerHTML;
    }

    const originalContent = document.querySelector("main.pdoc");
    let currentContent = originalContent;

    function setContent(innerHTML) {
        let elem;
        if (innerHTML) {
            elem = document.createElement("main");
            elem.classList.add("pdoc");
            elem.innerHTML = innerHTML;
        } else {
            elem = originalContent;
        }
        if (currentContent !== elem) {
            currentContent.replaceWith(elem);
            currentContent = elem;
        }
    }

    function getSearchTerm() {
        return (new URL(window.location)).searchParams.get("search");
    }

    const searchBox = document.querySelector(".pdoc input[type=search]");
    searchBox.addEventListener("input", function () {
        let url = new URL(window.location);
        if (searchBox.value.trim()) {
            url.hash = "";
            url.searchParams.set("search", searchBox.value);
        } else {
            url.searchParams.delete("search");
        }
        history.replaceState("", "", url.toString());
        onInput();
    });
    window.addEventListener("popstate", onInput);


    let search, searchErr;

    async function initialize() {
        try {
            search = await new Promise((resolve, reject) => {
                const script = document.createElement("script");
                script.type = "text/javascript";
                script.async = true;
                script.onload = () => resolve(window.pdocSearch);
                script.onerror = (e) => reject(e);
                script.src = "../../search.js";
                document.getElementsByTagName("head")[0].appendChild(script);
            });
        } catch (e) {
            console.error("Cannot fetch pdoc search index");
            searchErr = "Cannot fetch search index.";
        }
        onInput();

        document.querySelector("nav.pdoc").addEventListener("click", e => {
            if (e.target.hash) {
                searchBox.value = "";
                searchBox.dispatchEvent(new Event("input"));
            }
        });
    }

    function onInput() {
        setContent((() => {
            const term = getSearchTerm();
            if (!term) {
                return null
            }
            if (searchErr) {
                return `<h3>Error: ${searchErr}</h3>`
            }
            if (!search) {
                return "<h3>Searching...</h3>"
            }

            window.scrollTo({top: 0, left: 0, behavior: 'auto'});

            const results = search(term);

            let html;
            if (results.length === 0) {
                html = `No search results for '${escapeHTML(term)}'.`
            } else {
                html = `<h4>${results.length} search result${results.length > 1 ? "s" : ""} for '${escapeHTML(term)}'.</h4>`;
            }
            for (let result of results.slice(0, 10)) {
                let doc = result.doc;
                let url = `../../${doc.modulename.replaceAll(".", "/")}.html`;
                if (doc.qualname) {
                    url += `#${doc.qualname}`;
                }

                let heading;
                switch (result.doc.kind) {
                    case "function":
                        if (doc.fullname.endsWith(".__init__")) {
                            heading = `<span class="name">${doc.fullname.replace(/\.__init__$/, "")}</span>${doc.signature}`;
                        } else {
                            heading = `<span class="def">${doc.funcdef}</span> <span class="name">${doc.fullname}</span>${doc.signature}`;
                        }
                        break;
                    case "class":
                        heading = `<span class="def">class</span> <span class="name">${doc.fullname}</span>`;
                        if (doc.bases)
                            heading += `<wbr>(<span class="base">${doc.bases}</span>)`;
                        heading += `:`;
                        break;
                    case "variable":
                        heading = `<span class="name">${doc.fullname}</span>`;
                        if (doc.annotation)
                            heading += `<span class="annotation">${doc.annotation}</span>`;
                        if (doc.default_value)
                            heading += `<span class="default_value"> = ${doc.default_value}</span>`;
                        break;
                    default:
                        heading = `<span class="name">${doc.fullname}</span>`;
                        break;
                }
                html += `
                        <section class="search-result">
                        <a href="${url}" class="attr ${doc.kind}">${heading}</a>
                        <div class="docstring">${doc.doc}</div>
                        </section>
                    `;

            }
            return html;
        })());
    }

    if (getSearchTerm()) {
        initialize();
        searchBox.value = getSearchTerm();
        onInput();
    } else {
        searchBox.addEventListener("focus", initialize, {once: true});
    }

    searchBox.addEventListener("keydown", e => {
        if (["ArrowDown", "ArrowUp", "Enter"].includes(e.key)) {
            let focused = currentContent.querySelector(".search-result.focused");
            if (!focused) {
                currentContent.querySelector(".search-result").classList.add("focused");
            } else if (
                e.key === "ArrowDown"
                && focused.nextElementSibling
                && focused.nextElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.nextElementSibling.classList.add("focused");
                focused.nextElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "ArrowUp"
                && focused.previousElementSibling
                && focused.previousElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.previousElementSibling.classList.add("focused");
                focused.previousElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "Enter"
            ) {
                focused.querySelector("a").click();
            }
        }
    });
</script></body>
</html>