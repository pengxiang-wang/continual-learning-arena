<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="pdoc 15.0.1"/>
    <title>clarena.cl_algorithms.hat API documentation</title>

    <style>/*! * Bootstrap Reboot v5.0.0 (https://getbootstrap.com/) * Copyright 2011-2021 The Bootstrap Authors * Copyright 2011-2021 Twitter, Inc. * Licensed under MIT (https://github.com/twbs/bootstrap/blob/main/LICENSE) * Forked from Normalize.css, licensed MIT (https://github.com/necolas/normalize.css/blob/master/LICENSE.md) */*,::after,::before{box-sizing:border-box}@media (prefers-reduced-motion:no-preference){:root{scroll-behavior:smooth}}body{margin:0;font-family:system-ui,-apple-system,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans","Liberation Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";font-size:1rem;font-weight:400;line-height:1.5;color:#212529;background-color:#fff;-webkit-text-size-adjust:100%;-webkit-tap-highlight-color:transparent}hr{margin:1rem 0;color:inherit;background-color:currentColor;border:0;opacity:.25}hr:not([size]){height:1px}h1,h2,h3,h4,h5,h6{margin-top:0;margin-bottom:.5rem;font-weight:500;line-height:1.2}h1{font-size:calc(1.375rem + 1.5vw)}@media (min-width:1200px){h1{font-size:2.5rem}}h2{font-size:calc(1.325rem + .9vw)}@media (min-width:1200px){h2{font-size:2rem}}h3{font-size:calc(1.3rem + .6vw)}@media (min-width:1200px){h3{font-size:1.75rem}}h4{font-size:calc(1.275rem + .3vw)}@media (min-width:1200px){h4{font-size:1.5rem}}h5{font-size:1.25rem}h6{font-size:1rem}p{margin-top:0;margin-bottom:1rem}abbr[data-bs-original-title],abbr[title]{-webkit-text-decoration:underline dotted;text-decoration:underline dotted;cursor:help;-webkit-text-decoration-skip-ink:none;text-decoration-skip-ink:none}address{margin-bottom:1rem;font-style:normal;line-height:inherit}ol,ul{padding-left:2rem}dl,ol,ul{margin-top:0;margin-bottom:1rem}ol ol,ol ul,ul ol,ul ul{margin-bottom:0}dt{font-weight:700}dd{margin-bottom:.5rem;margin-left:0}blockquote{margin:0 0 1rem}b,strong{font-weight:bolder}small{font-size:.875em}mark{padding:.2em;background-color:#fcf8e3}sub,sup{position:relative;font-size:.75em;line-height:0;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}a{color:#0d6efd;text-decoration:underline}a:hover{color:#0a58ca}a:not([href]):not([class]),a:not([href]):not([class]):hover{color:inherit;text-decoration:none}code,kbd,pre,samp{font-family:SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;font-size:1em;direction:ltr;unicode-bidi:bidi-override}pre{display:block;margin-top:0;margin-bottom:1rem;overflow:auto;font-size:.875em}pre code{font-size:inherit;color:inherit;word-break:normal}code{font-size:.875em;color:#d63384;word-wrap:break-word}a>code{color:inherit}kbd{padding:.2rem .4rem;font-size:.875em;color:#fff;background-color:#212529;border-radius:.2rem}kbd kbd{padding:0;font-size:1em;font-weight:700}figure{margin:0 0 1rem}img,svg{vertical-align:middle}table{caption-side:bottom;border-collapse:collapse}caption{padding-top:.5rem;padding-bottom:.5rem;color:#6c757d;text-align:left}th{text-align:inherit;text-align:-webkit-match-parent}tbody,td,tfoot,th,thead,tr{border-color:inherit;border-style:solid;border-width:0}label{display:inline-block}button{border-radius:0}button:focus:not(:focus-visible){outline:0}button,input,optgroup,select,textarea{margin:0;font-family:inherit;font-size:inherit;line-height:inherit}button,select{text-transform:none}[role=button]{cursor:pointer}select{word-wrap:normal}select:disabled{opacity:1}[list]::-webkit-calendar-picker-indicator{display:none}[type=button],[type=reset],[type=submit],button{-webkit-appearance:button}[type=button]:not(:disabled),[type=reset]:not(:disabled),[type=submit]:not(:disabled),button:not(:disabled){cursor:pointer}::-moz-focus-inner{padding:0;border-style:none}textarea{resize:vertical}fieldset{min-width:0;padding:0;margin:0;border:0}legend{float:left;width:100%;padding:0;margin-bottom:.5rem;font-size:calc(1.275rem + .3vw);line-height:inherit}@media (min-width:1200px){legend{font-size:1.5rem}}legend+*{clear:left}::-webkit-datetime-edit-day-field,::-webkit-datetime-edit-fields-wrapper,::-webkit-datetime-edit-hour-field,::-webkit-datetime-edit-minute,::-webkit-datetime-edit-month-field,::-webkit-datetime-edit-text,::-webkit-datetime-edit-year-field{padding:0}::-webkit-inner-spin-button{height:auto}[type=search]{outline-offset:-2px;-webkit-appearance:textfield}::-webkit-search-decoration{-webkit-appearance:none}::-webkit-color-swatch-wrapper{padding:0}::file-selector-button{font:inherit}::-webkit-file-upload-button{font:inherit;-webkit-appearance:button}output{display:inline-block}iframe{border:0}summary{display:list-item;cursor:pointer}progress{vertical-align:baseline}[hidden]{display:none!important}</style>
    <style>/*! syntax-highlighting.css */pre{line-height:125%;}span.linenos{color:inherit; background-color:transparent; padding-left:5px; padding-right:20px;}.pdoc-code .hll{background-color:#ffffcc}.pdoc-code{background:#f8f8f8;}.pdoc-code .c{color:#3D7B7B; font-style:italic}.pdoc-code .err{border:1px solid #FF0000}.pdoc-code .k{color:#008000; font-weight:bold}.pdoc-code .o{color:#666666}.pdoc-code .ch{color:#3D7B7B; font-style:italic}.pdoc-code .cm{color:#3D7B7B; font-style:italic}.pdoc-code .cp{color:#9C6500}.pdoc-code .cpf{color:#3D7B7B; font-style:italic}.pdoc-code .c1{color:#3D7B7B; font-style:italic}.pdoc-code .cs{color:#3D7B7B; font-style:italic}.pdoc-code .gd{color:#A00000}.pdoc-code .ge{font-style:italic}.pdoc-code .gr{color:#E40000}.pdoc-code .gh{color:#000080; font-weight:bold}.pdoc-code .gi{color:#008400}.pdoc-code .go{color:#717171}.pdoc-code .gp{color:#000080; font-weight:bold}.pdoc-code .gs{font-weight:bold}.pdoc-code .gu{color:#800080; font-weight:bold}.pdoc-code .gt{color:#0044DD}.pdoc-code .kc{color:#008000; font-weight:bold}.pdoc-code .kd{color:#008000; font-weight:bold}.pdoc-code .kn{color:#008000; font-weight:bold}.pdoc-code .kp{color:#008000}.pdoc-code .kr{color:#008000; font-weight:bold}.pdoc-code .kt{color:#B00040}.pdoc-code .m{color:#666666}.pdoc-code .s{color:#BA2121}.pdoc-code .na{color:#687822}.pdoc-code .nb{color:#008000}.pdoc-code .nc{color:#0000FF; font-weight:bold}.pdoc-code .no{color:#880000}.pdoc-code .nd{color:#AA22FF}.pdoc-code .ni{color:#717171; font-weight:bold}.pdoc-code .ne{color:#CB3F38; font-weight:bold}.pdoc-code .nf{color:#0000FF}.pdoc-code .nl{color:#767600}.pdoc-code .nn{color:#0000FF; font-weight:bold}.pdoc-code .nt{color:#008000; font-weight:bold}.pdoc-code .nv{color:#19177C}.pdoc-code .ow{color:#AA22FF; font-weight:bold}.pdoc-code .w{color:#bbbbbb}.pdoc-code .mb{color:#666666}.pdoc-code .mf{color:#666666}.pdoc-code .mh{color:#666666}.pdoc-code .mi{color:#666666}.pdoc-code .mo{color:#666666}.pdoc-code .sa{color:#BA2121}.pdoc-code .sb{color:#BA2121}.pdoc-code .sc{color:#BA2121}.pdoc-code .dl{color:#BA2121}.pdoc-code .sd{color:#BA2121; font-style:italic}.pdoc-code .s2{color:#BA2121}.pdoc-code .se{color:#AA5D1F; font-weight:bold}.pdoc-code .sh{color:#BA2121}.pdoc-code .si{color:#A45A77; font-weight:bold}.pdoc-code .sx{color:#008000}.pdoc-code .sr{color:#A45A77}.pdoc-code .s1{color:#BA2121}.pdoc-code .ss{color:#19177C}.pdoc-code .bp{color:#008000}.pdoc-code .fm{color:#0000FF}.pdoc-code .vc{color:#19177C}.pdoc-code .vg{color:#19177C}.pdoc-code .vi{color:#19177C}.pdoc-code .vm{color:#19177C}.pdoc-code .il{color:#666666}</style>
    <style>/*! theme.css */:root{--pdoc-background:#fff;}.pdoc{--text:#212529;--muted:#6c757d;--link:#3660a5;--link-hover:#1659c5;--code:#f8f8f8;--active:#fff598;--accent:#eee;--accent2:#c1c1c1;--nav-hover:rgba(255, 255, 255, 0.5);--name:#0066BB;--def:#008800;--annotation:#007020;}</style>
    <style>/*! layout.css */html, body{width:100%;height:100%;}html, main{scroll-behavior:smooth;}body{background-color:var(--pdoc-background);}@media (max-width:769px){#navtoggle{cursor:pointer;position:absolute;width:50px;height:40px;top:1rem;right:1rem;border-color:var(--text);color:var(--text);display:flex;opacity:0.8;z-index:999;}#navtoggle:hover{opacity:1;}#togglestate + div{display:none;}#togglestate:checked + div{display:inherit;}main, header{padding:2rem 3vw;}header + main{margin-top:-3rem;}.git-button{display:none !important;}nav input[type="search"]{max-width:77%;}nav input[type="search"]:first-child{margin-top:-6px;}nav input[type="search"]:valid ~ *{display:none !important;}}@media (min-width:770px){:root{--sidebar-width:clamp(12.5rem, 28vw, 22rem);}nav{position:fixed;overflow:auto;height:100vh;width:var(--sidebar-width);}main, header{padding:3rem 2rem 3rem calc(var(--sidebar-width) + 3rem);width:calc(54rem + var(--sidebar-width));max-width:100%;}header + main{margin-top:-4rem;}#navtoggle{display:none;}}#togglestate{position:absolute;height:0;opacity:0;}nav.pdoc{--pad:clamp(0.5rem, 2vw, 1.75rem);--indent:1.5rem;background-color:var(--accent);border-right:1px solid var(--accent2);box-shadow:0 0 20px rgba(50, 50, 50, .2) inset;padding:0 0 0 var(--pad);overflow-wrap:anywhere;scrollbar-width:thin; scrollbar-color:var(--accent2) transparent; z-index:1}nav.pdoc::-webkit-scrollbar{width:.4rem; }nav.pdoc::-webkit-scrollbar-thumb{background-color:var(--accent2); }nav.pdoc > div{padding:var(--pad) 0;}nav.pdoc .module-list-button{display:inline-flex;align-items:center;color:var(--text);border-color:var(--muted);margin-bottom:1rem;}nav.pdoc .module-list-button:hover{border-color:var(--text);}nav.pdoc input[type=search]{display:block;outline-offset:0;width:calc(100% - var(--pad));}nav.pdoc .logo{max-width:calc(100% - var(--pad));max-height:35vh;display:block;margin:0 auto 1rem;transform:translate(calc(-.5 * var(--pad)), 0);}nav.pdoc ul{list-style:none;padding-left:0;}nav.pdoc > div > ul{margin-left:calc(0px - var(--pad));}nav.pdoc li a{padding:.2rem 0 .2rem calc(var(--pad) + var(--indent));}nav.pdoc > div > ul > li > a{padding-left:var(--pad);}nav.pdoc li{transition:all 100ms;}nav.pdoc li:hover{background-color:var(--nav-hover);}nav.pdoc a, nav.pdoc a:hover{color:var(--text);}nav.pdoc a{display:block;}nav.pdoc > h2:first-of-type{margin-top:1.5rem;}nav.pdoc .class:before{content:"class ";color:var(--muted);}nav.pdoc .function:after{content:"()";color:var(--muted);}nav.pdoc footer:before{content:"";display:block;width:calc(100% - var(--pad));border-top:solid var(--accent2) 1px;margin-top:1.5rem;padding-top:.5rem;}nav.pdoc footer{font-size:small;}</style>
    <style>/*! content.css */.pdoc{color:var(--text);box-sizing:border-box;line-height:1.5;background:none;}.pdoc .pdoc-button{cursor:pointer;display:inline-block;border:solid black 1px;border-radius:2px;font-size:.75rem;padding:calc(0.5em - 1px) 1em;transition:100ms all;}.pdoc .alert{padding:1rem 1rem 1rem calc(1.5rem + 24px);border:1px solid transparent;border-radius:.25rem;background-repeat:no-repeat;background-position:.75rem center;margin-bottom:1rem;}.pdoc .alert > em{display:none;}.pdoc .alert > *:last-child{margin-bottom:0;}.pdoc .alert.note {color:#084298;background-color:#cfe2ff;border-color:#b6d4fe;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23084298%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8%2016A8%208%200%201%200%208%200a8%208%200%200%200%200%2016zm.93-9.412-1%204.705c-.07.34.029.533.304.533.194%200%20.487-.07.686-.246l-.088.416c-.287.346-.92.598-1.465.598-.703%200-1.002-.422-.808-1.319l.738-3.468c.064-.293.006-.399-.287-.47l-.451-.081.082-.381%202.29-.287zM8%205.5a1%201%200%201%201%200-2%201%201%200%200%201%200%202z%22/%3E%3C/svg%3E");}.pdoc .alert.warning{color:#664d03;background-color:#fff3cd;border-color:#ffecb5;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23664d03%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8.982%201.566a1.13%201.13%200%200%200-1.96%200L.165%2013.233c-.457.778.091%201.767.98%201.767h13.713c.889%200%201.438-.99.98-1.767L8.982%201.566zM8%205c.535%200%20.954.462.9.995l-.35%203.507a.552.552%200%200%201-1.1%200L7.1%205.995A.905.905%200%200%201%208%205zm.002%206a1%201%200%201%201%200%202%201%201%200%200%201%200-2z%22/%3E%3C/svg%3E");}.pdoc .alert.danger{color:#842029;background-color:#f8d7da;border-color:#f5c2c7;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23842029%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M5.52.359A.5.5%200%200%201%206%200h4a.5.5%200%200%201%20.474.658L8.694%206H12.5a.5.5%200%200%201%20.395.807l-7%209a.5.5%200%200%201-.873-.454L6.823%209.5H3.5a.5.5%200%200%201-.48-.641l2.5-8.5z%22/%3E%3C/svg%3E");}.pdoc .visually-hidden{position:absolute !important;width:1px !important;height:1px !important;padding:0 !important;margin:-1px !important;overflow:hidden !important;clip:rect(0, 0, 0, 0) !important;white-space:nowrap !important;border:0 !important;}.pdoc h1, .pdoc h2, .pdoc h3{font-weight:300;margin:.3em 0;padding:.2em 0;}.pdoc > section:not(.module-info) h1{font-size:1.5rem;font-weight:500;}.pdoc > section:not(.module-info) h2{font-size:1.4rem;font-weight:500;}.pdoc > section:not(.module-info) h3{font-size:1.3rem;font-weight:500;}.pdoc > section:not(.module-info) h4{font-size:1.2rem;}.pdoc > section:not(.module-info) h5{font-size:1.1rem;}.pdoc a{text-decoration:none;color:var(--link);}.pdoc a:hover{color:var(--link-hover);}.pdoc blockquote{margin-left:2rem;}.pdoc pre{border-top:1px solid var(--accent2);border-bottom:1px solid var(--accent2);margin-top:0;margin-bottom:1em;padding:.5rem 0 .5rem .5rem;overflow-x:auto;background-color:var(--code);}.pdoc code{color:var(--text);padding:.2em .4em;margin:0;font-size:85%;background-color:var(--accent);border-radius:6px;}.pdoc a > code{color:inherit;}.pdoc pre > code{display:inline-block;font-size:inherit;background:none;border:none;padding:0;}.pdoc > section:not(.module-info){margin-bottom:1.5rem;}.pdoc .modulename{margin-top:0;font-weight:bold;}.pdoc .modulename a{color:var(--link);transition:100ms all;}.pdoc .git-button{float:right;border:solid var(--link) 1px;}.pdoc .git-button:hover{background-color:var(--link);color:var(--pdoc-background);}.view-source-toggle-state,.view-source-toggle-state ~ .pdoc-code{display:none;}.view-source-toggle-state:checked ~ .pdoc-code{display:block;}.view-source-button{display:inline-block;float:right;font-size:.75rem;line-height:1.5rem;color:var(--muted);padding:0 .4rem 0 1.3rem;cursor:pointer;text-indent:-2px;}.view-source-button > span{visibility:hidden;}.module-info .view-source-button{float:none;display:flex;justify-content:flex-end;margin:-1.2rem .4rem -.2rem 0;}.view-source-button::before{position:absolute;content:"View Source";display:list-item;list-style-type:disclosure-closed;}.view-source-toggle-state:checked ~ .attr .view-source-button::before,.view-source-toggle-state:checked ~ .view-source-button::before{list-style-type:disclosure-open;}.pdoc .docstring{margin-bottom:1.5rem;}.pdoc section:not(.module-info) .docstring{margin-left:clamp(0rem, 5vw - 2rem, 1rem);}.pdoc .docstring .pdoc-code{margin-left:1em;margin-right:1em;}.pdoc h1:target,.pdoc h2:target,.pdoc h3:target,.pdoc h4:target,.pdoc h5:target,.pdoc h6:target,.pdoc .pdoc-code > pre > span:target{background-color:var(--active);box-shadow:-1rem 0 0 0 var(--active);}.pdoc .pdoc-code > pre > span:target{display:block;}.pdoc div:target > .attr,.pdoc section:target > .attr,.pdoc dd:target > a{background-color:var(--active);}.pdoc *{scroll-margin:2rem;}.pdoc .pdoc-code .linenos{user-select:none;}.pdoc .attr:hover{filter:contrast(0.95);}.pdoc section, .pdoc .classattr{position:relative;}.pdoc .headerlink{--width:clamp(1rem, 3vw, 2rem);position:absolute;top:0;left:calc(0rem - var(--width));transition:all 100ms ease-in-out;opacity:0;}.pdoc .headerlink::before{content:"#";display:block;text-align:center;width:var(--width);height:2.3rem;line-height:2.3rem;font-size:1.5rem;}.pdoc .attr:hover ~ .headerlink,.pdoc *:target > .headerlink,.pdoc .headerlink:hover{opacity:1;}.pdoc .attr{display:block;margin:.5rem 0 .5rem;padding:.4rem .4rem .4rem 1rem;background-color:var(--accent);overflow-x:auto;}.pdoc .classattr{margin-left:2rem;}.pdoc .decorator-deprecated{color:#842029;}.pdoc .decorator-deprecated ~ span{filter:grayscale(1) opacity(0.8);}.pdoc .name{color:var(--name);font-weight:bold;}.pdoc .def{color:var(--def);font-weight:bold;}.pdoc .signature{background-color:transparent;}.pdoc .param, .pdoc .return-annotation{white-space:pre;}.pdoc .signature.multiline .param{display:block;}.pdoc .signature.condensed .param{display:inline-block;}.pdoc .annotation{color:var(--annotation);}.pdoc .view-value-toggle-state,.pdoc .view-value-toggle-state ~ .default_value{display:none;}.pdoc .view-value-toggle-state:checked ~ .default_value{display:inherit;}.pdoc .view-value-button{font-size:.5rem;vertical-align:middle;border-style:dashed;margin-top:-0.1rem;}.pdoc .view-value-button:hover{background:white;}.pdoc .view-value-button::before{content:"show";text-align:center;width:2.2em;display:inline-block;}.pdoc .view-value-toggle-state:checked ~ .view-value-button::before{content:"hide";}.pdoc .inherited{margin-left:2rem;}.pdoc .inherited dt{font-weight:700;}.pdoc .inherited dt, .pdoc .inherited dd{display:inline;margin-left:0;margin-bottom:.5rem;}.pdoc .inherited dd:not(:last-child):after{content:", ";}.pdoc .inherited .class:before{content:"class ";}.pdoc .inherited .function a:after{content:"()";}.pdoc .search-result .docstring{overflow:auto;max-height:25vh;}.pdoc .search-result.focused > .attr{background-color:var(--active);}.pdoc .attribution{margin-top:2rem;display:block;opacity:0.5;transition:all 200ms;filter:grayscale(100%);}.pdoc .attribution:hover{opacity:1;filter:grayscale(0%);}.pdoc .attribution img{margin-left:5px;height:35px;vertical-align:middle;width:70px;transition:all 200ms;}.pdoc table{display:block;width:max-content;max-width:100%;overflow:auto;margin-bottom:1rem;}.pdoc table th{font-weight:600;}.pdoc table th, .pdoc table td{padding:6px 13px;border:1px solid var(--accent2);}</style>
    <style>/*! custom.css */</style></head>
<body>
    <nav class="pdoc">
        <label id="navtoggle" for="togglestate" class="pdoc-button"><svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 30 30'><path stroke-linecap='round' stroke="currentColor" stroke-miterlimit='10' stroke-width='2' d='M4 7h22M4 15h22M4 23h22'/></svg></label>
        <input id="togglestate" type="checkbox" aria-hidden="true" tabindex="-1">
        <div>            <a class="pdoc-button module-list-button" href="../cl_algorithms.html">
<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-box-arrow-in-left" viewBox="0 0 16 16">
  <path fill-rule="evenodd" d="M10 3.5a.5.5 0 0 0-.5-.5h-8a.5.5 0 0 0-.5.5v9a.5.5 0 0 0 .5.5h8a.5.5 0 0 0 .5-.5v-2a.5.5 0 0 1 1 0v2A1.5 1.5 0 0 1 9.5 14h-8A1.5 1.5 0 0 1 0 12.5v-9A1.5 1.5 0 0 1 1.5 2h8A1.5 1.5 0 0 1 11 3.5v2a.5.5 0 0 1-1 0v-2z"/>
  <path fill-rule="evenodd" d="M4.146 8.354a.5.5 0 0 1 0-.708l3-3a.5.5 0 1 1 .708.708L5.707 7.5H14.5a.5.5 0 0 1 0 1H5.707l2.147 2.146a.5.5 0 0 1-.708.708l-3-3z"/>
</svg>                &nbsp;clarena.cl_algorithms</a>


            <input type="search" placeholder="Search..." role="searchbox" aria-label="search"
                   pattern=".+" required>



            <h2>API Documentation</h2>
                <ul class="memberlist">
            <li>
                    <a class="class" href="#HAT">HAT</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#HAT.__init__">HAT</a>
                        </li>
                        <li>
                                <a class="variable" href="#HAT.adjustment_mode">adjustment_mode</a>
                        </li>
                        <li>
                                <a class="variable" href="#HAT.s_max">s_max</a>
                        </li>
                        <li>
                                <a class="variable" href="#HAT.clamp_threshold">clamp_threshold</a>
                        </li>
                        <li>
                                <a class="variable" href="#HAT.mask_sparsity_reg_factor">mask_sparsity_reg_factor</a>
                        </li>
                        <li>
                                <a class="variable" href="#HAT.mask_sparsity_reg_mode">mask_sparsity_reg_mode</a>
                        </li>
                        <li>
                                <a class="variable" href="#HAT.mark_sparsity_reg">mark_sparsity_reg</a>
                        </li>
                        <li>
                                <a class="variable" href="#HAT.task_embedding_init_mode">task_embedding_init_mode</a>
                        </li>
                        <li>
                                <a class="variable" href="#HAT.alpha">alpha</a>
                        </li>
                        <li>
                                <a class="variable" href="#HAT.epsilon">epsilon</a>
                        </li>
                        <li>
                                <a class="variable" href="#HAT.masks">masks</a>
                        </li>
                        <li>
                                <a class="variable" href="#HAT.cumulative_mask_for_previous_tasks">cumulative_mask_for_previous_tasks</a>
                        </li>
                        <li>
                                <a class="variable" href="#HAT.automatic_optimization">automatic_optimization</a>
                        </li>
                        <li>
                                <a class="function" href="#HAT.sanity_check">sanity_check</a>
                        </li>
                        <li>
                                <a class="function" href="#HAT.on_train_start">on_train_start</a>
                        </li>
                        <li>
                                <a class="function" href="#HAT.clip_grad_by_adjustment">clip_grad_by_adjustment</a>
                        </li>
                        <li>
                                <a class="function" href="#HAT.compensate_task_embedding_gradients">compensate_task_embedding_gradients</a>
                        </li>
                        <li>
                                <a class="function" href="#HAT.forward">forward</a>
                        </li>
                        <li>
                                <a class="function" href="#HAT.training_step">training_step</a>
                        </li>
                        <li>
                                <a class="function" href="#HAT.on_train_end">on_train_end</a>
                        </li>
                        <li>
                                <a class="function" href="#HAT.validation_step">validation_step</a>
                        </li>
                        <li>
                                <a class="function" href="#HAT.test_step">test_step</a>
                        </li>
                </ul>

            </li>
    </ul>



        <a class="attribution" title="pdoc: Python API documentation generator" href="https://pdoc.dev" target="_blank">
            built with <span class="visually-hidden">pdoc</span><img
                alt="pdoc logo"
                src="data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20role%3D%22img%22%20aria-label%3D%22pdoc%20logo%22%20width%3D%22300%22%20height%3D%22150%22%20viewBox%3D%22-1%200%2060%2030%22%3E%3Ctitle%3Epdoc%3C/title%3E%3Cpath%20d%3D%22M29.621%2021.293c-.011-.273-.214-.475-.511-.481a.5.5%200%200%200-.489.503l-.044%201.393c-.097.551-.695%201.215-1.566%201.704-.577.428-1.306.486-2.193.182-1.426-.617-2.467-1.654-3.304-2.487l-.173-.172a3.43%203.43%200%200%200-.365-.306.49.49%200%200%200-.286-.196c-1.718-1.06-4.931-1.47-7.353.191l-.219.15c-1.707%201.187-3.413%202.131-4.328%201.03-.02-.027-.49-.685-.141-1.763.233-.721.546-2.408.772-4.076.042-.09.067-.187.046-.288.166-1.347.277-2.625.241-3.351%201.378-1.008%202.271-2.586%202.271-4.362%200-.976-.272-1.935-.788-2.774-.057-.094-.122-.18-.184-.268.033-.167.052-.339.052-.516%200-1.477-1.202-2.679-2.679-2.679-.791%200-1.496.352-1.987.9a6.3%206.3%200%200%200-1.001.029c-.492-.564-1.207-.929-2.012-.929-1.477%200-2.679%201.202-2.679%202.679A2.65%202.65%200%200%200%20.97%206.554c-.383.747-.595%201.572-.595%202.41%200%202.311%201.507%204.29%203.635%205.107-.037.699-.147%202.27-.423%203.294l-.137.461c-.622%202.042-2.515%208.257%201.727%2010.643%201.614.908%203.06%201.248%204.317%201.248%202.665%200%204.492-1.524%205.322-2.401%201.476-1.559%202.886-1.854%206.491.82%201.877%201.393%203.514%201.753%204.861%201.068%202.223-1.713%202.811-3.867%203.399-6.374.077-.846.056-1.469.054-1.537zm-4.835%204.313c-.054.305-.156.586-.242.629-.034-.007-.131-.022-.307-.157-.145-.111-.314-.478-.456-.908.221.121.432.25.675.355.115.039.219.051.33.081zm-2.251-1.238c-.05.33-.158.648-.252.694-.022.001-.125-.018-.307-.157-.217-.166-.488-.906-.639-1.573.358.344.754.693%201.198%201.036zm-3.887-2.337c-.006-.116-.018-.231-.041-.342.635.145%201.189.368%201.599.625.097.231.166.481.174.642-.03.049-.055.101-.067.158-.046.013-.128.026-.298.004-.278-.037-.901-.57-1.367-1.087zm-1.127-.497c.116.306.176.625.12.71-.019.014-.117.045-.345.016-.206-.027-.604-.332-.986-.695.41-.051.816-.056%201.211-.031zm-4.535%201.535c.209.22.379.47.358.598-.006.041-.088.138-.351.234-.144.055-.539-.063-.979-.259a11.66%2011.66%200%200%200%20.972-.573zm.983-.664c.359-.237.738-.418%201.126-.554.25.237.479.548.457.694-.006.042-.087.138-.351.235-.174.064-.694-.105-1.232-.375zm-3.381%201.794c-.022.145-.061.29-.149.401-.133.166-.358.248-.69.251h-.002c-.133%200-.306-.26-.45-.621.417.091.854.07%201.291-.031zm-2.066-8.077a4.78%204.78%200%200%201-.775-.584c.172-.115.505-.254.88-.378l-.105.962zm-.331%202.302a10.32%2010.32%200%200%201-.828-.502c.202-.143.576-.328.984-.49l-.156.992zm-.45%202.157l-.701-.403c.214-.115.536-.249.891-.376a11.57%2011.57%200%200%201-.19.779zm-.181%201.716c.064.398.194.702.298.893-.194-.051-.435-.162-.736-.398.061-.119.224-.3.438-.495zM8.87%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zm-.735-.389a1.15%201.15%200%200%200-.314.783%201.16%201.16%200%200%200%201.162%201.162c.457%200%20.842-.27%201.032-.653.026.117.042.238.042.362a1.68%201.68%200%200%201-1.679%201.679%201.68%201.68%200%200%201-1.679-1.679c0-.843.626-1.535%201.436-1.654zM5.059%205.406A1.68%201.68%200%200%201%203.38%207.085a1.68%201.68%200%200%201-1.679-1.679c0-.037.009-.072.011-.109.21.3.541.508.935.508a1.16%201.16%200%200%200%201.162-1.162%201.14%201.14%200%200%200-.474-.912c.015%200%20.03-.005.045-.005.926.001%201.679.754%201.679%201.68zM3.198%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zM1.375%208.964c0-.52.103-1.035.288-1.52.466.394%201.06.64%201.717.64%201.144%200%202.116-.725%202.499-1.738.383%201.012%201.355%201.738%202.499%201.738.867%200%201.631-.421%202.121-1.062.307.605.478%201.267.478%201.942%200%202.486-2.153%204.51-4.801%204.51s-4.801-2.023-4.801-4.51zm24.342%2019.349c-.985.498-2.267.168-3.813-.979-3.073-2.281-5.453-3.199-7.813-.705-1.315%201.391-4.163%203.365-8.423.97-3.174-1.786-2.239-6.266-1.261-9.479l.146-.492c.276-1.02.395-2.457.444-3.268a6.11%206.11%200%200%200%201.18.115%206.01%206.01%200%200%200%202.536-.562l-.006.175c-.802.215-1.848.612-2.021%201.25-.079.295.021.601.274.837.219.203.415.364.598.501-.667.304-1.243.698-1.311%201.179-.02.144-.022.507.393.787.213.144.395.26.564.365-1.285.521-1.361.96-1.381%201.126-.018.142-.011.496.427.746l.854.489c-.473.389-.971.914-.999%201.429-.018.278.095.532.316.713.675.556%201.231.721%201.653.721.059%200%20.104-.014.158-.02.207.707.641%201.64%201.513%201.64h.013c.8-.008%201.236-.345%201.462-.626.173-.216.268-.457.325-.692.424.195.93.374%201.372.374.151%200%20.294-.021.423-.068.732-.27.944-.704.993-1.021.009-.061.003-.119.002-.179.266.086.538.147.789.147.15%200%20.294-.021.423-.069.542-.2.797-.489.914-.754.237.147.478.258.704.288.106.014.205.021.296.021.356%200%20.595-.101.767-.229.438.435%201.094.992%201.656%201.067.106.014.205.021.296.021a1.56%201.56%200%200%200%20.323-.035c.17.575.453%201.289.866%201.605.358.273.665.362.914.362a.99.99%200%200%200%20.421-.093%201.03%201.03%200%200%200%20.245-.164c.168.428.39.846.68%201.068.358.273.665.362.913.362a.99.99%200%200%200%20.421-.093c.317-.148.512-.448.639-.762.251.157.495.257.726.257.127%200%20.25-.024.37-.071.427-.17.706-.617.841-1.314.022-.015.047-.022.068-.038.067-.051.133-.104.196-.159-.443%201.486-1.107%202.761-2.086%203.257zM8.66%209.925a.5.5%200%201%200-1%200c0%20.653-.818%201.205-1.787%201.205s-1.787-.552-1.787-1.205a.5.5%200%201%200-1%200c0%201.216%201.25%202.205%202.787%202.205s2.787-.989%202.787-2.205zm4.4%2015.965l-.208.097c-2.661%201.258-4.708%201.436-6.086.527-1.542-1.017-1.88-3.19-1.844-4.198a.4.4%200%200%200-.385-.414c-.242-.029-.406.164-.414.385-.046%201.249.367%203.686%202.202%204.896.708.467%201.547.7%202.51.7%201.248%200%202.706-.392%204.362-1.174l.185-.086a.4.4%200%200%200%20.205-.527c-.089-.204-.326-.291-.527-.206zM9.547%202.292c.093.077.205.114.317.114a.5.5%200%200%200%20.318-.886L8.817.397a.5.5%200%200%200-.703.068.5.5%200%200%200%20.069.703l1.364%201.124zm-7.661-.065c.086%200%20.173-.022.253-.068l1.523-.893a.5.5%200%200%200-.506-.863l-1.523.892a.5.5%200%200%200-.179.685c.094.158.261.247.432.247z%22%20transform%3D%22matrix%28-1%200%200%201%2058%200%29%22%20fill%3D%22%233bb300%22/%3E%3Cpath%20d%3D%22M.3%2021.86V10.18q0-.46.02-.68.04-.22.18-.5.28-.54%201.34-.54%201.06%200%201.42.28.38.26.44.78.76-1.04%202.38-1.04%201.64%200%203.1%201.54%201.46%201.54%201.46%203.58%200%202.04-1.46%203.58-1.44%201.54-3.08%201.54-1.64%200-2.38-.92v4.04q0%20.46-.04.68-.02.22-.18.5-.14.3-.5.42-.36.12-.98.12-.62%200-1-.12-.36-.12-.52-.4-.14-.28-.18-.5-.02-.22-.02-.68zm3.96-9.42q-.46.54-.46%201.18%200%20.64.46%201.18.48.52%201.2.52.74%200%201.24-.52.52-.52.52-1.18%200-.66-.48-1.18-.48-.54-1.26-.54-.76%200-1.22.54zm14.741-8.36q.16-.3.54-.42.38-.12%201-.12.64%200%201.02.12.38.12.52.42.16.3.18.54.04.22.04.68v11.94q0%20.46-.04.7-.02.22-.18.5-.3.54-1.7.54-1.38%200-1.54-.98-.84.96-2.34.96-1.8%200-3.28-1.56-1.48-1.58-1.48-3.66%200-2.1%201.48-3.68%201.5-1.58%203.28-1.58%201.48%200%202.3%201v-4.2q0-.46.02-.68.04-.24.18-.52zm-3.24%2010.86q.52.54%201.26.54.74%200%201.22-.54.5-.54.5-1.18%200-.66-.48-1.22-.46-.56-1.26-.56-.8%200-1.28.56-.48.54-.48%201.2%200%20.66.52%201.2zm7.833-1.2q0-2.4%201.68-3.96%201.68-1.56%203.84-1.56%202.16%200%203.82%201.56%201.66%201.54%201.66%203.94%200%201.66-.86%202.96-.86%201.28-2.1%201.9-1.22.6-2.54.6-1.32%200-2.56-.64-1.24-.66-2.1-1.92-.84-1.28-.84-2.88zm4.18%201.44q.64.48%201.3.48.66%200%201.32-.5.66-.5.66-1.48%200-.98-.62-1.46-.62-.48-1.34-.48-.72%200-1.34.5-.62.5-.62%201.48%200%20.96.64%201.46zm11.412-1.44q0%20.84.56%201.32.56.46%201.18.46.64%200%201.18-.36.56-.38.9-.38.6%200%201.46%201.06.46.58.46%201.04%200%20.76-1.1%201.42-1.14.8-2.8.8-1.86%200-3.58-1.34-.82-.64-1.34-1.7-.52-1.08-.52-2.36%200-1.3.52-2.34.52-1.06%201.34-1.7%201.66-1.32%203.54-1.32.76%200%201.48.22.72.2%201.06.4l.32.2q.36.24.56.38.52.4.52.92%200%20.5-.42%201.14-.72%201.1-1.38%201.1-.38%200-1.08-.44-.36-.34-1.04-.34-.66%200-1.24.48-.58.48-.58%201.34z%22%20fill%3D%22green%22/%3E%3C/svg%3E"/>
        </a>
</div>
    </nav>
    <main class="pdoc">
            <section class="module-info">
                    <h1 class="modulename">
<a href="./../../clarena.html">clarena</a><wbr>.<a href="./../cl_algorithms.html">cl_algorithms</a><wbr>.hat    </h1>

                        <div class="docstring"><p>The submodule in <code>cl_algorithms</code> for <a href="http://proceedings.mlr.press/v80/serra18a">HAT (Hard Attention to the Task) algorithm</a>.</p>
</div>

                        <input id="mod-hat-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">

                        <label class="view-source-button" for="mod-hat-view-source"><span>View Source</span></label>

                        <div class="pdoc-code codehilite"><pre><span></span><span id="L-1"><a href="#L-1"><span class="linenos">  1</span></a><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-2"><a href="#L-2"><span class="linenos">  2</span></a><span class="sd">The submodule in `cl_algorithms` for [HAT (Hard Attention to the Task) algorithm](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="L-3"><a href="#L-3"><span class="linenos">  3</span></a><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-4"><a href="#L-4"><span class="linenos">  4</span></a>
</span><span id="L-5"><a href="#L-5"><span class="linenos">  5</span></a><span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;HAT&quot;</span><span class="p">]</span>
</span><span id="L-6"><a href="#L-6"><span class="linenos">  6</span></a>
</span><span id="L-7"><a href="#L-7"><span class="linenos">  7</span></a><span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
</span><span id="L-8"><a href="#L-8"><span class="linenos">  8</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Any</span>
</span><span id="L-9"><a href="#L-9"><span class="linenos">  9</span></a>
</span><span id="L-10"><a href="#L-10"><span class="linenos"> 10</span></a><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span id="L-11"><a href="#L-11"><span class="linenos"> 11</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tensor</span>
</span><span id="L-12"><a href="#L-12"><span class="linenos"> 12</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">DataLoader</span>
</span><span id="L-13"><a href="#L-13"><span class="linenos"> 13</span></a>
</span><span id="L-14"><a href="#L-14"><span class="linenos"> 14</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">clarena.backbones</span><span class="w"> </span><span class="kn">import</span> <span class="n">HATMaskBackbone</span>
</span><span id="L-15"><a href="#L-15"><span class="linenos"> 15</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">clarena.cl_algorithms</span><span class="w"> </span><span class="kn">import</span> <span class="n">CLAlgorithm</span>
</span><span id="L-16"><a href="#L-16"><span class="linenos"> 16</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">clarena.cl_algorithms.regularisers</span><span class="w"> </span><span class="kn">import</span> <span class="n">HATMaskSparsityReg</span>
</span><span id="L-17"><a href="#L-17"><span class="linenos"> 17</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">clarena.cl_heads</span><span class="w"> </span><span class="kn">import</span> <span class="n">HeadsCIL</span><span class="p">,</span> <span class="n">HeadsTIL</span>
</span><span id="L-18"><a href="#L-18"><span class="linenos"> 18</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">clarena.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">HATNetworkCapacity</span>
</span><span id="L-19"><a href="#L-19"><span class="linenos"> 19</span></a>
</span><span id="L-20"><a href="#L-20"><span class="linenos"> 20</span></a><span class="c1"># always get logger for built-in logging in each module</span>
</span><span id="L-21"><a href="#L-21"><span class="linenos"> 21</span></a><span class="n">pylogger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>
</span><span id="L-22"><a href="#L-22"><span class="linenos"> 22</span></a>
</span><span id="L-23"><a href="#L-23"><span class="linenos"> 23</span></a>
</span><span id="L-24"><a href="#L-24"><span class="linenos"> 24</span></a><span class="k">class</span><span class="w"> </span><span class="nc">HAT</span><span class="p">(</span><span class="n">CLAlgorithm</span><span class="p">):</span>
</span><span id="L-25"><a href="#L-25"><span class="linenos"> 25</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;HAT (Hard Attention to the Task) algorithm.</span>
</span><span id="L-26"><a href="#L-26"><span class="linenos"> 26</span></a>
</span><span id="L-27"><a href="#L-27"><span class="linenos"> 27</span></a><span class="sd">    [HAT (Hard Attention to the Task, 2018)](http://proceedings.mlr.press/v80/serra18a) is an architecture-based continual learning approach that uses learnable hard attention masks to select the task-specific parameters.</span>
</span><span id="L-28"><a href="#L-28"><span class="linenos"> 28</span></a>
</span><span id="L-29"><a href="#L-29"><span class="linenos"> 29</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-30"><a href="#L-30"><span class="linenos"> 30</span></a>
</span><span id="L-31"><a href="#L-31"><span class="linenos"> 31</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-32"><a href="#L-32"><span class="linenos"> 32</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-33"><a href="#L-33"><span class="linenos"> 33</span></a>        <span class="n">backbone</span><span class="p">:</span> <span class="n">HATMaskBackbone</span><span class="p">,</span>
</span><span id="L-34"><a href="#L-34"><span class="linenos"> 34</span></a>        <span class="n">heads</span><span class="p">:</span> <span class="n">HeadsTIL</span> <span class="o">|</span> <span class="n">HeadsCIL</span><span class="p">,</span>
</span><span id="L-35"><a href="#L-35"><span class="linenos"> 35</span></a>        <span class="n">adjustment_mode</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-36"><a href="#L-36"><span class="linenos"> 36</span></a>        <span class="n">s_max</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="L-37"><a href="#L-37"><span class="linenos"> 37</span></a>        <span class="n">clamp_threshold</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="L-38"><a href="#L-38"><span class="linenos"> 38</span></a>        <span class="n">mask_sparsity_reg_factor</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="L-39"><a href="#L-39"><span class="linenos"> 39</span></a>        <span class="n">mask_sparsity_reg_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;original&quot;</span><span class="p">,</span>
</span><span id="L-40"><a href="#L-40"><span class="linenos"> 40</span></a>        <span class="n">task_embedding_init_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;N01&quot;</span><span class="p">,</span>
</span><span id="L-41"><a href="#L-41"><span class="linenos"> 41</span></a>        <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-42"><a href="#L-42"><span class="linenos"> 42</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-43"><a href="#L-43"><span class="linenos"> 43</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialise the HAT algorithm with the network.</span>
</span><span id="L-44"><a href="#L-44"><span class="linenos"> 44</span></a>
</span><span id="L-45"><a href="#L-45"><span class="linenos"> 45</span></a><span class="sd">        **Args:**</span>
</span><span id="L-46"><a href="#L-46"><span class="linenos"> 46</span></a><span class="sd">        - **backbone** (`HATMaskBackbone`): must be a backbone network with HAT mask mechanism.</span>
</span><span id="L-47"><a href="#L-47"><span class="linenos"> 47</span></a><span class="sd">        - **heads** (`HeadsTIL` | `HeadsCIL`): output heads.</span>
</span><span id="L-48"><a href="#L-48"><span class="linenos"> 48</span></a><span class="sd">        - **adjustment_mode** (`str`): the strategy of adjustment i.e. the mode of gradient clipping, should be one of the following:</span>
</span><span id="L-49"><a href="#L-49"><span class="linenos"> 49</span></a><span class="sd">            1. &#39;hat&#39;: set the gradients of parameters linking to masked units to zero. This is the way that HAT does, which fixes the part of network for previous tasks completely. See equation (2) in chapter 2.3 &quot;Network Training&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="L-50"><a href="#L-50"><span class="linenos"> 50</span></a><span class="sd">            2. &#39;hat_random&#39;: set the gradients of parameters linking to masked units to random 0-1 values. See the &quot;Baselines&quot; section in chapter 4.1 in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="L-51"><a href="#L-51"><span class="linenos"> 51</span></a><span class="sd">            3. &#39;hat_const_alpha&#39;: set the gradients of parameters linking to masked units to a constant value of `alpha`. See the &quot;Baselines&quot; section in chapter 4.1 in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="L-52"><a href="#L-52"><span class="linenos"> 52</span></a><span class="sd">            4. &#39;hat_const_1&#39;: set the gradients of parameters linking to masked units to a constant value of 1, which means no gradient constraint on any parameter at all. See the &quot;Baselines&quot; section in chapter 4.1 in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="L-53"><a href="#L-53"><span class="linenos"> 53</span></a><span class="sd">        - **s_max** (`float`): hyperparameter, the maximum scaling factor in the gate function. See chapter 2.4 &quot;Hard Attention Training&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="L-54"><a href="#L-54"><span class="linenos"> 54</span></a><span class="sd">        - **clamp_threshold** (`float`): the threshold for task embedding gradient compensation. See chapter 2.5 &quot;Embedding Gradient Compensation&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="L-55"><a href="#L-55"><span class="linenos"> 55</span></a><span class="sd">        - **mask_sparsity_reg_factor** (`float`): hyperparameter, the regularisation factor for mask sparsity.</span>
</span><span id="L-56"><a href="#L-56"><span class="linenos"> 56</span></a><span class="sd">        - **mask_sparsity_reg_mode** (`str`): the mode of mask sparsity regularisation, should be one of the following:</span>
</span><span id="L-57"><a href="#L-57"><span class="linenos"> 57</span></a><span class="sd">            1. &#39;original&#39; (default): the original mask sparsity regularisation in HAT paper.</span>
</span><span id="L-58"><a href="#L-58"><span class="linenos"> 58</span></a><span class="sd">            2. &#39;cross&#39;: the cross version mask sparsity regularisation.</span>
</span><span id="L-59"><a href="#L-59"><span class="linenos"> 59</span></a><span class="sd">        - **task_embedding_init_mode** (`str`): the initialisation mode for task embeddings, should be one of the following:</span>
</span><span id="L-60"><a href="#L-60"><span class="linenos"> 60</span></a><span class="sd">            1. &#39;N01&#39; (default): standard normal distribution $N(0, 1)$.</span>
</span><span id="L-61"><a href="#L-61"><span class="linenos"> 61</span></a><span class="sd">            2. &#39;U-11&#39;: uniform distribution $U(-1, 1)$.</span>
</span><span id="L-62"><a href="#L-62"><span class="linenos"> 62</span></a><span class="sd">            3. &#39;U01&#39;: uniform distribution $U(0, 1)$.</span>
</span><span id="L-63"><a href="#L-63"><span class="linenos"> 63</span></a><span class="sd">            4. &#39;U-10&#39;: uniform distribution $U(-1, 0)$.</span>
</span><span id="L-64"><a href="#L-64"><span class="linenos"> 64</span></a><span class="sd">            5. &#39;last&#39;: inherit task embedding from last task.</span>
</span><span id="L-65"><a href="#L-65"><span class="linenos"> 65</span></a><span class="sd">        - **alpha** (`float` | `None`): the `alpha` in the &#39;HAT-const-alpha&#39; mode. See the &quot;Baselines&quot; section in chapter 4.1 in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9). It applies only when adjustment_mode is &#39;hat_const_alpha&#39;.</span>
</span><span id="L-66"><a href="#L-66"><span class="linenos"> 66</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-67"><a href="#L-67"><span class="linenos"> 67</span></a>        <span class="n">CLAlgorithm</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">backbone</span><span class="o">=</span><span class="n">backbone</span><span class="p">,</span> <span class="n">heads</span><span class="o">=</span><span class="n">heads</span><span class="p">)</span>
</span><span id="L-68"><a href="#L-68"><span class="linenos"> 68</span></a>
</span><span id="L-69"><a href="#L-69"><span class="linenos"> 69</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">=</span> <span class="n">adjustment_mode</span>
</span><span id="L-70"><a href="#L-70"><span class="linenos"> 70</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the adjustment mode for gradient clipping.&quot;&quot;&quot;</span>
</span><span id="L-71"><a href="#L-71"><span class="linenos"> 71</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">s_max</span> <span class="o">=</span> <span class="n">s_max</span>
</span><span id="L-72"><a href="#L-72"><span class="linenos"> 72</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store s_max. &quot;&quot;&quot;</span>
</span><span id="L-73"><a href="#L-73"><span class="linenos"> 73</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">clamp_threshold</span> <span class="o">=</span> <span class="n">clamp_threshold</span>
</span><span id="L-74"><a href="#L-74"><span class="linenos"> 74</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the clamp threshold for task embedding gradient compensation.&quot;&quot;&quot;</span>
</span><span id="L-75"><a href="#L-75"><span class="linenos"> 75</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">mask_sparsity_reg_factor</span> <span class="o">=</span> <span class="n">mask_sparsity_reg_factor</span>
</span><span id="L-76"><a href="#L-76"><span class="linenos"> 76</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the mask sparsity regularisation factor.&quot;&quot;&quot;</span>
</span><span id="L-77"><a href="#L-77"><span class="linenos"> 77</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">mask_sparsity_reg_mode</span> <span class="o">=</span> <span class="n">mask_sparsity_reg_mode</span>
</span><span id="L-78"><a href="#L-78"><span class="linenos"> 78</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the mask sparsity regularisation mode.&quot;&quot;&quot;</span>
</span><span id="L-79"><a href="#L-79"><span class="linenos"> 79</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">mark_sparsity_reg</span> <span class="o">=</span> <span class="n">HATMaskSparsityReg</span><span class="p">(</span>
</span><span id="L-80"><a href="#L-80"><span class="linenos"> 80</span></a>            <span class="n">factor</span><span class="o">=</span><span class="n">mask_sparsity_reg_factor</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">mask_sparsity_reg_mode</span>
</span><span id="L-81"><a href="#L-81"><span class="linenos"> 81</span></a>        <span class="p">)</span>
</span><span id="L-82"><a href="#L-82"><span class="linenos"> 82</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialise and store the mask sparsity regulariser.&quot;&quot;&quot;</span>
</span><span id="L-83"><a href="#L-83"><span class="linenos"> 83</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_init_mode</span> <span class="o">=</span> <span class="n">task_embedding_init_mode</span>
</span><span id="L-84"><a href="#L-84"><span class="linenos"> 84</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the task embedding initialisation mode.&quot;&quot;&quot;</span>
</span><span id="L-85"><a href="#L-85"><span class="linenos"> 85</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span> <span class="k">if</span> <span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;hat_const_alpha&quot;</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="L-86"><a href="#L-86"><span class="linenos"> 86</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the alpha for `hat_const_alpha`.&quot;&quot;&quot;</span>
</span><span id="L-87"><a href="#L-87"><span class="linenos"> 87</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="L-88"><a href="#L-88"><span class="linenos"> 88</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;HAT doesn&#39;t use the epsilon for `hat_const_alpha`. We still set it here to be consistent with the `epsilon` in `clip_grad_by_adjustment()` method in `HATMaskBackbone`.&quot;&quot;&quot;</span>
</span><span id="L-89"><a href="#L-89"><span class="linenos"> 89</span></a>
</span><span id="L-90"><a href="#L-90"><span class="linenos"> 90</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">masks</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="L-91"><a href="#L-91"><span class="linenos"> 91</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the binary attention mask of each previous task gated from the task embedding. Keys are task IDs (string type) and values are the corresponding mask. Each mask is a dict where keys are layer names and values are the binary mask tensor for the layer. The mask tensor has size (number of units). &quot;&quot;&quot;</span>
</span><span id="L-92"><a href="#L-92"><span class="linenos"> 92</span></a>
</span><span id="L-93"><a href="#L-93"><span class="linenos"> 93</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">cumulative_mask_for_previous_tasks</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="L-94"><a href="#L-94"><span class="linenos"> 94</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the cumulative binary attention mask $\mathrm{M}^{&lt;t}$ of previous tasks $1,\cdots, t-1$, gated from the task embedding. Keys are task IDs and values are the corresponding cumulative mask. Each cumulative mask is a dict where keys are layer names and values are the binary mask tensor for the layer. The mask tensor has size (number of units). &quot;&quot;&quot;</span>
</span><span id="L-95"><a href="#L-95"><span class="linenos"> 95</span></a>
</span><span id="L-96"><a href="#L-96"><span class="linenos"> 96</span></a>        <span class="c1"># set manual optimisation</span>
</span><span id="L-97"><a href="#L-97"><span class="linenos"> 97</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">automatic_optimization</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="L-98"><a href="#L-98"><span class="linenos"> 98</span></a>
</span><span id="L-99"><a href="#L-99"><span class="linenos"> 99</span></a>        <span class="n">HAT</span><span class="o">.</span><span class="n">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</span><span id="L-100"><a href="#L-100"><span class="linenos">100</span></a>
</span><span id="L-101"><a href="#L-101"><span class="linenos">101</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-102"><a href="#L-102"><span class="linenos">102</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Check the sanity of the arguments.</span>
</span><span id="L-103"><a href="#L-103"><span class="linenos">103</span></a>
</span><span id="L-104"><a href="#L-104"><span class="linenos">104</span></a><span class="sd">        **Raises:**</span>
</span><span id="L-105"><a href="#L-105"><span class="linenos">105</span></a><span class="sd">        - **ValueError**: when backbone is not designed for HAT, or the `mask_sparsity_reg_mode` or `task_embedding_init_mode` is not one of the valid options. Also, if `alpha` is not given when `adjustment_mode` is &#39;hat_const_alpha&#39;.</span>
</span><span id="L-106"><a href="#L-106"><span class="linenos">106</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-107"><a href="#L-107"><span class="linenos">107</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="p">,</span> <span class="n">HATMaskBackbone</span><span class="p">):</span>
</span><span id="L-108"><a href="#L-108"><span class="linenos">108</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The backbone should be an instance of HATMaskBackbone.&quot;</span><span class="p">)</span>
</span><span id="L-109"><a href="#L-109"><span class="linenos">109</span></a>
</span><span id="L-110"><a href="#L-110"><span class="linenos">110</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask_sparsity_reg_mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;original&quot;</span><span class="p">,</span> <span class="s2">&quot;cross&quot;</span><span class="p">]:</span>
</span><span id="L-111"><a href="#L-111"><span class="linenos">111</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="L-112"><a href="#L-112"><span class="linenos">112</span></a>                <span class="s2">&quot;The mask_sparsity_reg_mode should be one of &#39;original&#39;, &#39;cross&#39;.&quot;</span>
</span><span id="L-113"><a href="#L-113"><span class="linenos">113</span></a>            <span class="p">)</span>
</span><span id="L-114"><a href="#L-114"><span class="linenos">114</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_init_mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span>
</span><span id="L-115"><a href="#L-115"><span class="linenos">115</span></a>            <span class="s2">&quot;N01&quot;</span><span class="p">,</span>
</span><span id="L-116"><a href="#L-116"><span class="linenos">116</span></a>            <span class="s2">&quot;U01&quot;</span><span class="p">,</span>
</span><span id="L-117"><a href="#L-117"><span class="linenos">117</span></a>            <span class="s2">&quot;U-10&quot;</span><span class="p">,</span>
</span><span id="L-118"><a href="#L-118"><span class="linenos">118</span></a>            <span class="s2">&quot;masked&quot;</span><span class="p">,</span>
</span><span id="L-119"><a href="#L-119"><span class="linenos">119</span></a>            <span class="s2">&quot;unmasked&quot;</span><span class="p">,</span>
</span><span id="L-120"><a href="#L-120"><span class="linenos">120</span></a>        <span class="p">]:</span>
</span><span id="L-121"><a href="#L-121"><span class="linenos">121</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="L-122"><a href="#L-122"><span class="linenos">122</span></a>                <span class="s2">&quot;The task_embedding_init_mode should be one of &#39;N01&#39;, &#39;U01&#39;, &#39;U-10&#39;, &#39;masked&#39;, &#39;unmasked&#39;.&quot;</span>
</span><span id="L-123"><a href="#L-123"><span class="linenos">123</span></a>            <span class="p">)</span>
</span><span id="L-124"><a href="#L-124"><span class="linenos">124</span></a>
</span><span id="L-125"><a href="#L-125"><span class="linenos">125</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;hat_const_alpha&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-126"><a href="#L-126"><span class="linenos">126</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="L-127"><a href="#L-127"><span class="linenos">127</span></a>                <span class="s2">&quot;Alpha should be given when the adjustment_mode is &#39;hat_const_alpha&#39;.&quot;</span>
</span><span id="L-128"><a href="#L-128"><span class="linenos">128</span></a>            <span class="p">)</span>
</span><span id="L-129"><a href="#L-129"><span class="linenos">129</span></a>
</span><span id="L-130"><a href="#L-130"><span class="linenos">130</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_start</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-131"><a href="#L-131"><span class="linenos">131</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialise the task embedding before training the next task and initialise the cumulative mask at the beginning of first task.&quot;&quot;&quot;</span>
</span><span id="L-132"><a href="#L-132"><span class="linenos">132</span></a>
</span><span id="L-133"><a href="#L-133"><span class="linenos">133</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">initialise_task_embedding</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_init_mode</span><span class="p">)</span>
</span><span id="L-134"><a href="#L-134"><span class="linenos">134</span></a>
</span><span id="L-135"><a href="#L-135"><span class="linenos">135</span></a>        <span class="c1"># initialise the cumulative mask at the beginning of first task. This should not be called in `__init__()` method as the `self.device` is not available at that time.</span>
</span><span id="L-136"><a href="#L-136"><span class="linenos">136</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="L-137"><a href="#L-137"><span class="linenos">137</span></a>            <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="L-138"><a href="#L-138"><span class="linenos">138</span></a>                <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span>
</span><span id="L-139"><a href="#L-139"><span class="linenos">139</span></a>                    <span class="n">layer_name</span>
</span><span id="L-140"><a href="#L-140"><span class="linenos">140</span></a>                <span class="p">)</span>  <span class="c1"># get the layer by its name</span>
</span><span id="L-141"><a href="#L-141"><span class="linenos">141</span></a>                <span class="n">num_units</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="L-142"><a href="#L-142"><span class="linenos">142</span></a>
</span><span id="L-143"><a href="#L-143"><span class="linenos">143</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">cumulative_mask_for_previous_tasks</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
</span><span id="L-144"><a href="#L-144"><span class="linenos">144</span></a>                    <span class="n">num_units</span>
</span><span id="L-145"><a href="#L-145"><span class="linenos">145</span></a>                <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
</span><span id="L-146"><a href="#L-146"><span class="linenos">146</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
</span><span id="L-147"><a href="#L-147"><span class="linenos">147</span></a>                <span class="p">)</span>  <span class="c1"># the cumulative mask $\mathrm{M}^{&lt;t}$ is initialised as zeros mask ($t = 1$). See equation (2) in chapter 3 in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9), or equation (5) in chapter 2.6 &quot;Promoting Low Capacity Usage&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="L-148"><a href="#L-148"><span class="linenos">148</span></a>
</span><span id="L-149"><a href="#L-149"><span class="linenos">149</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">clip_grad_by_adjustment</span><span class="p">(</span>
</span><span id="L-150"><a href="#L-150"><span class="linenos">150</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-151"><a href="#L-151"><span class="linenos">151</span></a>        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
</span><span id="L-152"><a href="#L-152"><span class="linenos">152</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-153"><a href="#L-153"><span class="linenos">153</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Clip the gradients by the adjustment rate.</span>
</span><span id="L-154"><a href="#L-154"><span class="linenos">154</span></a>
</span><span id="L-155"><a href="#L-155"><span class="linenos">155</span></a><span class="sd">        Note that as the task embedding fully covers every layer in the backbone network, no parameters are left out of this system. This applies not only the parameters in between layers with task embedding, but also those before the first layer. We designed it seperately in the codes.</span>
</span><span id="L-156"><a href="#L-156"><span class="linenos">156</span></a>
</span><span id="L-157"><a href="#L-157"><span class="linenos">157</span></a><span class="sd">        Network capacity is measured along with this method. Network capacity is defined as the average adjustment rate over all parameters. See chapter 4.1 in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="L-158"><a href="#L-158"><span class="linenos">158</span></a>
</span><span id="L-159"><a href="#L-159"><span class="linenos">159</span></a>
</span><span id="L-160"><a href="#L-160"><span class="linenos">160</span></a><span class="sd">        **Returns:**</span>
</span><span id="L-161"><a href="#L-161"><span class="linenos">161</span></a><span class="sd">        - **capacity** (`Tensor`): the calculated network capacity.</span>
</span><span id="L-162"><a href="#L-162"><span class="linenos">162</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-163"><a href="#L-163"><span class="linenos">163</span></a>
</span><span id="L-164"><a href="#L-164"><span class="linenos">164</span></a>        <span class="c1"># initialise network capacity metric</span>
</span><span id="L-165"><a href="#L-165"><span class="linenos">165</span></a>        <span class="n">capacity</span> <span class="o">=</span> <span class="n">HATNetworkCapacity</span><span class="p">()</span>
</span><span id="L-166"><a href="#L-166"><span class="linenos">166</span></a>
</span><span id="L-167"><a href="#L-167"><span class="linenos">167</span></a>        <span class="c1"># Calculate the adjustment rate for gradients of the parameters, both weights and biases (if exists)</span>
</span><span id="L-168"><a href="#L-168"><span class="linenos">168</span></a>        <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="L-169"><a href="#L-169"><span class="linenos">169</span></a>
</span><span id="L-170"><a href="#L-170"><span class="linenos">170</span></a>            <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span>
</span><span id="L-171"><a href="#L-171"><span class="linenos">171</span></a>                <span class="n">layer_name</span>
</span><span id="L-172"><a href="#L-172"><span class="linenos">172</span></a>            <span class="p">)</span>  <span class="c1"># get the layer by its name</span>
</span><span id="L-173"><a href="#L-173"><span class="linenos">173</span></a>
</span><span id="L-174"><a href="#L-174"><span class="linenos">174</span></a>            <span class="c1"># placeholder for the adjustment rate to avoid the error of using it before assignment</span>
</span><span id="L-175"><a href="#L-175"><span class="linenos">175</span></a>            <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="L-176"><a href="#L-176"><span class="linenos">176</span></a>            <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="L-177"><a href="#L-177"><span class="linenos">177</span></a>
</span><span id="L-178"><a href="#L-178"><span class="linenos">178</span></a>            <span class="n">weight_mask</span><span class="p">,</span> <span class="n">bias_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_measure_parameter_wise</span><span class="p">(</span>
</span><span id="L-179"><a href="#L-179"><span class="linenos">179</span></a>                <span class="n">unit_wise_measure</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">cumulative_mask_for_previous_tasks</span><span class="p">,</span>
</span><span id="L-180"><a href="#L-180"><span class="linenos">180</span></a>                <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="L-181"><a href="#L-181"><span class="linenos">181</span></a>                <span class="n">aggregation</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span>
</span><span id="L-182"><a href="#L-182"><span class="linenos">182</span></a>            <span class="p">)</span>
</span><span id="L-183"><a href="#L-183"><span class="linenos">183</span></a>
</span><span id="L-184"><a href="#L-184"><span class="linenos">184</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;hat&quot;</span><span class="p">:</span>
</span><span id="L-185"><a href="#L-185"><span class="linenos">185</span></a>                <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">weight_mask</span>
</span><span id="L-186"><a href="#L-186"><span class="linenos">186</span></a>                <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">bias_mask</span>
</span><span id="L-187"><a href="#L-187"><span class="linenos">187</span></a>
</span><span id="L-188"><a href="#L-188"><span class="linenos">188</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;hat_random&quot;</span><span class="p">:</span>
</span><span id="L-189"><a href="#L-189"><span class="linenos">189</span></a>                <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand_like</span><span class="p">(</span><span class="n">weight_mask</span><span class="p">)</span> <span class="o">*</span> <span class="n">weight_mask</span> <span class="o">+</span> <span class="p">(</span>
</span><span id="L-190"><a href="#L-190"><span class="linenos">190</span></a>                    <span class="mi">1</span> <span class="o">-</span> <span class="n">weight_mask</span>
</span><span id="L-191"><a href="#L-191"><span class="linenos">191</span></a>                <span class="p">)</span>
</span><span id="L-192"><a href="#L-192"><span class="linenos">192</span></a>                <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand_like</span><span class="p">(</span><span class="n">bias_mask</span><span class="p">)</span> <span class="o">*</span> <span class="n">bias_mask</span> <span class="o">+</span> <span class="p">(</span>
</span><span id="L-193"><a href="#L-193"><span class="linenos">193</span></a>                    <span class="mi">1</span> <span class="o">-</span> <span class="n">bias_mask</span>
</span><span id="L-194"><a href="#L-194"><span class="linenos">194</span></a>                <span class="p">)</span>
</span><span id="L-195"><a href="#L-195"><span class="linenos">195</span></a>
</span><span id="L-196"><a href="#L-196"><span class="linenos">196</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;hat_const_alpha&quot;</span><span class="p">:</span>
</span><span id="L-197"><a href="#L-197"><span class="linenos">197</span></a>                <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span>
</span><span id="L-198"><a href="#L-198"><span class="linenos">198</span></a>                    <span class="n">weight_mask</span>
</span><span id="L-199"><a href="#L-199"><span class="linenos">199</span></a>                <span class="p">)</span> <span class="o">*</span> <span class="n">weight_mask</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">weight_mask</span><span class="p">)</span>
</span><span id="L-200"><a href="#L-200"><span class="linenos">200</span></a>                <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span>
</span><span id="L-201"><a href="#L-201"><span class="linenos">201</span></a>                    <span class="n">bias_mask</span>
</span><span id="L-202"><a href="#L-202"><span class="linenos">202</span></a>                <span class="p">)</span> <span class="o">*</span> <span class="n">bias_mask</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">bias_mask</span><span class="p">)</span>
</span><span id="L-203"><a href="#L-203"><span class="linenos">203</span></a>
</span><span id="L-204"><a href="#L-204"><span class="linenos">204</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;hat_const_1&quot;</span><span class="p">:</span>
</span><span id="L-205"><a href="#L-205"><span class="linenos">205</span></a>                <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">weight_mask</span><span class="p">)</span> <span class="o">*</span> <span class="n">weight_mask</span> <span class="o">+</span> <span class="p">(</span>
</span><span id="L-206"><a href="#L-206"><span class="linenos">206</span></a>                    <span class="mi">1</span> <span class="o">-</span> <span class="n">weight_mask</span>
</span><span id="L-207"><a href="#L-207"><span class="linenos">207</span></a>                <span class="p">)</span>
</span><span id="L-208"><a href="#L-208"><span class="linenos">208</span></a>                <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">bias_mask</span><span class="p">)</span> <span class="o">*</span> <span class="n">bias_mask</span> <span class="o">+</span> <span class="p">(</span>
</span><span id="L-209"><a href="#L-209"><span class="linenos">209</span></a>                    <span class="mi">1</span> <span class="o">-</span> <span class="n">bias_mask</span>
</span><span id="L-210"><a href="#L-210"><span class="linenos">210</span></a>                <span class="p">)</span>
</span><span id="L-211"><a href="#L-211"><span class="linenos">211</span></a>
</span><span id="L-212"><a href="#L-212"><span class="linenos">212</span></a>            <span class="c1"># apply the adjustment rate to the gradients</span>
</span><span id="L-213"><a href="#L-213"><span class="linenos">213</span></a>            <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">adjustment_rate_weight_layer</span>
</span><span id="L-214"><a href="#L-214"><span class="linenos">214</span></a>            <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-215"><a href="#L-215"><span class="linenos">215</span></a>                <span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">adjustment_rate_bias_layer</span>
</span><span id="L-216"><a href="#L-216"><span class="linenos">216</span></a>
</span><span id="L-217"><a href="#L-217"><span class="linenos">217</span></a>            <span class="c1"># update network capacity metric</span>
</span><span id="L-218"><a href="#L-218"><span class="linenos">218</span></a>            <span class="n">capacity</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">adjustment_rate_weight_layer</span><span class="p">,</span> <span class="n">adjustment_rate_bias_layer</span><span class="p">)</span>
</span><span id="L-219"><a href="#L-219"><span class="linenos">219</span></a>
</span><span id="L-220"><a href="#L-220"><span class="linenos">220</span></a>        <span class="k">return</span> <span class="n">capacity</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
</span><span id="L-221"><a href="#L-221"><span class="linenos">221</span></a>
</span><span id="L-222"><a href="#L-222"><span class="linenos">222</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">compensate_task_embedding_gradients</span><span class="p">(</span>
</span><span id="L-223"><a href="#L-223"><span class="linenos">223</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-224"><a href="#L-224"><span class="linenos">224</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-225"><a href="#L-225"><span class="linenos">225</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-226"><a href="#L-226"><span class="linenos">226</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-227"><a href="#L-227"><span class="linenos">227</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Compensate the gradients of task embeddings during training. See chapter 2.5 &quot;Embedding Gradient Compensation&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="L-228"><a href="#L-228"><span class="linenos">228</span></a>
</span><span id="L-229"><a href="#L-229"><span class="linenos">229</span></a><span class="sd">        **Args:**</span>
</span><span id="L-230"><a href="#L-230"><span class="linenos">230</span></a><span class="sd">        - **batch_idx** (`int`): the current training batch index.</span>
</span><span id="L-231"><a href="#L-231"><span class="linenos">231</span></a><span class="sd">        - **num_batches** (`int`): the total number of training batches.</span>
</span><span id="L-232"><a href="#L-232"><span class="linenos">232</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-233"><a href="#L-233"><span class="linenos">233</span></a>
</span><span id="L-234"><a href="#L-234"><span class="linenos">234</span></a>        <span class="k">for</span> <span class="n">te</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
</span><span id="L-235"><a href="#L-235"><span class="linenos">235</span></a>            <span class="n">anneal_scalar</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_max</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">s_max</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_max</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span>
</span><span id="L-236"><a href="#L-236"><span class="linenos">236</span></a>                <span class="n">batch_idx</span> <span class="o">-</span> <span class="mi">1</span>
</span><span id="L-237"><a href="#L-237"><span class="linenos">237</span></a>            <span class="p">)</span> <span class="o">/</span> <span class="p">(</span>
</span><span id="L-238"><a href="#L-238"><span class="linenos">238</span></a>                <span class="n">num_batches</span> <span class="o">-</span> <span class="mi">1</span>
</span><span id="L-239"><a href="#L-239"><span class="linenos">239</span></a>            <span class="p">)</span>  <span class="c1"># see equation (3) in chapter 2.4 &quot;Hard Attention Training&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a)</span>
</span><span id="L-240"><a href="#L-240"><span class="linenos">240</span></a>
</span><span id="L-241"><a href="#L-241"><span class="linenos">241</span></a>            <span class="n">num</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-242"><a href="#L-242"><span class="linenos">242</span></a>                <span class="n">torch</span><span class="o">.</span><span class="n">cosh</span><span class="p">(</span>
</span><span id="L-243"><a href="#L-243"><span class="linenos">243</span></a>                    <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span>
</span><span id="L-244"><a href="#L-244"><span class="linenos">244</span></a>                        <span class="n">anneal_scalar</span> <span class="o">*</span> <span class="n">te</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">,</span>
</span><span id="L-245"><a href="#L-245"><span class="linenos">245</span></a>                        <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">clamp_threshold</span><span class="p">,</span>
</span><span id="L-246"><a href="#L-246"><span class="linenos">246</span></a>                        <span class="bp">self</span><span class="o">.</span><span class="n">clamp_threshold</span><span class="p">,</span>
</span><span id="L-247"><a href="#L-247"><span class="linenos">247</span></a>                    <span class="p">)</span>
</span><span id="L-248"><a href="#L-248"><span class="linenos">248</span></a>                <span class="p">)</span>
</span><span id="L-249"><a href="#L-249"><span class="linenos">249</span></a>                <span class="o">+</span> <span class="mi">1</span>
</span><span id="L-250"><a href="#L-250"><span class="linenos">250</span></a>            <span class="p">)</span>
</span><span id="L-251"><a href="#L-251"><span class="linenos">251</span></a>
</span><span id="L-252"><a href="#L-252"><span class="linenos">252</span></a>            <span class="n">den</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cosh</span><span class="p">(</span><span class="n">te</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="L-253"><a href="#L-253"><span class="linenos">253</span></a>
</span><span id="L-254"><a href="#L-254"><span class="linenos">254</span></a>            <span class="n">compensation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_max</span> <span class="o">/</span> <span class="n">anneal_scalar</span> <span class="o">*</span> <span class="n">num</span> <span class="o">/</span> <span class="n">den</span>
</span><span id="L-255"><a href="#L-255"><span class="linenos">255</span></a>
</span><span id="L-256"><a href="#L-256"><span class="linenos">256</span></a>            <span class="n">te</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">compensation</span>
</span><span id="L-257"><a href="#L-257"><span class="linenos">257</span></a>
</span><span id="L-258"><a href="#L-258"><span class="linenos">258</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="L-259"><a href="#L-259"><span class="linenos">259</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-260"><a href="#L-260"><span class="linenos">260</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-261"><a href="#L-261"><span class="linenos">261</span></a>        <span class="n">stage</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-262"><a href="#L-262"><span class="linenos">262</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-263"><a href="#L-263"><span class="linenos">263</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-264"><a href="#L-264"><span class="linenos">264</span></a>        <span class="n">task_id</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-265"><a href="#L-265"><span class="linenos">265</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
</span><span id="L-266"><a href="#L-266"><span class="linenos">266</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The forward pass for data from task `task_id`. Note that it is nothing to do with `forward()` method in `nn.Module`.</span>
</span><span id="L-267"><a href="#L-267"><span class="linenos">267</span></a>
</span><span id="L-268"><a href="#L-268"><span class="linenos">268</span></a><span class="sd">        **Args:**</span>
</span><span id="L-269"><a href="#L-269"><span class="linenos">269</span></a><span class="sd">        - **input** (`Tensor`): The input tensor from data.</span>
</span><span id="L-270"><a href="#L-270"><span class="linenos">270</span></a><span class="sd">        - **stage** (`str`): the stage of the forward pass, should be one of the following:</span>
</span><span id="L-271"><a href="#L-271"><span class="linenos">271</span></a><span class="sd">            1. &#39;train&#39;: training stage.</span>
</span><span id="L-272"><a href="#L-272"><span class="linenos">272</span></a><span class="sd">            2. &#39;validation&#39;: validation stage.</span>
</span><span id="L-273"><a href="#L-273"><span class="linenos">273</span></a><span class="sd">            3. &#39;test&#39;: testing stage.</span>
</span><span id="L-274"><a href="#L-274"><span class="linenos">274</span></a><span class="sd">        - **batch_idx** (`int` | `None`): the current batch index. Applies only to training stage. For other stages, it is default `None`.</span>
</span><span id="L-275"><a href="#L-275"><span class="linenos">275</span></a><span class="sd">        - **num_batches** (`int` | `None`): the total number of batches. Applies only to training stage. For other stages, it is default `None`.</span>
</span><span id="L-276"><a href="#L-276"><span class="linenos">276</span></a><span class="sd">        - **task_id** (`int`| `None`): the task ID where the data are from. If the stage is &#39;train&#39; or &#39;validation&#39;, it should be the current task `self.task_id`. If stage is &#39;test&#39;, it could be from any seen task. In TIL, the task IDs of test data are provided thus this argument can be used. HAT algorithm works only for TIL.</span>
</span><span id="L-277"><a href="#L-277"><span class="linenos">277</span></a>
</span><span id="L-278"><a href="#L-278"><span class="linenos">278</span></a><span class="sd">        **Returns:**</span>
</span><span id="L-279"><a href="#L-279"><span class="linenos">279</span></a><span class="sd">        - **logits** (`Tensor`): the output logits tensor.</span>
</span><span id="L-280"><a href="#L-280"><span class="linenos">280</span></a><span class="sd">        - **mask** (`dict[str, Tensor]`): the mask for the current task. Key (`str`) is layer name, value (`Tensor`) is the mask tensor. The mask tensor has size (number of units).</span>
</span><span id="L-281"><a href="#L-281"><span class="linenos">281</span></a><span class="sd">        - **activations** (`dict[str, Tensor]`): the hidden features (after activation) in each weighted layer. Key (`str`) is the weighted layer name, value (`Tensor`) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes. Although HAT algorithm does not need this, it is still provided for API consistence for other HAT-based algorithms inherited this `forward()` method of `HAT` class.</span>
</span><span id="L-282"><a href="#L-282"><span class="linenos">282</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-283"><a href="#L-283"><span class="linenos">283</span></a>        <span class="n">feature</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="p">(</span>
</span><span id="L-284"><a href="#L-284"><span class="linenos">284</span></a>            <span class="nb">input</span><span class="p">,</span>
</span><span id="L-285"><a href="#L-285"><span class="linenos">285</span></a>            <span class="n">stage</span><span class="o">=</span><span class="n">stage</span><span class="p">,</span>
</span><span id="L-286"><a href="#L-286"><span class="linenos">286</span></a>            <span class="n">s_max</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">s_max</span> <span class="k">if</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;train&quot;</span> <span class="ow">or</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;validation&quot;</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-287"><a href="#L-287"><span class="linenos">287</span></a>            <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span> <span class="k">if</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;train&quot;</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-288"><a href="#L-288"><span class="linenos">288</span></a>            <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span> <span class="k">if</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;train&quot;</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-289"><a href="#L-289"><span class="linenos">289</span></a>            <span class="n">test_mask</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">masks</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">task_id</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="k">if</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;test&quot;</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-290"><a href="#L-290"><span class="linenos">290</span></a>        <span class="p">)</span>
</span><span id="L-291"><a href="#L-291"><span class="linenos">291</span></a>        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">heads</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="n">task_id</span><span class="p">)</span>
</span><span id="L-292"><a href="#L-292"><span class="linenos">292</span></a>
</span><span id="L-293"><a href="#L-293"><span class="linenos">293</span></a>        <span class="k">return</span> <span class="n">logits</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">activations</span>
</span><span id="L-294"><a href="#L-294"><span class="linenos">294</span></a>
</span><span id="L-295"><a href="#L-295"><span class="linenos">295</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
</span><span id="L-296"><a href="#L-296"><span class="linenos">296</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Training step for current task `self.task_id`.</span>
</span><span id="L-297"><a href="#L-297"><span class="linenos">297</span></a>
</span><span id="L-298"><a href="#L-298"><span class="linenos">298</span></a><span class="sd">        **Args:**</span>
</span><span id="L-299"><a href="#L-299"><span class="linenos">299</span></a><span class="sd">        - **batch** (`Any`): a batch of training data.</span>
</span><span id="L-300"><a href="#L-300"><span class="linenos">300</span></a><span class="sd">        - **batch_idx** (`int`): the index of the batch. Used for calculating annealed scalar in HAT. See chapter 2.4 &quot;Hard Attention Training&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="L-301"><a href="#L-301"><span class="linenos">301</span></a>
</span><span id="L-302"><a href="#L-302"><span class="linenos">302</span></a><span class="sd">        **Returns:**</span>
</span><span id="L-303"><a href="#L-303"><span class="linenos">303</span></a><span class="sd">        - **outputs** (`dict[str, Tensor]`): a dictionary contains loss and other metrics from this training step. Key (`str`) is the metrics name, value (`Tensor`) is the metrics. Must include the key &#39;loss&#39; which is total loss in the case of automatic optimization, according to PyTorch Lightning docs. For HAT, it includes &#39;mask&#39; and &#39;capacity&#39; for logging.</span>
</span><span id="L-304"><a href="#L-304"><span class="linenos">304</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-305"><a href="#L-305"><span class="linenos">305</span></a>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
</span><span id="L-306"><a href="#L-306"><span class="linenos">306</span></a>
</span><span id="L-307"><a href="#L-307"><span class="linenos">307</span></a>        <span class="c1"># zero the gradients before forward pass in manual optimisation mode</span>
</span><span id="L-308"><a href="#L-308"><span class="linenos">308</span></a>        <span class="n">opt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizers</span><span class="p">()</span>
</span><span id="L-309"><a href="#L-309"><span class="linenos">309</span></a>        <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</span><span id="L-310"><a href="#L-310"><span class="linenos">310</span></a>
</span><span id="L-311"><a href="#L-311"><span class="linenos">311</span></a>        <span class="c1"># classification loss</span>
</span><span id="L-312"><a href="#L-312"><span class="linenos">312</span></a>        <span class="n">num_batches</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">num_training_batches</span>
</span><span id="L-313"><a href="#L-313"><span class="linenos">313</span></a>        <span class="n">logits</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
</span><span id="L-314"><a href="#L-314"><span class="linenos">314</span></a>            <span class="n">x</span><span class="p">,</span>
</span><span id="L-315"><a href="#L-315"><span class="linenos">315</span></a>            <span class="n">stage</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">,</span>
</span><span id="L-316"><a href="#L-316"><span class="linenos">316</span></a>            <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="L-317"><a href="#L-317"><span class="linenos">317</span></a>            <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="L-318"><a href="#L-318"><span class="linenos">318</span></a>            <span class="n">task_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">,</span>
</span><span id="L-319"><a href="#L-319"><span class="linenos">319</span></a>        <span class="p">)</span>
</span><span id="L-320"><a href="#L-320"><span class="linenos">320</span></a>        <span class="n">loss_cls</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span><span id="L-321"><a href="#L-321"><span class="linenos">321</span></a>
</span><span id="L-322"><a href="#L-322"><span class="linenos">322</span></a>        <span class="c1"># regularisation loss. See chapter 2.6 &quot;Promoting Low Capacity Usage&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="L-323"><a href="#L-323"><span class="linenos">323</span></a>        <span class="n">loss_reg</span><span class="p">,</span> <span class="n">network_sparsity</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mark_sparsity_reg</span><span class="p">(</span>
</span><span id="L-324"><a href="#L-324"><span class="linenos">324</span></a>            <span class="n">mask</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cumulative_mask_for_previous_tasks</span>
</span><span id="L-325"><a href="#L-325"><span class="linenos">325</span></a>        <span class="p">)</span>
</span><span id="L-326"><a href="#L-326"><span class="linenos">326</span></a>
</span><span id="L-327"><a href="#L-327"><span class="linenos">327</span></a>        <span class="c1"># total loss</span>
</span><span id="L-328"><a href="#L-328"><span class="linenos">328</span></a>        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_cls</span> <span class="o">+</span> <span class="n">loss_reg</span>
</span><span id="L-329"><a href="#L-329"><span class="linenos">329</span></a>
</span><span id="L-330"><a href="#L-330"><span class="linenos">330</span></a>        <span class="c1"># backward step (manually)</span>
</span><span id="L-331"><a href="#L-331"><span class="linenos">331</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">manual_backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>  <span class="c1"># calculate the gradients</span>
</span><span id="L-332"><a href="#L-332"><span class="linenos">332</span></a>        <span class="c1"># HAT hard clip gradients by the cumulative masks. See equation (2) inchapter 2.3 &quot;Network Training&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a). Network capacity is calculated along with this process. Network capacity is defined as the average adjustment rate over all parameters. See chapter 4.1 in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="L-333"><a href="#L-333"><span class="linenos">333</span></a>        <span class="n">capacity</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">clip_grad_by_adjustment</span><span class="p">(</span>
</span><span id="L-334"><a href="#L-334"><span class="linenos">334</span></a>            <span class="n">network_sparsity</span><span class="o">=</span><span class="n">network_sparsity</span><span class="p">,</span>  <span class="c1"># pass a keyword argument network sparsity here to make it compatible with AdaHAT. AdaHAT inherits this `training_step()` method.</span>
</span><span id="L-335"><a href="#L-335"><span class="linenos">335</span></a>        <span class="p">)</span>
</span><span id="L-336"><a href="#L-336"><span class="linenos">336</span></a>        <span class="c1"># compensate the gradients of task embedding. See chapter 2.5 &quot;Embedding Gradient Compensation&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="L-337"><a href="#L-337"><span class="linenos">337</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">compensate_task_embedding_gradients</span><span class="p">(</span>
</span><span id="L-338"><a href="#L-338"><span class="linenos">338</span></a>            <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="L-339"><a href="#L-339"><span class="linenos">339</span></a>            <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="L-340"><a href="#L-340"><span class="linenos">340</span></a>        <span class="p">)</span>
</span><span id="L-341"><a href="#L-341"><span class="linenos">341</span></a>        <span class="c1"># update parameters with the modified gradients</span>
</span><span id="L-342"><a href="#L-342"><span class="linenos">342</span></a>        <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span><span id="L-343"><a href="#L-343"><span class="linenos">343</span></a>
</span><span id="L-344"><a href="#L-344"><span class="linenos">344</span></a>        <span class="c1"># accuracy of the batch</span>
</span><span id="L-345"><a href="#L-345"><span class="linenos">345</span></a>        <span class="n">acc</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</span><span id="L-346"><a href="#L-346"><span class="linenos">346</span></a>
</span><span id="L-347"><a href="#L-347"><span class="linenos">347</span></a>        <span class="k">return</span> <span class="p">{</span>
</span><span id="L-348"><a href="#L-348"><span class="linenos">348</span></a>            <span class="s2">&quot;loss&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span>  <span class="c1"># Return loss is essential for training step, or backpropagation will fail</span>
</span><span id="L-349"><a href="#L-349"><span class="linenos">349</span></a>            <span class="s2">&quot;loss_cls&quot;</span><span class="p">:</span> <span class="n">loss_cls</span><span class="p">,</span>
</span><span id="L-350"><a href="#L-350"><span class="linenos">350</span></a>            <span class="s2">&quot;loss_reg&quot;</span><span class="p">:</span> <span class="n">loss_reg</span><span class="p">,</span>
</span><span id="L-351"><a href="#L-351"><span class="linenos">351</span></a>            <span class="s2">&quot;acc&quot;</span><span class="p">:</span> <span class="n">acc</span><span class="p">,</span>
</span><span id="L-352"><a href="#L-352"><span class="linenos">352</span></a>            <span class="s2">&quot;activations&quot;</span><span class="p">:</span> <span class="n">activations</span><span class="p">,</span>
</span><span id="L-353"><a href="#L-353"><span class="linenos">353</span></a>            <span class="s2">&quot;mask&quot;</span><span class="p">:</span> <span class="n">mask</span><span class="p">,</span>  <span class="c1"># Return other metrics for lightning loggers callback to handle at `on_train_batch_end()`</span>
</span><span id="L-354"><a href="#L-354"><span class="linenos">354</span></a>            <span class="s2">&quot;capacity&quot;</span><span class="p">:</span> <span class="n">capacity</span><span class="p">,</span>
</span><span id="L-355"><a href="#L-355"><span class="linenos">355</span></a>        <span class="p">}</span>
</span><span id="L-356"><a href="#L-356"><span class="linenos">356</span></a>
</span><span id="L-357"><a href="#L-357"><span class="linenos">357</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_end</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-358"><a href="#L-358"><span class="linenos">358</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the mask and update cumulative mask after training the task.&quot;&quot;&quot;</span>
</span><span id="L-359"><a href="#L-359"><span class="linenos">359</span></a>
</span><span id="L-360"><a href="#L-360"><span class="linenos">360</span></a>        <span class="c1"># store the mask for the current task</span>
</span><span id="L-361"><a href="#L-361"><span class="linenos">361</span></a>        <span class="n">mask_t</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="L-362"><a href="#L-362"><span class="linenos">362</span></a>            <span class="n">layer_name</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">gate_fn</span><span class="p">(</span>
</span><span id="L-363"><a href="#L-363"><span class="linenos">363</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_max</span>
</span><span id="L-364"><a href="#L-364"><span class="linenos">364</span></a>            <span class="p">)</span>
</span><span id="L-365"><a href="#L-365"><span class="linenos">365</span></a>            <span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
</span><span id="L-366"><a href="#L-366"><span class="linenos">366</span></a>            <span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="L-367"><a href="#L-367"><span class="linenos">367</span></a>            <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span>
</span><span id="L-368"><a href="#L-368"><span class="linenos">368</span></a>        <span class="p">}</span>
</span><span id="L-369"><a href="#L-369"><span class="linenos">369</span></a>
</span><span id="L-370"><a href="#L-370"><span class="linenos">370</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">masks</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mask_t</span>
</span><span id="L-371"><a href="#L-371"><span class="linenos">371</span></a>
</span><span id="L-372"><a href="#L-372"><span class="linenos">372</span></a>        <span class="c1"># update the cumulative and summative masks</span>
</span><span id="L-373"><a href="#L-373"><span class="linenos">373</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">cumulative_mask_for_previous_tasks</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="L-374"><a href="#L-374"><span class="linenos">374</span></a>            <span class="n">layer_name</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span>
</span><span id="L-375"><a href="#L-375"><span class="linenos">375</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">cumulative_mask_for_previous_tasks</span><span class="p">[</span><span class="n">layer_name</span><span class="p">],</span> <span class="n">mask_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="L-376"><a href="#L-376"><span class="linenos">376</span></a>            <span class="p">)</span>
</span><span id="L-377"><a href="#L-377"><span class="linenos">377</span></a>            <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span>
</span><span id="L-378"><a href="#L-378"><span class="linenos">378</span></a>        <span class="p">}</span>
</span><span id="L-379"><a href="#L-379"><span class="linenos">379</span></a>
</span><span id="L-380"><a href="#L-380"><span class="linenos">380</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
</span><span id="L-381"><a href="#L-381"><span class="linenos">381</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Validation step for current task `self.task_id`.</span>
</span><span id="L-382"><a href="#L-382"><span class="linenos">382</span></a>
</span><span id="L-383"><a href="#L-383"><span class="linenos">383</span></a><span class="sd">        **Args:**</span>
</span><span id="L-384"><a href="#L-384"><span class="linenos">384</span></a><span class="sd">        - **batch** (`Any`): a batch of validation data.</span>
</span><span id="L-385"><a href="#L-385"><span class="linenos">385</span></a>
</span><span id="L-386"><a href="#L-386"><span class="linenos">386</span></a><span class="sd">        **Returns:**</span>
</span><span id="L-387"><a href="#L-387"><span class="linenos">387</span></a><span class="sd">        - **outputs** (`dict[str, Tensor]`): a dictionary contains loss and other metrics from this validation step. Key (`str`) is the metrics name, value (`Tensor`) is the metrics.</span>
</span><span id="L-388"><a href="#L-388"><span class="linenos">388</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-389"><a href="#L-389"><span class="linenos">389</span></a>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
</span><span id="L-390"><a href="#L-390"><span class="linenos">390</span></a>        <span class="n">logits</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
</span><span id="L-391"><a href="#L-391"><span class="linenos">391</span></a>            <span class="n">x</span><span class="p">,</span> <span class="n">stage</span><span class="o">=</span><span class="s2">&quot;validation&quot;</span><span class="p">,</span> <span class="n">task_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span>
</span><span id="L-392"><a href="#L-392"><span class="linenos">392</span></a>        <span class="p">)</span>
</span><span id="L-393"><a href="#L-393"><span class="linenos">393</span></a>        <span class="n">loss_cls</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span><span id="L-394"><a href="#L-394"><span class="linenos">394</span></a>        <span class="n">acc</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</span><span id="L-395"><a href="#L-395"><span class="linenos">395</span></a>
</span><span id="L-396"><a href="#L-396"><span class="linenos">396</span></a>        <span class="k">return</span> <span class="p">{</span>
</span><span id="L-397"><a href="#L-397"><span class="linenos">397</span></a>            <span class="s2">&quot;loss_cls&quot;</span><span class="p">:</span> <span class="n">loss_cls</span><span class="p">,</span>
</span><span id="L-398"><a href="#L-398"><span class="linenos">398</span></a>            <span class="s2">&quot;acc&quot;</span><span class="p">:</span> <span class="n">acc</span><span class="p">,</span>  <span class="c1"># Return metrics for lightning loggers callback to handle at `on_validation_batch_end()`</span>
</span><span id="L-399"><a href="#L-399"><span class="linenos">399</span></a>        <span class="p">}</span>
</span><span id="L-400"><a href="#L-400"><span class="linenos">400</span></a>
</span><span id="L-401"><a href="#L-401"><span class="linenos">401</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">test_step</span><span class="p">(</span>
</span><span id="L-402"><a href="#L-402"><span class="linenos">402</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="L-403"><a href="#L-403"><span class="linenos">403</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
</span><span id="L-404"><a href="#L-404"><span class="linenos">404</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Test step for current task `self.task_id`, which tests for all seen tasks indexed by `dataloader_idx`.</span>
</span><span id="L-405"><a href="#L-405"><span class="linenos">405</span></a>
</span><span id="L-406"><a href="#L-406"><span class="linenos">406</span></a><span class="sd">        **Args:**</span>
</span><span id="L-407"><a href="#L-407"><span class="linenos">407</span></a><span class="sd">        - **batch** (`Any`): a batch of test data.</span>
</span><span id="L-408"><a href="#L-408"><span class="linenos">408</span></a><span class="sd">        - **dataloader_idx** (`int`): the task ID of seen tasks to be tested. A default value of 0 is given otherwise the LightningModule will raise a `RuntimeError`.</span>
</span><span id="L-409"><a href="#L-409"><span class="linenos">409</span></a>
</span><span id="L-410"><a href="#L-410"><span class="linenos">410</span></a><span class="sd">        **Returns:**</span>
</span><span id="L-411"><a href="#L-411"><span class="linenos">411</span></a><span class="sd">        - **outputs** (`dict[str, Tensor]`): a dictionary contains loss and other metrics from this test step. Key (`str`) is the metrics name, value (`Tensor`) is the metrics.</span>
</span><span id="L-412"><a href="#L-412"><span class="linenos">412</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-413"><a href="#L-413"><span class="linenos">413</span></a>        <span class="n">test_task_id</span> <span class="o">=</span> <span class="n">dataloader_idx</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="L-414"><a href="#L-414"><span class="linenos">414</span></a>
</span><span id="L-415"><a href="#L-415"><span class="linenos">415</span></a>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
</span><span id="L-416"><a href="#L-416"><span class="linenos">416</span></a>        <span class="n">logits</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
</span><span id="L-417"><a href="#L-417"><span class="linenos">417</span></a>            <span class="n">x</span><span class="p">,</span>
</span><span id="L-418"><a href="#L-418"><span class="linenos">418</span></a>            <span class="n">stage</span><span class="o">=</span><span class="s2">&quot;test&quot;</span><span class="p">,</span>
</span><span id="L-419"><a href="#L-419"><span class="linenos">419</span></a>            <span class="n">task_id</span><span class="o">=</span><span class="n">test_task_id</span><span class="p">,</span>
</span><span id="L-420"><a href="#L-420"><span class="linenos">420</span></a>        <span class="p">)</span>  <span class="c1"># use the corresponding head and mask to test (instead of the current task `self.task_id`)</span>
</span><span id="L-421"><a href="#L-421"><span class="linenos">421</span></a>        <span class="n">loss_cls</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span><span id="L-422"><a href="#L-422"><span class="linenos">422</span></a>        <span class="n">acc</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</span><span id="L-423"><a href="#L-423"><span class="linenos">423</span></a>
</span><span id="L-424"><a href="#L-424"><span class="linenos">424</span></a>        <span class="k">return</span> <span class="p">{</span>
</span><span id="L-425"><a href="#L-425"><span class="linenos">425</span></a>            <span class="s2">&quot;loss_cls&quot;</span><span class="p">:</span> <span class="n">loss_cls</span><span class="p">,</span>
</span><span id="L-426"><a href="#L-426"><span class="linenos">426</span></a>            <span class="s2">&quot;acc&quot;</span><span class="p">:</span> <span class="n">acc</span><span class="p">,</span>  <span class="c1"># Return metrics for lightning loggers callback to handle at `on_test_batch_end()`</span>
</span><span id="L-427"><a href="#L-427"><span class="linenos">427</span></a>        <span class="p">}</span>
</span></pre></div>


            </section>
                <section id="HAT">
                            <input id="HAT-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">HAT</span><wbr>(<span class="base">clarena.cl_algorithms.base.CLAlgorithm</span>):

                <label class="view-source-button" for="HAT-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HAT"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HAT-25"><a href="#HAT-25"><span class="linenos"> 25</span></a><span class="k">class</span><span class="w"> </span><span class="nc">HAT</span><span class="p">(</span><span class="n">CLAlgorithm</span><span class="p">):</span>
</span><span id="HAT-26"><a href="#HAT-26"><span class="linenos"> 26</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;HAT (Hard Attention to the Task) algorithm.</span>
</span><span id="HAT-27"><a href="#HAT-27"><span class="linenos"> 27</span></a>
</span><span id="HAT-28"><a href="#HAT-28"><span class="linenos"> 28</span></a><span class="sd">    [HAT (Hard Attention to the Task, 2018)](http://proceedings.mlr.press/v80/serra18a) is an architecture-based continual learning approach that uses learnable hard attention masks to select the task-specific parameters.</span>
</span><span id="HAT-29"><a href="#HAT-29"><span class="linenos"> 29</span></a>
</span><span id="HAT-30"><a href="#HAT-30"><span class="linenos"> 30</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="HAT-31"><a href="#HAT-31"><span class="linenos"> 31</span></a>
</span><span id="HAT-32"><a href="#HAT-32"><span class="linenos"> 32</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="HAT-33"><a href="#HAT-33"><span class="linenos"> 33</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="HAT-34"><a href="#HAT-34"><span class="linenos"> 34</span></a>        <span class="n">backbone</span><span class="p">:</span> <span class="n">HATMaskBackbone</span><span class="p">,</span>
</span><span id="HAT-35"><a href="#HAT-35"><span class="linenos"> 35</span></a>        <span class="n">heads</span><span class="p">:</span> <span class="n">HeadsTIL</span> <span class="o">|</span> <span class="n">HeadsCIL</span><span class="p">,</span>
</span><span id="HAT-36"><a href="#HAT-36"><span class="linenos"> 36</span></a>        <span class="n">adjustment_mode</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="HAT-37"><a href="#HAT-37"><span class="linenos"> 37</span></a>        <span class="n">s_max</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="HAT-38"><a href="#HAT-38"><span class="linenos"> 38</span></a>        <span class="n">clamp_threshold</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="HAT-39"><a href="#HAT-39"><span class="linenos"> 39</span></a>        <span class="n">mask_sparsity_reg_factor</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="HAT-40"><a href="#HAT-40"><span class="linenos"> 40</span></a>        <span class="n">mask_sparsity_reg_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;original&quot;</span><span class="p">,</span>
</span><span id="HAT-41"><a href="#HAT-41"><span class="linenos"> 41</span></a>        <span class="n">task_embedding_init_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;N01&quot;</span><span class="p">,</span>
</span><span id="HAT-42"><a href="#HAT-42"><span class="linenos"> 42</span></a>        <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HAT-43"><a href="#HAT-43"><span class="linenos"> 43</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HAT-44"><a href="#HAT-44"><span class="linenos"> 44</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialise the HAT algorithm with the network.</span>
</span><span id="HAT-45"><a href="#HAT-45"><span class="linenos"> 45</span></a>
</span><span id="HAT-46"><a href="#HAT-46"><span class="linenos"> 46</span></a><span class="sd">        **Args:**</span>
</span><span id="HAT-47"><a href="#HAT-47"><span class="linenos"> 47</span></a><span class="sd">        - **backbone** (`HATMaskBackbone`): must be a backbone network with HAT mask mechanism.</span>
</span><span id="HAT-48"><a href="#HAT-48"><span class="linenos"> 48</span></a><span class="sd">        - **heads** (`HeadsTIL` | `HeadsCIL`): output heads.</span>
</span><span id="HAT-49"><a href="#HAT-49"><span class="linenos"> 49</span></a><span class="sd">        - **adjustment_mode** (`str`): the strategy of adjustment i.e. the mode of gradient clipping, should be one of the following:</span>
</span><span id="HAT-50"><a href="#HAT-50"><span class="linenos"> 50</span></a><span class="sd">            1. &#39;hat&#39;: set the gradients of parameters linking to masked units to zero. This is the way that HAT does, which fixes the part of network for previous tasks completely. See equation (2) in chapter 2.3 &quot;Network Training&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="HAT-51"><a href="#HAT-51"><span class="linenos"> 51</span></a><span class="sd">            2. &#39;hat_random&#39;: set the gradients of parameters linking to masked units to random 0-1 values. See the &quot;Baselines&quot; section in chapter 4.1 in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="HAT-52"><a href="#HAT-52"><span class="linenos"> 52</span></a><span class="sd">            3. &#39;hat_const_alpha&#39;: set the gradients of parameters linking to masked units to a constant value of `alpha`. See the &quot;Baselines&quot; section in chapter 4.1 in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="HAT-53"><a href="#HAT-53"><span class="linenos"> 53</span></a><span class="sd">            4. &#39;hat_const_1&#39;: set the gradients of parameters linking to masked units to a constant value of 1, which means no gradient constraint on any parameter at all. See the &quot;Baselines&quot; section in chapter 4.1 in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="HAT-54"><a href="#HAT-54"><span class="linenos"> 54</span></a><span class="sd">        - **s_max** (`float`): hyperparameter, the maximum scaling factor in the gate function. See chapter 2.4 &quot;Hard Attention Training&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="HAT-55"><a href="#HAT-55"><span class="linenos"> 55</span></a><span class="sd">        - **clamp_threshold** (`float`): the threshold for task embedding gradient compensation. See chapter 2.5 &quot;Embedding Gradient Compensation&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="HAT-56"><a href="#HAT-56"><span class="linenos"> 56</span></a><span class="sd">        - **mask_sparsity_reg_factor** (`float`): hyperparameter, the regularisation factor for mask sparsity.</span>
</span><span id="HAT-57"><a href="#HAT-57"><span class="linenos"> 57</span></a><span class="sd">        - **mask_sparsity_reg_mode** (`str`): the mode of mask sparsity regularisation, should be one of the following:</span>
</span><span id="HAT-58"><a href="#HAT-58"><span class="linenos"> 58</span></a><span class="sd">            1. &#39;original&#39; (default): the original mask sparsity regularisation in HAT paper.</span>
</span><span id="HAT-59"><a href="#HAT-59"><span class="linenos"> 59</span></a><span class="sd">            2. &#39;cross&#39;: the cross version mask sparsity regularisation.</span>
</span><span id="HAT-60"><a href="#HAT-60"><span class="linenos"> 60</span></a><span class="sd">        - **task_embedding_init_mode** (`str`): the initialisation mode for task embeddings, should be one of the following:</span>
</span><span id="HAT-61"><a href="#HAT-61"><span class="linenos"> 61</span></a><span class="sd">            1. &#39;N01&#39; (default): standard normal distribution $N(0, 1)$.</span>
</span><span id="HAT-62"><a href="#HAT-62"><span class="linenos"> 62</span></a><span class="sd">            2. &#39;U-11&#39;: uniform distribution $U(-1, 1)$.</span>
</span><span id="HAT-63"><a href="#HAT-63"><span class="linenos"> 63</span></a><span class="sd">            3. &#39;U01&#39;: uniform distribution $U(0, 1)$.</span>
</span><span id="HAT-64"><a href="#HAT-64"><span class="linenos"> 64</span></a><span class="sd">            4. &#39;U-10&#39;: uniform distribution $U(-1, 0)$.</span>
</span><span id="HAT-65"><a href="#HAT-65"><span class="linenos"> 65</span></a><span class="sd">            5. &#39;last&#39;: inherit task embedding from last task.</span>
</span><span id="HAT-66"><a href="#HAT-66"><span class="linenos"> 66</span></a><span class="sd">        - **alpha** (`float` | `None`): the `alpha` in the &#39;HAT-const-alpha&#39; mode. See the &quot;Baselines&quot; section in chapter 4.1 in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9). It applies only when adjustment_mode is &#39;hat_const_alpha&#39;.</span>
</span><span id="HAT-67"><a href="#HAT-67"><span class="linenos"> 67</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HAT-68"><a href="#HAT-68"><span class="linenos"> 68</span></a>        <span class="n">CLAlgorithm</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">backbone</span><span class="o">=</span><span class="n">backbone</span><span class="p">,</span> <span class="n">heads</span><span class="o">=</span><span class="n">heads</span><span class="p">)</span>
</span><span id="HAT-69"><a href="#HAT-69"><span class="linenos"> 69</span></a>
</span><span id="HAT-70"><a href="#HAT-70"><span class="linenos"> 70</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">=</span> <span class="n">adjustment_mode</span>
</span><span id="HAT-71"><a href="#HAT-71"><span class="linenos"> 71</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the adjustment mode for gradient clipping.&quot;&quot;&quot;</span>
</span><span id="HAT-72"><a href="#HAT-72"><span class="linenos"> 72</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">s_max</span> <span class="o">=</span> <span class="n">s_max</span>
</span><span id="HAT-73"><a href="#HAT-73"><span class="linenos"> 73</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store s_max. &quot;&quot;&quot;</span>
</span><span id="HAT-74"><a href="#HAT-74"><span class="linenos"> 74</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">clamp_threshold</span> <span class="o">=</span> <span class="n">clamp_threshold</span>
</span><span id="HAT-75"><a href="#HAT-75"><span class="linenos"> 75</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the clamp threshold for task embedding gradient compensation.&quot;&quot;&quot;</span>
</span><span id="HAT-76"><a href="#HAT-76"><span class="linenos"> 76</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">mask_sparsity_reg_factor</span> <span class="o">=</span> <span class="n">mask_sparsity_reg_factor</span>
</span><span id="HAT-77"><a href="#HAT-77"><span class="linenos"> 77</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the mask sparsity regularisation factor.&quot;&quot;&quot;</span>
</span><span id="HAT-78"><a href="#HAT-78"><span class="linenos"> 78</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">mask_sparsity_reg_mode</span> <span class="o">=</span> <span class="n">mask_sparsity_reg_mode</span>
</span><span id="HAT-79"><a href="#HAT-79"><span class="linenos"> 79</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the mask sparsity regularisation mode.&quot;&quot;&quot;</span>
</span><span id="HAT-80"><a href="#HAT-80"><span class="linenos"> 80</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">mark_sparsity_reg</span> <span class="o">=</span> <span class="n">HATMaskSparsityReg</span><span class="p">(</span>
</span><span id="HAT-81"><a href="#HAT-81"><span class="linenos"> 81</span></a>            <span class="n">factor</span><span class="o">=</span><span class="n">mask_sparsity_reg_factor</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">mask_sparsity_reg_mode</span>
</span><span id="HAT-82"><a href="#HAT-82"><span class="linenos"> 82</span></a>        <span class="p">)</span>
</span><span id="HAT-83"><a href="#HAT-83"><span class="linenos"> 83</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialise and store the mask sparsity regulariser.&quot;&quot;&quot;</span>
</span><span id="HAT-84"><a href="#HAT-84"><span class="linenos"> 84</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_init_mode</span> <span class="o">=</span> <span class="n">task_embedding_init_mode</span>
</span><span id="HAT-85"><a href="#HAT-85"><span class="linenos"> 85</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the task embedding initialisation mode.&quot;&quot;&quot;</span>
</span><span id="HAT-86"><a href="#HAT-86"><span class="linenos"> 86</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span> <span class="k">if</span> <span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;hat_const_alpha&quot;</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="HAT-87"><a href="#HAT-87"><span class="linenos"> 87</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the alpha for `hat_const_alpha`.&quot;&quot;&quot;</span>
</span><span id="HAT-88"><a href="#HAT-88"><span class="linenos"> 88</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="HAT-89"><a href="#HAT-89"><span class="linenos"> 89</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;HAT doesn&#39;t use the epsilon for `hat_const_alpha`. We still set it here to be consistent with the `epsilon` in `clip_grad_by_adjustment()` method in `HATMaskBackbone`.&quot;&quot;&quot;</span>
</span><span id="HAT-90"><a href="#HAT-90"><span class="linenos"> 90</span></a>
</span><span id="HAT-91"><a href="#HAT-91"><span class="linenos"> 91</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">masks</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="HAT-92"><a href="#HAT-92"><span class="linenos"> 92</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the binary attention mask of each previous task gated from the task embedding. Keys are task IDs (string type) and values are the corresponding mask. Each mask is a dict where keys are layer names and values are the binary mask tensor for the layer. The mask tensor has size (number of units). &quot;&quot;&quot;</span>
</span><span id="HAT-93"><a href="#HAT-93"><span class="linenos"> 93</span></a>
</span><span id="HAT-94"><a href="#HAT-94"><span class="linenos"> 94</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">cumulative_mask_for_previous_tasks</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="HAT-95"><a href="#HAT-95"><span class="linenos"> 95</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the cumulative binary attention mask $\mathrm{M}^{&lt;t}$ of previous tasks $1,\cdots, t-1$, gated from the task embedding. Keys are task IDs and values are the corresponding cumulative mask. Each cumulative mask is a dict where keys are layer names and values are the binary mask tensor for the layer. The mask tensor has size (number of units). &quot;&quot;&quot;</span>
</span><span id="HAT-96"><a href="#HAT-96"><span class="linenos"> 96</span></a>
</span><span id="HAT-97"><a href="#HAT-97"><span class="linenos"> 97</span></a>        <span class="c1"># set manual optimisation</span>
</span><span id="HAT-98"><a href="#HAT-98"><span class="linenos"> 98</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">automatic_optimization</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="HAT-99"><a href="#HAT-99"><span class="linenos"> 99</span></a>
</span><span id="HAT-100"><a href="#HAT-100"><span class="linenos">100</span></a>        <span class="n">HAT</span><span class="o">.</span><span class="n">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</span><span id="HAT-101"><a href="#HAT-101"><span class="linenos">101</span></a>
</span><span id="HAT-102"><a href="#HAT-102"><span class="linenos">102</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HAT-103"><a href="#HAT-103"><span class="linenos">103</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Check the sanity of the arguments.</span>
</span><span id="HAT-104"><a href="#HAT-104"><span class="linenos">104</span></a>
</span><span id="HAT-105"><a href="#HAT-105"><span class="linenos">105</span></a><span class="sd">        **Raises:**</span>
</span><span id="HAT-106"><a href="#HAT-106"><span class="linenos">106</span></a><span class="sd">        - **ValueError**: when backbone is not designed for HAT, or the `mask_sparsity_reg_mode` or `task_embedding_init_mode` is not one of the valid options. Also, if `alpha` is not given when `adjustment_mode` is &#39;hat_const_alpha&#39;.</span>
</span><span id="HAT-107"><a href="#HAT-107"><span class="linenos">107</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HAT-108"><a href="#HAT-108"><span class="linenos">108</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="p">,</span> <span class="n">HATMaskBackbone</span><span class="p">):</span>
</span><span id="HAT-109"><a href="#HAT-109"><span class="linenos">109</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The backbone should be an instance of HATMaskBackbone.&quot;</span><span class="p">)</span>
</span><span id="HAT-110"><a href="#HAT-110"><span class="linenos">110</span></a>
</span><span id="HAT-111"><a href="#HAT-111"><span class="linenos">111</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask_sparsity_reg_mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;original&quot;</span><span class="p">,</span> <span class="s2">&quot;cross&quot;</span><span class="p">]:</span>
</span><span id="HAT-112"><a href="#HAT-112"><span class="linenos">112</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="HAT-113"><a href="#HAT-113"><span class="linenos">113</span></a>                <span class="s2">&quot;The mask_sparsity_reg_mode should be one of &#39;original&#39;, &#39;cross&#39;.&quot;</span>
</span><span id="HAT-114"><a href="#HAT-114"><span class="linenos">114</span></a>            <span class="p">)</span>
</span><span id="HAT-115"><a href="#HAT-115"><span class="linenos">115</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_init_mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span>
</span><span id="HAT-116"><a href="#HAT-116"><span class="linenos">116</span></a>            <span class="s2">&quot;N01&quot;</span><span class="p">,</span>
</span><span id="HAT-117"><a href="#HAT-117"><span class="linenos">117</span></a>            <span class="s2">&quot;U01&quot;</span><span class="p">,</span>
</span><span id="HAT-118"><a href="#HAT-118"><span class="linenos">118</span></a>            <span class="s2">&quot;U-10&quot;</span><span class="p">,</span>
</span><span id="HAT-119"><a href="#HAT-119"><span class="linenos">119</span></a>            <span class="s2">&quot;masked&quot;</span><span class="p">,</span>
</span><span id="HAT-120"><a href="#HAT-120"><span class="linenos">120</span></a>            <span class="s2">&quot;unmasked&quot;</span><span class="p">,</span>
</span><span id="HAT-121"><a href="#HAT-121"><span class="linenos">121</span></a>        <span class="p">]:</span>
</span><span id="HAT-122"><a href="#HAT-122"><span class="linenos">122</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="HAT-123"><a href="#HAT-123"><span class="linenos">123</span></a>                <span class="s2">&quot;The task_embedding_init_mode should be one of &#39;N01&#39;, &#39;U01&#39;, &#39;U-10&#39;, &#39;masked&#39;, &#39;unmasked&#39;.&quot;</span>
</span><span id="HAT-124"><a href="#HAT-124"><span class="linenos">124</span></a>            <span class="p">)</span>
</span><span id="HAT-125"><a href="#HAT-125"><span class="linenos">125</span></a>
</span><span id="HAT-126"><a href="#HAT-126"><span class="linenos">126</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;hat_const_alpha&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HAT-127"><a href="#HAT-127"><span class="linenos">127</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="HAT-128"><a href="#HAT-128"><span class="linenos">128</span></a>                <span class="s2">&quot;Alpha should be given when the adjustment_mode is &#39;hat_const_alpha&#39;.&quot;</span>
</span><span id="HAT-129"><a href="#HAT-129"><span class="linenos">129</span></a>            <span class="p">)</span>
</span><span id="HAT-130"><a href="#HAT-130"><span class="linenos">130</span></a>
</span><span id="HAT-131"><a href="#HAT-131"><span class="linenos">131</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_start</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HAT-132"><a href="#HAT-132"><span class="linenos">132</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialise the task embedding before training the next task and initialise the cumulative mask at the beginning of first task.&quot;&quot;&quot;</span>
</span><span id="HAT-133"><a href="#HAT-133"><span class="linenos">133</span></a>
</span><span id="HAT-134"><a href="#HAT-134"><span class="linenos">134</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">initialise_task_embedding</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_init_mode</span><span class="p">)</span>
</span><span id="HAT-135"><a href="#HAT-135"><span class="linenos">135</span></a>
</span><span id="HAT-136"><a href="#HAT-136"><span class="linenos">136</span></a>        <span class="c1"># initialise the cumulative mask at the beginning of first task. This should not be called in `__init__()` method as the `self.device` is not available at that time.</span>
</span><span id="HAT-137"><a href="#HAT-137"><span class="linenos">137</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="HAT-138"><a href="#HAT-138"><span class="linenos">138</span></a>            <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="HAT-139"><a href="#HAT-139"><span class="linenos">139</span></a>                <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span>
</span><span id="HAT-140"><a href="#HAT-140"><span class="linenos">140</span></a>                    <span class="n">layer_name</span>
</span><span id="HAT-141"><a href="#HAT-141"><span class="linenos">141</span></a>                <span class="p">)</span>  <span class="c1"># get the layer by its name</span>
</span><span id="HAT-142"><a href="#HAT-142"><span class="linenos">142</span></a>                <span class="n">num_units</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="HAT-143"><a href="#HAT-143"><span class="linenos">143</span></a>
</span><span id="HAT-144"><a href="#HAT-144"><span class="linenos">144</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">cumulative_mask_for_previous_tasks</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
</span><span id="HAT-145"><a href="#HAT-145"><span class="linenos">145</span></a>                    <span class="n">num_units</span>
</span><span id="HAT-146"><a href="#HAT-146"><span class="linenos">146</span></a>                <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
</span><span id="HAT-147"><a href="#HAT-147"><span class="linenos">147</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
</span><span id="HAT-148"><a href="#HAT-148"><span class="linenos">148</span></a>                <span class="p">)</span>  <span class="c1"># the cumulative mask $\mathrm{M}^{&lt;t}$ is initialised as zeros mask ($t = 1$). See equation (2) in chapter 3 in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9), or equation (5) in chapter 2.6 &quot;Promoting Low Capacity Usage&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="HAT-149"><a href="#HAT-149"><span class="linenos">149</span></a>
</span><span id="HAT-150"><a href="#HAT-150"><span class="linenos">150</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">clip_grad_by_adjustment</span><span class="p">(</span>
</span><span id="HAT-151"><a href="#HAT-151"><span class="linenos">151</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="HAT-152"><a href="#HAT-152"><span class="linenos">152</span></a>        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
</span><span id="HAT-153"><a href="#HAT-153"><span class="linenos">153</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="HAT-154"><a href="#HAT-154"><span class="linenos">154</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Clip the gradients by the adjustment rate.</span>
</span><span id="HAT-155"><a href="#HAT-155"><span class="linenos">155</span></a>
</span><span id="HAT-156"><a href="#HAT-156"><span class="linenos">156</span></a><span class="sd">        Note that as the task embedding fully covers every layer in the backbone network, no parameters are left out of this system. This applies not only the parameters in between layers with task embedding, but also those before the first layer. We designed it seperately in the codes.</span>
</span><span id="HAT-157"><a href="#HAT-157"><span class="linenos">157</span></a>
</span><span id="HAT-158"><a href="#HAT-158"><span class="linenos">158</span></a><span class="sd">        Network capacity is measured along with this method. Network capacity is defined as the average adjustment rate over all parameters. See chapter 4.1 in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="HAT-159"><a href="#HAT-159"><span class="linenos">159</span></a>
</span><span id="HAT-160"><a href="#HAT-160"><span class="linenos">160</span></a>
</span><span id="HAT-161"><a href="#HAT-161"><span class="linenos">161</span></a><span class="sd">        **Returns:**</span>
</span><span id="HAT-162"><a href="#HAT-162"><span class="linenos">162</span></a><span class="sd">        - **capacity** (`Tensor`): the calculated network capacity.</span>
</span><span id="HAT-163"><a href="#HAT-163"><span class="linenos">163</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HAT-164"><a href="#HAT-164"><span class="linenos">164</span></a>
</span><span id="HAT-165"><a href="#HAT-165"><span class="linenos">165</span></a>        <span class="c1"># initialise network capacity metric</span>
</span><span id="HAT-166"><a href="#HAT-166"><span class="linenos">166</span></a>        <span class="n">capacity</span> <span class="o">=</span> <span class="n">HATNetworkCapacity</span><span class="p">()</span>
</span><span id="HAT-167"><a href="#HAT-167"><span class="linenos">167</span></a>
</span><span id="HAT-168"><a href="#HAT-168"><span class="linenos">168</span></a>        <span class="c1"># Calculate the adjustment rate for gradients of the parameters, both weights and biases (if exists)</span>
</span><span id="HAT-169"><a href="#HAT-169"><span class="linenos">169</span></a>        <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="HAT-170"><a href="#HAT-170"><span class="linenos">170</span></a>
</span><span id="HAT-171"><a href="#HAT-171"><span class="linenos">171</span></a>            <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span>
</span><span id="HAT-172"><a href="#HAT-172"><span class="linenos">172</span></a>                <span class="n">layer_name</span>
</span><span id="HAT-173"><a href="#HAT-173"><span class="linenos">173</span></a>            <span class="p">)</span>  <span class="c1"># get the layer by its name</span>
</span><span id="HAT-174"><a href="#HAT-174"><span class="linenos">174</span></a>
</span><span id="HAT-175"><a href="#HAT-175"><span class="linenos">175</span></a>            <span class="c1"># placeholder for the adjustment rate to avoid the error of using it before assignment</span>
</span><span id="HAT-176"><a href="#HAT-176"><span class="linenos">176</span></a>            <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="HAT-177"><a href="#HAT-177"><span class="linenos">177</span></a>            <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="HAT-178"><a href="#HAT-178"><span class="linenos">178</span></a>
</span><span id="HAT-179"><a href="#HAT-179"><span class="linenos">179</span></a>            <span class="n">weight_mask</span><span class="p">,</span> <span class="n">bias_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_measure_parameter_wise</span><span class="p">(</span>
</span><span id="HAT-180"><a href="#HAT-180"><span class="linenos">180</span></a>                <span class="n">unit_wise_measure</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">cumulative_mask_for_previous_tasks</span><span class="p">,</span>
</span><span id="HAT-181"><a href="#HAT-181"><span class="linenos">181</span></a>                <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="HAT-182"><a href="#HAT-182"><span class="linenos">182</span></a>                <span class="n">aggregation</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span>
</span><span id="HAT-183"><a href="#HAT-183"><span class="linenos">183</span></a>            <span class="p">)</span>
</span><span id="HAT-184"><a href="#HAT-184"><span class="linenos">184</span></a>
</span><span id="HAT-185"><a href="#HAT-185"><span class="linenos">185</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;hat&quot;</span><span class="p">:</span>
</span><span id="HAT-186"><a href="#HAT-186"><span class="linenos">186</span></a>                <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">weight_mask</span>
</span><span id="HAT-187"><a href="#HAT-187"><span class="linenos">187</span></a>                <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">bias_mask</span>
</span><span id="HAT-188"><a href="#HAT-188"><span class="linenos">188</span></a>
</span><span id="HAT-189"><a href="#HAT-189"><span class="linenos">189</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;hat_random&quot;</span><span class="p">:</span>
</span><span id="HAT-190"><a href="#HAT-190"><span class="linenos">190</span></a>                <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand_like</span><span class="p">(</span><span class="n">weight_mask</span><span class="p">)</span> <span class="o">*</span> <span class="n">weight_mask</span> <span class="o">+</span> <span class="p">(</span>
</span><span id="HAT-191"><a href="#HAT-191"><span class="linenos">191</span></a>                    <span class="mi">1</span> <span class="o">-</span> <span class="n">weight_mask</span>
</span><span id="HAT-192"><a href="#HAT-192"><span class="linenos">192</span></a>                <span class="p">)</span>
</span><span id="HAT-193"><a href="#HAT-193"><span class="linenos">193</span></a>                <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand_like</span><span class="p">(</span><span class="n">bias_mask</span><span class="p">)</span> <span class="o">*</span> <span class="n">bias_mask</span> <span class="o">+</span> <span class="p">(</span>
</span><span id="HAT-194"><a href="#HAT-194"><span class="linenos">194</span></a>                    <span class="mi">1</span> <span class="o">-</span> <span class="n">bias_mask</span>
</span><span id="HAT-195"><a href="#HAT-195"><span class="linenos">195</span></a>                <span class="p">)</span>
</span><span id="HAT-196"><a href="#HAT-196"><span class="linenos">196</span></a>
</span><span id="HAT-197"><a href="#HAT-197"><span class="linenos">197</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;hat_const_alpha&quot;</span><span class="p">:</span>
</span><span id="HAT-198"><a href="#HAT-198"><span class="linenos">198</span></a>                <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span>
</span><span id="HAT-199"><a href="#HAT-199"><span class="linenos">199</span></a>                    <span class="n">weight_mask</span>
</span><span id="HAT-200"><a href="#HAT-200"><span class="linenos">200</span></a>                <span class="p">)</span> <span class="o">*</span> <span class="n">weight_mask</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">weight_mask</span><span class="p">)</span>
</span><span id="HAT-201"><a href="#HAT-201"><span class="linenos">201</span></a>                <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span>
</span><span id="HAT-202"><a href="#HAT-202"><span class="linenos">202</span></a>                    <span class="n">bias_mask</span>
</span><span id="HAT-203"><a href="#HAT-203"><span class="linenos">203</span></a>                <span class="p">)</span> <span class="o">*</span> <span class="n">bias_mask</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">bias_mask</span><span class="p">)</span>
</span><span id="HAT-204"><a href="#HAT-204"><span class="linenos">204</span></a>
</span><span id="HAT-205"><a href="#HAT-205"><span class="linenos">205</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;hat_const_1&quot;</span><span class="p">:</span>
</span><span id="HAT-206"><a href="#HAT-206"><span class="linenos">206</span></a>                <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">weight_mask</span><span class="p">)</span> <span class="o">*</span> <span class="n">weight_mask</span> <span class="o">+</span> <span class="p">(</span>
</span><span id="HAT-207"><a href="#HAT-207"><span class="linenos">207</span></a>                    <span class="mi">1</span> <span class="o">-</span> <span class="n">weight_mask</span>
</span><span id="HAT-208"><a href="#HAT-208"><span class="linenos">208</span></a>                <span class="p">)</span>
</span><span id="HAT-209"><a href="#HAT-209"><span class="linenos">209</span></a>                <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">bias_mask</span><span class="p">)</span> <span class="o">*</span> <span class="n">bias_mask</span> <span class="o">+</span> <span class="p">(</span>
</span><span id="HAT-210"><a href="#HAT-210"><span class="linenos">210</span></a>                    <span class="mi">1</span> <span class="o">-</span> <span class="n">bias_mask</span>
</span><span id="HAT-211"><a href="#HAT-211"><span class="linenos">211</span></a>                <span class="p">)</span>
</span><span id="HAT-212"><a href="#HAT-212"><span class="linenos">212</span></a>
</span><span id="HAT-213"><a href="#HAT-213"><span class="linenos">213</span></a>            <span class="c1"># apply the adjustment rate to the gradients</span>
</span><span id="HAT-214"><a href="#HAT-214"><span class="linenos">214</span></a>            <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">adjustment_rate_weight_layer</span>
</span><span id="HAT-215"><a href="#HAT-215"><span class="linenos">215</span></a>            <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HAT-216"><a href="#HAT-216"><span class="linenos">216</span></a>                <span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">adjustment_rate_bias_layer</span>
</span><span id="HAT-217"><a href="#HAT-217"><span class="linenos">217</span></a>
</span><span id="HAT-218"><a href="#HAT-218"><span class="linenos">218</span></a>            <span class="c1"># update network capacity metric</span>
</span><span id="HAT-219"><a href="#HAT-219"><span class="linenos">219</span></a>            <span class="n">capacity</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">adjustment_rate_weight_layer</span><span class="p">,</span> <span class="n">adjustment_rate_bias_layer</span><span class="p">)</span>
</span><span id="HAT-220"><a href="#HAT-220"><span class="linenos">220</span></a>
</span><span id="HAT-221"><a href="#HAT-221"><span class="linenos">221</span></a>        <span class="k">return</span> <span class="n">capacity</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
</span><span id="HAT-222"><a href="#HAT-222"><span class="linenos">222</span></a>
</span><span id="HAT-223"><a href="#HAT-223"><span class="linenos">223</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">compensate_task_embedding_gradients</span><span class="p">(</span>
</span><span id="HAT-224"><a href="#HAT-224"><span class="linenos">224</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="HAT-225"><a href="#HAT-225"><span class="linenos">225</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HAT-226"><a href="#HAT-226"><span class="linenos">226</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HAT-227"><a href="#HAT-227"><span class="linenos">227</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HAT-228"><a href="#HAT-228"><span class="linenos">228</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Compensate the gradients of task embeddings during training. See chapter 2.5 &quot;Embedding Gradient Compensation&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="HAT-229"><a href="#HAT-229"><span class="linenos">229</span></a>
</span><span id="HAT-230"><a href="#HAT-230"><span class="linenos">230</span></a><span class="sd">        **Args:**</span>
</span><span id="HAT-231"><a href="#HAT-231"><span class="linenos">231</span></a><span class="sd">        - **batch_idx** (`int`): the current training batch index.</span>
</span><span id="HAT-232"><a href="#HAT-232"><span class="linenos">232</span></a><span class="sd">        - **num_batches** (`int`): the total number of training batches.</span>
</span><span id="HAT-233"><a href="#HAT-233"><span class="linenos">233</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HAT-234"><a href="#HAT-234"><span class="linenos">234</span></a>
</span><span id="HAT-235"><a href="#HAT-235"><span class="linenos">235</span></a>        <span class="k">for</span> <span class="n">te</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
</span><span id="HAT-236"><a href="#HAT-236"><span class="linenos">236</span></a>            <span class="n">anneal_scalar</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_max</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">s_max</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_max</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span>
</span><span id="HAT-237"><a href="#HAT-237"><span class="linenos">237</span></a>                <span class="n">batch_idx</span> <span class="o">-</span> <span class="mi">1</span>
</span><span id="HAT-238"><a href="#HAT-238"><span class="linenos">238</span></a>            <span class="p">)</span> <span class="o">/</span> <span class="p">(</span>
</span><span id="HAT-239"><a href="#HAT-239"><span class="linenos">239</span></a>                <span class="n">num_batches</span> <span class="o">-</span> <span class="mi">1</span>
</span><span id="HAT-240"><a href="#HAT-240"><span class="linenos">240</span></a>            <span class="p">)</span>  <span class="c1"># see equation (3) in chapter 2.4 &quot;Hard Attention Training&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a)</span>
</span><span id="HAT-241"><a href="#HAT-241"><span class="linenos">241</span></a>
</span><span id="HAT-242"><a href="#HAT-242"><span class="linenos">242</span></a>            <span class="n">num</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="HAT-243"><a href="#HAT-243"><span class="linenos">243</span></a>                <span class="n">torch</span><span class="o">.</span><span class="n">cosh</span><span class="p">(</span>
</span><span id="HAT-244"><a href="#HAT-244"><span class="linenos">244</span></a>                    <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span>
</span><span id="HAT-245"><a href="#HAT-245"><span class="linenos">245</span></a>                        <span class="n">anneal_scalar</span> <span class="o">*</span> <span class="n">te</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">,</span>
</span><span id="HAT-246"><a href="#HAT-246"><span class="linenos">246</span></a>                        <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">clamp_threshold</span><span class="p">,</span>
</span><span id="HAT-247"><a href="#HAT-247"><span class="linenos">247</span></a>                        <span class="bp">self</span><span class="o">.</span><span class="n">clamp_threshold</span><span class="p">,</span>
</span><span id="HAT-248"><a href="#HAT-248"><span class="linenos">248</span></a>                    <span class="p">)</span>
</span><span id="HAT-249"><a href="#HAT-249"><span class="linenos">249</span></a>                <span class="p">)</span>
</span><span id="HAT-250"><a href="#HAT-250"><span class="linenos">250</span></a>                <span class="o">+</span> <span class="mi">1</span>
</span><span id="HAT-251"><a href="#HAT-251"><span class="linenos">251</span></a>            <span class="p">)</span>
</span><span id="HAT-252"><a href="#HAT-252"><span class="linenos">252</span></a>
</span><span id="HAT-253"><a href="#HAT-253"><span class="linenos">253</span></a>            <span class="n">den</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cosh</span><span class="p">(</span><span class="n">te</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="HAT-254"><a href="#HAT-254"><span class="linenos">254</span></a>
</span><span id="HAT-255"><a href="#HAT-255"><span class="linenos">255</span></a>            <span class="n">compensation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_max</span> <span class="o">/</span> <span class="n">anneal_scalar</span> <span class="o">*</span> <span class="n">num</span> <span class="o">/</span> <span class="n">den</span>
</span><span id="HAT-256"><a href="#HAT-256"><span class="linenos">256</span></a>
</span><span id="HAT-257"><a href="#HAT-257"><span class="linenos">257</span></a>            <span class="n">te</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">compensation</span>
</span><span id="HAT-258"><a href="#HAT-258"><span class="linenos">258</span></a>
</span><span id="HAT-259"><a href="#HAT-259"><span class="linenos">259</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="HAT-260"><a href="#HAT-260"><span class="linenos">260</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="HAT-261"><a href="#HAT-261"><span class="linenos">261</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="HAT-262"><a href="#HAT-262"><span class="linenos">262</span></a>        <span class="n">stage</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="HAT-263"><a href="#HAT-263"><span class="linenos">263</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HAT-264"><a href="#HAT-264"><span class="linenos">264</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HAT-265"><a href="#HAT-265"><span class="linenos">265</span></a>        <span class="n">task_id</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HAT-266"><a href="#HAT-266"><span class="linenos">266</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
</span><span id="HAT-267"><a href="#HAT-267"><span class="linenos">267</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The forward pass for data from task `task_id`. Note that it is nothing to do with `forward()` method in `nn.Module`.</span>
</span><span id="HAT-268"><a href="#HAT-268"><span class="linenos">268</span></a>
</span><span id="HAT-269"><a href="#HAT-269"><span class="linenos">269</span></a><span class="sd">        **Args:**</span>
</span><span id="HAT-270"><a href="#HAT-270"><span class="linenos">270</span></a><span class="sd">        - **input** (`Tensor`): The input tensor from data.</span>
</span><span id="HAT-271"><a href="#HAT-271"><span class="linenos">271</span></a><span class="sd">        - **stage** (`str`): the stage of the forward pass, should be one of the following:</span>
</span><span id="HAT-272"><a href="#HAT-272"><span class="linenos">272</span></a><span class="sd">            1. &#39;train&#39;: training stage.</span>
</span><span id="HAT-273"><a href="#HAT-273"><span class="linenos">273</span></a><span class="sd">            2. &#39;validation&#39;: validation stage.</span>
</span><span id="HAT-274"><a href="#HAT-274"><span class="linenos">274</span></a><span class="sd">            3. &#39;test&#39;: testing stage.</span>
</span><span id="HAT-275"><a href="#HAT-275"><span class="linenos">275</span></a><span class="sd">        - **batch_idx** (`int` | `None`): the current batch index. Applies only to training stage. For other stages, it is default `None`.</span>
</span><span id="HAT-276"><a href="#HAT-276"><span class="linenos">276</span></a><span class="sd">        - **num_batches** (`int` | `None`): the total number of batches. Applies only to training stage. For other stages, it is default `None`.</span>
</span><span id="HAT-277"><a href="#HAT-277"><span class="linenos">277</span></a><span class="sd">        - **task_id** (`int`| `None`): the task ID where the data are from. If the stage is &#39;train&#39; or &#39;validation&#39;, it should be the current task `self.task_id`. If stage is &#39;test&#39;, it could be from any seen task. In TIL, the task IDs of test data are provided thus this argument can be used. HAT algorithm works only for TIL.</span>
</span><span id="HAT-278"><a href="#HAT-278"><span class="linenos">278</span></a>
</span><span id="HAT-279"><a href="#HAT-279"><span class="linenos">279</span></a><span class="sd">        **Returns:**</span>
</span><span id="HAT-280"><a href="#HAT-280"><span class="linenos">280</span></a><span class="sd">        - **logits** (`Tensor`): the output logits tensor.</span>
</span><span id="HAT-281"><a href="#HAT-281"><span class="linenos">281</span></a><span class="sd">        - **mask** (`dict[str, Tensor]`): the mask for the current task. Key (`str`) is layer name, value (`Tensor`) is the mask tensor. The mask tensor has size (number of units).</span>
</span><span id="HAT-282"><a href="#HAT-282"><span class="linenos">282</span></a><span class="sd">        - **activations** (`dict[str, Tensor]`): the hidden features (after activation) in each weighted layer. Key (`str`) is the weighted layer name, value (`Tensor`) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes. Although HAT algorithm does not need this, it is still provided for API consistence for other HAT-based algorithms inherited this `forward()` method of `HAT` class.</span>
</span><span id="HAT-283"><a href="#HAT-283"><span class="linenos">283</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HAT-284"><a href="#HAT-284"><span class="linenos">284</span></a>        <span class="n">feature</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="p">(</span>
</span><span id="HAT-285"><a href="#HAT-285"><span class="linenos">285</span></a>            <span class="nb">input</span><span class="p">,</span>
</span><span id="HAT-286"><a href="#HAT-286"><span class="linenos">286</span></a>            <span class="n">stage</span><span class="o">=</span><span class="n">stage</span><span class="p">,</span>
</span><span id="HAT-287"><a href="#HAT-287"><span class="linenos">287</span></a>            <span class="n">s_max</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">s_max</span> <span class="k">if</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;train&quot;</span> <span class="ow">or</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;validation&quot;</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HAT-288"><a href="#HAT-288"><span class="linenos">288</span></a>            <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span> <span class="k">if</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;train&quot;</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HAT-289"><a href="#HAT-289"><span class="linenos">289</span></a>            <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span> <span class="k">if</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;train&quot;</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HAT-290"><a href="#HAT-290"><span class="linenos">290</span></a>            <span class="n">test_mask</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">masks</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">task_id</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="k">if</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;test&quot;</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HAT-291"><a href="#HAT-291"><span class="linenos">291</span></a>        <span class="p">)</span>
</span><span id="HAT-292"><a href="#HAT-292"><span class="linenos">292</span></a>        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">heads</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="n">task_id</span><span class="p">)</span>
</span><span id="HAT-293"><a href="#HAT-293"><span class="linenos">293</span></a>
</span><span id="HAT-294"><a href="#HAT-294"><span class="linenos">294</span></a>        <span class="k">return</span> <span class="n">logits</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">activations</span>
</span><span id="HAT-295"><a href="#HAT-295"><span class="linenos">295</span></a>
</span><span id="HAT-296"><a href="#HAT-296"><span class="linenos">296</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
</span><span id="HAT-297"><a href="#HAT-297"><span class="linenos">297</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Training step for current task `self.task_id`.</span>
</span><span id="HAT-298"><a href="#HAT-298"><span class="linenos">298</span></a>
</span><span id="HAT-299"><a href="#HAT-299"><span class="linenos">299</span></a><span class="sd">        **Args:**</span>
</span><span id="HAT-300"><a href="#HAT-300"><span class="linenos">300</span></a><span class="sd">        - **batch** (`Any`): a batch of training data.</span>
</span><span id="HAT-301"><a href="#HAT-301"><span class="linenos">301</span></a><span class="sd">        - **batch_idx** (`int`): the index of the batch. Used for calculating annealed scalar in HAT. See chapter 2.4 &quot;Hard Attention Training&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="HAT-302"><a href="#HAT-302"><span class="linenos">302</span></a>
</span><span id="HAT-303"><a href="#HAT-303"><span class="linenos">303</span></a><span class="sd">        **Returns:**</span>
</span><span id="HAT-304"><a href="#HAT-304"><span class="linenos">304</span></a><span class="sd">        - **outputs** (`dict[str, Tensor]`): a dictionary contains loss and other metrics from this training step. Key (`str`) is the metrics name, value (`Tensor`) is the metrics. Must include the key &#39;loss&#39; which is total loss in the case of automatic optimization, according to PyTorch Lightning docs. For HAT, it includes &#39;mask&#39; and &#39;capacity&#39; for logging.</span>
</span><span id="HAT-305"><a href="#HAT-305"><span class="linenos">305</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HAT-306"><a href="#HAT-306"><span class="linenos">306</span></a>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
</span><span id="HAT-307"><a href="#HAT-307"><span class="linenos">307</span></a>
</span><span id="HAT-308"><a href="#HAT-308"><span class="linenos">308</span></a>        <span class="c1"># zero the gradients before forward pass in manual optimisation mode</span>
</span><span id="HAT-309"><a href="#HAT-309"><span class="linenos">309</span></a>        <span class="n">opt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizers</span><span class="p">()</span>
</span><span id="HAT-310"><a href="#HAT-310"><span class="linenos">310</span></a>        <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</span><span id="HAT-311"><a href="#HAT-311"><span class="linenos">311</span></a>
</span><span id="HAT-312"><a href="#HAT-312"><span class="linenos">312</span></a>        <span class="c1"># classification loss</span>
</span><span id="HAT-313"><a href="#HAT-313"><span class="linenos">313</span></a>        <span class="n">num_batches</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">num_training_batches</span>
</span><span id="HAT-314"><a href="#HAT-314"><span class="linenos">314</span></a>        <span class="n">logits</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
</span><span id="HAT-315"><a href="#HAT-315"><span class="linenos">315</span></a>            <span class="n">x</span><span class="p">,</span>
</span><span id="HAT-316"><a href="#HAT-316"><span class="linenos">316</span></a>            <span class="n">stage</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">,</span>
</span><span id="HAT-317"><a href="#HAT-317"><span class="linenos">317</span></a>            <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="HAT-318"><a href="#HAT-318"><span class="linenos">318</span></a>            <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="HAT-319"><a href="#HAT-319"><span class="linenos">319</span></a>            <span class="n">task_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">,</span>
</span><span id="HAT-320"><a href="#HAT-320"><span class="linenos">320</span></a>        <span class="p">)</span>
</span><span id="HAT-321"><a href="#HAT-321"><span class="linenos">321</span></a>        <span class="n">loss_cls</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span><span id="HAT-322"><a href="#HAT-322"><span class="linenos">322</span></a>
</span><span id="HAT-323"><a href="#HAT-323"><span class="linenos">323</span></a>        <span class="c1"># regularisation loss. See chapter 2.6 &quot;Promoting Low Capacity Usage&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="HAT-324"><a href="#HAT-324"><span class="linenos">324</span></a>        <span class="n">loss_reg</span><span class="p">,</span> <span class="n">network_sparsity</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mark_sparsity_reg</span><span class="p">(</span>
</span><span id="HAT-325"><a href="#HAT-325"><span class="linenos">325</span></a>            <span class="n">mask</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cumulative_mask_for_previous_tasks</span>
</span><span id="HAT-326"><a href="#HAT-326"><span class="linenos">326</span></a>        <span class="p">)</span>
</span><span id="HAT-327"><a href="#HAT-327"><span class="linenos">327</span></a>
</span><span id="HAT-328"><a href="#HAT-328"><span class="linenos">328</span></a>        <span class="c1"># total loss</span>
</span><span id="HAT-329"><a href="#HAT-329"><span class="linenos">329</span></a>        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_cls</span> <span class="o">+</span> <span class="n">loss_reg</span>
</span><span id="HAT-330"><a href="#HAT-330"><span class="linenos">330</span></a>
</span><span id="HAT-331"><a href="#HAT-331"><span class="linenos">331</span></a>        <span class="c1"># backward step (manually)</span>
</span><span id="HAT-332"><a href="#HAT-332"><span class="linenos">332</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">manual_backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>  <span class="c1"># calculate the gradients</span>
</span><span id="HAT-333"><a href="#HAT-333"><span class="linenos">333</span></a>        <span class="c1"># HAT hard clip gradients by the cumulative masks. See equation (2) inchapter 2.3 &quot;Network Training&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a). Network capacity is calculated along with this process. Network capacity is defined as the average adjustment rate over all parameters. See chapter 4.1 in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="HAT-334"><a href="#HAT-334"><span class="linenos">334</span></a>        <span class="n">capacity</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">clip_grad_by_adjustment</span><span class="p">(</span>
</span><span id="HAT-335"><a href="#HAT-335"><span class="linenos">335</span></a>            <span class="n">network_sparsity</span><span class="o">=</span><span class="n">network_sparsity</span><span class="p">,</span>  <span class="c1"># pass a keyword argument network sparsity here to make it compatible with AdaHAT. AdaHAT inherits this `training_step()` method.</span>
</span><span id="HAT-336"><a href="#HAT-336"><span class="linenos">336</span></a>        <span class="p">)</span>
</span><span id="HAT-337"><a href="#HAT-337"><span class="linenos">337</span></a>        <span class="c1"># compensate the gradients of task embedding. See chapter 2.5 &quot;Embedding Gradient Compensation&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="HAT-338"><a href="#HAT-338"><span class="linenos">338</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">compensate_task_embedding_gradients</span><span class="p">(</span>
</span><span id="HAT-339"><a href="#HAT-339"><span class="linenos">339</span></a>            <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="HAT-340"><a href="#HAT-340"><span class="linenos">340</span></a>            <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="HAT-341"><a href="#HAT-341"><span class="linenos">341</span></a>        <span class="p">)</span>
</span><span id="HAT-342"><a href="#HAT-342"><span class="linenos">342</span></a>        <span class="c1"># update parameters with the modified gradients</span>
</span><span id="HAT-343"><a href="#HAT-343"><span class="linenos">343</span></a>        <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span><span id="HAT-344"><a href="#HAT-344"><span class="linenos">344</span></a>
</span><span id="HAT-345"><a href="#HAT-345"><span class="linenos">345</span></a>        <span class="c1"># accuracy of the batch</span>
</span><span id="HAT-346"><a href="#HAT-346"><span class="linenos">346</span></a>        <span class="n">acc</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</span><span id="HAT-347"><a href="#HAT-347"><span class="linenos">347</span></a>
</span><span id="HAT-348"><a href="#HAT-348"><span class="linenos">348</span></a>        <span class="k">return</span> <span class="p">{</span>
</span><span id="HAT-349"><a href="#HAT-349"><span class="linenos">349</span></a>            <span class="s2">&quot;loss&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span>  <span class="c1"># Return loss is essential for training step, or backpropagation will fail</span>
</span><span id="HAT-350"><a href="#HAT-350"><span class="linenos">350</span></a>            <span class="s2">&quot;loss_cls&quot;</span><span class="p">:</span> <span class="n">loss_cls</span><span class="p">,</span>
</span><span id="HAT-351"><a href="#HAT-351"><span class="linenos">351</span></a>            <span class="s2">&quot;loss_reg&quot;</span><span class="p">:</span> <span class="n">loss_reg</span><span class="p">,</span>
</span><span id="HAT-352"><a href="#HAT-352"><span class="linenos">352</span></a>            <span class="s2">&quot;acc&quot;</span><span class="p">:</span> <span class="n">acc</span><span class="p">,</span>
</span><span id="HAT-353"><a href="#HAT-353"><span class="linenos">353</span></a>            <span class="s2">&quot;activations&quot;</span><span class="p">:</span> <span class="n">activations</span><span class="p">,</span>
</span><span id="HAT-354"><a href="#HAT-354"><span class="linenos">354</span></a>            <span class="s2">&quot;mask&quot;</span><span class="p">:</span> <span class="n">mask</span><span class="p">,</span>  <span class="c1"># Return other metrics for lightning loggers callback to handle at `on_train_batch_end()`</span>
</span><span id="HAT-355"><a href="#HAT-355"><span class="linenos">355</span></a>            <span class="s2">&quot;capacity&quot;</span><span class="p">:</span> <span class="n">capacity</span><span class="p">,</span>
</span><span id="HAT-356"><a href="#HAT-356"><span class="linenos">356</span></a>        <span class="p">}</span>
</span><span id="HAT-357"><a href="#HAT-357"><span class="linenos">357</span></a>
</span><span id="HAT-358"><a href="#HAT-358"><span class="linenos">358</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_end</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HAT-359"><a href="#HAT-359"><span class="linenos">359</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the mask and update cumulative mask after training the task.&quot;&quot;&quot;</span>
</span><span id="HAT-360"><a href="#HAT-360"><span class="linenos">360</span></a>
</span><span id="HAT-361"><a href="#HAT-361"><span class="linenos">361</span></a>        <span class="c1"># store the mask for the current task</span>
</span><span id="HAT-362"><a href="#HAT-362"><span class="linenos">362</span></a>        <span class="n">mask_t</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="HAT-363"><a href="#HAT-363"><span class="linenos">363</span></a>            <span class="n">layer_name</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">gate_fn</span><span class="p">(</span>
</span><span id="HAT-364"><a href="#HAT-364"><span class="linenos">364</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_max</span>
</span><span id="HAT-365"><a href="#HAT-365"><span class="linenos">365</span></a>            <span class="p">)</span>
</span><span id="HAT-366"><a href="#HAT-366"><span class="linenos">366</span></a>            <span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
</span><span id="HAT-367"><a href="#HAT-367"><span class="linenos">367</span></a>            <span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="HAT-368"><a href="#HAT-368"><span class="linenos">368</span></a>            <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span>
</span><span id="HAT-369"><a href="#HAT-369"><span class="linenos">369</span></a>        <span class="p">}</span>
</span><span id="HAT-370"><a href="#HAT-370"><span class="linenos">370</span></a>
</span><span id="HAT-371"><a href="#HAT-371"><span class="linenos">371</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">masks</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mask_t</span>
</span><span id="HAT-372"><a href="#HAT-372"><span class="linenos">372</span></a>
</span><span id="HAT-373"><a href="#HAT-373"><span class="linenos">373</span></a>        <span class="c1"># update the cumulative and summative masks</span>
</span><span id="HAT-374"><a href="#HAT-374"><span class="linenos">374</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">cumulative_mask_for_previous_tasks</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="HAT-375"><a href="#HAT-375"><span class="linenos">375</span></a>            <span class="n">layer_name</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span>
</span><span id="HAT-376"><a href="#HAT-376"><span class="linenos">376</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">cumulative_mask_for_previous_tasks</span><span class="p">[</span><span class="n">layer_name</span><span class="p">],</span> <span class="n">mask_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="HAT-377"><a href="#HAT-377"><span class="linenos">377</span></a>            <span class="p">)</span>
</span><span id="HAT-378"><a href="#HAT-378"><span class="linenos">378</span></a>            <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span>
</span><span id="HAT-379"><a href="#HAT-379"><span class="linenos">379</span></a>        <span class="p">}</span>
</span><span id="HAT-380"><a href="#HAT-380"><span class="linenos">380</span></a>
</span><span id="HAT-381"><a href="#HAT-381"><span class="linenos">381</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
</span><span id="HAT-382"><a href="#HAT-382"><span class="linenos">382</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Validation step for current task `self.task_id`.</span>
</span><span id="HAT-383"><a href="#HAT-383"><span class="linenos">383</span></a>
</span><span id="HAT-384"><a href="#HAT-384"><span class="linenos">384</span></a><span class="sd">        **Args:**</span>
</span><span id="HAT-385"><a href="#HAT-385"><span class="linenos">385</span></a><span class="sd">        - **batch** (`Any`): a batch of validation data.</span>
</span><span id="HAT-386"><a href="#HAT-386"><span class="linenos">386</span></a>
</span><span id="HAT-387"><a href="#HAT-387"><span class="linenos">387</span></a><span class="sd">        **Returns:**</span>
</span><span id="HAT-388"><a href="#HAT-388"><span class="linenos">388</span></a><span class="sd">        - **outputs** (`dict[str, Tensor]`): a dictionary contains loss and other metrics from this validation step. Key (`str`) is the metrics name, value (`Tensor`) is the metrics.</span>
</span><span id="HAT-389"><a href="#HAT-389"><span class="linenos">389</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HAT-390"><a href="#HAT-390"><span class="linenos">390</span></a>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
</span><span id="HAT-391"><a href="#HAT-391"><span class="linenos">391</span></a>        <span class="n">logits</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
</span><span id="HAT-392"><a href="#HAT-392"><span class="linenos">392</span></a>            <span class="n">x</span><span class="p">,</span> <span class="n">stage</span><span class="o">=</span><span class="s2">&quot;validation&quot;</span><span class="p">,</span> <span class="n">task_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span>
</span><span id="HAT-393"><a href="#HAT-393"><span class="linenos">393</span></a>        <span class="p">)</span>
</span><span id="HAT-394"><a href="#HAT-394"><span class="linenos">394</span></a>        <span class="n">loss_cls</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span><span id="HAT-395"><a href="#HAT-395"><span class="linenos">395</span></a>        <span class="n">acc</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</span><span id="HAT-396"><a href="#HAT-396"><span class="linenos">396</span></a>
</span><span id="HAT-397"><a href="#HAT-397"><span class="linenos">397</span></a>        <span class="k">return</span> <span class="p">{</span>
</span><span id="HAT-398"><a href="#HAT-398"><span class="linenos">398</span></a>            <span class="s2">&quot;loss_cls&quot;</span><span class="p">:</span> <span class="n">loss_cls</span><span class="p">,</span>
</span><span id="HAT-399"><a href="#HAT-399"><span class="linenos">399</span></a>            <span class="s2">&quot;acc&quot;</span><span class="p">:</span> <span class="n">acc</span><span class="p">,</span>  <span class="c1"># Return metrics for lightning loggers callback to handle at `on_validation_batch_end()`</span>
</span><span id="HAT-400"><a href="#HAT-400"><span class="linenos">400</span></a>        <span class="p">}</span>
</span><span id="HAT-401"><a href="#HAT-401"><span class="linenos">401</span></a>
</span><span id="HAT-402"><a href="#HAT-402"><span class="linenos">402</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">test_step</span><span class="p">(</span>
</span><span id="HAT-403"><a href="#HAT-403"><span class="linenos">403</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="HAT-404"><a href="#HAT-404"><span class="linenos">404</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
</span><span id="HAT-405"><a href="#HAT-405"><span class="linenos">405</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Test step for current task `self.task_id`, which tests for all seen tasks indexed by `dataloader_idx`.</span>
</span><span id="HAT-406"><a href="#HAT-406"><span class="linenos">406</span></a>
</span><span id="HAT-407"><a href="#HAT-407"><span class="linenos">407</span></a><span class="sd">        **Args:**</span>
</span><span id="HAT-408"><a href="#HAT-408"><span class="linenos">408</span></a><span class="sd">        - **batch** (`Any`): a batch of test data.</span>
</span><span id="HAT-409"><a href="#HAT-409"><span class="linenos">409</span></a><span class="sd">        - **dataloader_idx** (`int`): the task ID of seen tasks to be tested. A default value of 0 is given otherwise the LightningModule will raise a `RuntimeError`.</span>
</span><span id="HAT-410"><a href="#HAT-410"><span class="linenos">410</span></a>
</span><span id="HAT-411"><a href="#HAT-411"><span class="linenos">411</span></a><span class="sd">        **Returns:**</span>
</span><span id="HAT-412"><a href="#HAT-412"><span class="linenos">412</span></a><span class="sd">        - **outputs** (`dict[str, Tensor]`): a dictionary contains loss and other metrics from this test step. Key (`str`) is the metrics name, value (`Tensor`) is the metrics.</span>
</span><span id="HAT-413"><a href="#HAT-413"><span class="linenos">413</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HAT-414"><a href="#HAT-414"><span class="linenos">414</span></a>        <span class="n">test_task_id</span> <span class="o">=</span> <span class="n">dataloader_idx</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="HAT-415"><a href="#HAT-415"><span class="linenos">415</span></a>
</span><span id="HAT-416"><a href="#HAT-416"><span class="linenos">416</span></a>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
</span><span id="HAT-417"><a href="#HAT-417"><span class="linenos">417</span></a>        <span class="n">logits</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
</span><span id="HAT-418"><a href="#HAT-418"><span class="linenos">418</span></a>            <span class="n">x</span><span class="p">,</span>
</span><span id="HAT-419"><a href="#HAT-419"><span class="linenos">419</span></a>            <span class="n">stage</span><span class="o">=</span><span class="s2">&quot;test&quot;</span><span class="p">,</span>
</span><span id="HAT-420"><a href="#HAT-420"><span class="linenos">420</span></a>            <span class="n">task_id</span><span class="o">=</span><span class="n">test_task_id</span><span class="p">,</span>
</span><span id="HAT-421"><a href="#HAT-421"><span class="linenos">421</span></a>        <span class="p">)</span>  <span class="c1"># use the corresponding head and mask to test (instead of the current task `self.task_id`)</span>
</span><span id="HAT-422"><a href="#HAT-422"><span class="linenos">422</span></a>        <span class="n">loss_cls</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span><span id="HAT-423"><a href="#HAT-423"><span class="linenos">423</span></a>        <span class="n">acc</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</span><span id="HAT-424"><a href="#HAT-424"><span class="linenos">424</span></a>
</span><span id="HAT-425"><a href="#HAT-425"><span class="linenos">425</span></a>        <span class="k">return</span> <span class="p">{</span>
</span><span id="HAT-426"><a href="#HAT-426"><span class="linenos">426</span></a>            <span class="s2">&quot;loss_cls&quot;</span><span class="p">:</span> <span class="n">loss_cls</span><span class="p">,</span>
</span><span id="HAT-427"><a href="#HAT-427"><span class="linenos">427</span></a>            <span class="s2">&quot;acc&quot;</span><span class="p">:</span> <span class="n">acc</span><span class="p">,</span>  <span class="c1"># Return metrics for lightning loggers callback to handle at `on_test_batch_end()`</span>
</span><span id="HAT-428"><a href="#HAT-428"><span class="linenos">428</span></a>        <span class="p">}</span>
</span></pre></div>


            <div class="docstring"><p>HAT (Hard Attention to the Task) algorithm.</p>

<p><a href="http://proceedings.mlr.press/v80/serra18a">HAT (Hard Attention to the Task, 2018)</a> is an architecture-based continual learning approach that uses learnable hard attention masks to select the task-specific parameters.</p>
</div>


                            <div id="HAT.__init__" class="classattr">
                                        <input id="HAT.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">HAT</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">backbone</span><span class="p">:</span> <span class="n"><a href="../backbones.html#HATMaskBackbone">clarena.backbones.HATMaskBackbone</a></span>,</span><span class="param">	<span class="n">heads</span><span class="p">:</span> <span class="n"><a href="../cl_heads.html#HeadsTIL">clarena.cl_heads.HeadsTIL</a></span> <span class="o">|</span> <span class="n"><a href="../cl_heads.html#HeadsCIL">clarena.cl_heads.HeadsCIL</a></span>,</span><span class="param">	<span class="n">adjustment_mode</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">s_max</span><span class="p">:</span> <span class="nb">float</span>,</span><span class="param">	<span class="n">clamp_threshold</span><span class="p">:</span> <span class="nb">float</span>,</span><span class="param">	<span class="n">mask_sparsity_reg_factor</span><span class="p">:</span> <span class="nb">float</span>,</span><span class="param">	<span class="n">mask_sparsity_reg_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;original&#39;</span>,</span><span class="param">	<span class="n">task_embedding_init_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;N01&#39;</span>,</span><span class="param">	<span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span></span>)</span>

                <label class="view-source-button" for="HAT.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HAT.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HAT.__init__-32"><a href="#HAT.__init__-32"><span class="linenos"> 32</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="HAT.__init__-33"><a href="#HAT.__init__-33"><span class="linenos"> 33</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="HAT.__init__-34"><a href="#HAT.__init__-34"><span class="linenos"> 34</span></a>        <span class="n">backbone</span><span class="p">:</span> <span class="n">HATMaskBackbone</span><span class="p">,</span>
</span><span id="HAT.__init__-35"><a href="#HAT.__init__-35"><span class="linenos"> 35</span></a>        <span class="n">heads</span><span class="p">:</span> <span class="n">HeadsTIL</span> <span class="o">|</span> <span class="n">HeadsCIL</span><span class="p">,</span>
</span><span id="HAT.__init__-36"><a href="#HAT.__init__-36"><span class="linenos"> 36</span></a>        <span class="n">adjustment_mode</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="HAT.__init__-37"><a href="#HAT.__init__-37"><span class="linenos"> 37</span></a>        <span class="n">s_max</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="HAT.__init__-38"><a href="#HAT.__init__-38"><span class="linenos"> 38</span></a>        <span class="n">clamp_threshold</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="HAT.__init__-39"><a href="#HAT.__init__-39"><span class="linenos"> 39</span></a>        <span class="n">mask_sparsity_reg_factor</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="HAT.__init__-40"><a href="#HAT.__init__-40"><span class="linenos"> 40</span></a>        <span class="n">mask_sparsity_reg_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;original&quot;</span><span class="p">,</span>
</span><span id="HAT.__init__-41"><a href="#HAT.__init__-41"><span class="linenos"> 41</span></a>        <span class="n">task_embedding_init_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;N01&quot;</span><span class="p">,</span>
</span><span id="HAT.__init__-42"><a href="#HAT.__init__-42"><span class="linenos"> 42</span></a>        <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HAT.__init__-43"><a href="#HAT.__init__-43"><span class="linenos"> 43</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HAT.__init__-44"><a href="#HAT.__init__-44"><span class="linenos"> 44</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialise the HAT algorithm with the network.</span>
</span><span id="HAT.__init__-45"><a href="#HAT.__init__-45"><span class="linenos"> 45</span></a>
</span><span id="HAT.__init__-46"><a href="#HAT.__init__-46"><span class="linenos"> 46</span></a><span class="sd">        **Args:**</span>
</span><span id="HAT.__init__-47"><a href="#HAT.__init__-47"><span class="linenos"> 47</span></a><span class="sd">        - **backbone** (`HATMaskBackbone`): must be a backbone network with HAT mask mechanism.</span>
</span><span id="HAT.__init__-48"><a href="#HAT.__init__-48"><span class="linenos"> 48</span></a><span class="sd">        - **heads** (`HeadsTIL` | `HeadsCIL`): output heads.</span>
</span><span id="HAT.__init__-49"><a href="#HAT.__init__-49"><span class="linenos"> 49</span></a><span class="sd">        - **adjustment_mode** (`str`): the strategy of adjustment i.e. the mode of gradient clipping, should be one of the following:</span>
</span><span id="HAT.__init__-50"><a href="#HAT.__init__-50"><span class="linenos"> 50</span></a><span class="sd">            1. &#39;hat&#39;: set the gradients of parameters linking to masked units to zero. This is the way that HAT does, which fixes the part of network for previous tasks completely. See equation (2) in chapter 2.3 &quot;Network Training&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="HAT.__init__-51"><a href="#HAT.__init__-51"><span class="linenos"> 51</span></a><span class="sd">            2. &#39;hat_random&#39;: set the gradients of parameters linking to masked units to random 0-1 values. See the &quot;Baselines&quot; section in chapter 4.1 in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="HAT.__init__-52"><a href="#HAT.__init__-52"><span class="linenos"> 52</span></a><span class="sd">            3. &#39;hat_const_alpha&#39;: set the gradients of parameters linking to masked units to a constant value of `alpha`. See the &quot;Baselines&quot; section in chapter 4.1 in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="HAT.__init__-53"><a href="#HAT.__init__-53"><span class="linenos"> 53</span></a><span class="sd">            4. &#39;hat_const_1&#39;: set the gradients of parameters linking to masked units to a constant value of 1, which means no gradient constraint on any parameter at all. See the &quot;Baselines&quot; section in chapter 4.1 in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="HAT.__init__-54"><a href="#HAT.__init__-54"><span class="linenos"> 54</span></a><span class="sd">        - **s_max** (`float`): hyperparameter, the maximum scaling factor in the gate function. See chapter 2.4 &quot;Hard Attention Training&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="HAT.__init__-55"><a href="#HAT.__init__-55"><span class="linenos"> 55</span></a><span class="sd">        - **clamp_threshold** (`float`): the threshold for task embedding gradient compensation. See chapter 2.5 &quot;Embedding Gradient Compensation&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="HAT.__init__-56"><a href="#HAT.__init__-56"><span class="linenos"> 56</span></a><span class="sd">        - **mask_sparsity_reg_factor** (`float`): hyperparameter, the regularisation factor for mask sparsity.</span>
</span><span id="HAT.__init__-57"><a href="#HAT.__init__-57"><span class="linenos"> 57</span></a><span class="sd">        - **mask_sparsity_reg_mode** (`str`): the mode of mask sparsity regularisation, should be one of the following:</span>
</span><span id="HAT.__init__-58"><a href="#HAT.__init__-58"><span class="linenos"> 58</span></a><span class="sd">            1. &#39;original&#39; (default): the original mask sparsity regularisation in HAT paper.</span>
</span><span id="HAT.__init__-59"><a href="#HAT.__init__-59"><span class="linenos"> 59</span></a><span class="sd">            2. &#39;cross&#39;: the cross version mask sparsity regularisation.</span>
</span><span id="HAT.__init__-60"><a href="#HAT.__init__-60"><span class="linenos"> 60</span></a><span class="sd">        - **task_embedding_init_mode** (`str`): the initialisation mode for task embeddings, should be one of the following:</span>
</span><span id="HAT.__init__-61"><a href="#HAT.__init__-61"><span class="linenos"> 61</span></a><span class="sd">            1. &#39;N01&#39; (default): standard normal distribution $N(0, 1)$.</span>
</span><span id="HAT.__init__-62"><a href="#HAT.__init__-62"><span class="linenos"> 62</span></a><span class="sd">            2. &#39;U-11&#39;: uniform distribution $U(-1, 1)$.</span>
</span><span id="HAT.__init__-63"><a href="#HAT.__init__-63"><span class="linenos"> 63</span></a><span class="sd">            3. &#39;U01&#39;: uniform distribution $U(0, 1)$.</span>
</span><span id="HAT.__init__-64"><a href="#HAT.__init__-64"><span class="linenos"> 64</span></a><span class="sd">            4. &#39;U-10&#39;: uniform distribution $U(-1, 0)$.</span>
</span><span id="HAT.__init__-65"><a href="#HAT.__init__-65"><span class="linenos"> 65</span></a><span class="sd">            5. &#39;last&#39;: inherit task embedding from last task.</span>
</span><span id="HAT.__init__-66"><a href="#HAT.__init__-66"><span class="linenos"> 66</span></a><span class="sd">        - **alpha** (`float` | `None`): the `alpha` in the &#39;HAT-const-alpha&#39; mode. See the &quot;Baselines&quot; section in chapter 4.1 in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9). It applies only when adjustment_mode is &#39;hat_const_alpha&#39;.</span>
</span><span id="HAT.__init__-67"><a href="#HAT.__init__-67"><span class="linenos"> 67</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HAT.__init__-68"><a href="#HAT.__init__-68"><span class="linenos"> 68</span></a>        <span class="n">CLAlgorithm</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">backbone</span><span class="o">=</span><span class="n">backbone</span><span class="p">,</span> <span class="n">heads</span><span class="o">=</span><span class="n">heads</span><span class="p">)</span>
</span><span id="HAT.__init__-69"><a href="#HAT.__init__-69"><span class="linenos"> 69</span></a>
</span><span id="HAT.__init__-70"><a href="#HAT.__init__-70"><span class="linenos"> 70</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">=</span> <span class="n">adjustment_mode</span>
</span><span id="HAT.__init__-71"><a href="#HAT.__init__-71"><span class="linenos"> 71</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the adjustment mode for gradient clipping.&quot;&quot;&quot;</span>
</span><span id="HAT.__init__-72"><a href="#HAT.__init__-72"><span class="linenos"> 72</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">s_max</span> <span class="o">=</span> <span class="n">s_max</span>
</span><span id="HAT.__init__-73"><a href="#HAT.__init__-73"><span class="linenos"> 73</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store s_max. &quot;&quot;&quot;</span>
</span><span id="HAT.__init__-74"><a href="#HAT.__init__-74"><span class="linenos"> 74</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">clamp_threshold</span> <span class="o">=</span> <span class="n">clamp_threshold</span>
</span><span id="HAT.__init__-75"><a href="#HAT.__init__-75"><span class="linenos"> 75</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the clamp threshold for task embedding gradient compensation.&quot;&quot;&quot;</span>
</span><span id="HAT.__init__-76"><a href="#HAT.__init__-76"><span class="linenos"> 76</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">mask_sparsity_reg_factor</span> <span class="o">=</span> <span class="n">mask_sparsity_reg_factor</span>
</span><span id="HAT.__init__-77"><a href="#HAT.__init__-77"><span class="linenos"> 77</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the mask sparsity regularisation factor.&quot;&quot;&quot;</span>
</span><span id="HAT.__init__-78"><a href="#HAT.__init__-78"><span class="linenos"> 78</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">mask_sparsity_reg_mode</span> <span class="o">=</span> <span class="n">mask_sparsity_reg_mode</span>
</span><span id="HAT.__init__-79"><a href="#HAT.__init__-79"><span class="linenos"> 79</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the mask sparsity regularisation mode.&quot;&quot;&quot;</span>
</span><span id="HAT.__init__-80"><a href="#HAT.__init__-80"><span class="linenos"> 80</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">mark_sparsity_reg</span> <span class="o">=</span> <span class="n">HATMaskSparsityReg</span><span class="p">(</span>
</span><span id="HAT.__init__-81"><a href="#HAT.__init__-81"><span class="linenos"> 81</span></a>            <span class="n">factor</span><span class="o">=</span><span class="n">mask_sparsity_reg_factor</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">mask_sparsity_reg_mode</span>
</span><span id="HAT.__init__-82"><a href="#HAT.__init__-82"><span class="linenos"> 82</span></a>        <span class="p">)</span>
</span><span id="HAT.__init__-83"><a href="#HAT.__init__-83"><span class="linenos"> 83</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialise and store the mask sparsity regulariser.&quot;&quot;&quot;</span>
</span><span id="HAT.__init__-84"><a href="#HAT.__init__-84"><span class="linenos"> 84</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_init_mode</span> <span class="o">=</span> <span class="n">task_embedding_init_mode</span>
</span><span id="HAT.__init__-85"><a href="#HAT.__init__-85"><span class="linenos"> 85</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the task embedding initialisation mode.&quot;&quot;&quot;</span>
</span><span id="HAT.__init__-86"><a href="#HAT.__init__-86"><span class="linenos"> 86</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span> <span class="k">if</span> <span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;hat_const_alpha&quot;</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="HAT.__init__-87"><a href="#HAT.__init__-87"><span class="linenos"> 87</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the alpha for `hat_const_alpha`.&quot;&quot;&quot;</span>
</span><span id="HAT.__init__-88"><a href="#HAT.__init__-88"><span class="linenos"> 88</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="HAT.__init__-89"><a href="#HAT.__init__-89"><span class="linenos"> 89</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;HAT doesn&#39;t use the epsilon for `hat_const_alpha`. We still set it here to be consistent with the `epsilon` in `clip_grad_by_adjustment()` method in `HATMaskBackbone`.&quot;&quot;&quot;</span>
</span><span id="HAT.__init__-90"><a href="#HAT.__init__-90"><span class="linenos"> 90</span></a>
</span><span id="HAT.__init__-91"><a href="#HAT.__init__-91"><span class="linenos"> 91</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">masks</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="HAT.__init__-92"><a href="#HAT.__init__-92"><span class="linenos"> 92</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the binary attention mask of each previous task gated from the task embedding. Keys are task IDs (string type) and values are the corresponding mask. Each mask is a dict where keys are layer names and values are the binary mask tensor for the layer. The mask tensor has size (number of units). &quot;&quot;&quot;</span>
</span><span id="HAT.__init__-93"><a href="#HAT.__init__-93"><span class="linenos"> 93</span></a>
</span><span id="HAT.__init__-94"><a href="#HAT.__init__-94"><span class="linenos"> 94</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">cumulative_mask_for_previous_tasks</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="HAT.__init__-95"><a href="#HAT.__init__-95"><span class="linenos"> 95</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the cumulative binary attention mask $\mathrm{M}^{&lt;t}$ of previous tasks $1,\cdots, t-1$, gated from the task embedding. Keys are task IDs and values are the corresponding cumulative mask. Each cumulative mask is a dict where keys are layer names and values are the binary mask tensor for the layer. The mask tensor has size (number of units). &quot;&quot;&quot;</span>
</span><span id="HAT.__init__-96"><a href="#HAT.__init__-96"><span class="linenos"> 96</span></a>
</span><span id="HAT.__init__-97"><a href="#HAT.__init__-97"><span class="linenos"> 97</span></a>        <span class="c1"># set manual optimisation</span>
</span><span id="HAT.__init__-98"><a href="#HAT.__init__-98"><span class="linenos"> 98</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">automatic_optimization</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="HAT.__init__-99"><a href="#HAT.__init__-99"><span class="linenos"> 99</span></a>
</span><span id="HAT.__init__-100"><a href="#HAT.__init__-100"><span class="linenos">100</span></a>        <span class="n">HAT</span><span class="o">.</span><span class="n">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Initialise the HAT algorithm with the network.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>backbone</strong> (<code>HATMaskBackbone</code>): must be a backbone network with HAT mask mechanism.</li>
<li><strong>heads</strong> (<code>HeadsTIL</code> | <code>HeadsCIL</code>): output heads.</li>
<li><strong>adjustment_mode</strong> (<code>str</code>): the strategy of adjustment i.e. the mode of gradient clipping, should be one of the following:
<ol>
<li>'hat': set the gradients of parameters linking to masked units to zero. This is the way that HAT does, which fixes the part of network for previous tasks completely. See equation (2) in chapter 2.3 "Network Training" in <a href="http://proceedings.mlr.press/v80/serra18a">HAT paper</a>.</li>
<li>'hat_random': set the gradients of parameters linking to masked units to random 0-1 values. See the "Baselines" section in chapter 4.1 in <a href="https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9">AdaHAT paper</a>.</li>
<li>'hat_const_alpha': set the gradients of parameters linking to masked units to a constant value of <code><a href="#HAT.alpha">alpha</a></code>. See the "Baselines" section in chapter 4.1 in <a href="https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9">AdaHAT paper</a>.</li>
<li>'hat_const_1': set the gradients of parameters linking to masked units to a constant value of 1, which means no gradient constraint on any parameter at all. See the "Baselines" section in chapter 4.1 in <a href="https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9">AdaHAT paper</a>.</li>
</ol></li>
<li><strong>s_max</strong> (<code><a href="#HAT.float">float</a></code>): hyperparameter, the maximum scaling factor in the gate function. See chapter 2.4 "Hard Attention Training" in <a href="http://proceedings.mlr.press/v80/serra18a">HAT paper</a>.</li>
<li><strong>clamp_threshold</strong> (<code><a href="#HAT.float">float</a></code>): the threshold for task embedding gradient compensation. See chapter 2.5 "Embedding Gradient Compensation" in <a href="http://proceedings.mlr.press/v80/serra18a">HAT paper</a>.</li>
<li><strong>mask_sparsity_reg_factor</strong> (<code><a href="#HAT.float">float</a></code>): hyperparameter, the regularisation factor for mask sparsity.</li>
<li><strong>mask_sparsity_reg_mode</strong> (<code>str</code>): the mode of mask sparsity regularisation, should be one of the following:
<ol>
<li>'original' (default): the original mask sparsity regularisation in HAT paper.</li>
<li>'cross': the cross version mask sparsity regularisation.</li>
</ol></li>
<li><strong>task_embedding_init_mode</strong> (<code>str</code>): the initialisation mode for task embeddings, should be one of the following:
<ol>
<li>'N01' (default): standard normal distribution $N(0, 1)$.</li>
<li>'U-11': uniform distribution $U(-1, 1)$.</li>
<li>'U01': uniform distribution $U(0, 1)$.</li>
<li>'U-10': uniform distribution $U(-1, 0)$.</li>
<li>'last': inherit task embedding from last task.</li>
</ol></li>
<li><strong>alpha</strong> (<code><a href="#HAT.float">float</a></code> | <code>None</code>): the <code><a href="#HAT.alpha">alpha</a></code> in the 'HAT-const-alpha' mode. See the "Baselines" section in chapter 4.1 in <a href="https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9">AdaHAT paper</a>. It applies only when adjustment_mode is 'hat_const_alpha'.</li>
</ul>
</div>


                            </div>
                            <div id="HAT.adjustment_mode" class="classattr">
                                <div class="attr variable">
            <span class="name">adjustment_mode</span>

        
    </div>
    <a class="headerlink" href="#HAT.adjustment_mode"></a>
    
            <div class="docstring"><p>Store the adjustment mode for gradient clipping.</p>
</div>


                            </div>
                            <div id="HAT.s_max" class="classattr">
                                <div class="attr variable">
            <span class="name">s_max</span>

        
    </div>
    <a class="headerlink" href="#HAT.s_max"></a>
    
            <div class="docstring"><p>Store s_max.</p>
</div>


                            </div>
                            <div id="HAT.clamp_threshold" class="classattr">
                                <div class="attr variable">
            <span class="name">clamp_threshold</span>

        
    </div>
    <a class="headerlink" href="#HAT.clamp_threshold"></a>
    
            <div class="docstring"><p>Store the clamp threshold for task embedding gradient compensation.</p>
</div>


                            </div>
                            <div id="HAT.mask_sparsity_reg_factor" class="classattr">
                                <div class="attr variable">
            <span class="name">mask_sparsity_reg_factor</span>

        
    </div>
    <a class="headerlink" href="#HAT.mask_sparsity_reg_factor"></a>
    
            <div class="docstring"><p>Store the mask sparsity regularisation factor.</p>
</div>


                            </div>
                            <div id="HAT.mask_sparsity_reg_mode" class="classattr">
                                <div class="attr variable">
            <span class="name">mask_sparsity_reg_mode</span>

        
    </div>
    <a class="headerlink" href="#HAT.mask_sparsity_reg_mode"></a>
    
            <div class="docstring"><p>Store the mask sparsity regularisation mode.</p>
</div>


                            </div>
                            <div id="HAT.mark_sparsity_reg" class="classattr">
                                <div class="attr variable">
            <span class="name">mark_sparsity_reg</span>

        
    </div>
    <a class="headerlink" href="#HAT.mark_sparsity_reg"></a>
    
            <div class="docstring"><p>Initialise and store the mask sparsity regulariser.</p>
</div>


                            </div>
                            <div id="HAT.task_embedding_init_mode" class="classattr">
                                <div class="attr variable">
            <span class="name">task_embedding_init_mode</span>

        
    </div>
    <a class="headerlink" href="#HAT.task_embedding_init_mode"></a>
    
            <div class="docstring"><p>Store the task embedding initialisation mode.</p>
</div>


                            </div>
                            <div id="HAT.alpha" class="classattr">
                                <div class="attr variable">
            <span class="name">alpha</span>

        
    </div>
    <a class="headerlink" href="#HAT.alpha"></a>
    
            <div class="docstring"><p>Store the alpha for <code>hat_const_alpha</code>.</p>
</div>


                            </div>
                            <div id="HAT.epsilon" class="classattr">
                                <div class="attr variable">
            <span class="name">epsilon</span>

        
    </div>
    <a class="headerlink" href="#HAT.epsilon"></a>
    
            <div class="docstring"><p>HAT doesn't use the epsilon for <code>hat_const_alpha</code>. We still set it here to be consistent with the <code><a href="#HAT.epsilon">epsilon</a></code> in <code><a href="#HAT.clip_grad_by_adjustment">clip_grad_by_adjustment()</a></code> method in <code>HATMaskBackbone</code>.</p>
</div>


                            </div>
                            <div id="HAT.masks" class="classattr">
                                <div class="attr variable">
            <span class="name">masks</span><span class="annotation">: dict[str, dict[str, torch.Tensor]]</span>

        
    </div>
    <a class="headerlink" href="#HAT.masks"></a>
    
            <div class="docstring"><p>Store the binary attention mask of each previous task gated from the task embedding. Keys are task IDs (string type) and values are the corresponding mask. Each mask is a dict where keys are layer names and values are the binary mask tensor for the layer. The mask tensor has size (number of units).</p>
</div>


                            </div>
                            <div id="HAT.cumulative_mask_for_previous_tasks" class="classattr">
                                <div class="attr variable">
            <span class="name">cumulative_mask_for_previous_tasks</span><span class="annotation">: dict[str, torch.Tensor]</span>

        
    </div>
    <a class="headerlink" href="#HAT.cumulative_mask_for_previous_tasks"></a>
    
            <div class="docstring"><p>Store the cumulative binary attention mask $\mathrm{M}^{<t}$ of previous tasks $1,\cdots, t-1$, gated from the task embedding. Keys are task IDs and values are the corresponding cumulative mask. Each cumulative mask is a dict where keys are layer names and values are the binary mask tensor for the layer. The mask tensor has size (number of units).</p>
</div>


                            </div>
                            <div id="HAT.automatic_optimization" class="classattr">
                                        <input id="HAT.automatic_optimization-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr variable">
            <span class="name">automatic_optimization</span><span class="annotation">: bool</span>

                <label class="view-source-button" for="HAT.automatic_optimization-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HAT.automatic_optimization"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HAT.automatic_optimization-290"><a href="#HAT.automatic_optimization-290"><span class="linenos">290</span></a>    <span class="nd">@property</span>
</span><span id="HAT.automatic_optimization-291"><a href="#HAT.automatic_optimization-291"><span class="linenos">291</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">automatic_optimization</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
</span><span id="HAT.automatic_optimization-292"><a href="#HAT.automatic_optimization-292"><span class="linenos">292</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``.&quot;&quot;&quot;</span>
</span><span id="HAT.automatic_optimization-293"><a href="#HAT.automatic_optimization-293"><span class="linenos">293</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_automatic_optimization</span>
</span></pre></div>


            <div class="docstring"><p>If set to <code>False</code> you are responsible for calling <code>.backward()</code>, <code>.step()</code>, <code>.zero_grad()</code>.</p>
</div>


                            </div>
                            <div id="HAT.sanity_check" class="classattr">
                                        <input id="HAT.sanity_check-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">sanity_check</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span></span><span class="return-annotation">) -> <span class="kc">None</span>:</span></span>

                <label class="view-source-button" for="HAT.sanity_check-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HAT.sanity_check"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HAT.sanity_check-102"><a href="#HAT.sanity_check-102"><span class="linenos">102</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HAT.sanity_check-103"><a href="#HAT.sanity_check-103"><span class="linenos">103</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Check the sanity of the arguments.</span>
</span><span id="HAT.sanity_check-104"><a href="#HAT.sanity_check-104"><span class="linenos">104</span></a>
</span><span id="HAT.sanity_check-105"><a href="#HAT.sanity_check-105"><span class="linenos">105</span></a><span class="sd">        **Raises:**</span>
</span><span id="HAT.sanity_check-106"><a href="#HAT.sanity_check-106"><span class="linenos">106</span></a><span class="sd">        - **ValueError**: when backbone is not designed for HAT, or the `mask_sparsity_reg_mode` or `task_embedding_init_mode` is not one of the valid options. Also, if `alpha` is not given when `adjustment_mode` is &#39;hat_const_alpha&#39;.</span>
</span><span id="HAT.sanity_check-107"><a href="#HAT.sanity_check-107"><span class="linenos">107</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HAT.sanity_check-108"><a href="#HAT.sanity_check-108"><span class="linenos">108</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="p">,</span> <span class="n">HATMaskBackbone</span><span class="p">):</span>
</span><span id="HAT.sanity_check-109"><a href="#HAT.sanity_check-109"><span class="linenos">109</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The backbone should be an instance of HATMaskBackbone.&quot;</span><span class="p">)</span>
</span><span id="HAT.sanity_check-110"><a href="#HAT.sanity_check-110"><span class="linenos">110</span></a>
</span><span id="HAT.sanity_check-111"><a href="#HAT.sanity_check-111"><span class="linenos">111</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask_sparsity_reg_mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;original&quot;</span><span class="p">,</span> <span class="s2">&quot;cross&quot;</span><span class="p">]:</span>
</span><span id="HAT.sanity_check-112"><a href="#HAT.sanity_check-112"><span class="linenos">112</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="HAT.sanity_check-113"><a href="#HAT.sanity_check-113"><span class="linenos">113</span></a>                <span class="s2">&quot;The mask_sparsity_reg_mode should be one of &#39;original&#39;, &#39;cross&#39;.&quot;</span>
</span><span id="HAT.sanity_check-114"><a href="#HAT.sanity_check-114"><span class="linenos">114</span></a>            <span class="p">)</span>
</span><span id="HAT.sanity_check-115"><a href="#HAT.sanity_check-115"><span class="linenos">115</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_init_mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span>
</span><span id="HAT.sanity_check-116"><a href="#HAT.sanity_check-116"><span class="linenos">116</span></a>            <span class="s2">&quot;N01&quot;</span><span class="p">,</span>
</span><span id="HAT.sanity_check-117"><a href="#HAT.sanity_check-117"><span class="linenos">117</span></a>            <span class="s2">&quot;U01&quot;</span><span class="p">,</span>
</span><span id="HAT.sanity_check-118"><a href="#HAT.sanity_check-118"><span class="linenos">118</span></a>            <span class="s2">&quot;U-10&quot;</span><span class="p">,</span>
</span><span id="HAT.sanity_check-119"><a href="#HAT.sanity_check-119"><span class="linenos">119</span></a>            <span class="s2">&quot;masked&quot;</span><span class="p">,</span>
</span><span id="HAT.sanity_check-120"><a href="#HAT.sanity_check-120"><span class="linenos">120</span></a>            <span class="s2">&quot;unmasked&quot;</span><span class="p">,</span>
</span><span id="HAT.sanity_check-121"><a href="#HAT.sanity_check-121"><span class="linenos">121</span></a>        <span class="p">]:</span>
</span><span id="HAT.sanity_check-122"><a href="#HAT.sanity_check-122"><span class="linenos">122</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="HAT.sanity_check-123"><a href="#HAT.sanity_check-123"><span class="linenos">123</span></a>                <span class="s2">&quot;The task_embedding_init_mode should be one of &#39;N01&#39;, &#39;U01&#39;, &#39;U-10&#39;, &#39;masked&#39;, &#39;unmasked&#39;.&quot;</span>
</span><span id="HAT.sanity_check-124"><a href="#HAT.sanity_check-124"><span class="linenos">124</span></a>            <span class="p">)</span>
</span><span id="HAT.sanity_check-125"><a href="#HAT.sanity_check-125"><span class="linenos">125</span></a>
</span><span id="HAT.sanity_check-126"><a href="#HAT.sanity_check-126"><span class="linenos">126</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;hat_const_alpha&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HAT.sanity_check-127"><a href="#HAT.sanity_check-127"><span class="linenos">127</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="HAT.sanity_check-128"><a href="#HAT.sanity_check-128"><span class="linenos">128</span></a>                <span class="s2">&quot;Alpha should be given when the adjustment_mode is &#39;hat_const_alpha&#39;.&quot;</span>
</span><span id="HAT.sanity_check-129"><a href="#HAT.sanity_check-129"><span class="linenos">129</span></a>            <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Check the sanity of the arguments.</p>

<p><strong>Raises:</strong></p>

<ul>
<li><strong>ValueError</strong>: when backbone is not designed for HAT, or the <code><a href="#HAT.mask_sparsity_reg_mode">mask_sparsity_reg_mode</a></code> or <code><a href="#HAT.task_embedding_init_mode">task_embedding_init_mode</a></code> is not one of the valid options. Also, if <code><a href="#HAT.alpha">alpha</a></code> is not given when <code><a href="#HAT.adjustment_mode">adjustment_mode</a></code> is 'hat_const_alpha'.</li>
</ul>
</div>


                            </div>
                            <div id="HAT.on_train_start" class="classattr">
                                        <input id="HAT.on_train_start-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">on_train_start</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span></span><span class="return-annotation">) -> <span class="kc">None</span>:</span></span>

                <label class="view-source-button" for="HAT.on_train_start-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HAT.on_train_start"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HAT.on_train_start-131"><a href="#HAT.on_train_start-131"><span class="linenos">131</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_start</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HAT.on_train_start-132"><a href="#HAT.on_train_start-132"><span class="linenos">132</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialise the task embedding before training the next task and initialise the cumulative mask at the beginning of first task.&quot;&quot;&quot;</span>
</span><span id="HAT.on_train_start-133"><a href="#HAT.on_train_start-133"><span class="linenos">133</span></a>
</span><span id="HAT.on_train_start-134"><a href="#HAT.on_train_start-134"><span class="linenos">134</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">initialise_task_embedding</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_init_mode</span><span class="p">)</span>
</span><span id="HAT.on_train_start-135"><a href="#HAT.on_train_start-135"><span class="linenos">135</span></a>
</span><span id="HAT.on_train_start-136"><a href="#HAT.on_train_start-136"><span class="linenos">136</span></a>        <span class="c1"># initialise the cumulative mask at the beginning of first task. This should not be called in `__init__()` method as the `self.device` is not available at that time.</span>
</span><span id="HAT.on_train_start-137"><a href="#HAT.on_train_start-137"><span class="linenos">137</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="HAT.on_train_start-138"><a href="#HAT.on_train_start-138"><span class="linenos">138</span></a>            <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="HAT.on_train_start-139"><a href="#HAT.on_train_start-139"><span class="linenos">139</span></a>                <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span>
</span><span id="HAT.on_train_start-140"><a href="#HAT.on_train_start-140"><span class="linenos">140</span></a>                    <span class="n">layer_name</span>
</span><span id="HAT.on_train_start-141"><a href="#HAT.on_train_start-141"><span class="linenos">141</span></a>                <span class="p">)</span>  <span class="c1"># get the layer by its name</span>
</span><span id="HAT.on_train_start-142"><a href="#HAT.on_train_start-142"><span class="linenos">142</span></a>                <span class="n">num_units</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="HAT.on_train_start-143"><a href="#HAT.on_train_start-143"><span class="linenos">143</span></a>
</span><span id="HAT.on_train_start-144"><a href="#HAT.on_train_start-144"><span class="linenos">144</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">cumulative_mask_for_previous_tasks</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
</span><span id="HAT.on_train_start-145"><a href="#HAT.on_train_start-145"><span class="linenos">145</span></a>                    <span class="n">num_units</span>
</span><span id="HAT.on_train_start-146"><a href="#HAT.on_train_start-146"><span class="linenos">146</span></a>                <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
</span><span id="HAT.on_train_start-147"><a href="#HAT.on_train_start-147"><span class="linenos">147</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
</span><span id="HAT.on_train_start-148"><a href="#HAT.on_train_start-148"><span class="linenos">148</span></a>                <span class="p">)</span>  <span class="c1"># the cumulative mask $\mathrm{M}^{&lt;t}$ is initialised as zeros mask ($t = 1$). See equation (2) in chapter 3 in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9), or equation (5) in chapter 2.6 &quot;Promoting Low Capacity Usage&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span></pre></div>


            <div class="docstring"><p>Initialise the task embedding before training the next task and initialise the cumulative mask at the beginning of first task.</p>
</div>


                            </div>
                            <div id="HAT.clip_grad_by_adjustment" class="classattr">
                                        <input id="HAT.clip_grad_by_adjustment-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">clip_grad_by_adjustment</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="o">**</span><span class="n">kwargs</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="HAT.clip_grad_by_adjustment-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HAT.clip_grad_by_adjustment"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HAT.clip_grad_by_adjustment-150"><a href="#HAT.clip_grad_by_adjustment-150"><span class="linenos">150</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">clip_grad_by_adjustment</span><span class="p">(</span>
</span><span id="HAT.clip_grad_by_adjustment-151"><a href="#HAT.clip_grad_by_adjustment-151"><span class="linenos">151</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="HAT.clip_grad_by_adjustment-152"><a href="#HAT.clip_grad_by_adjustment-152"><span class="linenos">152</span></a>        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
</span><span id="HAT.clip_grad_by_adjustment-153"><a href="#HAT.clip_grad_by_adjustment-153"><span class="linenos">153</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="HAT.clip_grad_by_adjustment-154"><a href="#HAT.clip_grad_by_adjustment-154"><span class="linenos">154</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Clip the gradients by the adjustment rate.</span>
</span><span id="HAT.clip_grad_by_adjustment-155"><a href="#HAT.clip_grad_by_adjustment-155"><span class="linenos">155</span></a>
</span><span id="HAT.clip_grad_by_adjustment-156"><a href="#HAT.clip_grad_by_adjustment-156"><span class="linenos">156</span></a><span class="sd">        Note that as the task embedding fully covers every layer in the backbone network, no parameters are left out of this system. This applies not only the parameters in between layers with task embedding, but also those before the first layer. We designed it seperately in the codes.</span>
</span><span id="HAT.clip_grad_by_adjustment-157"><a href="#HAT.clip_grad_by_adjustment-157"><span class="linenos">157</span></a>
</span><span id="HAT.clip_grad_by_adjustment-158"><a href="#HAT.clip_grad_by_adjustment-158"><span class="linenos">158</span></a><span class="sd">        Network capacity is measured along with this method. Network capacity is defined as the average adjustment rate over all parameters. See chapter 4.1 in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="HAT.clip_grad_by_adjustment-159"><a href="#HAT.clip_grad_by_adjustment-159"><span class="linenos">159</span></a>
</span><span id="HAT.clip_grad_by_adjustment-160"><a href="#HAT.clip_grad_by_adjustment-160"><span class="linenos">160</span></a>
</span><span id="HAT.clip_grad_by_adjustment-161"><a href="#HAT.clip_grad_by_adjustment-161"><span class="linenos">161</span></a><span class="sd">        **Returns:**</span>
</span><span id="HAT.clip_grad_by_adjustment-162"><a href="#HAT.clip_grad_by_adjustment-162"><span class="linenos">162</span></a><span class="sd">        - **capacity** (`Tensor`): the calculated network capacity.</span>
</span><span id="HAT.clip_grad_by_adjustment-163"><a href="#HAT.clip_grad_by_adjustment-163"><span class="linenos">163</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HAT.clip_grad_by_adjustment-164"><a href="#HAT.clip_grad_by_adjustment-164"><span class="linenos">164</span></a>
</span><span id="HAT.clip_grad_by_adjustment-165"><a href="#HAT.clip_grad_by_adjustment-165"><span class="linenos">165</span></a>        <span class="c1"># initialise network capacity metric</span>
</span><span id="HAT.clip_grad_by_adjustment-166"><a href="#HAT.clip_grad_by_adjustment-166"><span class="linenos">166</span></a>        <span class="n">capacity</span> <span class="o">=</span> <span class="n">HATNetworkCapacity</span><span class="p">()</span>
</span><span id="HAT.clip_grad_by_adjustment-167"><a href="#HAT.clip_grad_by_adjustment-167"><span class="linenos">167</span></a>
</span><span id="HAT.clip_grad_by_adjustment-168"><a href="#HAT.clip_grad_by_adjustment-168"><span class="linenos">168</span></a>        <span class="c1"># Calculate the adjustment rate for gradients of the parameters, both weights and biases (if exists)</span>
</span><span id="HAT.clip_grad_by_adjustment-169"><a href="#HAT.clip_grad_by_adjustment-169"><span class="linenos">169</span></a>        <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="HAT.clip_grad_by_adjustment-170"><a href="#HAT.clip_grad_by_adjustment-170"><span class="linenos">170</span></a>
</span><span id="HAT.clip_grad_by_adjustment-171"><a href="#HAT.clip_grad_by_adjustment-171"><span class="linenos">171</span></a>            <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span>
</span><span id="HAT.clip_grad_by_adjustment-172"><a href="#HAT.clip_grad_by_adjustment-172"><span class="linenos">172</span></a>                <span class="n">layer_name</span>
</span><span id="HAT.clip_grad_by_adjustment-173"><a href="#HAT.clip_grad_by_adjustment-173"><span class="linenos">173</span></a>            <span class="p">)</span>  <span class="c1"># get the layer by its name</span>
</span><span id="HAT.clip_grad_by_adjustment-174"><a href="#HAT.clip_grad_by_adjustment-174"><span class="linenos">174</span></a>
</span><span id="HAT.clip_grad_by_adjustment-175"><a href="#HAT.clip_grad_by_adjustment-175"><span class="linenos">175</span></a>            <span class="c1"># placeholder for the adjustment rate to avoid the error of using it before assignment</span>
</span><span id="HAT.clip_grad_by_adjustment-176"><a href="#HAT.clip_grad_by_adjustment-176"><span class="linenos">176</span></a>            <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="HAT.clip_grad_by_adjustment-177"><a href="#HAT.clip_grad_by_adjustment-177"><span class="linenos">177</span></a>            <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="HAT.clip_grad_by_adjustment-178"><a href="#HAT.clip_grad_by_adjustment-178"><span class="linenos">178</span></a>
</span><span id="HAT.clip_grad_by_adjustment-179"><a href="#HAT.clip_grad_by_adjustment-179"><span class="linenos">179</span></a>            <span class="n">weight_mask</span><span class="p">,</span> <span class="n">bias_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_measure_parameter_wise</span><span class="p">(</span>
</span><span id="HAT.clip_grad_by_adjustment-180"><a href="#HAT.clip_grad_by_adjustment-180"><span class="linenos">180</span></a>                <span class="n">unit_wise_measure</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">cumulative_mask_for_previous_tasks</span><span class="p">,</span>
</span><span id="HAT.clip_grad_by_adjustment-181"><a href="#HAT.clip_grad_by_adjustment-181"><span class="linenos">181</span></a>                <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="HAT.clip_grad_by_adjustment-182"><a href="#HAT.clip_grad_by_adjustment-182"><span class="linenos">182</span></a>                <span class="n">aggregation</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span>
</span><span id="HAT.clip_grad_by_adjustment-183"><a href="#HAT.clip_grad_by_adjustment-183"><span class="linenos">183</span></a>            <span class="p">)</span>
</span><span id="HAT.clip_grad_by_adjustment-184"><a href="#HAT.clip_grad_by_adjustment-184"><span class="linenos">184</span></a>
</span><span id="HAT.clip_grad_by_adjustment-185"><a href="#HAT.clip_grad_by_adjustment-185"><span class="linenos">185</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;hat&quot;</span><span class="p">:</span>
</span><span id="HAT.clip_grad_by_adjustment-186"><a href="#HAT.clip_grad_by_adjustment-186"><span class="linenos">186</span></a>                <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">weight_mask</span>
</span><span id="HAT.clip_grad_by_adjustment-187"><a href="#HAT.clip_grad_by_adjustment-187"><span class="linenos">187</span></a>                <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">bias_mask</span>
</span><span id="HAT.clip_grad_by_adjustment-188"><a href="#HAT.clip_grad_by_adjustment-188"><span class="linenos">188</span></a>
</span><span id="HAT.clip_grad_by_adjustment-189"><a href="#HAT.clip_grad_by_adjustment-189"><span class="linenos">189</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;hat_random&quot;</span><span class="p">:</span>
</span><span id="HAT.clip_grad_by_adjustment-190"><a href="#HAT.clip_grad_by_adjustment-190"><span class="linenos">190</span></a>                <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand_like</span><span class="p">(</span><span class="n">weight_mask</span><span class="p">)</span> <span class="o">*</span> <span class="n">weight_mask</span> <span class="o">+</span> <span class="p">(</span>
</span><span id="HAT.clip_grad_by_adjustment-191"><a href="#HAT.clip_grad_by_adjustment-191"><span class="linenos">191</span></a>                    <span class="mi">1</span> <span class="o">-</span> <span class="n">weight_mask</span>
</span><span id="HAT.clip_grad_by_adjustment-192"><a href="#HAT.clip_grad_by_adjustment-192"><span class="linenos">192</span></a>                <span class="p">)</span>
</span><span id="HAT.clip_grad_by_adjustment-193"><a href="#HAT.clip_grad_by_adjustment-193"><span class="linenos">193</span></a>                <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand_like</span><span class="p">(</span><span class="n">bias_mask</span><span class="p">)</span> <span class="o">*</span> <span class="n">bias_mask</span> <span class="o">+</span> <span class="p">(</span>
</span><span id="HAT.clip_grad_by_adjustment-194"><a href="#HAT.clip_grad_by_adjustment-194"><span class="linenos">194</span></a>                    <span class="mi">1</span> <span class="o">-</span> <span class="n">bias_mask</span>
</span><span id="HAT.clip_grad_by_adjustment-195"><a href="#HAT.clip_grad_by_adjustment-195"><span class="linenos">195</span></a>                <span class="p">)</span>
</span><span id="HAT.clip_grad_by_adjustment-196"><a href="#HAT.clip_grad_by_adjustment-196"><span class="linenos">196</span></a>
</span><span id="HAT.clip_grad_by_adjustment-197"><a href="#HAT.clip_grad_by_adjustment-197"><span class="linenos">197</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;hat_const_alpha&quot;</span><span class="p">:</span>
</span><span id="HAT.clip_grad_by_adjustment-198"><a href="#HAT.clip_grad_by_adjustment-198"><span class="linenos">198</span></a>                <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span>
</span><span id="HAT.clip_grad_by_adjustment-199"><a href="#HAT.clip_grad_by_adjustment-199"><span class="linenos">199</span></a>                    <span class="n">weight_mask</span>
</span><span id="HAT.clip_grad_by_adjustment-200"><a href="#HAT.clip_grad_by_adjustment-200"><span class="linenos">200</span></a>                <span class="p">)</span> <span class="o">*</span> <span class="n">weight_mask</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">weight_mask</span><span class="p">)</span>
</span><span id="HAT.clip_grad_by_adjustment-201"><a href="#HAT.clip_grad_by_adjustment-201"><span class="linenos">201</span></a>                <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span>
</span><span id="HAT.clip_grad_by_adjustment-202"><a href="#HAT.clip_grad_by_adjustment-202"><span class="linenos">202</span></a>                    <span class="n">bias_mask</span>
</span><span id="HAT.clip_grad_by_adjustment-203"><a href="#HAT.clip_grad_by_adjustment-203"><span class="linenos">203</span></a>                <span class="p">)</span> <span class="o">*</span> <span class="n">bias_mask</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">bias_mask</span><span class="p">)</span>
</span><span id="HAT.clip_grad_by_adjustment-204"><a href="#HAT.clip_grad_by_adjustment-204"><span class="linenos">204</span></a>
</span><span id="HAT.clip_grad_by_adjustment-205"><a href="#HAT.clip_grad_by_adjustment-205"><span class="linenos">205</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;hat_const_1&quot;</span><span class="p">:</span>
</span><span id="HAT.clip_grad_by_adjustment-206"><a href="#HAT.clip_grad_by_adjustment-206"><span class="linenos">206</span></a>                <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">weight_mask</span><span class="p">)</span> <span class="o">*</span> <span class="n">weight_mask</span> <span class="o">+</span> <span class="p">(</span>
</span><span id="HAT.clip_grad_by_adjustment-207"><a href="#HAT.clip_grad_by_adjustment-207"><span class="linenos">207</span></a>                    <span class="mi">1</span> <span class="o">-</span> <span class="n">weight_mask</span>
</span><span id="HAT.clip_grad_by_adjustment-208"><a href="#HAT.clip_grad_by_adjustment-208"><span class="linenos">208</span></a>                <span class="p">)</span>
</span><span id="HAT.clip_grad_by_adjustment-209"><a href="#HAT.clip_grad_by_adjustment-209"><span class="linenos">209</span></a>                <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">bias_mask</span><span class="p">)</span> <span class="o">*</span> <span class="n">bias_mask</span> <span class="o">+</span> <span class="p">(</span>
</span><span id="HAT.clip_grad_by_adjustment-210"><a href="#HAT.clip_grad_by_adjustment-210"><span class="linenos">210</span></a>                    <span class="mi">1</span> <span class="o">-</span> <span class="n">bias_mask</span>
</span><span id="HAT.clip_grad_by_adjustment-211"><a href="#HAT.clip_grad_by_adjustment-211"><span class="linenos">211</span></a>                <span class="p">)</span>
</span><span id="HAT.clip_grad_by_adjustment-212"><a href="#HAT.clip_grad_by_adjustment-212"><span class="linenos">212</span></a>
</span><span id="HAT.clip_grad_by_adjustment-213"><a href="#HAT.clip_grad_by_adjustment-213"><span class="linenos">213</span></a>            <span class="c1"># apply the adjustment rate to the gradients</span>
</span><span id="HAT.clip_grad_by_adjustment-214"><a href="#HAT.clip_grad_by_adjustment-214"><span class="linenos">214</span></a>            <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">adjustment_rate_weight_layer</span>
</span><span id="HAT.clip_grad_by_adjustment-215"><a href="#HAT.clip_grad_by_adjustment-215"><span class="linenos">215</span></a>            <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HAT.clip_grad_by_adjustment-216"><a href="#HAT.clip_grad_by_adjustment-216"><span class="linenos">216</span></a>                <span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">adjustment_rate_bias_layer</span>
</span><span id="HAT.clip_grad_by_adjustment-217"><a href="#HAT.clip_grad_by_adjustment-217"><span class="linenos">217</span></a>
</span><span id="HAT.clip_grad_by_adjustment-218"><a href="#HAT.clip_grad_by_adjustment-218"><span class="linenos">218</span></a>            <span class="c1"># update network capacity metric</span>
</span><span id="HAT.clip_grad_by_adjustment-219"><a href="#HAT.clip_grad_by_adjustment-219"><span class="linenos">219</span></a>            <span class="n">capacity</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">adjustment_rate_weight_layer</span><span class="p">,</span> <span class="n">adjustment_rate_bias_layer</span><span class="p">)</span>
</span><span id="HAT.clip_grad_by_adjustment-220"><a href="#HAT.clip_grad_by_adjustment-220"><span class="linenos">220</span></a>
</span><span id="HAT.clip_grad_by_adjustment-221"><a href="#HAT.clip_grad_by_adjustment-221"><span class="linenos">221</span></a>        <span class="k">return</span> <span class="n">capacity</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
</span></pre></div>


            <div class="docstring"><p>Clip the gradients by the adjustment rate.</p>

<p>Note that as the task embedding fully covers every layer in the backbone network, no parameters are left out of this system. This applies not only the parameters in between layers with task embedding, but also those before the first layer. We designed it seperately in the codes.</p>

<p>Network capacity is measured along with this method. Network capacity is defined as the average adjustment rate over all parameters. See chapter 4.1 in <a href="https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9">AdaHAT paper</a>.</p>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>capacity</strong> (<code>Tensor</code>): the calculated network capacity.</li>
</ul>
</div>


                            </div>
                            <div id="HAT.compensate_task_embedding_gradients" class="classattr">
                                        <input id="HAT.compensate_task_embedding_gradients-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">compensate_task_embedding_gradients</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span>, </span><span class="param"><span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span></span><span class="return-annotation">) -> <span class="kc">None</span>:</span></span>

                <label class="view-source-button" for="HAT.compensate_task_embedding_gradients-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HAT.compensate_task_embedding_gradients"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HAT.compensate_task_embedding_gradients-223"><a href="#HAT.compensate_task_embedding_gradients-223"><span class="linenos">223</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">compensate_task_embedding_gradients</span><span class="p">(</span>
</span><span id="HAT.compensate_task_embedding_gradients-224"><a href="#HAT.compensate_task_embedding_gradients-224"><span class="linenos">224</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="HAT.compensate_task_embedding_gradients-225"><a href="#HAT.compensate_task_embedding_gradients-225"><span class="linenos">225</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HAT.compensate_task_embedding_gradients-226"><a href="#HAT.compensate_task_embedding_gradients-226"><span class="linenos">226</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HAT.compensate_task_embedding_gradients-227"><a href="#HAT.compensate_task_embedding_gradients-227"><span class="linenos">227</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HAT.compensate_task_embedding_gradients-228"><a href="#HAT.compensate_task_embedding_gradients-228"><span class="linenos">228</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Compensate the gradients of task embeddings during training. See chapter 2.5 &quot;Embedding Gradient Compensation&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="HAT.compensate_task_embedding_gradients-229"><a href="#HAT.compensate_task_embedding_gradients-229"><span class="linenos">229</span></a>
</span><span id="HAT.compensate_task_embedding_gradients-230"><a href="#HAT.compensate_task_embedding_gradients-230"><span class="linenos">230</span></a><span class="sd">        **Args:**</span>
</span><span id="HAT.compensate_task_embedding_gradients-231"><a href="#HAT.compensate_task_embedding_gradients-231"><span class="linenos">231</span></a><span class="sd">        - **batch_idx** (`int`): the current training batch index.</span>
</span><span id="HAT.compensate_task_embedding_gradients-232"><a href="#HAT.compensate_task_embedding_gradients-232"><span class="linenos">232</span></a><span class="sd">        - **num_batches** (`int`): the total number of training batches.</span>
</span><span id="HAT.compensate_task_embedding_gradients-233"><a href="#HAT.compensate_task_embedding_gradients-233"><span class="linenos">233</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HAT.compensate_task_embedding_gradients-234"><a href="#HAT.compensate_task_embedding_gradients-234"><span class="linenos">234</span></a>
</span><span id="HAT.compensate_task_embedding_gradients-235"><a href="#HAT.compensate_task_embedding_gradients-235"><span class="linenos">235</span></a>        <span class="k">for</span> <span class="n">te</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
</span><span id="HAT.compensate_task_embedding_gradients-236"><a href="#HAT.compensate_task_embedding_gradients-236"><span class="linenos">236</span></a>            <span class="n">anneal_scalar</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_max</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">s_max</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_max</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span>
</span><span id="HAT.compensate_task_embedding_gradients-237"><a href="#HAT.compensate_task_embedding_gradients-237"><span class="linenos">237</span></a>                <span class="n">batch_idx</span> <span class="o">-</span> <span class="mi">1</span>
</span><span id="HAT.compensate_task_embedding_gradients-238"><a href="#HAT.compensate_task_embedding_gradients-238"><span class="linenos">238</span></a>            <span class="p">)</span> <span class="o">/</span> <span class="p">(</span>
</span><span id="HAT.compensate_task_embedding_gradients-239"><a href="#HAT.compensate_task_embedding_gradients-239"><span class="linenos">239</span></a>                <span class="n">num_batches</span> <span class="o">-</span> <span class="mi">1</span>
</span><span id="HAT.compensate_task_embedding_gradients-240"><a href="#HAT.compensate_task_embedding_gradients-240"><span class="linenos">240</span></a>            <span class="p">)</span>  <span class="c1"># see equation (3) in chapter 2.4 &quot;Hard Attention Training&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a)</span>
</span><span id="HAT.compensate_task_embedding_gradients-241"><a href="#HAT.compensate_task_embedding_gradients-241"><span class="linenos">241</span></a>
</span><span id="HAT.compensate_task_embedding_gradients-242"><a href="#HAT.compensate_task_embedding_gradients-242"><span class="linenos">242</span></a>            <span class="n">num</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="HAT.compensate_task_embedding_gradients-243"><a href="#HAT.compensate_task_embedding_gradients-243"><span class="linenos">243</span></a>                <span class="n">torch</span><span class="o">.</span><span class="n">cosh</span><span class="p">(</span>
</span><span id="HAT.compensate_task_embedding_gradients-244"><a href="#HAT.compensate_task_embedding_gradients-244"><span class="linenos">244</span></a>                    <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span>
</span><span id="HAT.compensate_task_embedding_gradients-245"><a href="#HAT.compensate_task_embedding_gradients-245"><span class="linenos">245</span></a>                        <span class="n">anneal_scalar</span> <span class="o">*</span> <span class="n">te</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">,</span>
</span><span id="HAT.compensate_task_embedding_gradients-246"><a href="#HAT.compensate_task_embedding_gradients-246"><span class="linenos">246</span></a>                        <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">clamp_threshold</span><span class="p">,</span>
</span><span id="HAT.compensate_task_embedding_gradients-247"><a href="#HAT.compensate_task_embedding_gradients-247"><span class="linenos">247</span></a>                        <span class="bp">self</span><span class="o">.</span><span class="n">clamp_threshold</span><span class="p">,</span>
</span><span id="HAT.compensate_task_embedding_gradients-248"><a href="#HAT.compensate_task_embedding_gradients-248"><span class="linenos">248</span></a>                    <span class="p">)</span>
</span><span id="HAT.compensate_task_embedding_gradients-249"><a href="#HAT.compensate_task_embedding_gradients-249"><span class="linenos">249</span></a>                <span class="p">)</span>
</span><span id="HAT.compensate_task_embedding_gradients-250"><a href="#HAT.compensate_task_embedding_gradients-250"><span class="linenos">250</span></a>                <span class="o">+</span> <span class="mi">1</span>
</span><span id="HAT.compensate_task_embedding_gradients-251"><a href="#HAT.compensate_task_embedding_gradients-251"><span class="linenos">251</span></a>            <span class="p">)</span>
</span><span id="HAT.compensate_task_embedding_gradients-252"><a href="#HAT.compensate_task_embedding_gradients-252"><span class="linenos">252</span></a>
</span><span id="HAT.compensate_task_embedding_gradients-253"><a href="#HAT.compensate_task_embedding_gradients-253"><span class="linenos">253</span></a>            <span class="n">den</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cosh</span><span class="p">(</span><span class="n">te</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="HAT.compensate_task_embedding_gradients-254"><a href="#HAT.compensate_task_embedding_gradients-254"><span class="linenos">254</span></a>
</span><span id="HAT.compensate_task_embedding_gradients-255"><a href="#HAT.compensate_task_embedding_gradients-255"><span class="linenos">255</span></a>            <span class="n">compensation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_max</span> <span class="o">/</span> <span class="n">anneal_scalar</span> <span class="o">*</span> <span class="n">num</span> <span class="o">/</span> <span class="n">den</span>
</span><span id="HAT.compensate_task_embedding_gradients-256"><a href="#HAT.compensate_task_embedding_gradients-256"><span class="linenos">256</span></a>
</span><span id="HAT.compensate_task_embedding_gradients-257"><a href="#HAT.compensate_task_embedding_gradients-257"><span class="linenos">257</span></a>            <span class="n">te</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">compensation</span>
</span></pre></div>


            <div class="docstring"><p>Compensate the gradients of task embeddings during training. See chapter 2.5 "Embedding Gradient Compensation" in <a href="http://proceedings.mlr.press/v80/serra18a">HAT paper</a>.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>batch_idx</strong> (<code>int</code>): the current training batch index.</li>
<li><strong>num_batches</strong> (<code>int</code>): the total number of training batches.</li>
</ul>
</div>


                            </div>
                            <div id="HAT.forward" class="classattr">
                                        <input id="HAT.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">stage</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">task_id</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span></span><span class="return-annotation">) -> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span>:</span></span>

                <label class="view-source-button" for="HAT.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HAT.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HAT.forward-259"><a href="#HAT.forward-259"><span class="linenos">259</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="HAT.forward-260"><a href="#HAT.forward-260"><span class="linenos">260</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="HAT.forward-261"><a href="#HAT.forward-261"><span class="linenos">261</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="HAT.forward-262"><a href="#HAT.forward-262"><span class="linenos">262</span></a>        <span class="n">stage</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="HAT.forward-263"><a href="#HAT.forward-263"><span class="linenos">263</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HAT.forward-264"><a href="#HAT.forward-264"><span class="linenos">264</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HAT.forward-265"><a href="#HAT.forward-265"><span class="linenos">265</span></a>        <span class="n">task_id</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HAT.forward-266"><a href="#HAT.forward-266"><span class="linenos">266</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
</span><span id="HAT.forward-267"><a href="#HAT.forward-267"><span class="linenos">267</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The forward pass for data from task `task_id`. Note that it is nothing to do with `forward()` method in `nn.Module`.</span>
</span><span id="HAT.forward-268"><a href="#HAT.forward-268"><span class="linenos">268</span></a>
</span><span id="HAT.forward-269"><a href="#HAT.forward-269"><span class="linenos">269</span></a><span class="sd">        **Args:**</span>
</span><span id="HAT.forward-270"><a href="#HAT.forward-270"><span class="linenos">270</span></a><span class="sd">        - **input** (`Tensor`): The input tensor from data.</span>
</span><span id="HAT.forward-271"><a href="#HAT.forward-271"><span class="linenos">271</span></a><span class="sd">        - **stage** (`str`): the stage of the forward pass, should be one of the following:</span>
</span><span id="HAT.forward-272"><a href="#HAT.forward-272"><span class="linenos">272</span></a><span class="sd">            1. &#39;train&#39;: training stage.</span>
</span><span id="HAT.forward-273"><a href="#HAT.forward-273"><span class="linenos">273</span></a><span class="sd">            2. &#39;validation&#39;: validation stage.</span>
</span><span id="HAT.forward-274"><a href="#HAT.forward-274"><span class="linenos">274</span></a><span class="sd">            3. &#39;test&#39;: testing stage.</span>
</span><span id="HAT.forward-275"><a href="#HAT.forward-275"><span class="linenos">275</span></a><span class="sd">        - **batch_idx** (`int` | `None`): the current batch index. Applies only to training stage. For other stages, it is default `None`.</span>
</span><span id="HAT.forward-276"><a href="#HAT.forward-276"><span class="linenos">276</span></a><span class="sd">        - **num_batches** (`int` | `None`): the total number of batches. Applies only to training stage. For other stages, it is default `None`.</span>
</span><span id="HAT.forward-277"><a href="#HAT.forward-277"><span class="linenos">277</span></a><span class="sd">        - **task_id** (`int`| `None`): the task ID where the data are from. If the stage is &#39;train&#39; or &#39;validation&#39;, it should be the current task `self.task_id`. If stage is &#39;test&#39;, it could be from any seen task. In TIL, the task IDs of test data are provided thus this argument can be used. HAT algorithm works only for TIL.</span>
</span><span id="HAT.forward-278"><a href="#HAT.forward-278"><span class="linenos">278</span></a>
</span><span id="HAT.forward-279"><a href="#HAT.forward-279"><span class="linenos">279</span></a><span class="sd">        **Returns:**</span>
</span><span id="HAT.forward-280"><a href="#HAT.forward-280"><span class="linenos">280</span></a><span class="sd">        - **logits** (`Tensor`): the output logits tensor.</span>
</span><span id="HAT.forward-281"><a href="#HAT.forward-281"><span class="linenos">281</span></a><span class="sd">        - **mask** (`dict[str, Tensor]`): the mask for the current task. Key (`str`) is layer name, value (`Tensor`) is the mask tensor. The mask tensor has size (number of units).</span>
</span><span id="HAT.forward-282"><a href="#HAT.forward-282"><span class="linenos">282</span></a><span class="sd">        - **activations** (`dict[str, Tensor]`): the hidden features (after activation) in each weighted layer. Key (`str`) is the weighted layer name, value (`Tensor`) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes. Although HAT algorithm does not need this, it is still provided for API consistence for other HAT-based algorithms inherited this `forward()` method of `HAT` class.</span>
</span><span id="HAT.forward-283"><a href="#HAT.forward-283"><span class="linenos">283</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HAT.forward-284"><a href="#HAT.forward-284"><span class="linenos">284</span></a>        <span class="n">feature</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="p">(</span>
</span><span id="HAT.forward-285"><a href="#HAT.forward-285"><span class="linenos">285</span></a>            <span class="nb">input</span><span class="p">,</span>
</span><span id="HAT.forward-286"><a href="#HAT.forward-286"><span class="linenos">286</span></a>            <span class="n">stage</span><span class="o">=</span><span class="n">stage</span><span class="p">,</span>
</span><span id="HAT.forward-287"><a href="#HAT.forward-287"><span class="linenos">287</span></a>            <span class="n">s_max</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">s_max</span> <span class="k">if</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;train&quot;</span> <span class="ow">or</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;validation&quot;</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HAT.forward-288"><a href="#HAT.forward-288"><span class="linenos">288</span></a>            <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span> <span class="k">if</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;train&quot;</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HAT.forward-289"><a href="#HAT.forward-289"><span class="linenos">289</span></a>            <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span> <span class="k">if</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;train&quot;</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HAT.forward-290"><a href="#HAT.forward-290"><span class="linenos">290</span></a>            <span class="n">test_mask</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">masks</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">task_id</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="k">if</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;test&quot;</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HAT.forward-291"><a href="#HAT.forward-291"><span class="linenos">291</span></a>        <span class="p">)</span>
</span><span id="HAT.forward-292"><a href="#HAT.forward-292"><span class="linenos">292</span></a>        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">heads</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="n">task_id</span><span class="p">)</span>
</span><span id="HAT.forward-293"><a href="#HAT.forward-293"><span class="linenos">293</span></a>
</span><span id="HAT.forward-294"><a href="#HAT.forward-294"><span class="linenos">294</span></a>        <span class="k">return</span> <span class="n">logits</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">activations</span>
</span></pre></div>


            <div class="docstring"><p>The forward pass for data from task <code><a href="#HAT.task_id">task_id</a></code>. Note that it is nothing to do with <code><a href="#HAT.forward">forward()</a></code> method in <code>nn.Module</code>.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>input</strong> (<code>Tensor</code>): The input tensor from data.</li>
<li><strong>stage</strong> (<code>str</code>): the stage of the forward pass, should be one of the following:
<ol>
<li>'train': training stage.</li>
<li>'validation': validation stage.</li>
<li>'test': testing stage.</li>
</ol></li>
<li><strong>batch_idx</strong> (<code>int</code> | <code>None</code>): the current batch index. Applies only to training stage. For other stages, it is default <code>None</code>.</li>
<li><strong>num_batches</strong> (<code>int</code> | <code>None</code>): the total number of batches. Applies only to training stage. For other stages, it is default <code>None</code>.</li>
<li><strong>task_id</strong> (<code>int</code>| <code>None</code>): the task ID where the data are from. If the stage is 'train' or 'validation', it should be the current task <code>self.task_id</code>. If stage is 'test', it could be from any seen task. In TIL, the task IDs of test data are provided thus this argument can be used. HAT algorithm works only for TIL.</li>
</ul>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>logits</strong> (<code>Tensor</code>): the output logits tensor.</li>
<li><strong>mask</strong> (<code>dict[str, Tensor]</code>): the mask for the current task. Key (<code>str</code>) is layer name, value (<code>Tensor</code>) is the mask tensor. The mask tensor has size (number of units).</li>
<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): the hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name, value (<code>Tensor</code>) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes. Although HAT algorithm does not need this, it is still provided for API consistence for other HAT-based algorithms inherited this <code><a href="#HAT.forward">forward()</a></code> method of <code><a href="#HAT">HAT</a></code> class.</li>
</ul>
</div>


                            </div>
                            <div id="HAT.training_step" class="classattr">
                                        <input id="HAT.training_step-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">training_step</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">batch</span><span class="p">:</span> <span class="n">Any</span>, </span><span class="param"><span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span></span><span class="return-annotation">) -> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>:</span></span>

                <label class="view-source-button" for="HAT.training_step-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HAT.training_step"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HAT.training_step-296"><a href="#HAT.training_step-296"><span class="linenos">296</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
</span><span id="HAT.training_step-297"><a href="#HAT.training_step-297"><span class="linenos">297</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Training step for current task `self.task_id`.</span>
</span><span id="HAT.training_step-298"><a href="#HAT.training_step-298"><span class="linenos">298</span></a>
</span><span id="HAT.training_step-299"><a href="#HAT.training_step-299"><span class="linenos">299</span></a><span class="sd">        **Args:**</span>
</span><span id="HAT.training_step-300"><a href="#HAT.training_step-300"><span class="linenos">300</span></a><span class="sd">        - **batch** (`Any`): a batch of training data.</span>
</span><span id="HAT.training_step-301"><a href="#HAT.training_step-301"><span class="linenos">301</span></a><span class="sd">        - **batch_idx** (`int`): the index of the batch. Used for calculating annealed scalar in HAT. See chapter 2.4 &quot;Hard Attention Training&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="HAT.training_step-302"><a href="#HAT.training_step-302"><span class="linenos">302</span></a>
</span><span id="HAT.training_step-303"><a href="#HAT.training_step-303"><span class="linenos">303</span></a><span class="sd">        **Returns:**</span>
</span><span id="HAT.training_step-304"><a href="#HAT.training_step-304"><span class="linenos">304</span></a><span class="sd">        - **outputs** (`dict[str, Tensor]`): a dictionary contains loss and other metrics from this training step. Key (`str`) is the metrics name, value (`Tensor`) is the metrics. Must include the key &#39;loss&#39; which is total loss in the case of automatic optimization, according to PyTorch Lightning docs. For HAT, it includes &#39;mask&#39; and &#39;capacity&#39; for logging.</span>
</span><span id="HAT.training_step-305"><a href="#HAT.training_step-305"><span class="linenos">305</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HAT.training_step-306"><a href="#HAT.training_step-306"><span class="linenos">306</span></a>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
</span><span id="HAT.training_step-307"><a href="#HAT.training_step-307"><span class="linenos">307</span></a>
</span><span id="HAT.training_step-308"><a href="#HAT.training_step-308"><span class="linenos">308</span></a>        <span class="c1"># zero the gradients before forward pass in manual optimisation mode</span>
</span><span id="HAT.training_step-309"><a href="#HAT.training_step-309"><span class="linenos">309</span></a>        <span class="n">opt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizers</span><span class="p">()</span>
</span><span id="HAT.training_step-310"><a href="#HAT.training_step-310"><span class="linenos">310</span></a>        <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</span><span id="HAT.training_step-311"><a href="#HAT.training_step-311"><span class="linenos">311</span></a>
</span><span id="HAT.training_step-312"><a href="#HAT.training_step-312"><span class="linenos">312</span></a>        <span class="c1"># classification loss</span>
</span><span id="HAT.training_step-313"><a href="#HAT.training_step-313"><span class="linenos">313</span></a>        <span class="n">num_batches</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">num_training_batches</span>
</span><span id="HAT.training_step-314"><a href="#HAT.training_step-314"><span class="linenos">314</span></a>        <span class="n">logits</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
</span><span id="HAT.training_step-315"><a href="#HAT.training_step-315"><span class="linenos">315</span></a>            <span class="n">x</span><span class="p">,</span>
</span><span id="HAT.training_step-316"><a href="#HAT.training_step-316"><span class="linenos">316</span></a>            <span class="n">stage</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">,</span>
</span><span id="HAT.training_step-317"><a href="#HAT.training_step-317"><span class="linenos">317</span></a>            <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="HAT.training_step-318"><a href="#HAT.training_step-318"><span class="linenos">318</span></a>            <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="HAT.training_step-319"><a href="#HAT.training_step-319"><span class="linenos">319</span></a>            <span class="n">task_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">,</span>
</span><span id="HAT.training_step-320"><a href="#HAT.training_step-320"><span class="linenos">320</span></a>        <span class="p">)</span>
</span><span id="HAT.training_step-321"><a href="#HAT.training_step-321"><span class="linenos">321</span></a>        <span class="n">loss_cls</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span><span id="HAT.training_step-322"><a href="#HAT.training_step-322"><span class="linenos">322</span></a>
</span><span id="HAT.training_step-323"><a href="#HAT.training_step-323"><span class="linenos">323</span></a>        <span class="c1"># regularisation loss. See chapter 2.6 &quot;Promoting Low Capacity Usage&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="HAT.training_step-324"><a href="#HAT.training_step-324"><span class="linenos">324</span></a>        <span class="n">loss_reg</span><span class="p">,</span> <span class="n">network_sparsity</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mark_sparsity_reg</span><span class="p">(</span>
</span><span id="HAT.training_step-325"><a href="#HAT.training_step-325"><span class="linenos">325</span></a>            <span class="n">mask</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cumulative_mask_for_previous_tasks</span>
</span><span id="HAT.training_step-326"><a href="#HAT.training_step-326"><span class="linenos">326</span></a>        <span class="p">)</span>
</span><span id="HAT.training_step-327"><a href="#HAT.training_step-327"><span class="linenos">327</span></a>
</span><span id="HAT.training_step-328"><a href="#HAT.training_step-328"><span class="linenos">328</span></a>        <span class="c1"># total loss</span>
</span><span id="HAT.training_step-329"><a href="#HAT.training_step-329"><span class="linenos">329</span></a>        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_cls</span> <span class="o">+</span> <span class="n">loss_reg</span>
</span><span id="HAT.training_step-330"><a href="#HAT.training_step-330"><span class="linenos">330</span></a>
</span><span id="HAT.training_step-331"><a href="#HAT.training_step-331"><span class="linenos">331</span></a>        <span class="c1"># backward step (manually)</span>
</span><span id="HAT.training_step-332"><a href="#HAT.training_step-332"><span class="linenos">332</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">manual_backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>  <span class="c1"># calculate the gradients</span>
</span><span id="HAT.training_step-333"><a href="#HAT.training_step-333"><span class="linenos">333</span></a>        <span class="c1"># HAT hard clip gradients by the cumulative masks. See equation (2) inchapter 2.3 &quot;Network Training&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a). Network capacity is calculated along with this process. Network capacity is defined as the average adjustment rate over all parameters. See chapter 4.1 in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="HAT.training_step-334"><a href="#HAT.training_step-334"><span class="linenos">334</span></a>        <span class="n">capacity</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">clip_grad_by_adjustment</span><span class="p">(</span>
</span><span id="HAT.training_step-335"><a href="#HAT.training_step-335"><span class="linenos">335</span></a>            <span class="n">network_sparsity</span><span class="o">=</span><span class="n">network_sparsity</span><span class="p">,</span>  <span class="c1"># pass a keyword argument network sparsity here to make it compatible with AdaHAT. AdaHAT inherits this `training_step()` method.</span>
</span><span id="HAT.training_step-336"><a href="#HAT.training_step-336"><span class="linenos">336</span></a>        <span class="p">)</span>
</span><span id="HAT.training_step-337"><a href="#HAT.training_step-337"><span class="linenos">337</span></a>        <span class="c1"># compensate the gradients of task embedding. See chapter 2.5 &quot;Embedding Gradient Compensation&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="HAT.training_step-338"><a href="#HAT.training_step-338"><span class="linenos">338</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">compensate_task_embedding_gradients</span><span class="p">(</span>
</span><span id="HAT.training_step-339"><a href="#HAT.training_step-339"><span class="linenos">339</span></a>            <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="HAT.training_step-340"><a href="#HAT.training_step-340"><span class="linenos">340</span></a>            <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="HAT.training_step-341"><a href="#HAT.training_step-341"><span class="linenos">341</span></a>        <span class="p">)</span>
</span><span id="HAT.training_step-342"><a href="#HAT.training_step-342"><span class="linenos">342</span></a>        <span class="c1"># update parameters with the modified gradients</span>
</span><span id="HAT.training_step-343"><a href="#HAT.training_step-343"><span class="linenos">343</span></a>        <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span><span id="HAT.training_step-344"><a href="#HAT.training_step-344"><span class="linenos">344</span></a>
</span><span id="HAT.training_step-345"><a href="#HAT.training_step-345"><span class="linenos">345</span></a>        <span class="c1"># accuracy of the batch</span>
</span><span id="HAT.training_step-346"><a href="#HAT.training_step-346"><span class="linenos">346</span></a>        <span class="n">acc</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</span><span id="HAT.training_step-347"><a href="#HAT.training_step-347"><span class="linenos">347</span></a>
</span><span id="HAT.training_step-348"><a href="#HAT.training_step-348"><span class="linenos">348</span></a>        <span class="k">return</span> <span class="p">{</span>
</span><span id="HAT.training_step-349"><a href="#HAT.training_step-349"><span class="linenos">349</span></a>            <span class="s2">&quot;loss&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span>  <span class="c1"># Return loss is essential for training step, or backpropagation will fail</span>
</span><span id="HAT.training_step-350"><a href="#HAT.training_step-350"><span class="linenos">350</span></a>            <span class="s2">&quot;loss_cls&quot;</span><span class="p">:</span> <span class="n">loss_cls</span><span class="p">,</span>
</span><span id="HAT.training_step-351"><a href="#HAT.training_step-351"><span class="linenos">351</span></a>            <span class="s2">&quot;loss_reg&quot;</span><span class="p">:</span> <span class="n">loss_reg</span><span class="p">,</span>
</span><span id="HAT.training_step-352"><a href="#HAT.training_step-352"><span class="linenos">352</span></a>            <span class="s2">&quot;acc&quot;</span><span class="p">:</span> <span class="n">acc</span><span class="p">,</span>
</span><span id="HAT.training_step-353"><a href="#HAT.training_step-353"><span class="linenos">353</span></a>            <span class="s2">&quot;activations&quot;</span><span class="p">:</span> <span class="n">activations</span><span class="p">,</span>
</span><span id="HAT.training_step-354"><a href="#HAT.training_step-354"><span class="linenos">354</span></a>            <span class="s2">&quot;mask&quot;</span><span class="p">:</span> <span class="n">mask</span><span class="p">,</span>  <span class="c1"># Return other metrics for lightning loggers callback to handle at `on_train_batch_end()`</span>
</span><span id="HAT.training_step-355"><a href="#HAT.training_step-355"><span class="linenos">355</span></a>            <span class="s2">&quot;capacity&quot;</span><span class="p">:</span> <span class="n">capacity</span><span class="p">,</span>
</span><span id="HAT.training_step-356"><a href="#HAT.training_step-356"><span class="linenos">356</span></a>        <span class="p">}</span>
</span></pre></div>


            <div class="docstring"><p>Training step for current task <code>self.task_id</code>.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>batch</strong> (<code>Any</code>): a batch of training data.</li>
<li><strong>batch_idx</strong> (<code>int</code>): the index of the batch. Used for calculating annealed scalar in HAT. See chapter 2.4 "Hard Attention Training" in <a href="http://proceedings.mlr.press/v80/serra18a">HAT paper</a>.</li>
</ul>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and other metrics from this training step. Key (<code>str</code>) is the metrics name, value (<code>Tensor</code>) is the metrics. Must include the key 'loss' which is total loss in the case of automatic optimization, according to PyTorch Lightning docs. For HAT, it includes 'mask' and 'capacity' for logging.</li>
</ul>
</div>


                            </div>
                            <div id="HAT.on_train_end" class="classattr">
                                        <input id="HAT.on_train_end-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">on_train_end</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span></span><span class="return-annotation">) -> <span class="kc">None</span>:</span></span>

                <label class="view-source-button" for="HAT.on_train_end-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HAT.on_train_end"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HAT.on_train_end-358"><a href="#HAT.on_train_end-358"><span class="linenos">358</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_end</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HAT.on_train_end-359"><a href="#HAT.on_train_end-359"><span class="linenos">359</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the mask and update cumulative mask after training the task.&quot;&quot;&quot;</span>
</span><span id="HAT.on_train_end-360"><a href="#HAT.on_train_end-360"><span class="linenos">360</span></a>
</span><span id="HAT.on_train_end-361"><a href="#HAT.on_train_end-361"><span class="linenos">361</span></a>        <span class="c1"># store the mask for the current task</span>
</span><span id="HAT.on_train_end-362"><a href="#HAT.on_train_end-362"><span class="linenos">362</span></a>        <span class="n">mask_t</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="HAT.on_train_end-363"><a href="#HAT.on_train_end-363"><span class="linenos">363</span></a>            <span class="n">layer_name</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">gate_fn</span><span class="p">(</span>
</span><span id="HAT.on_train_end-364"><a href="#HAT.on_train_end-364"><span class="linenos">364</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_max</span>
</span><span id="HAT.on_train_end-365"><a href="#HAT.on_train_end-365"><span class="linenos">365</span></a>            <span class="p">)</span>
</span><span id="HAT.on_train_end-366"><a href="#HAT.on_train_end-366"><span class="linenos">366</span></a>            <span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
</span><span id="HAT.on_train_end-367"><a href="#HAT.on_train_end-367"><span class="linenos">367</span></a>            <span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="HAT.on_train_end-368"><a href="#HAT.on_train_end-368"><span class="linenos">368</span></a>            <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span>
</span><span id="HAT.on_train_end-369"><a href="#HAT.on_train_end-369"><span class="linenos">369</span></a>        <span class="p">}</span>
</span><span id="HAT.on_train_end-370"><a href="#HAT.on_train_end-370"><span class="linenos">370</span></a>
</span><span id="HAT.on_train_end-371"><a href="#HAT.on_train_end-371"><span class="linenos">371</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">masks</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mask_t</span>
</span><span id="HAT.on_train_end-372"><a href="#HAT.on_train_end-372"><span class="linenos">372</span></a>
</span><span id="HAT.on_train_end-373"><a href="#HAT.on_train_end-373"><span class="linenos">373</span></a>        <span class="c1"># update the cumulative and summative masks</span>
</span><span id="HAT.on_train_end-374"><a href="#HAT.on_train_end-374"><span class="linenos">374</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">cumulative_mask_for_previous_tasks</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="HAT.on_train_end-375"><a href="#HAT.on_train_end-375"><span class="linenos">375</span></a>            <span class="n">layer_name</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span>
</span><span id="HAT.on_train_end-376"><a href="#HAT.on_train_end-376"><span class="linenos">376</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">cumulative_mask_for_previous_tasks</span><span class="p">[</span><span class="n">layer_name</span><span class="p">],</span> <span class="n">mask_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="HAT.on_train_end-377"><a href="#HAT.on_train_end-377"><span class="linenos">377</span></a>            <span class="p">)</span>
</span><span id="HAT.on_train_end-378"><a href="#HAT.on_train_end-378"><span class="linenos">378</span></a>            <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span>
</span><span id="HAT.on_train_end-379"><a href="#HAT.on_train_end-379"><span class="linenos">379</span></a>        <span class="p">}</span>
</span></pre></div>


            <div class="docstring"><p>Store the mask and update cumulative mask after training the task.</p>
</div>


                            </div>
                            <div id="HAT.validation_step" class="classattr">
                                        <input id="HAT.validation_step-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">validation_step</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">batch</span><span class="p">:</span> <span class="n">Any</span></span><span class="return-annotation">) -> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>:</span></span>

                <label class="view-source-button" for="HAT.validation_step-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HAT.validation_step"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HAT.validation_step-381"><a href="#HAT.validation_step-381"><span class="linenos">381</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
</span><span id="HAT.validation_step-382"><a href="#HAT.validation_step-382"><span class="linenos">382</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Validation step for current task `self.task_id`.</span>
</span><span id="HAT.validation_step-383"><a href="#HAT.validation_step-383"><span class="linenos">383</span></a>
</span><span id="HAT.validation_step-384"><a href="#HAT.validation_step-384"><span class="linenos">384</span></a><span class="sd">        **Args:**</span>
</span><span id="HAT.validation_step-385"><a href="#HAT.validation_step-385"><span class="linenos">385</span></a><span class="sd">        - **batch** (`Any`): a batch of validation data.</span>
</span><span id="HAT.validation_step-386"><a href="#HAT.validation_step-386"><span class="linenos">386</span></a>
</span><span id="HAT.validation_step-387"><a href="#HAT.validation_step-387"><span class="linenos">387</span></a><span class="sd">        **Returns:**</span>
</span><span id="HAT.validation_step-388"><a href="#HAT.validation_step-388"><span class="linenos">388</span></a><span class="sd">        - **outputs** (`dict[str, Tensor]`): a dictionary contains loss and other metrics from this validation step. Key (`str`) is the metrics name, value (`Tensor`) is the metrics.</span>
</span><span id="HAT.validation_step-389"><a href="#HAT.validation_step-389"><span class="linenos">389</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HAT.validation_step-390"><a href="#HAT.validation_step-390"><span class="linenos">390</span></a>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
</span><span id="HAT.validation_step-391"><a href="#HAT.validation_step-391"><span class="linenos">391</span></a>        <span class="n">logits</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
</span><span id="HAT.validation_step-392"><a href="#HAT.validation_step-392"><span class="linenos">392</span></a>            <span class="n">x</span><span class="p">,</span> <span class="n">stage</span><span class="o">=</span><span class="s2">&quot;validation&quot;</span><span class="p">,</span> <span class="n">task_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span>
</span><span id="HAT.validation_step-393"><a href="#HAT.validation_step-393"><span class="linenos">393</span></a>        <span class="p">)</span>
</span><span id="HAT.validation_step-394"><a href="#HAT.validation_step-394"><span class="linenos">394</span></a>        <span class="n">loss_cls</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span><span id="HAT.validation_step-395"><a href="#HAT.validation_step-395"><span class="linenos">395</span></a>        <span class="n">acc</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</span><span id="HAT.validation_step-396"><a href="#HAT.validation_step-396"><span class="linenos">396</span></a>
</span><span id="HAT.validation_step-397"><a href="#HAT.validation_step-397"><span class="linenos">397</span></a>        <span class="k">return</span> <span class="p">{</span>
</span><span id="HAT.validation_step-398"><a href="#HAT.validation_step-398"><span class="linenos">398</span></a>            <span class="s2">&quot;loss_cls&quot;</span><span class="p">:</span> <span class="n">loss_cls</span><span class="p">,</span>
</span><span id="HAT.validation_step-399"><a href="#HAT.validation_step-399"><span class="linenos">399</span></a>            <span class="s2">&quot;acc&quot;</span><span class="p">:</span> <span class="n">acc</span><span class="p">,</span>  <span class="c1"># Return metrics for lightning loggers callback to handle at `on_validation_batch_end()`</span>
</span><span id="HAT.validation_step-400"><a href="#HAT.validation_step-400"><span class="linenos">400</span></a>        <span class="p">}</span>
</span></pre></div>


            <div class="docstring"><p>Validation step for current task <code>self.task_id</code>.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>batch</strong> (<code>Any</code>): a batch of validation data.</li>
</ul>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and other metrics from this validation step. Key (<code>str</code>) is the metrics name, value (<code>Tensor</code>) is the metrics.</li>
</ul>
</div>


                            </div>
                            <div id="HAT.test_step" class="classattr">
                                        <input id="HAT.test_step-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">test_step</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">batch</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dataloader</span><span class="o">.</span><span class="n">DataLoader</span>,</span><span class="param">	<span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">dataloader_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span></span><span class="return-annotation">) -> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>:</span></span>

                <label class="view-source-button" for="HAT.test_step-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HAT.test_step"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HAT.test_step-402"><a href="#HAT.test_step-402"><span class="linenos">402</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">test_step</span><span class="p">(</span>
</span><span id="HAT.test_step-403"><a href="#HAT.test_step-403"><span class="linenos">403</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="HAT.test_step-404"><a href="#HAT.test_step-404"><span class="linenos">404</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
</span><span id="HAT.test_step-405"><a href="#HAT.test_step-405"><span class="linenos">405</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Test step for current task `self.task_id`, which tests for all seen tasks indexed by `dataloader_idx`.</span>
</span><span id="HAT.test_step-406"><a href="#HAT.test_step-406"><span class="linenos">406</span></a>
</span><span id="HAT.test_step-407"><a href="#HAT.test_step-407"><span class="linenos">407</span></a><span class="sd">        **Args:**</span>
</span><span id="HAT.test_step-408"><a href="#HAT.test_step-408"><span class="linenos">408</span></a><span class="sd">        - **batch** (`Any`): a batch of test data.</span>
</span><span id="HAT.test_step-409"><a href="#HAT.test_step-409"><span class="linenos">409</span></a><span class="sd">        - **dataloader_idx** (`int`): the task ID of seen tasks to be tested. A default value of 0 is given otherwise the LightningModule will raise a `RuntimeError`.</span>
</span><span id="HAT.test_step-410"><a href="#HAT.test_step-410"><span class="linenos">410</span></a>
</span><span id="HAT.test_step-411"><a href="#HAT.test_step-411"><span class="linenos">411</span></a><span class="sd">        **Returns:**</span>
</span><span id="HAT.test_step-412"><a href="#HAT.test_step-412"><span class="linenos">412</span></a><span class="sd">        - **outputs** (`dict[str, Tensor]`): a dictionary contains loss and other metrics from this test step. Key (`str`) is the metrics name, value (`Tensor`) is the metrics.</span>
</span><span id="HAT.test_step-413"><a href="#HAT.test_step-413"><span class="linenos">413</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HAT.test_step-414"><a href="#HAT.test_step-414"><span class="linenos">414</span></a>        <span class="n">test_task_id</span> <span class="o">=</span> <span class="n">dataloader_idx</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="HAT.test_step-415"><a href="#HAT.test_step-415"><span class="linenos">415</span></a>
</span><span id="HAT.test_step-416"><a href="#HAT.test_step-416"><span class="linenos">416</span></a>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
</span><span id="HAT.test_step-417"><a href="#HAT.test_step-417"><span class="linenos">417</span></a>        <span class="n">logits</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
</span><span id="HAT.test_step-418"><a href="#HAT.test_step-418"><span class="linenos">418</span></a>            <span class="n">x</span><span class="p">,</span>
</span><span id="HAT.test_step-419"><a href="#HAT.test_step-419"><span class="linenos">419</span></a>            <span class="n">stage</span><span class="o">=</span><span class="s2">&quot;test&quot;</span><span class="p">,</span>
</span><span id="HAT.test_step-420"><a href="#HAT.test_step-420"><span class="linenos">420</span></a>            <span class="n">task_id</span><span class="o">=</span><span class="n">test_task_id</span><span class="p">,</span>
</span><span id="HAT.test_step-421"><a href="#HAT.test_step-421"><span class="linenos">421</span></a>        <span class="p">)</span>  <span class="c1"># use the corresponding head and mask to test (instead of the current task `self.task_id`)</span>
</span><span id="HAT.test_step-422"><a href="#HAT.test_step-422"><span class="linenos">422</span></a>        <span class="n">loss_cls</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span><span id="HAT.test_step-423"><a href="#HAT.test_step-423"><span class="linenos">423</span></a>        <span class="n">acc</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</span><span id="HAT.test_step-424"><a href="#HAT.test_step-424"><span class="linenos">424</span></a>
</span><span id="HAT.test_step-425"><a href="#HAT.test_step-425"><span class="linenos">425</span></a>        <span class="k">return</span> <span class="p">{</span>
</span><span id="HAT.test_step-426"><a href="#HAT.test_step-426"><span class="linenos">426</span></a>            <span class="s2">&quot;loss_cls&quot;</span><span class="p">:</span> <span class="n">loss_cls</span><span class="p">,</span>
</span><span id="HAT.test_step-427"><a href="#HAT.test_step-427"><span class="linenos">427</span></a>            <span class="s2">&quot;acc&quot;</span><span class="p">:</span> <span class="n">acc</span><span class="p">,</span>  <span class="c1"># Return metrics for lightning loggers callback to handle at `on_test_batch_end()`</span>
</span><span id="HAT.test_step-428"><a href="#HAT.test_step-428"><span class="linenos">428</span></a>        <span class="p">}</span>
</span></pre></div>


            <div class="docstring"><p>Test step for current task <code>self.task_id</code>, which tests for all seen tasks indexed by <code>dataloader_idx</code>.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>batch</strong> (<code>Any</code>): a batch of test data.</li>
<li><strong>dataloader_idx</strong> (<code>int</code>): the task ID of seen tasks to be tested. A default value of 0 is given otherwise the LightningModule will raise a <code>RuntimeError</code>.</li>
</ul>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and other metrics from this test step. Key (<code>str</code>) is the metrics name, value (<code>Tensor</code>) is the metrics.</li>
</ul>
</div>


                            </div>
                </section>
    </main>
<script>
    function escapeHTML(html) {
        return document.createElement('div').appendChild(document.createTextNode(html)).parentNode.innerHTML;
    }

    const originalContent = document.querySelector("main.pdoc");
    let currentContent = originalContent;

    function setContent(innerHTML) {
        let elem;
        if (innerHTML) {
            elem = document.createElement("main");
            elem.classList.add("pdoc");
            elem.innerHTML = innerHTML;
        } else {
            elem = originalContent;
        }
        if (currentContent !== elem) {
            currentContent.replaceWith(elem);
            currentContent = elem;
        }
    }

    function getSearchTerm() {
        return (new URL(window.location)).searchParams.get("search");
    }

    const searchBox = document.querySelector(".pdoc input[type=search]");
    searchBox.addEventListener("input", function () {
        let url = new URL(window.location);
        if (searchBox.value.trim()) {
            url.hash = "";
            url.searchParams.set("search", searchBox.value);
        } else {
            url.searchParams.delete("search");
        }
        history.replaceState("", "", url.toString());
        onInput();
    });
    window.addEventListener("popstate", onInput);


    let search, searchErr;

    async function initialize() {
        try {
            search = await new Promise((resolve, reject) => {
                const script = document.createElement("script");
                script.type = "text/javascript";
                script.async = true;
                script.onload = () => resolve(window.pdocSearch);
                script.onerror = (e) => reject(e);
                script.src = "../../search.js";
                document.getElementsByTagName("head")[0].appendChild(script);
            });
        } catch (e) {
            console.error("Cannot fetch pdoc search index");
            searchErr = "Cannot fetch search index.";
        }
        onInput();

        document.querySelector("nav.pdoc").addEventListener("click", e => {
            if (e.target.hash) {
                searchBox.value = "";
                searchBox.dispatchEvent(new Event("input"));
            }
        });
    }

    function onInput() {
        setContent((() => {
            const term = getSearchTerm();
            if (!term) {
                return null
            }
            if (searchErr) {
                return `<h3>Error: ${searchErr}</h3>`
            }
            if (!search) {
                return "<h3>Searching...</h3>"
            }

            window.scrollTo({top: 0, left: 0, behavior: 'auto'});

            const results = search(term);

            let html;
            if (results.length === 0) {
                html = `No search results for '${escapeHTML(term)}'.`
            } else {
                html = `<h4>${results.length} search result${results.length > 1 ? "s" : ""} for '${escapeHTML(term)}'.</h4>`;
            }
            for (let result of results.slice(0, 10)) {
                let doc = result.doc;
                let url = `../../${doc.modulename.replaceAll(".", "/")}.html`;
                if (doc.qualname) {
                    url += `#${doc.qualname}`;
                }

                let heading;
                switch (result.doc.kind) {
                    case "function":
                        if (doc.fullname.endsWith(".__init__")) {
                            heading = `<span class="name">${doc.fullname.replace(/\.__init__$/, "")}</span>${doc.signature}`;
                        } else {
                            heading = `<span class="def">${doc.funcdef}</span> <span class="name">${doc.fullname}</span>${doc.signature}`;
                        }
                        break;
                    case "class":
                        heading = `<span class="def">class</span> <span class="name">${doc.fullname}</span>`;
                        if (doc.bases)
                            heading += `<wbr>(<span class="base">${doc.bases}</span>)`;
                        heading += `:`;
                        break;
                    case "variable":
                        heading = `<span class="name">${doc.fullname}</span>`;
                        if (doc.annotation)
                            heading += `<span class="annotation">${doc.annotation}</span>`;
                        if (doc.default_value)
                            heading += `<span class="default_value"> = ${doc.default_value}</span>`;
                        break;
                    default:
                        heading = `<span class="name">${doc.fullname}</span>`;
                        break;
                }
                html += `
                        <section class="search-result">
                        <a href="${url}" class="attr ${doc.kind}">${heading}</a>
                        <div class="docstring">${doc.doc}</div>
                        </section>
                    `;

            }
            return html;
        })());
    }

    if (getSearchTerm()) {
        initialize();
        searchBox.value = getSearchTerm();
        onInput();
    } else {
        searchBox.addEventListener("focus", initialize, {once: true});
    }

    searchBox.addEventListener("keydown", e => {
        if (["ArrowDown", "ArrowUp", "Enter"].includes(e.key)) {
            let focused = currentContent.querySelector(".search-result.focused");
            if (!focused) {
                currentContent.querySelector(".search-result").classList.add("focused");
            } else if (
                e.key === "ArrowDown"
                && focused.nextElementSibling
                && focused.nextElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.nextElementSibling.classList.add("focused");
                focused.nextElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "ArrowUp"
                && focused.previousElementSibling
                && focused.previousElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.previousElementSibling.classList.add("focused");
                focused.previousElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "Enter"
            ) {
                focused.querySelector("a").click();
            }
        }
    });
</script></body>
</html>