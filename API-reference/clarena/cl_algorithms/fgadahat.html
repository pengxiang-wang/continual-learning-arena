<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="pdoc 15.0.1"/>
    <title>clarena.cl_algorithms.fgadahat API documentation</title>

    <style>/*! * Bootstrap Reboot v5.0.0 (https://getbootstrap.com/) * Copyright 2011-2021 The Bootstrap Authors * Copyright 2011-2021 Twitter, Inc. * Licensed under MIT (https://github.com/twbs/bootstrap/blob/main/LICENSE) * Forked from Normalize.css, licensed MIT (https://github.com/necolas/normalize.css/blob/master/LICENSE.md) */*,::after,::before{box-sizing:border-box}@media (prefers-reduced-motion:no-preference){:root{scroll-behavior:smooth}}body{margin:0;font-family:system-ui,-apple-system,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans","Liberation Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";font-size:1rem;font-weight:400;line-height:1.5;color:#212529;background-color:#fff;-webkit-text-size-adjust:100%;-webkit-tap-highlight-color:transparent}hr{margin:1rem 0;color:inherit;background-color:currentColor;border:0;opacity:.25}hr:not([size]){height:1px}h1,h2,h3,h4,h5,h6{margin-top:0;margin-bottom:.5rem;font-weight:500;line-height:1.2}h1{font-size:calc(1.375rem + 1.5vw)}@media (min-width:1200px){h1{font-size:2.5rem}}h2{font-size:calc(1.325rem + .9vw)}@media (min-width:1200px){h2{font-size:2rem}}h3{font-size:calc(1.3rem + .6vw)}@media (min-width:1200px){h3{font-size:1.75rem}}h4{font-size:calc(1.275rem + .3vw)}@media (min-width:1200px){h4{font-size:1.5rem}}h5{font-size:1.25rem}h6{font-size:1rem}p{margin-top:0;margin-bottom:1rem}abbr[data-bs-original-title],abbr[title]{-webkit-text-decoration:underline dotted;text-decoration:underline dotted;cursor:help;-webkit-text-decoration-skip-ink:none;text-decoration-skip-ink:none}address{margin-bottom:1rem;font-style:normal;line-height:inherit}ol,ul{padding-left:2rem}dl,ol,ul{margin-top:0;margin-bottom:1rem}ol ol,ol ul,ul ol,ul ul{margin-bottom:0}dt{font-weight:700}dd{margin-bottom:.5rem;margin-left:0}blockquote{margin:0 0 1rem}b,strong{font-weight:bolder}small{font-size:.875em}mark{padding:.2em;background-color:#fcf8e3}sub,sup{position:relative;font-size:.75em;line-height:0;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}a{color:#0d6efd;text-decoration:underline}a:hover{color:#0a58ca}a:not([href]):not([class]),a:not([href]):not([class]):hover{color:inherit;text-decoration:none}code,kbd,pre,samp{font-family:SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;font-size:1em;direction:ltr;unicode-bidi:bidi-override}pre{display:block;margin-top:0;margin-bottom:1rem;overflow:auto;font-size:.875em}pre code{font-size:inherit;color:inherit;word-break:normal}code{font-size:.875em;color:#d63384;word-wrap:break-word}a>code{color:inherit}kbd{padding:.2rem .4rem;font-size:.875em;color:#fff;background-color:#212529;border-radius:.2rem}kbd kbd{padding:0;font-size:1em;font-weight:700}figure{margin:0 0 1rem}img,svg{vertical-align:middle}table{caption-side:bottom;border-collapse:collapse}caption{padding-top:.5rem;padding-bottom:.5rem;color:#6c757d;text-align:left}th{text-align:inherit;text-align:-webkit-match-parent}tbody,td,tfoot,th,thead,tr{border-color:inherit;border-style:solid;border-width:0}label{display:inline-block}button{border-radius:0}button:focus:not(:focus-visible){outline:0}button,input,optgroup,select,textarea{margin:0;font-family:inherit;font-size:inherit;line-height:inherit}button,select{text-transform:none}[role=button]{cursor:pointer}select{word-wrap:normal}select:disabled{opacity:1}[list]::-webkit-calendar-picker-indicator{display:none}[type=button],[type=reset],[type=submit],button{-webkit-appearance:button}[type=button]:not(:disabled),[type=reset]:not(:disabled),[type=submit]:not(:disabled),button:not(:disabled){cursor:pointer}::-moz-focus-inner{padding:0;border-style:none}textarea{resize:vertical}fieldset{min-width:0;padding:0;margin:0;border:0}legend{float:left;width:100%;padding:0;margin-bottom:.5rem;font-size:calc(1.275rem + .3vw);line-height:inherit}@media (min-width:1200px){legend{font-size:1.5rem}}legend+*{clear:left}::-webkit-datetime-edit-day-field,::-webkit-datetime-edit-fields-wrapper,::-webkit-datetime-edit-hour-field,::-webkit-datetime-edit-minute,::-webkit-datetime-edit-month-field,::-webkit-datetime-edit-text,::-webkit-datetime-edit-year-field{padding:0}::-webkit-inner-spin-button{height:auto}[type=search]{outline-offset:-2px;-webkit-appearance:textfield}::-webkit-search-decoration{-webkit-appearance:none}::-webkit-color-swatch-wrapper{padding:0}::file-selector-button{font:inherit}::-webkit-file-upload-button{font:inherit;-webkit-appearance:button}output{display:inline-block}iframe{border:0}summary{display:list-item;cursor:pointer}progress{vertical-align:baseline}[hidden]{display:none!important}</style>
    <style>/*! syntax-highlighting.css */pre{line-height:125%;}span.linenos{color:inherit; background-color:transparent; padding-left:5px; padding-right:20px;}.pdoc-code .hll{background-color:#ffffcc}.pdoc-code{background:#f8f8f8;}.pdoc-code .c{color:#3D7B7B; font-style:italic}.pdoc-code .err{border:1px solid #FF0000}.pdoc-code .k{color:#008000; font-weight:bold}.pdoc-code .o{color:#666666}.pdoc-code .ch{color:#3D7B7B; font-style:italic}.pdoc-code .cm{color:#3D7B7B; font-style:italic}.pdoc-code .cp{color:#9C6500}.pdoc-code .cpf{color:#3D7B7B; font-style:italic}.pdoc-code .c1{color:#3D7B7B; font-style:italic}.pdoc-code .cs{color:#3D7B7B; font-style:italic}.pdoc-code .gd{color:#A00000}.pdoc-code .ge{font-style:italic}.pdoc-code .gr{color:#E40000}.pdoc-code .gh{color:#000080; font-weight:bold}.pdoc-code .gi{color:#008400}.pdoc-code .go{color:#717171}.pdoc-code .gp{color:#000080; font-weight:bold}.pdoc-code .gs{font-weight:bold}.pdoc-code .gu{color:#800080; font-weight:bold}.pdoc-code .gt{color:#0044DD}.pdoc-code .kc{color:#008000; font-weight:bold}.pdoc-code .kd{color:#008000; font-weight:bold}.pdoc-code .kn{color:#008000; font-weight:bold}.pdoc-code .kp{color:#008000}.pdoc-code .kr{color:#008000; font-weight:bold}.pdoc-code .kt{color:#B00040}.pdoc-code .m{color:#666666}.pdoc-code .s{color:#BA2121}.pdoc-code .na{color:#687822}.pdoc-code .nb{color:#008000}.pdoc-code .nc{color:#0000FF; font-weight:bold}.pdoc-code .no{color:#880000}.pdoc-code .nd{color:#AA22FF}.pdoc-code .ni{color:#717171; font-weight:bold}.pdoc-code .ne{color:#CB3F38; font-weight:bold}.pdoc-code .nf{color:#0000FF}.pdoc-code .nl{color:#767600}.pdoc-code .nn{color:#0000FF; font-weight:bold}.pdoc-code .nt{color:#008000; font-weight:bold}.pdoc-code .nv{color:#19177C}.pdoc-code .ow{color:#AA22FF; font-weight:bold}.pdoc-code .w{color:#bbbbbb}.pdoc-code .mb{color:#666666}.pdoc-code .mf{color:#666666}.pdoc-code .mh{color:#666666}.pdoc-code .mi{color:#666666}.pdoc-code .mo{color:#666666}.pdoc-code .sa{color:#BA2121}.pdoc-code .sb{color:#BA2121}.pdoc-code .sc{color:#BA2121}.pdoc-code .dl{color:#BA2121}.pdoc-code .sd{color:#BA2121; font-style:italic}.pdoc-code .s2{color:#BA2121}.pdoc-code .se{color:#AA5D1F; font-weight:bold}.pdoc-code .sh{color:#BA2121}.pdoc-code .si{color:#A45A77; font-weight:bold}.pdoc-code .sx{color:#008000}.pdoc-code .sr{color:#A45A77}.pdoc-code .s1{color:#BA2121}.pdoc-code .ss{color:#19177C}.pdoc-code .bp{color:#008000}.pdoc-code .fm{color:#0000FF}.pdoc-code .vc{color:#19177C}.pdoc-code .vg{color:#19177C}.pdoc-code .vi{color:#19177C}.pdoc-code .vm{color:#19177C}.pdoc-code .il{color:#666666}</style>
    <style>/*! theme.css */:root{--pdoc-background:#fff;}.pdoc{--text:#212529;--muted:#6c757d;--link:#3660a5;--link-hover:#1659c5;--code:#f8f8f8;--active:#fff598;--accent:#eee;--accent2:#c1c1c1;--nav-hover:rgba(255, 255, 255, 0.5);--name:#0066BB;--def:#008800;--annotation:#007020;}</style>
    <style>/*! layout.css */html, body{width:100%;height:100%;}html, main{scroll-behavior:smooth;}body{background-color:var(--pdoc-background);}@media (max-width:769px){#navtoggle{cursor:pointer;position:absolute;width:50px;height:40px;top:1rem;right:1rem;border-color:var(--text);color:var(--text);display:flex;opacity:0.8;z-index:999;}#navtoggle:hover{opacity:1;}#togglestate + div{display:none;}#togglestate:checked + div{display:inherit;}main, header{padding:2rem 3vw;}header + main{margin-top:-3rem;}.git-button{display:none !important;}nav input[type="search"]{max-width:77%;}nav input[type="search"]:first-child{margin-top:-6px;}nav input[type="search"]:valid ~ *{display:none !important;}}@media (min-width:770px){:root{--sidebar-width:clamp(12.5rem, 28vw, 22rem);}nav{position:fixed;overflow:auto;height:100vh;width:var(--sidebar-width);}main, header{padding:3rem 2rem 3rem calc(var(--sidebar-width) + 3rem);width:calc(54rem + var(--sidebar-width));max-width:100%;}header + main{margin-top:-4rem;}#navtoggle{display:none;}}#togglestate{position:absolute;height:0;opacity:0;}nav.pdoc{--pad:clamp(0.5rem, 2vw, 1.75rem);--indent:1.5rem;background-color:var(--accent);border-right:1px solid var(--accent2);box-shadow:0 0 20px rgba(50, 50, 50, .2) inset;padding:0 0 0 var(--pad);overflow-wrap:anywhere;scrollbar-width:thin; scrollbar-color:var(--accent2) transparent; z-index:1}nav.pdoc::-webkit-scrollbar{width:.4rem; }nav.pdoc::-webkit-scrollbar-thumb{background-color:var(--accent2); }nav.pdoc > div{padding:var(--pad) 0;}nav.pdoc .module-list-button{display:inline-flex;align-items:center;color:var(--text);border-color:var(--muted);margin-bottom:1rem;}nav.pdoc .module-list-button:hover{border-color:var(--text);}nav.pdoc input[type=search]{display:block;outline-offset:0;width:calc(100% - var(--pad));}nav.pdoc .logo{max-width:calc(100% - var(--pad));max-height:35vh;display:block;margin:0 auto 1rem;transform:translate(calc(-.5 * var(--pad)), 0);}nav.pdoc ul{list-style:none;padding-left:0;}nav.pdoc > div > ul{margin-left:calc(0px - var(--pad));}nav.pdoc li a{padding:.2rem 0 .2rem calc(var(--pad) + var(--indent));}nav.pdoc > div > ul > li > a{padding-left:var(--pad);}nav.pdoc li{transition:all 100ms;}nav.pdoc li:hover{background-color:var(--nav-hover);}nav.pdoc a, nav.pdoc a:hover{color:var(--text);}nav.pdoc a{display:block;}nav.pdoc > h2:first-of-type{margin-top:1.5rem;}nav.pdoc .class:before{content:"class ";color:var(--muted);}nav.pdoc .function:after{content:"()";color:var(--muted);}nav.pdoc footer:before{content:"";display:block;width:calc(100% - var(--pad));border-top:solid var(--accent2) 1px;margin-top:1.5rem;padding-top:.5rem;}nav.pdoc footer{font-size:small;}</style>
    <style>/*! content.css */.pdoc{color:var(--text);box-sizing:border-box;line-height:1.5;background:none;}.pdoc .pdoc-button{cursor:pointer;display:inline-block;border:solid black 1px;border-radius:2px;font-size:.75rem;padding:calc(0.5em - 1px) 1em;transition:100ms all;}.pdoc .alert{padding:1rem 1rem 1rem calc(1.5rem + 24px);border:1px solid transparent;border-radius:.25rem;background-repeat:no-repeat;background-position:.75rem center;margin-bottom:1rem;}.pdoc .alert > em{display:none;}.pdoc .alert > *:last-child{margin-bottom:0;}.pdoc .alert.note {color:#084298;background-color:#cfe2ff;border-color:#b6d4fe;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23084298%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8%2016A8%208%200%201%200%208%200a8%208%200%200%200%200%2016zm.93-9.412-1%204.705c-.07.34.029.533.304.533.194%200%20.487-.07.686-.246l-.088.416c-.287.346-.92.598-1.465.598-.703%200-1.002-.422-.808-1.319l.738-3.468c.064-.293.006-.399-.287-.47l-.451-.081.082-.381%202.29-.287zM8%205.5a1%201%200%201%201%200-2%201%201%200%200%201%200%202z%22/%3E%3C/svg%3E");}.pdoc .alert.warning{color:#664d03;background-color:#fff3cd;border-color:#ffecb5;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23664d03%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8.982%201.566a1.13%201.13%200%200%200-1.96%200L.165%2013.233c-.457.778.091%201.767.98%201.767h13.713c.889%200%201.438-.99.98-1.767L8.982%201.566zM8%205c.535%200%20.954.462.9.995l-.35%203.507a.552.552%200%200%201-1.1%200L7.1%205.995A.905.905%200%200%201%208%205zm.002%206a1%201%200%201%201%200%202%201%201%200%200%201%200-2z%22/%3E%3C/svg%3E");}.pdoc .alert.danger{color:#842029;background-color:#f8d7da;border-color:#f5c2c7;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23842029%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M5.52.359A.5.5%200%200%201%206%200h4a.5.5%200%200%201%20.474.658L8.694%206H12.5a.5.5%200%200%201%20.395.807l-7%209a.5.5%200%200%201-.873-.454L6.823%209.5H3.5a.5.5%200%200%201-.48-.641l2.5-8.5z%22/%3E%3C/svg%3E");}.pdoc .visually-hidden{position:absolute !important;width:1px !important;height:1px !important;padding:0 !important;margin:-1px !important;overflow:hidden !important;clip:rect(0, 0, 0, 0) !important;white-space:nowrap !important;border:0 !important;}.pdoc h1, .pdoc h2, .pdoc h3{font-weight:300;margin:.3em 0;padding:.2em 0;}.pdoc > section:not(.module-info) h1{font-size:1.5rem;font-weight:500;}.pdoc > section:not(.module-info) h2{font-size:1.4rem;font-weight:500;}.pdoc > section:not(.module-info) h3{font-size:1.3rem;font-weight:500;}.pdoc > section:not(.module-info) h4{font-size:1.2rem;}.pdoc > section:not(.module-info) h5{font-size:1.1rem;}.pdoc a{text-decoration:none;color:var(--link);}.pdoc a:hover{color:var(--link-hover);}.pdoc blockquote{margin-left:2rem;}.pdoc pre{border-top:1px solid var(--accent2);border-bottom:1px solid var(--accent2);margin-top:0;margin-bottom:1em;padding:.5rem 0 .5rem .5rem;overflow-x:auto;background-color:var(--code);}.pdoc code{color:var(--text);padding:.2em .4em;margin:0;font-size:85%;background-color:var(--accent);border-radius:6px;}.pdoc a > code{color:inherit;}.pdoc pre > code{display:inline-block;font-size:inherit;background:none;border:none;padding:0;}.pdoc > section:not(.module-info){margin-bottom:1.5rem;}.pdoc .modulename{margin-top:0;font-weight:bold;}.pdoc .modulename a{color:var(--link);transition:100ms all;}.pdoc .git-button{float:right;border:solid var(--link) 1px;}.pdoc .git-button:hover{background-color:var(--link);color:var(--pdoc-background);}.view-source-toggle-state,.view-source-toggle-state ~ .pdoc-code{display:none;}.view-source-toggle-state:checked ~ .pdoc-code{display:block;}.view-source-button{display:inline-block;float:right;font-size:.75rem;line-height:1.5rem;color:var(--muted);padding:0 .4rem 0 1.3rem;cursor:pointer;text-indent:-2px;}.view-source-button > span{visibility:hidden;}.module-info .view-source-button{float:none;display:flex;justify-content:flex-end;margin:-1.2rem .4rem -.2rem 0;}.view-source-button::before{position:absolute;content:"View Source";display:list-item;list-style-type:disclosure-closed;}.view-source-toggle-state:checked ~ .attr .view-source-button::before,.view-source-toggle-state:checked ~ .view-source-button::before{list-style-type:disclosure-open;}.pdoc .docstring{margin-bottom:1.5rem;}.pdoc section:not(.module-info) .docstring{margin-left:clamp(0rem, 5vw - 2rem, 1rem);}.pdoc .docstring .pdoc-code{margin-left:1em;margin-right:1em;}.pdoc h1:target,.pdoc h2:target,.pdoc h3:target,.pdoc h4:target,.pdoc h5:target,.pdoc h6:target,.pdoc .pdoc-code > pre > span:target{background-color:var(--active);box-shadow:-1rem 0 0 0 var(--active);}.pdoc .pdoc-code > pre > span:target{display:block;}.pdoc div:target > .attr,.pdoc section:target > .attr,.pdoc dd:target > a{background-color:var(--active);}.pdoc *{scroll-margin:2rem;}.pdoc .pdoc-code .linenos{user-select:none;}.pdoc .attr:hover{filter:contrast(0.95);}.pdoc section, .pdoc .classattr{position:relative;}.pdoc .headerlink{--width:clamp(1rem, 3vw, 2rem);position:absolute;top:0;left:calc(0rem - var(--width));transition:all 100ms ease-in-out;opacity:0;}.pdoc .headerlink::before{content:"#";display:block;text-align:center;width:var(--width);height:2.3rem;line-height:2.3rem;font-size:1.5rem;}.pdoc .attr:hover ~ .headerlink,.pdoc *:target > .headerlink,.pdoc .headerlink:hover{opacity:1;}.pdoc .attr{display:block;margin:.5rem 0 .5rem;padding:.4rem .4rem .4rem 1rem;background-color:var(--accent);overflow-x:auto;}.pdoc .classattr{margin-left:2rem;}.pdoc .decorator-deprecated{color:#842029;}.pdoc .decorator-deprecated ~ span{filter:grayscale(1) opacity(0.8);}.pdoc .name{color:var(--name);font-weight:bold;}.pdoc .def{color:var(--def);font-weight:bold;}.pdoc .signature{background-color:transparent;}.pdoc .param, .pdoc .return-annotation{white-space:pre;}.pdoc .signature.multiline .param{display:block;}.pdoc .signature.condensed .param{display:inline-block;}.pdoc .annotation{color:var(--annotation);}.pdoc .view-value-toggle-state,.pdoc .view-value-toggle-state ~ .default_value{display:none;}.pdoc .view-value-toggle-state:checked ~ .default_value{display:inherit;}.pdoc .view-value-button{font-size:.5rem;vertical-align:middle;border-style:dashed;margin-top:-0.1rem;}.pdoc .view-value-button:hover{background:white;}.pdoc .view-value-button::before{content:"show";text-align:center;width:2.2em;display:inline-block;}.pdoc .view-value-toggle-state:checked ~ .view-value-button::before{content:"hide";}.pdoc .inherited{margin-left:2rem;}.pdoc .inherited dt{font-weight:700;}.pdoc .inherited dt, .pdoc .inherited dd{display:inline;margin-left:0;margin-bottom:.5rem;}.pdoc .inherited dd:not(:last-child):after{content:", ";}.pdoc .inherited .class:before{content:"class ";}.pdoc .inherited .function a:after{content:"()";}.pdoc .search-result .docstring{overflow:auto;max-height:25vh;}.pdoc .search-result.focused > .attr{background-color:var(--active);}.pdoc .attribution{margin-top:2rem;display:block;opacity:0.5;transition:all 200ms;filter:grayscale(100%);}.pdoc .attribution:hover{opacity:1;filter:grayscale(0%);}.pdoc .attribution img{margin-left:5px;height:35px;vertical-align:middle;width:70px;transition:all 200ms;}.pdoc table{display:block;width:max-content;max-width:100%;overflow:auto;margin-bottom:1rem;}.pdoc table th{font-weight:600;}.pdoc table th, .pdoc table td{padding:6px 13px;border:1px solid var(--accent2);}</style>
    <style>/*! custom.css */</style><script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']]
        }
    };
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>
    /* Re-invoke MathJax when DOM content changes, for example during search. */
    document.addEventListener("DOMContentLoaded", () => {
        new MutationObserver(() => MathJax.typeset()).observe(
            document.querySelector("main.pdoc").parentNode,
            {childList: true}
        );
    })
</script>
<style>
    mjx-container {
        overflow-x: auto;
        overflow-y: hidden;
    }
</style></head>
<body>
    <nav class="pdoc">
        <label id="navtoggle" for="togglestate" class="pdoc-button"><svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 30 30'><path stroke-linecap='round' stroke="currentColor" stroke-miterlimit='10' stroke-width='2' d='M4 7h22M4 15h22M4 23h22'/></svg></label>
        <input id="togglestate" type="checkbox" aria-hidden="true" tabindex="-1">
        <div>            <a class="pdoc-button module-list-button" href="../cl_algorithms.html">
<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-box-arrow-in-left" viewBox="0 0 16 16">
  <path fill-rule="evenodd" d="M10 3.5a.5.5 0 0 0-.5-.5h-8a.5.5 0 0 0-.5.5v9a.5.5 0 0 0 .5.5h8a.5.5 0 0 0 .5-.5v-2a.5.5 0 0 1 1 0v2A1.5 1.5 0 0 1 9.5 14h-8A1.5 1.5 0 0 1 0 12.5v-9A1.5 1.5 0 0 1 1.5 2h8A1.5 1.5 0 0 1 11 3.5v2a.5.5 0 0 1-1 0v-2z"/>
  <path fill-rule="evenodd" d="M4.146 8.354a.5.5 0 0 1 0-.708l3-3a.5.5 0 1 1 .708.708L5.707 7.5H14.5a.5.5 0 0 1 0 1H5.707l2.147 2.146a.5.5 0 0 1-.708.708l-3-3z"/>
</svg>                &nbsp;clarena.cl_algorithms</a>


            <input type="search" placeholder="Search..." role="searchbox" aria-label="search"
                   pattern=".+" required>



            <h2>API Documentation</h2>
                <ul class="memberlist">
            <li>
                    <a class="class" href="#FGAdaHAT">FGAdaHAT</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#FGAdaHAT.__init__">FGAdaHAT</a>
                        </li>
                        <li>
                                <a class="variable" href="#FGAdaHAT.importance_type">importance_type</a>
                        </li>
                        <li>
                                <a class="variable" href="#FGAdaHAT.importance_scheduler_type">importance_scheduler_type</a>
                        </li>
                        <li>
                                <a class="variable" href="#FGAdaHAT.neuron_to_weight_importance_aggregation_mode">neuron_to_weight_importance_aggregation_mode</a>
                        </li>
                        <li>
                                <a class="variable" href="#FGAdaHAT.filter_by_cumulative_mask">filter_by_cumulative_mask</a>
                        </li>
                        <li>
                                <a class="variable" href="#FGAdaHAT.filter_unmasked_importance">filter_unmasked_importance</a>
                        </li>
                        <li>
                                <a class="variable" href="#FGAdaHAT.step_multiply_training_mask">step_multiply_training_mask</a>
                        </li>
                        <li>
                                <a class="variable" href="#FGAdaHAT.importance_summing_strategy">importance_summing_strategy</a>
                        </li>
                        <li>
                                <a class="variable" href="#FGAdaHAT.base_importance">base_importance</a>
                        </li>
                        <li>
                                <a class="variable" href="#FGAdaHAT.base_mask_sparsity_reg">base_mask_sparsity_reg</a>
                        </li>
                        <li>
                                <a class="variable" href="#FGAdaHAT.base_linear">base_linear</a>
                        </li>
                        <li>
                                <a class="variable" href="#FGAdaHAT.importances">importances</a>
                        </li>
                        <li>
                                <a class="variable" href="#FGAdaHAT.summative_importance_for_previous_tasks">summative_importance_for_previous_tasks</a>
                        </li>
                        <li>
                                <a class="variable" href="#FGAdaHAT.num_steps_t">num_steps_t</a>
                        </li>
                        <li>
                                <a class="variable" href="#FGAdaHAT.automatic_optimization">automatic_optimization</a>
                        </li>
                        <li>
                                <a class="function" href="#FGAdaHAT.sanity_check">sanity_check</a>
                        </li>
                        <li>
                                <a class="function" href="#FGAdaHAT.on_train_start">on_train_start</a>
                        </li>
                        <li>
                                <a class="function" href="#FGAdaHAT.clip_grad_by_adjustment">clip_grad_by_adjustment</a>
                        </li>
                        <li>
                                <a class="function" href="#FGAdaHAT.on_train_batch_end">on_train_batch_end</a>
                        </li>
                        <li>
                                <a class="function" href="#FGAdaHAT.on_train_end">on_train_end</a>
                        </li>
                        <li>
                                <a class="function" href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum">get_importance_step_layer_weight_abs_sum</a>
                        </li>
                        <li>
                                <a class="function" href="#FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum">get_importance_step_layer_weight_gradient_abs_sum</a>
                        </li>
                        <li>
                                <a class="function" href="#FGAdaHAT.get_importance_step_layer_activation_abs">get_importance_step_layer_activation_abs</a>
                        </li>
                        <li>
                                <a class="function" href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs">get_importance_step_layer_weight_abs_sum_x_activation_abs</a>
                        </li>
                        <li>
                                <a class="function" href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs">get_importance_step_layer_gradient_x_activation_abs</a>
                        </li>
                        <li>
                                <a class="function" href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum">get_importance_step_layer_weight_gradient_square_sum</a>
                        </li>
                        <li>
                                <a class="function" href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs">get_importance_step_layer_weight_gradient_square_sum_x_activation_abs</a>
                        </li>
                        <li>
                                <a class="function" href="#FGAdaHAT.get_importance_step_layer_conductance_abs">get_importance_step_layer_conductance_abs</a>
                        </li>
                        <li>
                                <a class="function" href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs">get_importance_step_layer_internal_influence_abs</a>
                        </li>
                        <li>
                                <a class="function" href="#FGAdaHAT.get_importance_step_layer_gradcam_abs">get_importance_step_layer_gradcam_abs</a>
                        </li>
                        <li>
                                <a class="function" href="#FGAdaHAT.get_importance_step_layer_deeplift_abs">get_importance_step_layer_deeplift_abs</a>
                        </li>
                        <li>
                                <a class="function" href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs">get_importance_step_layer_deepliftshap_abs</a>
                        </li>
                        <li>
                                <a class="function" href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs">get_importance_step_layer_gradientshap_abs</a>
                        </li>
                        <li>
                                <a class="function" href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs">get_importance_step_layer_integrated_gradients_abs</a>
                        </li>
                        <li>
                                <a class="function" href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs">get_importance_step_layer_feature_ablation_abs</a>
                        </li>
                        <li>
                                <a class="function" href="#FGAdaHAT.get_importance_step_layer_lrp_abs">get_importance_step_layer_lrp_abs</a>
                        </li>
                        <li>
                                <a class="function" href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution">get_importance_step_layer_cbp_adaptive_contribution</a>
                        </li>
                </ul>

            </li>
    </ul>



        <a class="attribution" title="pdoc: Python API documentation generator" href="https://pdoc.dev" target="_blank">
            built with <span class="visually-hidden">pdoc</span><img
                alt="pdoc logo"
                src="data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20role%3D%22img%22%20aria-label%3D%22pdoc%20logo%22%20width%3D%22300%22%20height%3D%22150%22%20viewBox%3D%22-1%200%2060%2030%22%3E%3Ctitle%3Epdoc%3C/title%3E%3Cpath%20d%3D%22M29.621%2021.293c-.011-.273-.214-.475-.511-.481a.5.5%200%200%200-.489.503l-.044%201.393c-.097.551-.695%201.215-1.566%201.704-.577.428-1.306.486-2.193.182-1.426-.617-2.467-1.654-3.304-2.487l-.173-.172a3.43%203.43%200%200%200-.365-.306.49.49%200%200%200-.286-.196c-1.718-1.06-4.931-1.47-7.353.191l-.219.15c-1.707%201.187-3.413%202.131-4.328%201.03-.02-.027-.49-.685-.141-1.763.233-.721.546-2.408.772-4.076.042-.09.067-.187.046-.288.166-1.347.277-2.625.241-3.351%201.378-1.008%202.271-2.586%202.271-4.362%200-.976-.272-1.935-.788-2.774-.057-.094-.122-.18-.184-.268.033-.167.052-.339.052-.516%200-1.477-1.202-2.679-2.679-2.679-.791%200-1.496.352-1.987.9a6.3%206.3%200%200%200-1.001.029c-.492-.564-1.207-.929-2.012-.929-1.477%200-2.679%201.202-2.679%202.679A2.65%202.65%200%200%200%20.97%206.554c-.383.747-.595%201.572-.595%202.41%200%202.311%201.507%204.29%203.635%205.107-.037.699-.147%202.27-.423%203.294l-.137.461c-.622%202.042-2.515%208.257%201.727%2010.643%201.614.908%203.06%201.248%204.317%201.248%202.665%200%204.492-1.524%205.322-2.401%201.476-1.559%202.886-1.854%206.491.82%201.877%201.393%203.514%201.753%204.861%201.068%202.223-1.713%202.811-3.867%203.399-6.374.077-.846.056-1.469.054-1.537zm-4.835%204.313c-.054.305-.156.586-.242.629-.034-.007-.131-.022-.307-.157-.145-.111-.314-.478-.456-.908.221.121.432.25.675.355.115.039.219.051.33.081zm-2.251-1.238c-.05.33-.158.648-.252.694-.022.001-.125-.018-.307-.157-.217-.166-.488-.906-.639-1.573.358.344.754.693%201.198%201.036zm-3.887-2.337c-.006-.116-.018-.231-.041-.342.635.145%201.189.368%201.599.625.097.231.166.481.174.642-.03.049-.055.101-.067.158-.046.013-.128.026-.298.004-.278-.037-.901-.57-1.367-1.087zm-1.127-.497c.116.306.176.625.12.71-.019.014-.117.045-.345.016-.206-.027-.604-.332-.986-.695.41-.051.816-.056%201.211-.031zm-4.535%201.535c.209.22.379.47.358.598-.006.041-.088.138-.351.234-.144.055-.539-.063-.979-.259a11.66%2011.66%200%200%200%20.972-.573zm.983-.664c.359-.237.738-.418%201.126-.554.25.237.479.548.457.694-.006.042-.087.138-.351.235-.174.064-.694-.105-1.232-.375zm-3.381%201.794c-.022.145-.061.29-.149.401-.133.166-.358.248-.69.251h-.002c-.133%200-.306-.26-.45-.621.417.091.854.07%201.291-.031zm-2.066-8.077a4.78%204.78%200%200%201-.775-.584c.172-.115.505-.254.88-.378l-.105.962zm-.331%202.302a10.32%2010.32%200%200%201-.828-.502c.202-.143.576-.328.984-.49l-.156.992zm-.45%202.157l-.701-.403c.214-.115.536-.249.891-.376a11.57%2011.57%200%200%201-.19.779zm-.181%201.716c.064.398.194.702.298.893-.194-.051-.435-.162-.736-.398.061-.119.224-.3.438-.495zM8.87%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zm-.735-.389a1.15%201.15%200%200%200-.314.783%201.16%201.16%200%200%200%201.162%201.162c.457%200%20.842-.27%201.032-.653.026.117.042.238.042.362a1.68%201.68%200%200%201-1.679%201.679%201.68%201.68%200%200%201-1.679-1.679c0-.843.626-1.535%201.436-1.654zM5.059%205.406A1.68%201.68%200%200%201%203.38%207.085a1.68%201.68%200%200%201-1.679-1.679c0-.037.009-.072.011-.109.21.3.541.508.935.508a1.16%201.16%200%200%200%201.162-1.162%201.14%201.14%200%200%200-.474-.912c.015%200%20.03-.005.045-.005.926.001%201.679.754%201.679%201.68zM3.198%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zM1.375%208.964c0-.52.103-1.035.288-1.52.466.394%201.06.64%201.717.64%201.144%200%202.116-.725%202.499-1.738.383%201.012%201.355%201.738%202.499%201.738.867%200%201.631-.421%202.121-1.062.307.605.478%201.267.478%201.942%200%202.486-2.153%204.51-4.801%204.51s-4.801-2.023-4.801-4.51zm24.342%2019.349c-.985.498-2.267.168-3.813-.979-3.073-2.281-5.453-3.199-7.813-.705-1.315%201.391-4.163%203.365-8.423.97-3.174-1.786-2.239-6.266-1.261-9.479l.146-.492c.276-1.02.395-2.457.444-3.268a6.11%206.11%200%200%200%201.18.115%206.01%206.01%200%200%200%202.536-.562l-.006.175c-.802.215-1.848.612-2.021%201.25-.079.295.021.601.274.837.219.203.415.364.598.501-.667.304-1.243.698-1.311%201.179-.02.144-.022.507.393.787.213.144.395.26.564.365-1.285.521-1.361.96-1.381%201.126-.018.142-.011.496.427.746l.854.489c-.473.389-.971.914-.999%201.429-.018.278.095.532.316.713.675.556%201.231.721%201.653.721.059%200%20.104-.014.158-.02.207.707.641%201.64%201.513%201.64h.013c.8-.008%201.236-.345%201.462-.626.173-.216.268-.457.325-.692.424.195.93.374%201.372.374.151%200%20.294-.021.423-.068.732-.27.944-.704.993-1.021.009-.061.003-.119.002-.179.266.086.538.147.789.147.15%200%20.294-.021.423-.069.542-.2.797-.489.914-.754.237.147.478.258.704.288.106.014.205.021.296.021.356%200%20.595-.101.767-.229.438.435%201.094.992%201.656%201.067.106.014.205.021.296.021a1.56%201.56%200%200%200%20.323-.035c.17.575.453%201.289.866%201.605.358.273.665.362.914.362a.99.99%200%200%200%20.421-.093%201.03%201.03%200%200%200%20.245-.164c.168.428.39.846.68%201.068.358.273.665.362.913.362a.99.99%200%200%200%20.421-.093c.317-.148.512-.448.639-.762.251.157.495.257.726.257.127%200%20.25-.024.37-.071.427-.17.706-.617.841-1.314.022-.015.047-.022.068-.038.067-.051.133-.104.196-.159-.443%201.486-1.107%202.761-2.086%203.257zM8.66%209.925a.5.5%200%201%200-1%200c0%20.653-.818%201.205-1.787%201.205s-1.787-.552-1.787-1.205a.5.5%200%201%200-1%200c0%201.216%201.25%202.205%202.787%202.205s2.787-.989%202.787-2.205zm4.4%2015.965l-.208.097c-2.661%201.258-4.708%201.436-6.086.527-1.542-1.017-1.88-3.19-1.844-4.198a.4.4%200%200%200-.385-.414c-.242-.029-.406.164-.414.385-.046%201.249.367%203.686%202.202%204.896.708.467%201.547.7%202.51.7%201.248%200%202.706-.392%204.362-1.174l.185-.086a.4.4%200%200%200%20.205-.527c-.089-.204-.326-.291-.527-.206zM9.547%202.292c.093.077.205.114.317.114a.5.5%200%200%200%20.318-.886L8.817.397a.5.5%200%200%200-.703.068.5.5%200%200%200%20.069.703l1.364%201.124zm-7.661-.065c.086%200%20.173-.022.253-.068l1.523-.893a.5.5%200%200%200-.506-.863l-1.523.892a.5.5%200%200%200-.179.685c.094.158.261.247.432.247z%22%20transform%3D%22matrix%28-1%200%200%201%2058%200%29%22%20fill%3D%22%233bb300%22/%3E%3Cpath%20d%3D%22M.3%2021.86V10.18q0-.46.02-.68.04-.22.18-.5.28-.54%201.34-.54%201.06%200%201.42.28.38.26.44.78.76-1.04%202.38-1.04%201.64%200%203.1%201.54%201.46%201.54%201.46%203.58%200%202.04-1.46%203.58-1.44%201.54-3.08%201.54-1.64%200-2.38-.92v4.04q0%20.46-.04.68-.02.22-.18.5-.14.3-.5.42-.36.12-.98.12-.62%200-1-.12-.36-.12-.52-.4-.14-.28-.18-.5-.02-.22-.02-.68zm3.96-9.42q-.46.54-.46%201.18%200%20.64.46%201.18.48.52%201.2.52.74%200%201.24-.52.52-.52.52-1.18%200-.66-.48-1.18-.48-.54-1.26-.54-.76%200-1.22.54zm14.741-8.36q.16-.3.54-.42.38-.12%201-.12.64%200%201.02.12.38.12.52.42.16.3.18.54.04.22.04.68v11.94q0%20.46-.04.7-.02.22-.18.5-.3.54-1.7.54-1.38%200-1.54-.98-.84.96-2.34.96-1.8%200-3.28-1.56-1.48-1.58-1.48-3.66%200-2.1%201.48-3.68%201.5-1.58%203.28-1.58%201.48%200%202.3%201v-4.2q0-.46.02-.68.04-.24.18-.52zm-3.24%2010.86q.52.54%201.26.54.74%200%201.22-.54.5-.54.5-1.18%200-.66-.48-1.22-.46-.56-1.26-.56-.8%200-1.28.56-.48.54-.48%201.2%200%20.66.52%201.2zm7.833-1.2q0-2.4%201.68-3.96%201.68-1.56%203.84-1.56%202.16%200%203.82%201.56%201.66%201.54%201.66%203.94%200%201.66-.86%202.96-.86%201.28-2.1%201.9-1.22.6-2.54.6-1.32%200-2.56-.64-1.24-.66-2.1-1.92-.84-1.28-.84-2.88zm4.18%201.44q.64.48%201.3.48.66%200%201.32-.5.66-.5.66-1.48%200-.98-.62-1.46-.62-.48-1.34-.48-.72%200-1.34.5-.62.5-.62%201.48%200%20.96.64%201.46zm11.412-1.44q0%20.84.56%201.32.56.46%201.18.46.64%200%201.18-.36.56-.38.9-.38.6%200%201.46%201.06.46.58.46%201.04%200%20.76-1.1%201.42-1.14.8-2.8.8-1.86%200-3.58-1.34-.82-.64-1.34-1.7-.52-1.08-.52-2.36%200-1.3.52-2.34.52-1.06%201.34-1.7%201.66-1.32%203.54-1.32.76%200%201.48.22.72.2%201.06.4l.32.2q.36.24.56.38.52.4.52.92%200%20.5-.42%201.14-.72%201.1-1.38%201.1-.38%200-1.08-.44-.36-.34-1.04-.34-.66%200-1.24.48-.58.48-.58%201.34z%22%20fill%3D%22green%22/%3E%3C/svg%3E"/>
        </a>
</div>
    </nav>
    <main class="pdoc">
            <section class="module-info">
                    <h1 class="modulename">
<a href="./../../clarena.html">clarena</a><wbr>.<a href="./../cl_algorithms.html">cl_algorithms</a><wbr>.fgadahat    </h1>

                        <div class="docstring"><p>The submodule in <code>cl_algorithms</code> for FG-AdaHAT algorithm.</p>
</div>

                        <input id="mod-fgadahat-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">

                        <label class="view-source-button" for="mod-fgadahat-view-source"><span>View Source</span></label>

                        <div class="pdoc-code codehilite"><pre><span></span><span id="L-1"><a href="#L-1"><span class="linenos">   1</span></a><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-2"><a href="#L-2"><span class="linenos">   2</span></a><span class="sd">The submodule in `cl_algorithms` for FG-AdaHAT algorithm.</span>
</span><span id="L-3"><a href="#L-3"><span class="linenos">   3</span></a><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-4"><a href="#L-4"><span class="linenos">   4</span></a>
</span><span id="L-5"><a href="#L-5"><span class="linenos">   5</span></a><span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;FGAdaHAT&quot;</span><span class="p">]</span>
</span><span id="L-6"><a href="#L-6"><span class="linenos">   6</span></a>
</span><span id="L-7"><a href="#L-7"><span class="linenos">   7</span></a><span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
</span><span id="L-8"><a href="#L-8"><span class="linenos">   8</span></a><span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
</span><span id="L-9"><a href="#L-9"><span class="linenos">   9</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Any</span>
</span><span id="L-10"><a href="#L-10"><span class="linenos">  10</span></a>
</span><span id="L-11"><a href="#L-11"><span class="linenos">  11</span></a><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span id="L-12"><a href="#L-12"><span class="linenos">  12</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">captum.attr</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
</span><span id="L-13"><a href="#L-13"><span class="linenos">  13</span></a>    <span class="n">InternalInfluence</span><span class="p">,</span>
</span><span id="L-14"><a href="#L-14"><span class="linenos">  14</span></a>    <span class="n">LayerConductance</span><span class="p">,</span>
</span><span id="L-15"><a href="#L-15"><span class="linenos">  15</span></a>    <span class="n">LayerDeepLift</span><span class="p">,</span>
</span><span id="L-16"><a href="#L-16"><span class="linenos">  16</span></a>    <span class="n">LayerDeepLiftShap</span><span class="p">,</span>
</span><span id="L-17"><a href="#L-17"><span class="linenos">  17</span></a>    <span class="n">LayerFeatureAblation</span><span class="p">,</span>
</span><span id="L-18"><a href="#L-18"><span class="linenos">  18</span></a>    <span class="n">LayerGradCam</span><span class="p">,</span>
</span><span id="L-19"><a href="#L-19"><span class="linenos">  19</span></a>    <span class="n">LayerGradientShap</span><span class="p">,</span>
</span><span id="L-20"><a href="#L-20"><span class="linenos">  20</span></a>    <span class="n">LayerGradientXActivation</span><span class="p">,</span>
</span><span id="L-21"><a href="#L-21"><span class="linenos">  21</span></a>    <span class="n">LayerIntegratedGradients</span><span class="p">,</span>
</span><span id="L-22"><a href="#L-22"><span class="linenos">  22</span></a>    <span class="n">LayerLRP</span><span class="p">,</span>
</span><span id="L-23"><a href="#L-23"><span class="linenos">  23</span></a><span class="p">)</span>
</span><span id="L-24"><a href="#L-24"><span class="linenos">  24</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tensor</span>
</span><span id="L-25"><a href="#L-25"><span class="linenos">  25</span></a>
</span><span id="L-26"><a href="#L-26"><span class="linenos">  26</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">clarena.backbones</span><span class="w"> </span><span class="kn">import</span> <span class="n">HATMaskBackbone</span>
</span><span id="L-27"><a href="#L-27"><span class="linenos">  27</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">clarena.cl_algorithms</span><span class="w"> </span><span class="kn">import</span> <span class="n">AdaHAT</span>
</span><span id="L-28"><a href="#L-28"><span class="linenos">  28</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">clarena.heads</span><span class="w"> </span><span class="kn">import</span> <span class="n">HeadsTIL</span>
</span><span id="L-29"><a href="#L-29"><span class="linenos">  29</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">clarena.utils.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">HATNetworkCapacityMetric</span>
</span><span id="L-30"><a href="#L-30"><span class="linenos">  30</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">clarena.utils.transforms</span><span class="w"> </span><span class="kn">import</span> <span class="n">min_max_normalize</span>
</span><span id="L-31"><a href="#L-31"><span class="linenos">  31</span></a>
</span><span id="L-32"><a href="#L-32"><span class="linenos">  32</span></a><span class="c1"># always get logger for built-in logging in each module</span>
</span><span id="L-33"><a href="#L-33"><span class="linenos">  33</span></a><span class="n">pylogger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>
</span><span id="L-34"><a href="#L-34"><span class="linenos">  34</span></a>
</span><span id="L-35"><a href="#L-35"><span class="linenos">  35</span></a>
</span><span id="L-36"><a href="#L-36"><span class="linenos">  36</span></a><span class="k">class</span><span class="w"> </span><span class="nc">FGAdaHAT</span><span class="p">(</span><span class="n">AdaHAT</span><span class="p">):</span>
</span><span id="L-37"><a href="#L-37"><span class="linenos">  37</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;FG-AdaHAT (Fine-Grained Adaptive Hard Attention to the Task) algorithm.</span>
</span><span id="L-38"><a href="#L-38"><span class="linenos">  38</span></a>
</span><span id="L-39"><a href="#L-39"><span class="linenos">  39</span></a><span class="sd">    An architecture-based continual learning approach that improves [AdaHAT (Adaptive Hard Attention to the Task)](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9) by introducing fine-grained neuron-wise importance measures guiding the adaptive adjustment mechanism in AdaHAT.</span>
</span><span id="L-40"><a href="#L-40"><span class="linenos">  40</span></a>
</span><span id="L-41"><a href="#L-41"><span class="linenos">  41</span></a><span class="sd">    We implement FG-AdaHAT as a subclass of AdaHAT, as it reuses AdaHAT&#39;s summative mask and other components.</span>
</span><span id="L-42"><a href="#L-42"><span class="linenos">  42</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-43"><a href="#L-43"><span class="linenos">  43</span></a>
</span><span id="L-44"><a href="#L-44"><span class="linenos">  44</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-45"><a href="#L-45"><span class="linenos">  45</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-46"><a href="#L-46"><span class="linenos">  46</span></a>        <span class="n">backbone</span><span class="p">:</span> <span class="n">HATMaskBackbone</span><span class="p">,</span>
</span><span id="L-47"><a href="#L-47"><span class="linenos">  47</span></a>        <span class="n">heads</span><span class="p">:</span> <span class="n">HeadsTIL</span><span class="p">,</span>
</span><span id="L-48"><a href="#L-48"><span class="linenos">  48</span></a>        <span class="n">adjustment_intensity</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="L-49"><a href="#L-49"><span class="linenos">  49</span></a>        <span class="n">importance_type</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-50"><a href="#L-50"><span class="linenos">  50</span></a>        <span class="n">importance_summing_strategy</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-51"><a href="#L-51"><span class="linenos">  51</span></a>        <span class="n">importance_scheduler_type</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-52"><a href="#L-52"><span class="linenos">  52</span></a>        <span class="n">neuron_to_weight_importance_aggregation_mode</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-53"><a href="#L-53"><span class="linenos">  53</span></a>        <span class="n">s_max</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="L-54"><a href="#L-54"><span class="linenos">  54</span></a>        <span class="n">clamp_threshold</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="L-55"><a href="#L-55"><span class="linenos">  55</span></a>        <span class="n">mask_sparsity_reg_factor</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="L-56"><a href="#L-56"><span class="linenos">  56</span></a>        <span class="n">mask_sparsity_reg_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;original&quot;</span><span class="p">,</span>
</span><span id="L-57"><a href="#L-57"><span class="linenos">  57</span></a>        <span class="n">base_importance</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
</span><span id="L-58"><a href="#L-58"><span class="linenos">  58</span></a>        <span class="n">base_mask_sparsity_reg</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
</span><span id="L-59"><a href="#L-59"><span class="linenos">  59</span></a>        <span class="n">base_linear</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
</span><span id="L-60"><a href="#L-60"><span class="linenos">  60</span></a>        <span class="n">filter_by_cumulative_mask</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-61"><a href="#L-61"><span class="linenos">  61</span></a>        <span class="n">filter_unmasked_importance</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="L-62"><a href="#L-62"><span class="linenos">  62</span></a>        <span class="n">step_multiply_training_mask</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="L-63"><a href="#L-63"><span class="linenos">  63</span></a>        <span class="n">task_embedding_init_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;N01&quot;</span><span class="p">,</span>
</span><span id="L-64"><a href="#L-64"><span class="linenos">  64</span></a>        <span class="n">importance_summing_strategy_linear_step</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-65"><a href="#L-65"><span class="linenos">  65</span></a>        <span class="n">importance_summing_strategy_exponential_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-66"><a href="#L-66"><span class="linenos">  66</span></a>        <span class="n">importance_summing_strategy_log_base</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-67"><a href="#L-67"><span class="linenos">  67</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-68"><a href="#L-68"><span class="linenos">  68</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialize the FG-AdaHAT algorithm with the network.</span>
</span><span id="L-69"><a href="#L-69"><span class="linenos">  69</span></a>
</span><span id="L-70"><a href="#L-70"><span class="linenos">  70</span></a><span class="sd">        **Args:**</span>
</span><span id="L-71"><a href="#L-71"><span class="linenos">  71</span></a><span class="sd">        - **backbone** (`HATMaskBackbone`): must be a backbone network with the HAT mask mechanism.</span>
</span><span id="L-72"><a href="#L-72"><span class="linenos">  72</span></a><span class="sd">        - **heads** (`HeadsTIL`): output heads. FG-AdaHAT supports only TIL (Task-Incremental Learning).</span>
</span><span id="L-73"><a href="#L-73"><span class="linenos">  73</span></a><span class="sd">        - **adjustment_intensity** (`float`): hyperparameter, controls the overall intensity of gradient adjustment (the $\alpha$ in the paper).</span>
</span><span id="L-74"><a href="#L-74"><span class="linenos">  74</span></a><span class="sd">        - **importance_type** (`str`): the type of neuron-wise importance, must be one of:</span>
</span><span id="L-75"><a href="#L-75"><span class="linenos">  75</span></a><span class="sd">            1. &#39;input_weight_abs_sum&#39;: sum of absolute input weights;</span>
</span><span id="L-76"><a href="#L-76"><span class="linenos">  76</span></a><span class="sd">            2. &#39;output_weight_abs_sum&#39;: sum of absolute output weights;</span>
</span><span id="L-77"><a href="#L-77"><span class="linenos">  77</span></a><span class="sd">            3. &#39;input_weight_gradient_abs_sum&#39;: sum of absolute gradients of the input weights (Input Gradients (IG) in the paper);</span>
</span><span id="L-78"><a href="#L-78"><span class="linenos">  78</span></a><span class="sd">            4. &#39;output_weight_gradient_abs_sum&#39;: sum of absolute gradients of the output weights (Output Gradients (OG) in the paper);</span>
</span><span id="L-79"><a href="#L-79"><span class="linenos">  79</span></a><span class="sd">            5. &#39;activation_abs&#39;: absolute activation;</span>
</span><span id="L-80"><a href="#L-80"><span class="linenos">  80</span></a><span class="sd">            6. &#39;input_weight_abs_sum_x_activation_abs&#39;: sum of absolute input weights multiplied by absolute activation (Input Contribution Utility (ICU) in the paper);</span>
</span><span id="L-81"><a href="#L-81"><span class="linenos">  81</span></a><span class="sd">            7. &#39;output_weight_abs_sum_x_activation_abs&#39;: sum of absolute output weights multiplied by absolute activation (Contribution Utility (CU) in the paper);</span>
</span><span id="L-82"><a href="#L-82"><span class="linenos">  82</span></a><span class="sd">            8. &#39;gradient_x_activation_abs&#39;: absolute gradient (the saliency) multiplied by activation;</span>
</span><span id="L-83"><a href="#L-83"><span class="linenos">  83</span></a><span class="sd">            9. &#39;input_weight_gradient_square_sum&#39;: sum of squared gradients of the input weights;</span>
</span><span id="L-84"><a href="#L-84"><span class="linenos">  84</span></a><span class="sd">            10. &#39;output_weight_gradient_square_sum&#39;: sum of squared gradients of the output weights;</span>
</span><span id="L-85"><a href="#L-85"><span class="linenos">  85</span></a><span class="sd">            11. &#39;input_weight_gradient_square_sum_x_activation_abs&#39;: sum of squared gradients of the input weights multiplied by absolute activation (Activation Fisher Information (AFI) in the paper);</span>
</span><span id="L-86"><a href="#L-86"><span class="linenos">  86</span></a><span class="sd">            12. &#39;output_weight_gradient_square_sum_x_activation_abs&#39;: sum of squared gradients of the output weights multiplied by absolute activation;</span>
</span><span id="L-87"><a href="#L-87"><span class="linenos">  87</span></a><span class="sd">            13. &#39;conductance_abs&#39;: absolute layer conductance;</span>
</span><span id="L-88"><a href="#L-88"><span class="linenos">  88</span></a><span class="sd">            14. &#39;internal_influence_abs&#39;: absolute internal influence (Internal Influence (II) in the paper);</span>
</span><span id="L-89"><a href="#L-89"><span class="linenos">  89</span></a><span class="sd">            15. &#39;gradcam_abs&#39;: absolute Grad-CAM;</span>
</span><span id="L-90"><a href="#L-90"><span class="linenos">  90</span></a><span class="sd">            16. &#39;deeplift_abs&#39;: absolute DeepLIFT (DeepLIFT (DL) in the paper);</span>
</span><span id="L-91"><a href="#L-91"><span class="linenos">  91</span></a><span class="sd">            17. &#39;deepliftshap_abs&#39;: absolute DeepLIFT-SHAP;</span>
</span><span id="L-92"><a href="#L-92"><span class="linenos">  92</span></a><span class="sd">            18. &#39;gradientshap_abs&#39;: absolute Gradient-SHAP (Gradient SHAP (GS) in the paper);</span>
</span><span id="L-93"><a href="#L-93"><span class="linenos">  93</span></a><span class="sd">            19. &#39;integrated_gradients_abs&#39;: absolute Integrated Gradients;</span>
</span><span id="L-94"><a href="#L-94"><span class="linenos">  94</span></a><span class="sd">            20. &#39;feature_ablation_abs&#39;: absolute Feature Ablation (Feature Ablation (FA) in the paper);</span>
</span><span id="L-95"><a href="#L-95"><span class="linenos">  95</span></a><span class="sd">            21. &#39;lrp_abs&#39;: absolute Layer-wise Relevance Propagation (LRP);</span>
</span><span id="L-96"><a href="#L-96"><span class="linenos">  96</span></a><span class="sd">            22. &#39;cbp_adaptation&#39;: the adaptation function in [Continual Backpropagation (CBP)](https://www.nature.com/articles/s41586-024-07711-7);</span>
</span><span id="L-97"><a href="#L-97"><span class="linenos">  97</span></a><span class="sd">            23. &#39;cbp_adaptive_contribution&#39;: the adaptive contribution function in [Continual Backpropagation (CBP)](https://www.nature.com/articles/s41586-024-07711-7);</span>
</span><span id="L-98"><a href="#L-98"><span class="linenos">  98</span></a><span class="sd">        - **importance_summing_strategy** (`str`): the strategy to sum neuron-wise importance for previous tasks, must be one of:</span>
</span><span id="L-99"><a href="#L-99"><span class="linenos">  99</span></a><span class="sd">            1. &#39;add_latest&#39;: add the latest neuron-wise importance to the summative importance;</span>
</span><span id="L-100"><a href="#L-100"><span class="linenos"> 100</span></a><span class="sd">            2. &#39;add_all&#39;: add all previous neuron-wise importance (including the latest) to the summative importance;</span>
</span><span id="L-101"><a href="#L-101"><span class="linenos"> 101</span></a><span class="sd">            3. &#39;add_average&#39;: add the average of all previous neuron-wise importance (including the latest) to the summative importance;</span>
</span><span id="L-102"><a href="#L-102"><span class="linenos"> 102</span></a><span class="sd">            4. &#39;linear_decrease&#39;: weigh the previous neuron-wise importance by a linear factor that decreases with the task ID;</span>
</span><span id="L-103"><a href="#L-103"><span class="linenos"> 103</span></a><span class="sd">            5. &#39;quadratic_decrease&#39;: weigh the previous neuron-wise importance that decreases quadratically with the task ID;</span>
</span><span id="L-104"><a href="#L-104"><span class="linenos"> 104</span></a><span class="sd">            6. &#39;cubic_decrease&#39;: weigh the previous neuron-wise importance that decreases cubically with the task ID;</span>
</span><span id="L-105"><a href="#L-105"><span class="linenos"> 105</span></a><span class="sd">            7. &#39;exponential_decrease&#39;: weigh the previous neuron-wise importance by an exponential factor that decreases with the task ID;</span>
</span><span id="L-106"><a href="#L-106"><span class="linenos"> 106</span></a><span class="sd">            8. &#39;log_decrease&#39;: weigh the previous neuron-wise importance by a logarithmic factor that decreases with the task ID;</span>
</span><span id="L-107"><a href="#L-107"><span class="linenos"> 107</span></a><span class="sd">            9. &#39;factorial_decrease&#39;: weigh the previous neuron-wise importance that decreases factorially with the task ID;</span>
</span><span id="L-108"><a href="#L-108"><span class="linenos"> 108</span></a><span class="sd">        - **importance_scheduler_type** (`str`): the scheduler for importance, i.e., the factor $c^t$ multiplied to parameter importance. Must be one of:</span>
</span><span id="L-109"><a href="#L-109"><span class="linenos"> 109</span></a><span class="sd">            1. &#39;linear_sparsity_reg&#39;: $c^t = (t+b_L) \cdot [R(M^t, M^{&lt;t}) + b_R]$, where $R(M^t, M^{&lt;t})$ is the mask sparsity regularization betwwen the current task and previous tasks, $b_L$ is the base linear factor (see argument `base_linear`), and $b_R$ is the base mask sparsity regularization factor (see argument `base_mask_sparsity_reg`);</span>
</span><span id="L-110"><a href="#L-110"><span class="linenos"> 110</span></a><span class="sd">            2. &#39;sparsity_reg&#39;: $c^t = [R(M^t, M^{&lt;t}) + b_R]$;</span>
</span><span id="L-111"><a href="#L-111"><span class="linenos"> 111</span></a><span class="sd">            3. &#39;summative_mask_sparsity_reg&#39;: $c^t_{l,ij} = \left(\min \left(m^{&lt;t, \text{sum}}_{l,i}, m^{&lt;t, \text{sum}}_{l-1,j}\right)+b_L\right) \cdot [R(M^t, M^{&lt;t}) + b_R]$.</span>
</span><span id="L-112"><a href="#L-112"><span class="linenos"> 112</span></a><span class="sd">        - **neuron_to_weight_importance_aggregation_mode** (`str`): aggregation mode from neuron-wise to weight-wise importance ($\text{Agg}(\cdot)$ in the paper), must be one of:</span>
</span><span id="L-113"><a href="#L-113"><span class="linenos"> 113</span></a><span class="sd">            1. &#39;min&#39;: take the minimum of neuron-wise importance for each weight;</span>
</span><span id="L-114"><a href="#L-114"><span class="linenos"> 114</span></a><span class="sd">            2. &#39;max&#39;: take the maximum of neuron-wise importance for each weight;</span>
</span><span id="L-115"><a href="#L-115"><span class="linenos"> 115</span></a><span class="sd">            3. &#39;mean&#39;: take the mean of neuron-wise importance for each weight.</span>
</span><span id="L-116"><a href="#L-116"><span class="linenos"> 116</span></a><span class="sd">        - **s_max** (`float`): hyperparameter, the maximum scaling factor in the gate function. See Sec. 2.4 &quot;Hard Attention Training&quot; in the [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="L-117"><a href="#L-117"><span class="linenos"> 117</span></a><span class="sd">        - **clamp_threshold** (`float`): the threshold for task embedding gradient compensation. See Sec. 2.5 &quot;Embedding Gradient Compensation&quot; in the [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="L-118"><a href="#L-118"><span class="linenos"> 118</span></a><span class="sd">        - **mask_sparsity_reg_factor** (`float`): hyperparameter, the regularization factor for mask sparsity.</span>
</span><span id="L-119"><a href="#L-119"><span class="linenos"> 119</span></a><span class="sd">        - **mask_sparsity_reg_mode** (`str`): the mode of mask sparsity regularization, must be one of:</span>
</span><span id="L-120"><a href="#L-120"><span class="linenos"> 120</span></a><span class="sd">            1. &#39;original&#39; (default): the original mask sparsity regularization in the [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="L-121"><a href="#L-121"><span class="linenos"> 121</span></a><span class="sd">            2. &#39;cross&#39;: the cross version of mask sparsity regularization.</span>
</span><span id="L-122"><a href="#L-122"><span class="linenos"> 122</span></a><span class="sd">        - **base_importance** (`float`): base value added to importance ($b_I$ in the paper). Default: 0.01.</span>
</span><span id="L-123"><a href="#L-123"><span class="linenos"> 123</span></a><span class="sd">        - **base_mask_sparsity_reg** (`float`): base value added to mask sparsity regularization factor in the importance scheduler ($b_R$ in the paper). Default: 0.1.</span>
</span><span id="L-124"><a href="#L-124"><span class="linenos"> 124</span></a><span class="sd">        - **base_linear** (`float`): base value added to the linear factor in the importance scheduler ($b_L$ in the paper). Default: 10.</span>
</span><span id="L-125"><a href="#L-125"><span class="linenos"> 125</span></a><span class="sd">        - **filter_by_cumulative_mask** (`bool`): whether to multiply the cumulative mask to the importance when calculating adjustment rate. Default: False.</span>
</span><span id="L-126"><a href="#L-126"><span class="linenos"> 126</span></a><span class="sd">        - **filter_unmasked_importance** (`bool`): whether to filter unmasked importance values (set to 0) at the end of task training. Default: False.</span>
</span><span id="L-127"><a href="#L-127"><span class="linenos"> 127</span></a><span class="sd">        - **step_multiply_training_mask** (`bool`): whether to multiply the training mask to the importance at each training step. Default: True.</span>
</span><span id="L-128"><a href="#L-128"><span class="linenos"> 128</span></a><span class="sd">        - **task_embedding_init_mode** (`str`): the initialization mode for task embeddings, must be one of:</span>
</span><span id="L-129"><a href="#L-129"><span class="linenos"> 129</span></a><span class="sd">            1. &#39;N01&#39; (default): standard normal distribution $N(0, 1)$.</span>
</span><span id="L-130"><a href="#L-130"><span class="linenos"> 130</span></a><span class="sd">            2. &#39;U-11&#39;: uniform distribution $U(-1, 1)$.</span>
</span><span id="L-131"><a href="#L-131"><span class="linenos"> 131</span></a><span class="sd">            3. &#39;U01&#39;: uniform distribution $U(0, 1)$.</span>
</span><span id="L-132"><a href="#L-132"><span class="linenos"> 132</span></a><span class="sd">            4. &#39;U-10&#39;: uniform distribution $U(-1, 0)$.</span>
</span><span id="L-133"><a href="#L-133"><span class="linenos"> 133</span></a><span class="sd">            5. &#39;last&#39;: inherit the task embedding from the last task.</span>
</span><span id="L-134"><a href="#L-134"><span class="linenos"> 134</span></a><span class="sd">        - **importance_summing_strategy_linear_step** (`float` | `None`): linear step for the importance summing strategy (used when `importance_summing_strategy` is &#39;linear_decrease&#39;). Must be &gt; 0.</span>
</span><span id="L-135"><a href="#L-135"><span class="linenos"> 135</span></a><span class="sd">        - **importance_summing_strategy_exponential_rate** (`float` | `None`): exponential rate for the importance summing strategy (used when `importance_summing_strategy` is &#39;exponential_decrease&#39;). Must be &gt; 1.</span>
</span><span id="L-136"><a href="#L-136"><span class="linenos"> 136</span></a><span class="sd">        - **importance_summing_strategy_log_base** (`float` | `None`): base for the logarithm in the importance summing strategy (used when `importance_summing_strategy` is &#39;log_decrease&#39;). Must be &gt; 1.</span>
</span><span id="L-137"><a href="#L-137"><span class="linenos"> 137</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-138"><a href="#L-138"><span class="linenos"> 138</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-139"><a href="#L-139"><span class="linenos"> 139</span></a>            <span class="n">backbone</span><span class="o">=</span><span class="n">backbone</span><span class="p">,</span>
</span><span id="L-140"><a href="#L-140"><span class="linenos"> 140</span></a>            <span class="n">heads</span><span class="o">=</span><span class="n">heads</span><span class="p">,</span>
</span><span id="L-141"><a href="#L-141"><span class="linenos"> 141</span></a>            <span class="n">adjustment_mode</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="c1"># use the own adjustment mechanism of FG-AdaHAT</span>
</span><span id="L-142"><a href="#L-142"><span class="linenos"> 142</span></a>            <span class="n">adjustment_intensity</span><span class="o">=</span><span class="n">adjustment_intensity</span><span class="p">,</span>
</span><span id="L-143"><a href="#L-143"><span class="linenos"> 143</span></a>            <span class="n">s_max</span><span class="o">=</span><span class="n">s_max</span><span class="p">,</span>
</span><span id="L-144"><a href="#L-144"><span class="linenos"> 144</span></a>            <span class="n">clamp_threshold</span><span class="o">=</span><span class="n">clamp_threshold</span><span class="p">,</span>
</span><span id="L-145"><a href="#L-145"><span class="linenos"> 145</span></a>            <span class="n">mask_sparsity_reg_factor</span><span class="o">=</span><span class="n">mask_sparsity_reg_factor</span><span class="p">,</span>
</span><span id="L-146"><a href="#L-146"><span class="linenos"> 146</span></a>            <span class="n">mask_sparsity_reg_mode</span><span class="o">=</span><span class="n">mask_sparsity_reg_mode</span><span class="p">,</span>
</span><span id="L-147"><a href="#L-147"><span class="linenos"> 147</span></a>            <span class="n">task_embedding_init_mode</span><span class="o">=</span><span class="n">task_embedding_init_mode</span><span class="p">,</span>
</span><span id="L-148"><a href="#L-148"><span class="linenos"> 148</span></a>            <span class="n">epsilon</span><span class="o">=</span><span class="n">base_mask_sparsity_reg</span><span class="p">,</span>  <span class="c1"># the epsilon is now the base mask sparsity regularization factor</span>
</span><span id="L-149"><a href="#L-149"><span class="linenos"> 149</span></a>        <span class="p">)</span>
</span><span id="L-150"><a href="#L-150"><span class="linenos"> 150</span></a>
</span><span id="L-151"><a href="#L-151"><span class="linenos"> 151</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">importance_type</span>
</span><span id="L-152"><a href="#L-152"><span class="linenos"> 152</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The type of the neuron-wise importance added to AdaHAT importance.&quot;&quot;&quot;</span>
</span><span id="L-153"><a href="#L-153"><span class="linenos"> 153</span></a>
</span><span id="L-154"><a href="#L-154"><span class="linenos"> 154</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">importance_scheduler_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">importance_scheduler_type</span>
</span><span id="L-155"><a href="#L-155"><span class="linenos"> 155</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the type of the importance scheduler.&quot;&quot;&quot;</span>
</span><span id="L-156"><a href="#L-156"><span class="linenos"> 156</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">neuron_to_weight_importance_aggregation_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-157"><a href="#L-157"><span class="linenos"> 157</span></a>            <span class="n">neuron_to_weight_importance_aggregation_mode</span>
</span><span id="L-158"><a href="#L-158"><span class="linenos"> 158</span></a>        <span class="p">)</span>
</span><span id="L-159"><a href="#L-159"><span class="linenos"> 159</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The mode of aggregation from neuron-wise to weight-wise importance. &quot;&quot;&quot;</span>
</span><span id="L-160"><a href="#L-160"><span class="linenos"> 160</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">filter_by_cumulative_mask</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">filter_by_cumulative_mask</span>
</span><span id="L-161"><a href="#L-161"><span class="linenos"> 161</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The flag to filter importance by the cumulative mask when calculating the adjustment rate.&quot;&quot;&quot;</span>
</span><span id="L-162"><a href="#L-162"><span class="linenos"> 162</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">filter_unmasked_importance</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">filter_unmasked_importance</span>
</span><span id="L-163"><a href="#L-163"><span class="linenos"> 163</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The flag to filter unmasked importance values (set them to 0) at the end of task training.&quot;&quot;&quot;</span>
</span><span id="L-164"><a href="#L-164"><span class="linenos"> 164</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">step_multiply_training_mask</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">step_multiply_training_mask</span>
</span><span id="L-165"><a href="#L-165"><span class="linenos"> 165</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The flag to multiply the training mask to the importance at each training step.&quot;&quot;&quot;</span>
</span><span id="L-166"><a href="#L-166"><span class="linenos"> 166</span></a>
</span><span id="L-167"><a href="#L-167"><span class="linenos"> 167</span></a>        <span class="c1"># importance summing strategy</span>
</span><span id="L-168"><a href="#L-168"><span class="linenos"> 168</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">importance_summing_strategy</span>
</span><span id="L-169"><a href="#L-169"><span class="linenos"> 169</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The strategy to sum the neuron-wise importance for previous tasks.&quot;&quot;&quot;</span>
</span><span id="L-170"><a href="#L-170"><span class="linenos"> 170</span></a>        <span class="k">if</span> <span class="n">importance_summing_strategy_linear_step</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-171"><a href="#L-171"><span class="linenos"> 171</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy_linear_step</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-172"><a href="#L-172"><span class="linenos"> 172</span></a>                <span class="n">importance_summing_strategy_linear_step</span>
</span><span id="L-173"><a href="#L-173"><span class="linenos"> 173</span></a>            <span class="p">)</span>
</span><span id="L-174"><a href="#L-174"><span class="linenos"> 174</span></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The linear step for the importance summing strategy (only when `importance_summing_strategy` is &#39;linear_decrease&#39;).&quot;&quot;&quot;</span>
</span><span id="L-175"><a href="#L-175"><span class="linenos"> 175</span></a>        <span class="k">if</span> <span class="n">importance_summing_strategy_exponential_rate</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-176"><a href="#L-176"><span class="linenos"> 176</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy_exponential_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-177"><a href="#L-177"><span class="linenos"> 177</span></a>                <span class="n">importance_summing_strategy_exponential_rate</span>
</span><span id="L-178"><a href="#L-178"><span class="linenos"> 178</span></a>            <span class="p">)</span>
</span><span id="L-179"><a href="#L-179"><span class="linenos"> 179</span></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The exponential rate for the importance summing strategy (only when `importance_summing_strategy` is &#39;exponential_decrease&#39;). &quot;&quot;&quot;</span>
</span><span id="L-180"><a href="#L-180"><span class="linenos"> 180</span></a>        <span class="k">if</span> <span class="n">importance_summing_strategy_log_base</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-181"><a href="#L-181"><span class="linenos"> 181</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy_log_base</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-182"><a href="#L-182"><span class="linenos"> 182</span></a>                <span class="n">importance_summing_strategy_log_base</span>
</span><span id="L-183"><a href="#L-183"><span class="linenos"> 183</span></a>            <span class="p">)</span>
</span><span id="L-184"><a href="#L-184"><span class="linenos"> 184</span></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The base for the logarithm in the importance summing strategy (only when `importance_summing_strategy` is &#39;log_decrease&#39;). &quot;&quot;&quot;</span>
</span><span id="L-185"><a href="#L-185"><span class="linenos"> 185</span></a>
</span><span id="L-186"><a href="#L-186"><span class="linenos"> 186</span></a>        <span class="c1"># base values</span>
</span><span id="L-187"><a href="#L-187"><span class="linenos"> 187</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">base_importance</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">base_importance</span>
</span><span id="L-188"><a href="#L-188"><span class="linenos"> 188</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The base value added to the importance to avoid zero. &quot;&quot;&quot;</span>
</span><span id="L-189"><a href="#L-189"><span class="linenos"> 189</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">base_mask_sparsity_reg</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">base_mask_sparsity_reg</span>
</span><span id="L-190"><a href="#L-190"><span class="linenos"> 190</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The base value added to the mask sparsity regularization to avoid zero. &quot;&quot;&quot;</span>
</span><span id="L-191"><a href="#L-191"><span class="linenos"> 191</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">base_linear</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">base_linear</span>
</span><span id="L-192"><a href="#L-192"><span class="linenos"> 192</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The base value added to the linear layer to avoid zero. &quot;&quot;&quot;</span>
</span><span id="L-193"><a href="#L-193"><span class="linenos"> 193</span></a>
</span><span id="L-194"><a href="#L-194"><span class="linenos"> 194</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">importances</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="L-195"><a href="#L-195"><span class="linenos"> 195</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The min-max scaled ($[0, 1]$) neuron-wise importance of units. It is $I^{\tau}_{l}$ in the paper. Keys are task IDs and values are the corresponding importance tensors. Each importance tensor is a dict where keys are layer names and values are the importance tensor for the layer. The utility tensor is the same size as the feature tensor with size (number of units, ). &quot;&quot;&quot;</span>
</span><span id="L-196"><a href="#L-196"><span class="linenos"> 196</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">summative_importance_for_previous_tasks</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="L-197"><a href="#L-197"><span class="linenos"> 197</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The summative neuron-wise importance values of units for previous tasks before the current task `self.task_id`. See $I^{&lt;t}_{l}$ in the paper. Keys are layer names and values are the summative importance tensor for the layer. The summative importance tensor has the same size as the feature tensor with size (number of units, ). &quot;&quot;&quot;</span>
</span><span id="L-198"><a href="#L-198"><span class="linenos"> 198</span></a>
</span><span id="L-199"><a href="#L-199"><span class="linenos"> 199</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">num_steps_t</span><span class="p">:</span> <span class="nb">int</span>
</span><span id="L-200"><a href="#L-200"><span class="linenos"> 200</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the number of training steps for the current task `self.task_id`.&quot;&quot;&quot;</span>
</span><span id="L-201"><a href="#L-201"><span class="linenos"> 201</span></a>        <span class="c1"># set manual optimization</span>
</span><span id="L-202"><a href="#L-202"><span class="linenos"> 202</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">automatic_optimization</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="L-203"><a href="#L-203"><span class="linenos"> 203</span></a>
</span><span id="L-204"><a href="#L-204"><span class="linenos"> 204</span></a>        <span class="n">FGAdaHAT</span><span class="o">.</span><span class="n">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</span><span id="L-205"><a href="#L-205"><span class="linenos"> 205</span></a>
</span><span id="L-206"><a href="#L-206"><span class="linenos"> 206</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-207"><a href="#L-207"><span class="linenos"> 207</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Check the sanity of the arguments.&quot;&quot;&quot;</span>
</span><span id="L-208"><a href="#L-208"><span class="linenos"> 208</span></a>
</span><span id="L-209"><a href="#L-209"><span class="linenos"> 209</span></a>        <span class="c1"># check importance type</span>
</span><span id="L-210"><a href="#L-210"><span class="linenos"> 210</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span>
</span><span id="L-211"><a href="#L-211"><span class="linenos"> 211</span></a>            <span class="s2">&quot;input_weight_abs_sum&quot;</span><span class="p">,</span>
</span><span id="L-212"><a href="#L-212"><span class="linenos"> 212</span></a>            <span class="s2">&quot;output_weight_abs_sum&quot;</span><span class="p">,</span>
</span><span id="L-213"><a href="#L-213"><span class="linenos"> 213</span></a>            <span class="s2">&quot;input_weight_gradient_abs_sum&quot;</span><span class="p">,</span>
</span><span id="L-214"><a href="#L-214"><span class="linenos"> 214</span></a>            <span class="s2">&quot;output_weight_gradient_abs_sum&quot;</span><span class="p">,</span>
</span><span id="L-215"><a href="#L-215"><span class="linenos"> 215</span></a>            <span class="s2">&quot;activation_abs&quot;</span><span class="p">,</span>
</span><span id="L-216"><a href="#L-216"><span class="linenos"> 216</span></a>            <span class="s2">&quot;input_weight_abs_sum_x_activation_abs&quot;</span><span class="p">,</span>
</span><span id="L-217"><a href="#L-217"><span class="linenos"> 217</span></a>            <span class="s2">&quot;output_weight_abs_sum_x_activation_abs&quot;</span><span class="p">,</span>
</span><span id="L-218"><a href="#L-218"><span class="linenos"> 218</span></a>            <span class="s2">&quot;gradient_x_activation_abs&quot;</span><span class="p">,</span>
</span><span id="L-219"><a href="#L-219"><span class="linenos"> 219</span></a>            <span class="s2">&quot;input_weight_gradient_square_sum&quot;</span><span class="p">,</span>
</span><span id="L-220"><a href="#L-220"><span class="linenos"> 220</span></a>            <span class="s2">&quot;output_weight_gradient_square_sum&quot;</span><span class="p">,</span>
</span><span id="L-221"><a href="#L-221"><span class="linenos"> 221</span></a>            <span class="s2">&quot;input_weight_gradient_square_sum_x_activation_abs&quot;</span><span class="p">,</span>
</span><span id="L-222"><a href="#L-222"><span class="linenos"> 222</span></a>            <span class="s2">&quot;output_weight_gradient_square_sum_x_activation_abs&quot;</span><span class="p">,</span>
</span><span id="L-223"><a href="#L-223"><span class="linenos"> 223</span></a>            <span class="s2">&quot;conductance_abs&quot;</span><span class="p">,</span>
</span><span id="L-224"><a href="#L-224"><span class="linenos"> 224</span></a>            <span class="s2">&quot;internal_influence_abs&quot;</span><span class="p">,</span>
</span><span id="L-225"><a href="#L-225"><span class="linenos"> 225</span></a>            <span class="s2">&quot;gradcam_abs&quot;</span><span class="p">,</span>
</span><span id="L-226"><a href="#L-226"><span class="linenos"> 226</span></a>            <span class="s2">&quot;deeplift_abs&quot;</span><span class="p">,</span>
</span><span id="L-227"><a href="#L-227"><span class="linenos"> 227</span></a>            <span class="s2">&quot;deepliftshap_abs&quot;</span><span class="p">,</span>
</span><span id="L-228"><a href="#L-228"><span class="linenos"> 228</span></a>            <span class="s2">&quot;gradientshap_abs&quot;</span><span class="p">,</span>
</span><span id="L-229"><a href="#L-229"><span class="linenos"> 229</span></a>            <span class="s2">&quot;integrated_gradients_abs&quot;</span><span class="p">,</span>
</span><span id="L-230"><a href="#L-230"><span class="linenos"> 230</span></a>            <span class="s2">&quot;feature_ablation_abs&quot;</span><span class="p">,</span>
</span><span id="L-231"><a href="#L-231"><span class="linenos"> 231</span></a>            <span class="s2">&quot;lrp_abs&quot;</span><span class="p">,</span>
</span><span id="L-232"><a href="#L-232"><span class="linenos"> 232</span></a>            <span class="s2">&quot;cbp_adaptation&quot;</span><span class="p">,</span>
</span><span id="L-233"><a href="#L-233"><span class="linenos"> 233</span></a>            <span class="s2">&quot;cbp_adaptive_contribution&quot;</span><span class="p">,</span>
</span><span id="L-234"><a href="#L-234"><span class="linenos"> 234</span></a>        <span class="p">]:</span>
</span><span id="L-235"><a href="#L-235"><span class="linenos"> 235</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="L-236"><a href="#L-236"><span class="linenos"> 236</span></a>                <span class="sa">f</span><span class="s2">&quot;importance_type must be one of the predefined types, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="L-237"><a href="#L-237"><span class="linenos"> 237</span></a>            <span class="p">)</span>
</span><span id="L-238"><a href="#L-238"><span class="linenos"> 238</span></a>
</span><span id="L-239"><a href="#L-239"><span class="linenos"> 239</span></a>        <span class="c1"># check importance summing strategy</span>
</span><span id="L-240"><a href="#L-240"><span class="linenos"> 240</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span>
</span><span id="L-241"><a href="#L-241"><span class="linenos"> 241</span></a>            <span class="s2">&quot;add_latest&quot;</span><span class="p">,</span>
</span><span id="L-242"><a href="#L-242"><span class="linenos"> 242</span></a>            <span class="s2">&quot;add_all&quot;</span><span class="p">,</span>
</span><span id="L-243"><a href="#L-243"><span class="linenos"> 243</span></a>            <span class="s2">&quot;add_average&quot;</span><span class="p">,</span>
</span><span id="L-244"><a href="#L-244"><span class="linenos"> 244</span></a>            <span class="s2">&quot;linear_decrease&quot;</span><span class="p">,</span>
</span><span id="L-245"><a href="#L-245"><span class="linenos"> 245</span></a>            <span class="s2">&quot;quadratic_decrease&quot;</span><span class="p">,</span>
</span><span id="L-246"><a href="#L-246"><span class="linenos"> 246</span></a>            <span class="s2">&quot;cubic_decrease&quot;</span><span class="p">,</span>
</span><span id="L-247"><a href="#L-247"><span class="linenos"> 247</span></a>            <span class="s2">&quot;exponential_decrease&quot;</span><span class="p">,</span>
</span><span id="L-248"><a href="#L-248"><span class="linenos"> 248</span></a>            <span class="s2">&quot;log_decrease&quot;</span><span class="p">,</span>
</span><span id="L-249"><a href="#L-249"><span class="linenos"> 249</span></a>            <span class="s2">&quot;factorial_decrease&quot;</span><span class="p">,</span>
</span><span id="L-250"><a href="#L-250"><span class="linenos"> 250</span></a>        <span class="p">]:</span>
</span><span id="L-251"><a href="#L-251"><span class="linenos"> 251</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="L-252"><a href="#L-252"><span class="linenos"> 252</span></a>                <span class="sa">f</span><span class="s2">&quot;importance_summing_strategy must be one of the predefined strategies, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="L-253"><a href="#L-253"><span class="linenos"> 253</span></a>            <span class="p">)</span>
</span><span id="L-254"><a href="#L-254"><span class="linenos"> 254</span></a>
</span><span id="L-255"><a href="#L-255"><span class="linenos"> 255</span></a>        <span class="c1"># check importance scheduler type</span>
</span><span id="L-256"><a href="#L-256"><span class="linenos"> 256</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_scheduler_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span>
</span><span id="L-257"><a href="#L-257"><span class="linenos"> 257</span></a>            <span class="s2">&quot;linear_sparsity_reg&quot;</span><span class="p">,</span>
</span><span id="L-258"><a href="#L-258"><span class="linenos"> 258</span></a>            <span class="s2">&quot;sparsity_reg&quot;</span><span class="p">,</span>
</span><span id="L-259"><a href="#L-259"><span class="linenos"> 259</span></a>            <span class="s2">&quot;summative_mask_sparsity_reg&quot;</span><span class="p">,</span>
</span><span id="L-260"><a href="#L-260"><span class="linenos"> 260</span></a>        <span class="p">]:</span>
</span><span id="L-261"><a href="#L-261"><span class="linenos"> 261</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="L-262"><a href="#L-262"><span class="linenos"> 262</span></a>                <span class="sa">f</span><span class="s2">&quot;importance_scheduler_type must be one of the predefined types, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">importance_scheduler_type</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="L-263"><a href="#L-263"><span class="linenos"> 263</span></a>            <span class="p">)</span>
</span><span id="L-264"><a href="#L-264"><span class="linenos"> 264</span></a>
</span><span id="L-265"><a href="#L-265"><span class="linenos"> 265</span></a>        <span class="c1"># check neuron to weight importance aggregation mode</span>
</span><span id="L-266"><a href="#L-266"><span class="linenos"> 266</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">neuron_to_weight_importance_aggregation_mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span>
</span><span id="L-267"><a href="#L-267"><span class="linenos"> 267</span></a>            <span class="s2">&quot;min&quot;</span><span class="p">,</span>
</span><span id="L-268"><a href="#L-268"><span class="linenos"> 268</span></a>            <span class="s2">&quot;max&quot;</span><span class="p">,</span>
</span><span id="L-269"><a href="#L-269"><span class="linenos"> 269</span></a>            <span class="s2">&quot;mean&quot;</span><span class="p">,</span>
</span><span id="L-270"><a href="#L-270"><span class="linenos"> 270</span></a>        <span class="p">]:</span>
</span><span id="L-271"><a href="#L-271"><span class="linenos"> 271</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="L-272"><a href="#L-272"><span class="linenos"> 272</span></a>                <span class="sa">f</span><span class="s2">&quot;neuron_to_weight_importance_aggregation_mode must be one of the predefined modes, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">neuron_to_weight_importance_aggregation_mode</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="L-273"><a href="#L-273"><span class="linenos"> 273</span></a>            <span class="p">)</span>
</span><span id="L-274"><a href="#L-274"><span class="linenos"> 274</span></a>
</span><span id="L-275"><a href="#L-275"><span class="linenos"> 275</span></a>        <span class="c1"># check base values</span>
</span><span id="L-276"><a href="#L-276"><span class="linenos"> 276</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_importance</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="L-277"><a href="#L-277"><span class="linenos"> 277</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="L-278"><a href="#L-278"><span class="linenos"> 278</span></a>                <span class="sa">f</span><span class="s2">&quot;base_importance must be &gt;= 0, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">base_importance</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="L-279"><a href="#L-279"><span class="linenos"> 279</span></a>            <span class="p">)</span>
</span><span id="L-280"><a href="#L-280"><span class="linenos"> 280</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_mask_sparsity_reg</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="L-281"><a href="#L-281"><span class="linenos"> 281</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="L-282"><a href="#L-282"><span class="linenos"> 282</span></a>                <span class="sa">f</span><span class="s2">&quot;base_mask_sparsity_reg must be &gt; 0, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">base_mask_sparsity_reg</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="L-283"><a href="#L-283"><span class="linenos"> 283</span></a>            <span class="p">)</span>
</span><span id="L-284"><a href="#L-284"><span class="linenos"> 284</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_linear</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="L-285"><a href="#L-285"><span class="linenos"> 285</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;base_linear must be &gt; 0, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">base_linear</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="L-286"><a href="#L-286"><span class="linenos"> 286</span></a>
</span><span id="L-287"><a href="#L-287"><span class="linenos"> 287</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_start</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-288"><a href="#L-288"><span class="linenos"> 288</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialize neuron importance accumulation variable for each layer as zeros, in addition to AdaHAT&#39;s summative mask initialization.&quot;&quot;&quot;</span>
</span><span id="L-289"><a href="#L-289"><span class="linenos"> 289</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">on_train_start</span><span class="p">()</span>
</span><span id="L-290"><a href="#L-290"><span class="linenos"> 290</span></a>
</span><span id="L-291"><a href="#L-291"><span class="linenos"> 291</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">importances</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-292"><a href="#L-292"><span class="linenos"> 292</span></a>            <span class="p">{}</span>
</span><span id="L-293"><a href="#L-293"><span class="linenos"> 293</span></a>        <span class="p">)</span>  <span class="c1"># initialize the importance for the current task</span>
</span><span id="L-294"><a href="#L-294"><span class="linenos"> 294</span></a>
</span><span id="L-295"><a href="#L-295"><span class="linenos"> 295</span></a>        <span class="c1"># initialize the neuron importance at the beginning of each task. This should not be called in `__init__()` method because `self.device` is not available at that time.</span>
</span><span id="L-296"><a href="#L-296"><span class="linenos"> 296</span></a>        <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="L-297"><a href="#L-297"><span class="linenos"> 297</span></a>            <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span>
</span><span id="L-298"><a href="#L-298"><span class="linenos"> 298</span></a>                <span class="n">layer_name</span>
</span><span id="L-299"><a href="#L-299"><span class="linenos"> 299</span></a>            <span class="p">)</span>  <span class="c1"># get the layer by its name</span>
</span><span id="L-300"><a href="#L-300"><span class="linenos"> 300</span></a>            <span class="n">num_units</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="L-301"><a href="#L-301"><span class="linenos"> 301</span></a>
</span><span id="L-302"><a href="#L-302"><span class="linenos"> 302</span></a>            <span class="c1"># initialize the accumulated importance at the beginning of each task</span>
</span><span id="L-303"><a href="#L-303"><span class="linenos"> 303</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">importances</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">][</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_units</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
</span><span id="L-304"><a href="#L-304"><span class="linenos"> 304</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
</span><span id="L-305"><a href="#L-305"><span class="linenos"> 305</span></a>            <span class="p">)</span>
</span><span id="L-306"><a href="#L-306"><span class="linenos"> 306</span></a>
</span><span id="L-307"><a href="#L-307"><span class="linenos"> 307</span></a>            <span class="c1"># reset the number of steps counter for the current task</span>
</span><span id="L-308"><a href="#L-308"><span class="linenos"> 308</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">num_steps_t</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="L-309"><a href="#L-309"><span class="linenos"> 309</span></a>
</span><span id="L-310"><a href="#L-310"><span class="linenos"> 310</span></a>            <span class="c1"># initialize the summative neuron-wise importance at the beginning of the first task</span>
</span><span id="L-311"><a href="#L-311"><span class="linenos"> 311</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="L-312"><a href="#L-312"><span class="linenos"> 312</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">summative_importance_for_previous_tasks</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
</span><span id="L-313"><a href="#L-313"><span class="linenos"> 313</span></a>                    <span class="n">num_units</span>
</span><span id="L-314"><a href="#L-314"><span class="linenos"> 314</span></a>                <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
</span><span id="L-315"><a href="#L-315"><span class="linenos"> 315</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
</span><span id="L-316"><a href="#L-316"><span class="linenos"> 316</span></a>                <span class="p">)</span>  <span class="c1"># the summative neuron-wise importance for previous tasks $I^{&lt;t}_{l}$ is initialized as zeros mask when $t=1$</span>
</span><span id="L-317"><a href="#L-317"><span class="linenos"> 317</span></a>
</span><span id="L-318"><a href="#L-318"><span class="linenos"> 318</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">clip_grad_by_adjustment</span><span class="p">(</span>
</span><span id="L-319"><a href="#L-319"><span class="linenos"> 319</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-320"><a href="#L-320"><span class="linenos"> 320</span></a>        <span class="n">network_sparsity</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span>
</span><span id="L-321"><a href="#L-321"><span class="linenos"> 321</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="n">Tensor</span><span class="p">]:</span>
</span><span id="L-322"><a href="#L-322"><span class="linenos"> 322</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Clip the gradients by the adjustment rate. See Eq. (1) in the paper.</span>
</span><span id="L-323"><a href="#L-323"><span class="linenos"> 323</span></a>
</span><span id="L-324"><a href="#L-324"><span class="linenos"> 324</span></a><span class="sd">        Note that because the task embedding fully covers every layer in the backbone network, no parameters are left out of this system. This applies not only to parameters between layers with task embeddings, but also to those before the first layer. We design it separately in the code.</span>
</span><span id="L-325"><a href="#L-325"><span class="linenos"> 325</span></a>
</span><span id="L-326"><a href="#L-326"><span class="linenos"> 326</span></a><span class="sd">        Network capacity is measured alongside this method. Network capacity is defined as the average adjustment rate over all parameters. See Sec. 4.1 in the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="L-327"><a href="#L-327"><span class="linenos"> 327</span></a>
</span><span id="L-328"><a href="#L-328"><span class="linenos"> 328</span></a><span class="sd">        **Args:**</span>
</span><span id="L-329"><a href="#L-329"><span class="linenos"> 329</span></a><span class="sd">        - **network_sparsity** (`dict[str, Tensor]`): the network sparsity (i.e., mask sparsity loss of each layer) for the current task. Keys are layer names and values are the network sparsity values. It is used to calculate the adjustment rate for gradients. In FG-AdaHAT, it is used to construct the importance scheduler.</span>
</span><span id="L-330"><a href="#L-330"><span class="linenos"> 330</span></a>
</span><span id="L-331"><a href="#L-331"><span class="linenos"> 331</span></a><span class="sd">        **Returns:**</span>
</span><span id="L-332"><a href="#L-332"><span class="linenos"> 332</span></a><span class="sd">        - **adjustment_rate_weight** (`dict[str, Tensor]`): the adjustment rate for weights. Keys (`str`) are layer names and values (`Tensor`) are the adjustment rate tensors.</span>
</span><span id="L-333"><a href="#L-333"><span class="linenos"> 333</span></a><span class="sd">        - **adjustment_rate_bias** (`dict[str, Tensor]`): the adjustment rate for biases. Keys (`str`) are layer names and values (`Tensor`) are the adjustment rate tensors.</span>
</span><span id="L-334"><a href="#L-334"><span class="linenos"> 334</span></a><span class="sd">        - **capacity** (`Tensor`): the calculated network capacity.</span>
</span><span id="L-335"><a href="#L-335"><span class="linenos"> 335</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-336"><a href="#L-336"><span class="linenos"> 336</span></a>
</span><span id="L-337"><a href="#L-337"><span class="linenos"> 337</span></a>        <span class="c1"># initialize network capacity metric</span>
</span><span id="L-338"><a href="#L-338"><span class="linenos"> 338</span></a>        <span class="n">capacity</span> <span class="o">=</span> <span class="n">HATNetworkCapacityMetric</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="L-339"><a href="#L-339"><span class="linenos"> 339</span></a>        <span class="n">adjustment_rate_weight</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="L-340"><a href="#L-340"><span class="linenos"> 340</span></a>        <span class="n">adjustment_rate_bias</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="L-341"><a href="#L-341"><span class="linenos"> 341</span></a>
</span><span id="L-342"><a href="#L-342"><span class="linenos"> 342</span></a>        <span class="c1"># calculate the adjustment rate for gradients of the parameters, both weights and biases (if they exist). See Eq. (2) in the paper</span>
</span><span id="L-343"><a href="#L-343"><span class="linenos"> 343</span></a>        <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="L-344"><a href="#L-344"><span class="linenos"> 344</span></a>
</span><span id="L-345"><a href="#L-345"><span class="linenos"> 345</span></a>            <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span>
</span><span id="L-346"><a href="#L-346"><span class="linenos"> 346</span></a>                <span class="n">layer_name</span>
</span><span id="L-347"><a href="#L-347"><span class="linenos"> 347</span></a>            <span class="p">)</span>  <span class="c1"># get the layer by its name</span>
</span><span id="L-348"><a href="#L-348"><span class="linenos"> 348</span></a>
</span><span id="L-349"><a href="#L-349"><span class="linenos"> 349</span></a>            <span class="c1"># placeholder for the adjustment rate to avoid the error of using it before assignment</span>
</span><span id="L-350"><a href="#L-350"><span class="linenos"> 350</span></a>            <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="L-351"><a href="#L-351"><span class="linenos"> 351</span></a>            <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="L-352"><a href="#L-352"><span class="linenos"> 352</span></a>
</span><span id="L-353"><a href="#L-353"><span class="linenos"> 353</span></a>            <span class="c1"># aggregate the neuron-wise importance to weight-wise importance. Note that the neuron-wise importance has already been min-max scaled to $[0, 1]$ in the `on_train_batch_end()` method, added the base value, and filtered by the mask</span>
</span><span id="L-354"><a href="#L-354"><span class="linenos"> 354</span></a>            <span class="n">weight_importance</span><span class="p">,</span> <span class="n">bias_importance</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-355"><a href="#L-355"><span class="linenos"> 355</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_measure_parameter_wise</span><span class="p">(</span>
</span><span id="L-356"><a href="#L-356"><span class="linenos"> 356</span></a>                    <span class="n">unit_wise_measure</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">summative_importance_for_previous_tasks</span><span class="p">,</span>
</span><span id="L-357"><a href="#L-357"><span class="linenos"> 357</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="L-358"><a href="#L-358"><span class="linenos"> 358</span></a>                    <span class="n">aggregation_mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">neuron_to_weight_importance_aggregation_mode</span><span class="p">,</span>
</span><span id="L-359"><a href="#L-359"><span class="linenos"> 359</span></a>                <span class="p">)</span>
</span><span id="L-360"><a href="#L-360"><span class="linenos"> 360</span></a>            <span class="p">)</span>
</span><span id="L-361"><a href="#L-361"><span class="linenos"> 361</span></a>
</span><span id="L-362"><a href="#L-362"><span class="linenos"> 362</span></a>            <span class="n">weight_mask</span><span class="p">,</span> <span class="n">bias_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_measure_parameter_wise</span><span class="p">(</span>
</span><span id="L-363"><a href="#L-363"><span class="linenos"> 363</span></a>                <span class="n">unit_wise_measure</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">cumulative_mask_for_previous_tasks</span><span class="p">,</span>
</span><span id="L-364"><a href="#L-364"><span class="linenos"> 364</span></a>                <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="L-365"><a href="#L-365"><span class="linenos"> 365</span></a>                <span class="n">aggregation_mode</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span>
</span><span id="L-366"><a href="#L-366"><span class="linenos"> 366</span></a>            <span class="p">)</span>
</span><span id="L-367"><a href="#L-367"><span class="linenos"> 367</span></a>
</span><span id="L-368"><a href="#L-368"><span class="linenos"> 368</span></a>            <span class="c1"># filter the weight importance by the cumulative mask</span>
</span><span id="L-369"><a href="#L-369"><span class="linenos"> 369</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">filter_by_cumulative_mask</span><span class="p">:</span>
</span><span id="L-370"><a href="#L-370"><span class="linenos"> 370</span></a>                <span class="n">weight_importance</span> <span class="o">=</span> <span class="n">weight_importance</span> <span class="o">*</span> <span class="n">weight_mask</span>
</span><span id="L-371"><a href="#L-371"><span class="linenos"> 371</span></a>                <span class="n">bias_importance</span> <span class="o">=</span> <span class="n">bias_importance</span> <span class="o">*</span> <span class="n">bias_mask</span>
</span><span id="L-372"><a href="#L-372"><span class="linenos"> 372</span></a>
</span><span id="L-373"><a href="#L-373"><span class="linenos"> 373</span></a>            <span class="n">network_sparsity_layer</span> <span class="o">=</span> <span class="n">network_sparsity</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="L-374"><a href="#L-374"><span class="linenos"> 374</span></a>
</span><span id="L-375"><a href="#L-375"><span class="linenos"> 375</span></a>            <span class="c1"># calculate importance scheduler (the factor of importance). See Eq. (3) in the paper</span>
</span><span id="L-376"><a href="#L-376"><span class="linenos"> 376</span></a>            <span class="n">factor</span> <span class="o">=</span> <span class="n">network_sparsity_layer</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_mask_sparsity_reg</span>
</span><span id="L-377"><a href="#L-377"><span class="linenos"> 377</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_scheduler_type</span> <span class="o">==</span> <span class="s2">&quot;linear_sparsity_reg&quot;</span><span class="p">:</span>
</span><span id="L-378"><a href="#L-378"><span class="linenos"> 378</span></a>                <span class="n">factor</span> <span class="o">=</span> <span class="n">factor</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_linear</span><span class="p">)</span>
</span><span id="L-379"><a href="#L-379"><span class="linenos"> 379</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_scheduler_type</span> <span class="o">==</span> <span class="s2">&quot;sparsity_reg&quot;</span><span class="p">:</span>
</span><span id="L-380"><a href="#L-380"><span class="linenos"> 380</span></a>                <span class="k">pass</span>
</span><span id="L-381"><a href="#L-381"><span class="linenos"> 381</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_scheduler_type</span> <span class="o">==</span> <span class="s2">&quot;summative_mask_sparsity_reg&quot;</span><span class="p">:</span>
</span><span id="L-382"><a href="#L-382"><span class="linenos"> 382</span></a>                <span class="n">factor</span> <span class="o">=</span> <span class="n">factor</span> <span class="o">*</span> <span class="p">(</span>
</span><span id="L-383"><a href="#L-383"><span class="linenos"> 383</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">summative_mask_for_previous_tasks</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_linear</span>
</span><span id="L-384"><a href="#L-384"><span class="linenos"> 384</span></a>                <span class="p">)</span>
</span><span id="L-385"><a href="#L-385"><span class="linenos"> 385</span></a>
</span><span id="L-386"><a href="#L-386"><span class="linenos"> 386</span></a>            <span class="c1"># calculate the adjustment rate</span>
</span><span id="L-387"><a href="#L-387"><span class="linenos"> 387</span></a>            <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span>
</span><span id="L-388"><a href="#L-388"><span class="linenos"> 388</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span><span class="p">,</span>
</span><span id="L-389"><a href="#L-389"><span class="linenos"> 389</span></a>                <span class="p">(</span><span class="n">factor</span> <span class="o">*</span> <span class="n">weight_importance</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span><span class="p">),</span>
</span><span id="L-390"><a href="#L-390"><span class="linenos"> 390</span></a>            <span class="p">)</span>
</span><span id="L-391"><a href="#L-391"><span class="linenos"> 391</span></a>
</span><span id="L-392"><a href="#L-392"><span class="linenos"> 392</span></a>            <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span>
</span><span id="L-393"><a href="#L-393"><span class="linenos"> 393</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span><span class="p">,</span>
</span><span id="L-394"><a href="#L-394"><span class="linenos"> 394</span></a>                <span class="p">(</span><span class="n">factor</span> <span class="o">*</span> <span class="n">bias_importance</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span><span class="p">),</span>
</span><span id="L-395"><a href="#L-395"><span class="linenos"> 395</span></a>            <span class="p">)</span>
</span><span id="L-396"><a href="#L-396"><span class="linenos"> 396</span></a>
</span><span id="L-397"><a href="#L-397"><span class="linenos"> 397</span></a>            <span class="c1"># apply the adjustment rate to the gradients</span>
</span><span id="L-398"><a href="#L-398"><span class="linenos"> 398</span></a>            <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">adjustment_rate_weight_layer</span>
</span><span id="L-399"><a href="#L-399"><span class="linenos"> 399</span></a>            <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-400"><a href="#L-400"><span class="linenos"> 400</span></a>                <span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">adjustment_rate_bias_layer</span>
</span><span id="L-401"><a href="#L-401"><span class="linenos"> 401</span></a>
</span><span id="L-402"><a href="#L-402"><span class="linenos"> 402</span></a>            <span class="c1"># store the adjustment rate for logging</span>
</span><span id="L-403"><a href="#L-403"><span class="linenos"> 403</span></a>            <span class="n">adjustment_rate_weight</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">adjustment_rate_weight_layer</span>
</span><span id="L-404"><a href="#L-404"><span class="linenos"> 404</span></a>            <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-405"><a href="#L-405"><span class="linenos"> 405</span></a>                <span class="n">adjustment_rate_bias</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">adjustment_rate_bias_layer</span>
</span><span id="L-406"><a href="#L-406"><span class="linenos"> 406</span></a>
</span><span id="L-407"><a href="#L-407"><span class="linenos"> 407</span></a>            <span class="c1"># update network capacity metric</span>
</span><span id="L-408"><a href="#L-408"><span class="linenos"> 408</span></a>            <span class="n">capacity</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">adjustment_rate_weight_layer</span><span class="p">,</span> <span class="n">adjustment_rate_bias_layer</span><span class="p">)</span>
</span><span id="L-409"><a href="#L-409"><span class="linenos"> 409</span></a>
</span><span id="L-410"><a href="#L-410"><span class="linenos"> 410</span></a>        <span class="k">return</span> <span class="n">adjustment_rate_weight</span><span class="p">,</span> <span class="n">adjustment_rate_bias</span><span class="p">,</span> <span class="n">capacity</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
</span><span id="L-411"><a href="#L-411"><span class="linenos"> 411</span></a>
</span><span id="L-412"><a href="#L-412"><span class="linenos"> 412</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_batch_end</span><span class="p">(</span>
</span><span id="L-413"><a href="#L-413"><span class="linenos"> 413</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span>
</span><span id="L-414"><a href="#L-414"><span class="linenos"> 414</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-415"><a href="#L-415"><span class="linenos"> 415</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Calculate the step-wise importance, update the accumulated importance and number of steps counter after each training step.</span>
</span><span id="L-416"><a href="#L-416"><span class="linenos"> 416</span></a>
</span><span id="L-417"><a href="#L-417"><span class="linenos"> 417</span></a><span class="sd">        **Args:**</span>
</span><span id="L-418"><a href="#L-418"><span class="linenos"> 418</span></a><span class="sd">        - **outputs** (`dict[str, Any]`): outputs of the training step (returns of `training_step()` in `CLAlgorithm`).</span>
</span><span id="L-419"><a href="#L-419"><span class="linenos"> 419</span></a><span class="sd">        - **batch** (`Any`): training data batch.</span>
</span><span id="L-420"><a href="#L-420"><span class="linenos"> 420</span></a><span class="sd">        - **batch_idx** (`int`): index of the current batch (for mask figure file name).</span>
</span><span id="L-421"><a href="#L-421"><span class="linenos"> 421</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-422"><a href="#L-422"><span class="linenos"> 422</span></a>
</span><span id="L-423"><a href="#L-423"><span class="linenos"> 423</span></a>        <span class="c1"># get potential useful information from training batch</span>
</span><span id="L-424"><a href="#L-424"><span class="linenos"> 424</span></a>        <span class="n">activations</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;activations&quot;</span><span class="p">]</span>
</span><span id="L-425"><a href="#L-425"><span class="linenos"> 425</span></a>        <span class="nb">input</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">]</span>
</span><span id="L-426"><a href="#L-426"><span class="linenos"> 426</span></a>        <span class="n">target</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;target&quot;</span><span class="p">]</span>
</span><span id="L-427"><a href="#L-427"><span class="linenos"> 427</span></a>        <span class="n">mask</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;mask&quot;</span><span class="p">]</span>
</span><span id="L-428"><a href="#L-428"><span class="linenos"> 428</span></a>        <span class="n">num_batches</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">num_training_batches</span>
</span><span id="L-429"><a href="#L-429"><span class="linenos"> 429</span></a>
</span><span id="L-430"><a href="#L-430"><span class="linenos"> 430</span></a>        <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="L-431"><a href="#L-431"><span class="linenos"> 431</span></a>            <span class="c1"># layer-wise operation</span>
</span><span id="L-432"><a href="#L-432"><span class="linenos"> 432</span></a>
</span><span id="L-433"><a href="#L-433"><span class="linenos"> 433</span></a>            <span class="n">activation</span> <span class="o">=</span> <span class="n">activations</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="L-434"><a href="#L-434"><span class="linenos"> 434</span></a>
</span><span id="L-435"><a href="#L-435"><span class="linenos"> 435</span></a>            <span class="c1"># calculate neuron-wise importance of the training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper.</span>
</span><span id="L-436"><a href="#L-436"><span class="linenos"> 436</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;input_weight_abs_sum&quot;</span><span class="p">:</span>
</span><span id="L-437"><a href="#L-437"><span class="linenos"> 437</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_weight_abs_sum</span><span class="p">(</span>
</span><span id="L-438"><a href="#L-438"><span class="linenos"> 438</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="L-439"><a href="#L-439"><span class="linenos"> 439</span></a>                    <span class="n">if_output_weight</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="L-440"><a href="#L-440"><span class="linenos"> 440</span></a>                    <span class="n">reciprocal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="L-441"><a href="#L-441"><span class="linenos"> 441</span></a>                <span class="p">)</span>
</span><span id="L-442"><a href="#L-442"><span class="linenos"> 442</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;output_weight_abs_sum&quot;</span><span class="p">:</span>
</span><span id="L-443"><a href="#L-443"><span class="linenos"> 443</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_weight_abs_sum</span><span class="p">(</span>
</span><span id="L-444"><a href="#L-444"><span class="linenos"> 444</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="L-445"><a href="#L-445"><span class="linenos"> 445</span></a>                    <span class="n">if_output_weight</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="L-446"><a href="#L-446"><span class="linenos"> 446</span></a>                    <span class="n">reciprocal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="L-447"><a href="#L-447"><span class="linenos"> 447</span></a>                <span class="p">)</span>
</span><span id="L-448"><a href="#L-448"><span class="linenos"> 448</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;input_weight_gradient_abs_sum&quot;</span><span class="p">:</span>
</span><span id="L-449"><a href="#L-449"><span class="linenos"> 449</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-450"><a href="#L-450"><span class="linenos"> 450</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_weight_gradient_abs_sum</span><span class="p">(</span>
</span><span id="L-451"><a href="#L-451"><span class="linenos"> 451</span></a>                        <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span> <span class="n">if_output_weight</span><span class="o">=</span><span class="kc">False</span>
</span><span id="L-452"><a href="#L-452"><span class="linenos"> 452</span></a>                    <span class="p">)</span>
</span><span id="L-453"><a href="#L-453"><span class="linenos"> 453</span></a>                <span class="p">)</span>
</span><span id="L-454"><a href="#L-454"><span class="linenos"> 454</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;output_weight_gradient_abs_sum&quot;</span><span class="p">:</span>
</span><span id="L-455"><a href="#L-455"><span class="linenos"> 455</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-456"><a href="#L-456"><span class="linenos"> 456</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_weight_gradient_abs_sum</span><span class="p">(</span>
</span><span id="L-457"><a href="#L-457"><span class="linenos"> 457</span></a>                        <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span> <span class="n">if_output_weight</span><span class="o">=</span><span class="kc">True</span>
</span><span id="L-458"><a href="#L-458"><span class="linenos"> 458</span></a>                    <span class="p">)</span>
</span><span id="L-459"><a href="#L-459"><span class="linenos"> 459</span></a>                <span class="p">)</span>
</span><span id="L-460"><a href="#L-460"><span class="linenos"> 460</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;activation_abs&quot;</span><span class="p">:</span>
</span><span id="L-461"><a href="#L-461"><span class="linenos"> 461</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_activation_abs</span><span class="p">(</span>
</span><span id="L-462"><a href="#L-462"><span class="linenos"> 462</span></a>                    <span class="n">activation</span><span class="o">=</span><span class="n">activation</span>
</span><span id="L-463"><a href="#L-463"><span class="linenos"> 463</span></a>                <span class="p">)</span>
</span><span id="L-464"><a href="#L-464"><span class="linenos"> 464</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;input_weight_abs_sum_x_activation_abs&quot;</span><span class="p">:</span>
</span><span id="L-465"><a href="#L-465"><span class="linenos"> 465</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-466"><a href="#L-466"><span class="linenos"> 466</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_weight_abs_sum_x_activation_abs</span><span class="p">(</span>
</span><span id="L-467"><a href="#L-467"><span class="linenos"> 467</span></a>                        <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="L-468"><a href="#L-468"><span class="linenos"> 468</span></a>                        <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
</span><span id="L-469"><a href="#L-469"><span class="linenos"> 469</span></a>                        <span class="n">if_output_weight</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="L-470"><a href="#L-470"><span class="linenos"> 470</span></a>                    <span class="p">)</span>
</span><span id="L-471"><a href="#L-471"><span class="linenos"> 471</span></a>                <span class="p">)</span>
</span><span id="L-472"><a href="#L-472"><span class="linenos"> 472</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;output_weight_abs_sum_x_activation_abs&quot;</span><span class="p">:</span>
</span><span id="L-473"><a href="#L-473"><span class="linenos"> 473</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-474"><a href="#L-474"><span class="linenos"> 474</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_weight_abs_sum_x_activation_abs</span><span class="p">(</span>
</span><span id="L-475"><a href="#L-475"><span class="linenos"> 475</span></a>                        <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="L-476"><a href="#L-476"><span class="linenos"> 476</span></a>                        <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
</span><span id="L-477"><a href="#L-477"><span class="linenos"> 477</span></a>                        <span class="n">if_output_weight</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="L-478"><a href="#L-478"><span class="linenos"> 478</span></a>                    <span class="p">)</span>
</span><span id="L-479"><a href="#L-479"><span class="linenos"> 479</span></a>                <span class="p">)</span>
</span><span id="L-480"><a href="#L-480"><span class="linenos"> 480</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;gradient_x_activation_abs&quot;</span><span class="p">:</span>
</span><span id="L-481"><a href="#L-481"><span class="linenos"> 481</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-482"><a href="#L-482"><span class="linenos"> 482</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_gradient_x_activation_abs</span><span class="p">(</span>
</span><span id="L-483"><a href="#L-483"><span class="linenos"> 483</span></a>                        <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="L-484"><a href="#L-484"><span class="linenos"> 484</span></a>                        <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="L-485"><a href="#L-485"><span class="linenos"> 485</span></a>                        <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="L-486"><a href="#L-486"><span class="linenos"> 486</span></a>                        <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="L-487"><a href="#L-487"><span class="linenos"> 487</span></a>                        <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="L-488"><a href="#L-488"><span class="linenos"> 488</span></a>                    <span class="p">)</span>
</span><span id="L-489"><a href="#L-489"><span class="linenos"> 489</span></a>                <span class="p">)</span>
</span><span id="L-490"><a href="#L-490"><span class="linenos"> 490</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;input_weight_gradient_square_sum&quot;</span><span class="p">:</span>
</span><span id="L-491"><a href="#L-491"><span class="linenos"> 491</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-492"><a href="#L-492"><span class="linenos"> 492</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_weight_gradient_square_sum</span><span class="p">(</span>
</span><span id="L-493"><a href="#L-493"><span class="linenos"> 493</span></a>                        <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="L-494"><a href="#L-494"><span class="linenos"> 494</span></a>                        <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
</span><span id="L-495"><a href="#L-495"><span class="linenos"> 495</span></a>                        <span class="n">if_output_weight</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="L-496"><a href="#L-496"><span class="linenos"> 496</span></a>                    <span class="p">)</span>
</span><span id="L-497"><a href="#L-497"><span class="linenos"> 497</span></a>                <span class="p">)</span>
</span><span id="L-498"><a href="#L-498"><span class="linenos"> 498</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;output_weight_gradient_square_sum&quot;</span><span class="p">:</span>
</span><span id="L-499"><a href="#L-499"><span class="linenos"> 499</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-500"><a href="#L-500"><span class="linenos"> 500</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_weight_gradient_square_sum</span><span class="p">(</span>
</span><span id="L-501"><a href="#L-501"><span class="linenos"> 501</span></a>                        <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="L-502"><a href="#L-502"><span class="linenos"> 502</span></a>                        <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
</span><span id="L-503"><a href="#L-503"><span class="linenos"> 503</span></a>                        <span class="n">if_output_weight</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="L-504"><a href="#L-504"><span class="linenos"> 504</span></a>                    <span class="p">)</span>
</span><span id="L-505"><a href="#L-505"><span class="linenos"> 505</span></a>                <span class="p">)</span>
</span><span id="L-506"><a href="#L-506"><span class="linenos"> 506</span></a>            <span class="k">elif</span> <span class="p">(</span>
</span><span id="L-507"><a href="#L-507"><span class="linenos"> 507</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span>
</span><span id="L-508"><a href="#L-508"><span class="linenos"> 508</span></a>                <span class="o">==</span> <span class="s2">&quot;input_weight_gradient_square_sum_x_activation_abs&quot;</span>
</span><span id="L-509"><a href="#L-509"><span class="linenos"> 509</span></a>            <span class="p">):</span>
</span><span id="L-510"><a href="#L-510"><span class="linenos"> 510</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_weight_gradient_square_sum_x_activation_abs</span><span class="p">(</span>
</span><span id="L-511"><a href="#L-511"><span class="linenos"> 511</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="L-512"><a href="#L-512"><span class="linenos"> 512</span></a>                    <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
</span><span id="L-513"><a href="#L-513"><span class="linenos"> 513</span></a>                    <span class="n">if_output_weight</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="L-514"><a href="#L-514"><span class="linenos"> 514</span></a>                <span class="p">)</span>
</span><span id="L-515"><a href="#L-515"><span class="linenos"> 515</span></a>            <span class="k">elif</span> <span class="p">(</span>
</span><span id="L-516"><a href="#L-516"><span class="linenos"> 516</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span>
</span><span id="L-517"><a href="#L-517"><span class="linenos"> 517</span></a>                <span class="o">==</span> <span class="s2">&quot;output_weight_gradient_square_sum_x_activation_abs&quot;</span>
</span><span id="L-518"><a href="#L-518"><span class="linenos"> 518</span></a>            <span class="p">):</span>
</span><span id="L-519"><a href="#L-519"><span class="linenos"> 519</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_weight_gradient_square_sum_x_activation_abs</span><span class="p">(</span>
</span><span id="L-520"><a href="#L-520"><span class="linenos"> 520</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="L-521"><a href="#L-521"><span class="linenos"> 521</span></a>                    <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
</span><span id="L-522"><a href="#L-522"><span class="linenos"> 522</span></a>                    <span class="n">if_output_weight</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="L-523"><a href="#L-523"><span class="linenos"> 523</span></a>                <span class="p">)</span>
</span><span id="L-524"><a href="#L-524"><span class="linenos"> 524</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;conductance_abs&quot;</span><span class="p">:</span>
</span><span id="L-525"><a href="#L-525"><span class="linenos"> 525</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_conductance_abs</span><span class="p">(</span>
</span><span id="L-526"><a href="#L-526"><span class="linenos"> 526</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="L-527"><a href="#L-527"><span class="linenos"> 527</span></a>                    <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="L-528"><a href="#L-528"><span class="linenos"> 528</span></a>                    <span class="n">baselines</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="L-529"><a href="#L-529"><span class="linenos"> 529</span></a>                    <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="L-530"><a href="#L-530"><span class="linenos"> 530</span></a>                    <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="L-531"><a href="#L-531"><span class="linenos"> 531</span></a>                    <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="L-532"><a href="#L-532"><span class="linenos"> 532</span></a>                <span class="p">)</span>
</span><span id="L-533"><a href="#L-533"><span class="linenos"> 533</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;internal_influence_abs&quot;</span><span class="p">:</span>
</span><span id="L-534"><a href="#L-534"><span class="linenos"> 534</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_internal_influence_abs</span><span class="p">(</span>
</span><span id="L-535"><a href="#L-535"><span class="linenos"> 535</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="L-536"><a href="#L-536"><span class="linenos"> 536</span></a>                    <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="L-537"><a href="#L-537"><span class="linenos"> 537</span></a>                    <span class="n">baselines</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="L-538"><a href="#L-538"><span class="linenos"> 538</span></a>                    <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="L-539"><a href="#L-539"><span class="linenos"> 539</span></a>                    <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="L-540"><a href="#L-540"><span class="linenos"> 540</span></a>                    <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="L-541"><a href="#L-541"><span class="linenos"> 541</span></a>                <span class="p">)</span>
</span><span id="L-542"><a href="#L-542"><span class="linenos"> 542</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;gradcam_abs&quot;</span><span class="p">:</span>
</span><span id="L-543"><a href="#L-543"><span class="linenos"> 543</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_gradcam_abs</span><span class="p">(</span>
</span><span id="L-544"><a href="#L-544"><span class="linenos"> 544</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="L-545"><a href="#L-545"><span class="linenos"> 545</span></a>                    <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="L-546"><a href="#L-546"><span class="linenos"> 546</span></a>                    <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="L-547"><a href="#L-547"><span class="linenos"> 547</span></a>                    <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="L-548"><a href="#L-548"><span class="linenos"> 548</span></a>                    <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="L-549"><a href="#L-549"><span class="linenos"> 549</span></a>                <span class="p">)</span>
</span><span id="L-550"><a href="#L-550"><span class="linenos"> 550</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;deeplift_abs&quot;</span><span class="p">:</span>
</span><span id="L-551"><a href="#L-551"><span class="linenos"> 551</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_deeplift_abs</span><span class="p">(</span>
</span><span id="L-552"><a href="#L-552"><span class="linenos"> 552</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="L-553"><a href="#L-553"><span class="linenos"> 553</span></a>                    <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="L-554"><a href="#L-554"><span class="linenos"> 554</span></a>                    <span class="n">baselines</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="L-555"><a href="#L-555"><span class="linenos"> 555</span></a>                    <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="L-556"><a href="#L-556"><span class="linenos"> 556</span></a>                    <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="L-557"><a href="#L-557"><span class="linenos"> 557</span></a>                    <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="L-558"><a href="#L-558"><span class="linenos"> 558</span></a>                <span class="p">)</span>
</span><span id="L-559"><a href="#L-559"><span class="linenos"> 559</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;deepliftshap_abs&quot;</span><span class="p">:</span>
</span><span id="L-560"><a href="#L-560"><span class="linenos"> 560</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_deepliftshap_abs</span><span class="p">(</span>
</span><span id="L-561"><a href="#L-561"><span class="linenos"> 561</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="L-562"><a href="#L-562"><span class="linenos"> 562</span></a>                    <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="L-563"><a href="#L-563"><span class="linenos"> 563</span></a>                    <span class="n">baselines</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="L-564"><a href="#L-564"><span class="linenos"> 564</span></a>                    <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="L-565"><a href="#L-565"><span class="linenos"> 565</span></a>                    <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="L-566"><a href="#L-566"><span class="linenos"> 566</span></a>                    <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="L-567"><a href="#L-567"><span class="linenos"> 567</span></a>                <span class="p">)</span>
</span><span id="L-568"><a href="#L-568"><span class="linenos"> 568</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;gradientshap_abs&quot;</span><span class="p">:</span>
</span><span id="L-569"><a href="#L-569"><span class="linenos"> 569</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_gradientshap_abs</span><span class="p">(</span>
</span><span id="L-570"><a href="#L-570"><span class="linenos"> 570</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="L-571"><a href="#L-571"><span class="linenos"> 571</span></a>                    <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="L-572"><a href="#L-572"><span class="linenos"> 572</span></a>                    <span class="n">baselines</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="L-573"><a href="#L-573"><span class="linenos"> 573</span></a>                    <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="L-574"><a href="#L-574"><span class="linenos"> 574</span></a>                    <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="L-575"><a href="#L-575"><span class="linenos"> 575</span></a>                    <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="L-576"><a href="#L-576"><span class="linenos"> 576</span></a>                <span class="p">)</span>
</span><span id="L-577"><a href="#L-577"><span class="linenos"> 577</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;integrated_gradients_abs&quot;</span><span class="p">:</span>
</span><span id="L-578"><a href="#L-578"><span class="linenos"> 578</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-579"><a href="#L-579"><span class="linenos"> 579</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_integrated_gradients_abs</span><span class="p">(</span>
</span><span id="L-580"><a href="#L-580"><span class="linenos"> 580</span></a>                        <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="L-581"><a href="#L-581"><span class="linenos"> 581</span></a>                        <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="L-582"><a href="#L-582"><span class="linenos"> 582</span></a>                        <span class="n">baselines</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="L-583"><a href="#L-583"><span class="linenos"> 583</span></a>                        <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="L-584"><a href="#L-584"><span class="linenos"> 584</span></a>                        <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="L-585"><a href="#L-585"><span class="linenos"> 585</span></a>                        <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="L-586"><a href="#L-586"><span class="linenos"> 586</span></a>                    <span class="p">)</span>
</span><span id="L-587"><a href="#L-587"><span class="linenos"> 587</span></a>                <span class="p">)</span>
</span><span id="L-588"><a href="#L-588"><span class="linenos"> 588</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;feature_ablation_abs&quot;</span><span class="p">:</span>
</span><span id="L-589"><a href="#L-589"><span class="linenos"> 589</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_feature_ablation_abs</span><span class="p">(</span>
</span><span id="L-590"><a href="#L-590"><span class="linenos"> 590</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="L-591"><a href="#L-591"><span class="linenos"> 591</span></a>                    <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="L-592"><a href="#L-592"><span class="linenos"> 592</span></a>                    <span class="n">layer_baselines</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="L-593"><a href="#L-593"><span class="linenos"> 593</span></a>                    <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="L-594"><a href="#L-594"><span class="linenos"> 594</span></a>                    <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="L-595"><a href="#L-595"><span class="linenos"> 595</span></a>                    <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="L-596"><a href="#L-596"><span class="linenos"> 596</span></a>                <span class="p">)</span>
</span><span id="L-597"><a href="#L-597"><span class="linenos"> 597</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;lrp_abs&quot;</span><span class="p">:</span>
</span><span id="L-598"><a href="#L-598"><span class="linenos"> 598</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_lrp_abs</span><span class="p">(</span>
</span><span id="L-599"><a href="#L-599"><span class="linenos"> 599</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="L-600"><a href="#L-600"><span class="linenos"> 600</span></a>                    <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="L-601"><a href="#L-601"><span class="linenos"> 601</span></a>                    <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="L-602"><a href="#L-602"><span class="linenos"> 602</span></a>                    <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="L-603"><a href="#L-603"><span class="linenos"> 603</span></a>                    <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="L-604"><a href="#L-604"><span class="linenos"> 604</span></a>                <span class="p">)</span>
</span><span id="L-605"><a href="#L-605"><span class="linenos"> 605</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;cbp_adaptation&quot;</span><span class="p">:</span>
</span><span id="L-606"><a href="#L-606"><span class="linenos"> 606</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_weight_abs_sum</span><span class="p">(</span>
</span><span id="L-607"><a href="#L-607"><span class="linenos"> 607</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="L-608"><a href="#L-608"><span class="linenos"> 608</span></a>                    <span class="n">if_output_weight</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="L-609"><a href="#L-609"><span class="linenos"> 609</span></a>                    <span class="n">reciprocal</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="L-610"><a href="#L-610"><span class="linenos"> 610</span></a>                <span class="p">)</span>
</span><span id="L-611"><a href="#L-611"><span class="linenos"> 611</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;cbp_adaptive_contribution&quot;</span><span class="p">:</span>
</span><span id="L-612"><a href="#L-612"><span class="linenos"> 612</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-613"><a href="#L-613"><span class="linenos"> 613</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_cbp_adaptive_contribution</span><span class="p">(</span>
</span><span id="L-614"><a href="#L-614"><span class="linenos"> 614</span></a>                        <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="L-615"><a href="#L-615"><span class="linenos"> 615</span></a>                        <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
</span><span id="L-616"><a href="#L-616"><span class="linenos"> 616</span></a>                    <span class="p">)</span>
</span><span id="L-617"><a href="#L-617"><span class="linenos"> 617</span></a>                <span class="p">)</span>
</span><span id="L-618"><a href="#L-618"><span class="linenos"> 618</span></a>
</span><span id="L-619"><a href="#L-619"><span class="linenos"> 619</span></a>            <span class="n">importance_step</span> <span class="o">=</span> <span class="n">min_max_normalize</span><span class="p">(</span>
</span><span id="L-620"><a href="#L-620"><span class="linenos"> 620</span></a>                <span class="n">importance_step</span>
</span><span id="L-621"><a href="#L-621"><span class="linenos"> 621</span></a>            <span class="p">)</span>  <span class="c1"># min-max scaling the utility to $[0, 1]$. See Eq. (5) in the paper</span>
</span><span id="L-622"><a href="#L-622"><span class="linenos"> 622</span></a>
</span><span id="L-623"><a href="#L-623"><span class="linenos"> 623</span></a>            <span class="c1"># multiply the importance by the training mask. See Eq. (6) in the paper</span>
</span><span id="L-624"><a href="#L-624"><span class="linenos"> 624</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_multiply_training_mask</span><span class="p">:</span>
</span><span id="L-625"><a href="#L-625"><span class="linenos"> 625</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="n">importance_step</span> <span class="o">*</span> <span class="n">mask</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="L-626"><a href="#L-626"><span class="linenos"> 626</span></a>
</span><span id="L-627"><a href="#L-627"><span class="linenos"> 627</span></a>            <span class="c1"># update accumulated importance</span>
</span><span id="L-628"><a href="#L-628"><span class="linenos"> 628</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">importances</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">][</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-629"><a href="#L-629"><span class="linenos"> 629</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">importances</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">][</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">+</span> <span class="n">importance_step</span>
</span><span id="L-630"><a href="#L-630"><span class="linenos"> 630</span></a>            <span class="p">)</span>
</span><span id="L-631"><a href="#L-631"><span class="linenos"> 631</span></a>
</span><span id="L-632"><a href="#L-632"><span class="linenos"> 632</span></a>        <span class="c1"># update number of steps counter</span>
</span><span id="L-633"><a href="#L-633"><span class="linenos"> 633</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">num_steps_t</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span id="L-634"><a href="#L-634"><span class="linenos"> 634</span></a>
</span><span id="L-635"><a href="#L-635"><span class="linenos"> 635</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_end</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-636"><a href="#L-636"><span class="linenos"> 636</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Additionally calculate neuron-wise importance for previous tasks at the end of training each task.&quot;&quot;&quot;</span>
</span><span id="L-637"><a href="#L-637"><span class="linenos"> 637</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">on_train_end</span><span class="p">()</span>  <span class="c1"># store the mask and update cumulative and summative masks</span>
</span><span id="L-638"><a href="#L-638"><span class="linenos"> 638</span></a>
</span><span id="L-639"><a href="#L-639"><span class="linenos"> 639</span></a>        <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="L-640"><a href="#L-640"><span class="linenos"> 640</span></a>
</span><span id="L-641"><a href="#L-641"><span class="linenos"> 641</span></a>            <span class="c1"># average the neuron-wise step importance. See Eq. (4) in the paper</span>
</span><span id="L-642"><a href="#L-642"><span class="linenos"> 642</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">importances</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">][</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-643"><a href="#L-643"><span class="linenos"> 643</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">importances</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">][</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="L-644"><a href="#L-644"><span class="linenos"> 644</span></a>            <span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_steps_t</span>
</span><span id="L-645"><a href="#L-645"><span class="linenos"> 645</span></a>
</span><span id="L-646"><a href="#L-646"><span class="linenos"> 646</span></a>            <span class="c1"># add the base importance. See Eq. (6) in the paper</span>
</span><span id="L-647"><a href="#L-647"><span class="linenos"> 647</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">importances</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">][</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-648"><a href="#L-648"><span class="linenos"> 648</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">importances</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">][</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_importance</span>
</span><span id="L-649"><a href="#L-649"><span class="linenos"> 649</span></a>            <span class="p">)</span>
</span><span id="L-650"><a href="#L-650"><span class="linenos"> 650</span></a>
</span><span id="L-651"><a href="#L-651"><span class="linenos"> 651</span></a>            <span class="c1"># filter unmasked importance</span>
</span><span id="L-652"><a href="#L-652"><span class="linenos"> 652</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">filter_unmasked_importance</span><span class="p">:</span>
</span><span id="L-653"><a href="#L-653"><span class="linenos"> 653</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">importances</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">][</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-654"><a href="#L-654"><span class="linenos"> 654</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">importances</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">][</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="L-655"><a href="#L-655"><span class="linenos"> 655</span></a>                    <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">masks</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">][</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="L-656"><a href="#L-656"><span class="linenos"> 656</span></a>                <span class="p">)</span>
</span><span id="L-657"><a href="#L-657"><span class="linenos"> 657</span></a>
</span><span id="L-658"><a href="#L-658"><span class="linenos"> 658</span></a>            <span class="c1"># calculate the summative neuron-wise importance for previous tasks. See Eq. (4) in the paper</span>
</span><span id="L-659"><a href="#L-659"><span class="linenos"> 659</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy</span> <span class="o">==</span> <span class="s2">&quot;add_latest&quot;</span><span class="p">:</span>
</span><span id="L-660"><a href="#L-660"><span class="linenos"> 660</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">summative_importance_for_previous_tasks</span><span class="p">[</span>
</span><span id="L-661"><a href="#L-661"><span class="linenos"> 661</span></a>                    <span class="n">layer_name</span>
</span><span id="L-662"><a href="#L-662"><span class="linenos"> 662</span></a>                <span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">importances</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">][</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="L-663"><a href="#L-663"><span class="linenos"> 663</span></a>
</span><span id="L-664"><a href="#L-664"><span class="linenos"> 664</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy</span> <span class="o">==</span> <span class="s2">&quot;add_all&quot;</span><span class="p">:</span>
</span><span id="L-665"><a href="#L-665"><span class="linenos"> 665</span></a>                <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
</span><span id="L-666"><a href="#L-666"><span class="linenos"> 666</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">summative_importance_for_previous_tasks</span><span class="p">[</span>
</span><span id="L-667"><a href="#L-667"><span class="linenos"> 667</span></a>                        <span class="n">layer_name</span>
</span><span id="L-668"><a href="#L-668"><span class="linenos"> 668</span></a>                    <span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">importances</span><span class="p">[</span><span class="n">t</span><span class="p">][</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="L-669"><a href="#L-669"><span class="linenos"> 669</span></a>
</span><span id="L-670"><a href="#L-670"><span class="linenos"> 670</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy</span> <span class="o">==</span> <span class="s2">&quot;add_average&quot;</span><span class="p">:</span>
</span><span id="L-671"><a href="#L-671"><span class="linenos"> 671</span></a>                <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
</span><span id="L-672"><a href="#L-672"><span class="linenos"> 672</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">summative_importance_for_previous_tasks</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span>
</span><span id="L-673"><a href="#L-673"><span class="linenos"> 673</span></a>                        <span class="bp">self</span><span class="o">.</span><span class="n">importances</span><span class="p">[</span><span class="n">t</span><span class="p">][</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span>
</span><span id="L-674"><a href="#L-674"><span class="linenos"> 674</span></a>                    <span class="p">)</span>
</span><span id="L-675"><a href="#L-675"><span class="linenos"> 675</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="L-676"><a href="#L-676"><span class="linenos"> 676</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">summative_importance_for_previous_tasks</span><span class="p">[</span>
</span><span id="L-677"><a href="#L-677"><span class="linenos"> 677</span></a>                    <span class="n">layer_name</span>
</span><span id="L-678"><a href="#L-678"><span class="linenos"> 678</span></a>                <span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span>
</span><span id="L-679"><a href="#L-679"><span class="linenos"> 679</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">summative_importance_for_previous_tasks</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="L-680"><a href="#L-680"><span class="linenos"> 680</span></a>                <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
</span><span id="L-681"><a href="#L-681"><span class="linenos"> 681</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
</span><span id="L-682"><a href="#L-682"><span class="linenos"> 682</span></a>                <span class="p">)</span>  <span class="c1"># starting adding from 0</span>
</span><span id="L-683"><a href="#L-683"><span class="linenos"> 683</span></a>
</span><span id="L-684"><a href="#L-684"><span class="linenos"> 684</span></a>                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy</span> <span class="o">==</span> <span class="s2">&quot;linear_decrease&quot;</span><span class="p">:</span>
</span><span id="L-685"><a href="#L-685"><span class="linenos"> 685</span></a>                    <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy_linear_step</span>
</span><span id="L-686"><a href="#L-686"><span class="linenos"> 686</span></a>                    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
</span><span id="L-687"><a href="#L-687"><span class="linenos"> 687</span></a>                        <span class="n">w_t</span> <span class="o">=</span> <span class="n">s</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">-</span> <span class="n">t</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="L-688"><a href="#L-688"><span class="linenos"> 688</span></a>
</span><span id="L-689"><a href="#L-689"><span class="linenos"> 689</span></a>                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy</span> <span class="o">==</span> <span class="s2">&quot;quadratic_decrease&quot;</span><span class="p">:</span>
</span><span id="L-690"><a href="#L-690"><span class="linenos"> 690</span></a>                    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
</span><span id="L-691"><a href="#L-691"><span class="linenos"> 691</span></a>                        <span class="n">w_t</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">-</span> <span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
</span><span id="L-692"><a href="#L-692"><span class="linenos"> 692</span></a>                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy</span> <span class="o">==</span> <span class="s2">&quot;cubic_decrease&quot;</span><span class="p">:</span>
</span><span id="L-693"><a href="#L-693"><span class="linenos"> 693</span></a>                    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
</span><span id="L-694"><a href="#L-694"><span class="linenos"> 694</span></a>                        <span class="n">w_t</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">-</span> <span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">3</span>
</span><span id="L-695"><a href="#L-695"><span class="linenos"> 695</span></a>                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy</span> <span class="o">==</span> <span class="s2">&quot;exponential_decrease&quot;</span><span class="p">:</span>
</span><span id="L-696"><a href="#L-696"><span class="linenos"> 696</span></a>                    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
</span><span id="L-697"><a href="#L-697"><span class="linenos"> 697</span></a>                        <span class="n">r</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy_exponential_rate</span>
</span><span id="L-698"><a href="#L-698"><span class="linenos"> 698</span></a>
</span><span id="L-699"><a href="#L-699"><span class="linenos"> 699</span></a>                        <span class="n">w_t</span> <span class="o">=</span> <span class="n">r</span> <span class="o">**</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">-</span> <span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="L-700"><a href="#L-700"><span class="linenos"> 700</span></a>                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy</span> <span class="o">==</span> <span class="s2">&quot;log_decrease&quot;</span><span class="p">:</span>
</span><span id="L-701"><a href="#L-701"><span class="linenos"> 701</span></a>                    <span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy_log_base</span>
</span><span id="L-702"><a href="#L-702"><span class="linenos"> 702</span></a>                    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
</span><span id="L-703"><a href="#L-703"><span class="linenos"> 703</span></a>                        <span class="n">w_t</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">-</span> <span class="n">t</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="L-704"><a href="#L-704"><span class="linenos"> 704</span></a>                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy</span> <span class="o">==</span> <span class="s2">&quot;factorial_decrease&quot;</span><span class="p">:</span>
</span><span id="L-705"><a href="#L-705"><span class="linenos"> 705</span></a>                    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
</span><span id="L-706"><a href="#L-706"><span class="linenos"> 706</span></a>                        <span class="n">w_t</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">factorial</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">-</span> <span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="L-707"><a href="#L-707"><span class="linenos"> 707</span></a>                <span class="k">else</span><span class="p">:</span>
</span><span id="L-708"><a href="#L-708"><span class="linenos"> 708</span></a>                    <span class="k">raise</span> <span class="ne">ValueError</span>
</span><span id="L-709"><a href="#L-709"><span class="linenos"> 709</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">summative_importance_for_previous_tasks</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span>
</span><span id="L-710"><a href="#L-710"><span class="linenos"> 710</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">importances</span><span class="p">[</span><span class="n">t</span><span class="p">][</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">*</span> <span class="n">w_t</span>
</span><span id="L-711"><a href="#L-711"><span class="linenos"> 711</span></a>                <span class="p">)</span>
</span><span id="L-712"><a href="#L-712"><span class="linenos"> 712</span></a>
</span><span id="L-713"><a href="#L-713"><span class="linenos"> 713</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_weight_abs_sum</span><span class="p">(</span>
</span><span id="L-714"><a href="#L-714"><span class="linenos"> 714</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-715"><a href="#L-715"><span class="linenos"> 715</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-716"><a href="#L-716"><span class="linenos"> 716</span></a>        <span class="n">if_output_weight</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
</span><span id="L-717"><a href="#L-717"><span class="linenos"> 717</span></a>        <span class="n">reciprocal</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
</span><span id="L-718"><a href="#L-718"><span class="linenos"> 718</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-719"><a href="#L-719"><span class="linenos"> 719</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the sum of absolute values of layer input or output weights.</span>
</span><span id="L-720"><a href="#L-720"><span class="linenos"> 720</span></a>
</span><span id="L-721"><a href="#L-721"><span class="linenos"> 721</span></a><span class="sd">        **Args:**</span>
</span><span id="L-722"><a href="#L-722"><span class="linenos"> 722</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="L-723"><a href="#L-723"><span class="linenos"> 723</span></a><span class="sd">        - **if_output_weight** (`bool`): whether to use the output weights or input weights.</span>
</span><span id="L-724"><a href="#L-724"><span class="linenos"> 724</span></a><span class="sd">        - **reciprocal** (`bool`): whether to take reciprocal.</span>
</span><span id="L-725"><a href="#L-725"><span class="linenos"> 725</span></a>
</span><span id="L-726"><a href="#L-726"><span class="linenos"> 726</span></a><span class="sd">        **Returns:**</span>
</span><span id="L-727"><a href="#L-727"><span class="linenos"> 727</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="L-728"><a href="#L-728"><span class="linenos"> 728</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-729"><a href="#L-729"><span class="linenos"> 729</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="L-730"><a href="#L-730"><span class="linenos"> 730</span></a>
</span><span id="L-731"><a href="#L-731"><span class="linenos"> 731</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="n">if_output_weight</span><span class="p">:</span>
</span><span id="L-732"><a href="#L-732"><span class="linenos"> 732</span></a>            <span class="n">weight_abs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span><span id="L-733"><a href="#L-733"><span class="linenos"> 733</span></a>            <span class="n">weight_abs_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
</span><span id="L-734"><a href="#L-734"><span class="linenos"> 734</span></a>                <span class="n">weight_abs</span><span class="p">,</span>
</span><span id="L-735"><a href="#L-735"><span class="linenos"> 735</span></a>                <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="L-736"><a href="#L-736"><span class="linenos"> 736</span></a>                    <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">weight_abs</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">0</span>
</span><span id="L-737"><a href="#L-737"><span class="linenos"> 737</span></a>                <span class="p">],</span>  <span class="c1"># sum over the input dimension</span>
</span><span id="L-738"><a href="#L-738"><span class="linenos"> 738</span></a>            <span class="p">)</span>
</span><span id="L-739"><a href="#L-739"><span class="linenos"> 739</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-740"><a href="#L-740"><span class="linenos"> 740</span></a>            <span class="n">weight_abs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">next_layer</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span><span id="L-741"><a href="#L-741"><span class="linenos"> 741</span></a>            <span class="n">weight_abs_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
</span><span id="L-742"><a href="#L-742"><span class="linenos"> 742</span></a>                <span class="n">weight_abs</span><span class="p">,</span>
</span><span id="L-743"><a href="#L-743"><span class="linenos"> 743</span></a>                <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="L-744"><a href="#L-744"><span class="linenos"> 744</span></a>                    <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">weight_abs</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="L-745"><a href="#L-745"><span class="linenos"> 745</span></a>                <span class="p">],</span>  <span class="c1"># sum over the output dimension</span>
</span><span id="L-746"><a href="#L-746"><span class="linenos"> 746</span></a>            <span class="p">)</span>
</span><span id="L-747"><a href="#L-747"><span class="linenos"> 747</span></a>
</span><span id="L-748"><a href="#L-748"><span class="linenos"> 748</span></a>        <span class="k">if</span> <span class="n">reciprocal</span><span class="p">:</span>
</span><span id="L-749"><a href="#L-749"><span class="linenos"> 749</span></a>            <span class="n">weight_abs_sum_reciprocal</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">reciprocal</span><span class="p">(</span><span class="n">weight_abs_sum</span><span class="p">)</span>
</span><span id="L-750"><a href="#L-750"><span class="linenos"> 750</span></a>            <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">weight_abs_sum_reciprocal</span>
</span><span id="L-751"><a href="#L-751"><span class="linenos"> 751</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-752"><a href="#L-752"><span class="linenos"> 752</span></a>            <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">weight_abs_sum</span>
</span><span id="L-753"><a href="#L-753"><span class="linenos"> 753</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="L-754"><a href="#L-754"><span class="linenos"> 754</span></a>
</span><span id="L-755"><a href="#L-755"><span class="linenos"> 755</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span><span id="L-756"><a href="#L-756"><span class="linenos"> 756</span></a>
</span><span id="L-757"><a href="#L-757"><span class="linenos"> 757</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_weight_gradient_abs_sum</span><span class="p">(</span>
</span><span id="L-758"><a href="#L-758"><span class="linenos"> 758</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-759"><a href="#L-759"><span class="linenos"> 759</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-760"><a href="#L-760"><span class="linenos"> 760</span></a>        <span class="n">if_output_weight</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
</span><span id="L-761"><a href="#L-761"><span class="linenos"> 761</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-762"><a href="#L-762"><span class="linenos"> 762</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the sum of absolute values of gradients of the layer input or output weights.</span>
</span><span id="L-763"><a href="#L-763"><span class="linenos"> 763</span></a>
</span><span id="L-764"><a href="#L-764"><span class="linenos"> 764</span></a><span class="sd">        **Args:**</span>
</span><span id="L-765"><a href="#L-765"><span class="linenos"> 765</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="L-766"><a href="#L-766"><span class="linenos"> 766</span></a><span class="sd">        - **if_output_weight** (`bool`): whether to use the output weights or input weights.</span>
</span><span id="L-767"><a href="#L-767"><span class="linenos"> 767</span></a>
</span><span id="L-768"><a href="#L-768"><span class="linenos"> 768</span></a><span class="sd">        **Returns:**</span>
</span><span id="L-769"><a href="#L-769"><span class="linenos"> 769</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="L-770"><a href="#L-770"><span class="linenos"> 770</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-771"><a href="#L-771"><span class="linenos"> 771</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="L-772"><a href="#L-772"><span class="linenos"> 772</span></a>
</span><span id="L-773"><a href="#L-773"><span class="linenos"> 773</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="n">if_output_weight</span><span class="p">:</span>
</span><span id="L-774"><a href="#L-774"><span class="linenos"> 774</span></a>            <span class="n">gradient_abs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span><span id="L-775"><a href="#L-775"><span class="linenos"> 775</span></a>            <span class="n">gradient_abs_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
</span><span id="L-776"><a href="#L-776"><span class="linenos"> 776</span></a>                <span class="n">gradient_abs</span><span class="p">,</span>
</span><span id="L-777"><a href="#L-777"><span class="linenos"> 777</span></a>                <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="L-778"><a href="#L-778"><span class="linenos"> 778</span></a>                    <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">gradient_abs</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">0</span>
</span><span id="L-779"><a href="#L-779"><span class="linenos"> 779</span></a>                <span class="p">],</span>  <span class="c1"># sum over the input dimension</span>
</span><span id="L-780"><a href="#L-780"><span class="linenos"> 780</span></a>            <span class="p">)</span>
</span><span id="L-781"><a href="#L-781"><span class="linenos"> 781</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-782"><a href="#L-782"><span class="linenos"> 782</span></a>            <span class="n">gradient_abs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">next_layer</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span><span id="L-783"><a href="#L-783"><span class="linenos"> 783</span></a>            <span class="n">gradient_abs_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
</span><span id="L-784"><a href="#L-784"><span class="linenos"> 784</span></a>                <span class="n">gradient_abs</span><span class="p">,</span>
</span><span id="L-785"><a href="#L-785"><span class="linenos"> 785</span></a>                <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="L-786"><a href="#L-786"><span class="linenos"> 786</span></a>                    <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">gradient_abs</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="L-787"><a href="#L-787"><span class="linenos"> 787</span></a>                <span class="p">],</span>  <span class="c1"># sum over the output dimension</span>
</span><span id="L-788"><a href="#L-788"><span class="linenos"> 788</span></a>            <span class="p">)</span>
</span><span id="L-789"><a href="#L-789"><span class="linenos"> 789</span></a>
</span><span id="L-790"><a href="#L-790"><span class="linenos"> 790</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">gradient_abs_sum</span>
</span><span id="L-791"><a href="#L-791"><span class="linenos"> 791</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="L-792"><a href="#L-792"><span class="linenos"> 792</span></a>
</span><span id="L-793"><a href="#L-793"><span class="linenos"> 793</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span><span id="L-794"><a href="#L-794"><span class="linenos"> 794</span></a>
</span><span id="L-795"><a href="#L-795"><span class="linenos"> 795</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_activation_abs</span><span class="p">(</span>
</span><span id="L-796"><a href="#L-796"><span class="linenos"> 796</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-797"><a href="#L-797"><span class="linenos"> 797</span></a>        <span class="n">activation</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-798"><a href="#L-798"><span class="linenos"> 798</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-799"><a href="#L-799"><span class="linenos"> 799</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute value of activation of the layer. This is our own implementation of [Layer Activation](https://captum.ai/api/layer.html#layer-activation) in Captum.</span>
</span><span id="L-800"><a href="#L-800"><span class="linenos"> 800</span></a>
</span><span id="L-801"><a href="#L-801"><span class="linenos"> 801</span></a><span class="sd">        **Args:**</span>
</span><span id="L-802"><a href="#L-802"><span class="linenos"> 802</span></a><span class="sd">        - **activation** (`Tensor`): the activation tensor of the layer. It has the same size of (number of units, ).</span>
</span><span id="L-803"><a href="#L-803"><span class="linenos"> 803</span></a>
</span><span id="L-804"><a href="#L-804"><span class="linenos"> 804</span></a><span class="sd">        **Returns:**</span>
</span><span id="L-805"><a href="#L-805"><span class="linenos"> 805</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="L-806"><a href="#L-806"><span class="linenos"> 806</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-807"><a href="#L-807"><span class="linenos"> 807</span></a>        <span class="n">activation_abs_batch_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
</span><span id="L-808"><a href="#L-808"><span class="linenos"> 808</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">activation</span><span class="p">),</span>
</span><span id="L-809"><a href="#L-809"><span class="linenos"> 809</span></a>            <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="L-810"><a href="#L-810"><span class="linenos"> 810</span></a>                <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">activation</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="L-811"><a href="#L-811"><span class="linenos"> 811</span></a>            <span class="p">],</span>  <span class="c1"># average the features over batch samples</span>
</span><span id="L-812"><a href="#L-812"><span class="linenos"> 812</span></a>        <span class="p">)</span>
</span><span id="L-813"><a href="#L-813"><span class="linenos"> 813</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">activation_abs_batch_mean</span>
</span><span id="L-814"><a href="#L-814"><span class="linenos"> 814</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="L-815"><a href="#L-815"><span class="linenos"> 815</span></a>
</span><span id="L-816"><a href="#L-816"><span class="linenos"> 816</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span><span id="L-817"><a href="#L-817"><span class="linenos"> 817</span></a>
</span><span id="L-818"><a href="#L-818"><span class="linenos"> 818</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_weight_abs_sum_x_activation_abs</span><span class="p">(</span>
</span><span id="L-819"><a href="#L-819"><span class="linenos"> 819</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-820"><a href="#L-820"><span class="linenos"> 820</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-821"><a href="#L-821"><span class="linenos"> 821</span></a>        <span class="n">activation</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-822"><a href="#L-822"><span class="linenos"> 822</span></a>        <span class="n">if_output_weight</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
</span><span id="L-823"><a href="#L-823"><span class="linenos"> 823</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-824"><a href="#L-824"><span class="linenos"> 824</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the sum of absolute values of layer input / output weights multiplied by absolute values of activation. The input weights version is equal to the contribution utility in [CBP](https://www.nature.com/articles/s41586-024-07711-7).</span>
</span><span id="L-825"><a href="#L-825"><span class="linenos"> 825</span></a>
</span><span id="L-826"><a href="#L-826"><span class="linenos"> 826</span></a><span class="sd">        **Args:**</span>
</span><span id="L-827"><a href="#L-827"><span class="linenos"> 827</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="L-828"><a href="#L-828"><span class="linenos"> 828</span></a><span class="sd">        - **activation** (`Tensor`): the activation tensor of the layer. It has the same size of (number of units, ).</span>
</span><span id="L-829"><a href="#L-829"><span class="linenos"> 829</span></a><span class="sd">        - **if_output_weight** (`bool`): whether to use the output weights or input weights.</span>
</span><span id="L-830"><a href="#L-830"><span class="linenos"> 830</span></a>
</span><span id="L-831"><a href="#L-831"><span class="linenos"> 831</span></a><span class="sd">        **Returns:**</span>
</span><span id="L-832"><a href="#L-832"><span class="linenos"> 832</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="L-833"><a href="#L-833"><span class="linenos"> 833</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-834"><a href="#L-834"><span class="linenos"> 834</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="L-835"><a href="#L-835"><span class="linenos"> 835</span></a>
</span><span id="L-836"><a href="#L-836"><span class="linenos"> 836</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="n">if_output_weight</span><span class="p">:</span>
</span><span id="L-837"><a href="#L-837"><span class="linenos"> 837</span></a>            <span class="n">weight_abs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span><span id="L-838"><a href="#L-838"><span class="linenos"> 838</span></a>            <span class="n">weight_abs_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
</span><span id="L-839"><a href="#L-839"><span class="linenos"> 839</span></a>                <span class="n">weight_abs</span><span class="p">,</span>
</span><span id="L-840"><a href="#L-840"><span class="linenos"> 840</span></a>                <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="L-841"><a href="#L-841"><span class="linenos"> 841</span></a>                    <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">weight_abs</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">0</span>
</span><span id="L-842"><a href="#L-842"><span class="linenos"> 842</span></a>                <span class="p">],</span>  <span class="c1"># sum over the input dimension</span>
</span><span id="L-843"><a href="#L-843"><span class="linenos"> 843</span></a>            <span class="p">)</span>
</span><span id="L-844"><a href="#L-844"><span class="linenos"> 844</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-845"><a href="#L-845"><span class="linenos"> 845</span></a>            <span class="n">weight_abs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">next_layer</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span><span id="L-846"><a href="#L-846"><span class="linenos"> 846</span></a>            <span class="n">weight_abs_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
</span><span id="L-847"><a href="#L-847"><span class="linenos"> 847</span></a>                <span class="n">weight_abs</span><span class="p">,</span>
</span><span id="L-848"><a href="#L-848"><span class="linenos"> 848</span></a>                <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="L-849"><a href="#L-849"><span class="linenos"> 849</span></a>                    <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">weight_abs</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="L-850"><a href="#L-850"><span class="linenos"> 850</span></a>                <span class="p">],</span>  <span class="c1"># sum over the output dimension</span>
</span><span id="L-851"><a href="#L-851"><span class="linenos"> 851</span></a>            <span class="p">)</span>
</span><span id="L-852"><a href="#L-852"><span class="linenos"> 852</span></a>
</span><span id="L-853"><a href="#L-853"><span class="linenos"> 853</span></a>        <span class="n">activation_abs_batch_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
</span><span id="L-854"><a href="#L-854"><span class="linenos"> 854</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">activation</span><span class="p">),</span>
</span><span id="L-855"><a href="#L-855"><span class="linenos"> 855</span></a>            <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="L-856"><a href="#L-856"><span class="linenos"> 856</span></a>                <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">activation</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="L-857"><a href="#L-857"><span class="linenos"> 857</span></a>            <span class="p">],</span>  <span class="c1"># average the features over batch samples</span>
</span><span id="L-858"><a href="#L-858"><span class="linenos"> 858</span></a>        <span class="p">)</span>
</span><span id="L-859"><a href="#L-859"><span class="linenos"> 859</span></a>
</span><span id="L-860"><a href="#L-860"><span class="linenos"> 860</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">weight_abs_sum</span> <span class="o">*</span> <span class="n">activation_abs_batch_mean</span>
</span><span id="L-861"><a href="#L-861"><span class="linenos"> 861</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="L-862"><a href="#L-862"><span class="linenos"> 862</span></a>
</span><span id="L-863"><a href="#L-863"><span class="linenos"> 863</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span><span id="L-864"><a href="#L-864"><span class="linenos"> 864</span></a>
</span><span id="L-865"><a href="#L-865"><span class="linenos"> 865</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_gradient_x_activation_abs</span><span class="p">(</span>
</span><span id="L-866"><a href="#L-866"><span class="linenos"> 866</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-867"><a href="#L-867"><span class="linenos"> 867</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-868"><a href="#L-868"><span class="linenos"> 868</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="L-869"><a href="#L-869"><span class="linenos"> 869</span></a>        <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-870"><a href="#L-870"><span class="linenos"> 870</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-871"><a href="#L-871"><span class="linenos"> 871</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-872"><a href="#L-872"><span class="linenos"> 872</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-873"><a href="#L-873"><span class="linenos"> 873</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of the gradient of layer activation multiplied by the activation. We implement this using [Layer Gradient X Activation](https://captum.ai/api/layer.html#layer-gradient-x-activation) in Captum.</span>
</span><span id="L-874"><a href="#L-874"><span class="linenos"> 874</span></a>
</span><span id="L-875"><a href="#L-875"><span class="linenos"> 875</span></a><span class="sd">        **Args:**</span>
</span><span id="L-876"><a href="#L-876"><span class="linenos"> 876</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="L-877"><a href="#L-877"><span class="linenos"> 877</span></a><span class="sd">        - **input** (`Tensor` | `tuple[Tensor, ...]`): the input batch of the training step.</span>
</span><span id="L-878"><a href="#L-878"><span class="linenos"> 878</span></a><span class="sd">        - **target** (`Tensor` | `None`): the target batch of the training step.</span>
</span><span id="L-879"><a href="#L-879"><span class="linenos"> 879</span></a><span class="sd">        - **batch_idx** (`int`): the index of the current batch. This is an argument of the forward function during training.</span>
</span><span id="L-880"><a href="#L-880"><span class="linenos"> 880</span></a><span class="sd">        - **num_batches** (`int`): the number of batches in the training step. This is an argument of the forward function during training.</span>
</span><span id="L-881"><a href="#L-881"><span class="linenos"> 881</span></a>
</span><span id="L-882"><a href="#L-882"><span class="linenos"> 882</span></a><span class="sd">        **Returns:**</span>
</span><span id="L-883"><a href="#L-883"><span class="linenos"> 883</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="L-884"><a href="#L-884"><span class="linenos"> 884</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-885"><a href="#L-885"><span class="linenos"> 885</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="L-886"><a href="#L-886"><span class="linenos"> 886</span></a>
</span><span id="L-887"><a href="#L-887"><span class="linenos"> 887</span></a>        <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
</span><span id="L-888"><a href="#L-888"><span class="linenos"> 888</span></a>
</span><span id="L-889"><a href="#L-889"><span class="linenos"> 889</span></a>        <span class="c1"># initialize the Layer Gradient X Activation object</span>
</span><span id="L-890"><a href="#L-890"><span class="linenos"> 890</span></a>        <span class="n">layer_gradient_x_activation</span> <span class="o">=</span> <span class="n">LayerGradientXActivation</span><span class="p">(</span>
</span><span id="L-891"><a href="#L-891"><span class="linenos"> 891</span></a>            <span class="n">forward_func</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="n">layer</span>
</span><span id="L-892"><a href="#L-892"><span class="linenos"> 892</span></a>        <span class="p">)</span>
</span><span id="L-893"><a href="#L-893"><span class="linenos"> 893</span></a>
</span><span id="L-894"><a href="#L-894"><span class="linenos"> 894</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</span><span id="L-895"><a href="#L-895"><span class="linenos"> 895</span></a>        <span class="c1"># calculate layer attribution of the step</span>
</span><span id="L-896"><a href="#L-896"><span class="linenos"> 896</span></a>        <span class="n">attribution</span> <span class="o">=</span> <span class="n">layer_gradient_x_activation</span><span class="o">.</span><span class="n">attribute</span><span class="p">(</span>
</span><span id="L-897"><a href="#L-897"><span class="linenos"> 897</span></a>            <span class="n">inputs</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="L-898"><a href="#L-898"><span class="linenos"> 898</span></a>            <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="L-899"><a href="#L-899"><span class="linenos"> 899</span></a>            <span class="n">additional_forward_args</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">num_batches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">),</span>
</span><span id="L-900"><a href="#L-900"><span class="linenos"> 900</span></a>        <span class="p">)</span>
</span><span id="L-901"><a href="#L-901"><span class="linenos"> 901</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</span><span id="L-902"><a href="#L-902"><span class="linenos"> 902</span></a>
</span><span id="L-903"><a href="#L-903"><span class="linenos"> 903</span></a>        <span class="n">attribution_abs_batch_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
</span><span id="L-904"><a href="#L-904"><span class="linenos"> 904</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">attribution</span><span class="p">),</span>
</span><span id="L-905"><a href="#L-905"><span class="linenos"> 905</span></a>            <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="L-906"><a href="#L-906"><span class="linenos"> 906</span></a>                <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">attribution</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="L-907"><a href="#L-907"><span class="linenos"> 907</span></a>            <span class="p">],</span>  <span class="c1"># average the features over batch samples</span>
</span><span id="L-908"><a href="#L-908"><span class="linenos"> 908</span></a>        <span class="p">)</span>
</span><span id="L-909"><a href="#L-909"><span class="linenos"> 909</span></a>
</span><span id="L-910"><a href="#L-910"><span class="linenos"> 910</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">attribution_abs_batch_mean</span>
</span><span id="L-911"><a href="#L-911"><span class="linenos"> 911</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="L-912"><a href="#L-912"><span class="linenos"> 912</span></a>
</span><span id="L-913"><a href="#L-913"><span class="linenos"> 913</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span><span id="L-914"><a href="#L-914"><span class="linenos"> 914</span></a>
</span><span id="L-915"><a href="#L-915"><span class="linenos"> 915</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_weight_gradient_square_sum</span><span class="p">(</span>
</span><span id="L-916"><a href="#L-916"><span class="linenos"> 916</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-917"><a href="#L-917"><span class="linenos"> 917</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-918"><a href="#L-918"><span class="linenos"> 918</span></a>        <span class="n">activation</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-919"><a href="#L-919"><span class="linenos"> 919</span></a>        <span class="n">if_output_weight</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
</span><span id="L-920"><a href="#L-920"><span class="linenos"> 920</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-921"><a href="#L-921"><span class="linenos"> 921</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the sum of layer weight gradient squares. The weight gradient square is equal to fisher information in [EWC](https://www.pnas.org/doi/10.1073/pnas.1611835114).</span>
</span><span id="L-922"><a href="#L-922"><span class="linenos"> 922</span></a>
</span><span id="L-923"><a href="#L-923"><span class="linenos"> 923</span></a><span class="sd">        **Args:**</span>
</span><span id="L-924"><a href="#L-924"><span class="linenos"> 924</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="L-925"><a href="#L-925"><span class="linenos"> 925</span></a><span class="sd">        - **activation** (`Tensor`): the activation tensor of the layer. It has the same size of (number of units, ).</span>
</span><span id="L-926"><a href="#L-926"><span class="linenos"> 926</span></a><span class="sd">        - **if_output_weight** (`bool`): whether to use the output weights or input weights.</span>
</span><span id="L-927"><a href="#L-927"><span class="linenos"> 927</span></a>
</span><span id="L-928"><a href="#L-928"><span class="linenos"> 928</span></a><span class="sd">        **Returns:**</span>
</span><span id="L-929"><a href="#L-929"><span class="linenos"> 929</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="L-930"><a href="#L-930"><span class="linenos"> 930</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-931"><a href="#L-931"><span class="linenos"> 931</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="L-932"><a href="#L-932"><span class="linenos"> 932</span></a>
</span><span id="L-933"><a href="#L-933"><span class="linenos"> 933</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="n">if_output_weight</span><span class="p">:</span>
</span><span id="L-934"><a href="#L-934"><span class="linenos"> 934</span></a>            <span class="n">gradient_square</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">**</span><span class="mi">2</span>
</span><span id="L-935"><a href="#L-935"><span class="linenos"> 935</span></a>            <span class="n">gradient_square_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
</span><span id="L-936"><a href="#L-936"><span class="linenos"> 936</span></a>                <span class="n">gradient_square</span><span class="p">,</span>
</span><span id="L-937"><a href="#L-937"><span class="linenos"> 937</span></a>                <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="L-938"><a href="#L-938"><span class="linenos"> 938</span></a>                    <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">gradient_square</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">0</span>
</span><span id="L-939"><a href="#L-939"><span class="linenos"> 939</span></a>                <span class="p">],</span>  <span class="c1"># sum over the input dimension</span>
</span><span id="L-940"><a href="#L-940"><span class="linenos"> 940</span></a>            <span class="p">)</span>
</span><span id="L-941"><a href="#L-941"><span class="linenos"> 941</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-942"><a href="#L-942"><span class="linenos"> 942</span></a>            <span class="n">gradient_square</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">next_layer</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">**</span><span class="mi">2</span>
</span><span id="L-943"><a href="#L-943"><span class="linenos"> 943</span></a>            <span class="n">gradient_square_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
</span><span id="L-944"><a href="#L-944"><span class="linenos"> 944</span></a>                <span class="n">gradient_square</span><span class="p">,</span>
</span><span id="L-945"><a href="#L-945"><span class="linenos"> 945</span></a>                <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="L-946"><a href="#L-946"><span class="linenos"> 946</span></a>                    <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">gradient_square</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="L-947"><a href="#L-947"><span class="linenos"> 947</span></a>                <span class="p">],</span>  <span class="c1"># sum over the output dimension</span>
</span><span id="L-948"><a href="#L-948"><span class="linenos"> 948</span></a>            <span class="p">)</span>
</span><span id="L-949"><a href="#L-949"><span class="linenos"> 949</span></a>
</span><span id="L-950"><a href="#L-950"><span class="linenos"> 950</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">gradient_square_sum</span>
</span><span id="L-951"><a href="#L-951"><span class="linenos"> 951</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="L-952"><a href="#L-952"><span class="linenos"> 952</span></a>
</span><span id="L-953"><a href="#L-953"><span class="linenos"> 953</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span><span id="L-954"><a href="#L-954"><span class="linenos"> 954</span></a>
</span><span id="L-955"><a href="#L-955"><span class="linenos"> 955</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_weight_gradient_square_sum_x_activation_abs</span><span class="p">(</span>
</span><span id="L-956"><a href="#L-956"><span class="linenos"> 956</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-957"><a href="#L-957"><span class="linenos"> 957</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-958"><a href="#L-958"><span class="linenos"> 958</span></a>        <span class="n">activation</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-959"><a href="#L-959"><span class="linenos"> 959</span></a>        <span class="n">if_output_weight</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
</span><span id="L-960"><a href="#L-960"><span class="linenos"> 960</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-961"><a href="#L-961"><span class="linenos"> 961</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the sum of layer weight gradient squares multiplied by absolute values of activation. The weight gradient square is equal to fisher information in [EWC](https://www.pnas.org/doi/10.1073/pnas.1611835114).</span>
</span><span id="L-962"><a href="#L-962"><span class="linenos"> 962</span></a>
</span><span id="L-963"><a href="#L-963"><span class="linenos"> 963</span></a><span class="sd">        **Args:**</span>
</span><span id="L-964"><a href="#L-964"><span class="linenos"> 964</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="L-965"><a href="#L-965"><span class="linenos"> 965</span></a><span class="sd">        - **activation** (`Tensor`): the activation tensor of the layer. It has the same size of (number of units, ).</span>
</span><span id="L-966"><a href="#L-966"><span class="linenos"> 966</span></a><span class="sd">        - **if_output_weight** (`bool`): whether to use the output weights or input weights.</span>
</span><span id="L-967"><a href="#L-967"><span class="linenos"> 967</span></a>
</span><span id="L-968"><a href="#L-968"><span class="linenos"> 968</span></a><span class="sd">        **Returns:**</span>
</span><span id="L-969"><a href="#L-969"><span class="linenos"> 969</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="L-970"><a href="#L-970"><span class="linenos"> 970</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-971"><a href="#L-971"><span class="linenos"> 971</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="L-972"><a href="#L-972"><span class="linenos"> 972</span></a>
</span><span id="L-973"><a href="#L-973"><span class="linenos"> 973</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="n">if_output_weight</span><span class="p">:</span>
</span><span id="L-974"><a href="#L-974"><span class="linenos"> 974</span></a>            <span class="n">gradient_square</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">**</span><span class="mi">2</span>
</span><span id="L-975"><a href="#L-975"><span class="linenos"> 975</span></a>            <span class="n">gradient_square_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
</span><span id="L-976"><a href="#L-976"><span class="linenos"> 976</span></a>                <span class="n">gradient_square</span><span class="p">,</span>
</span><span id="L-977"><a href="#L-977"><span class="linenos"> 977</span></a>                <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="L-978"><a href="#L-978"><span class="linenos"> 978</span></a>                    <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">gradient_square</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">0</span>
</span><span id="L-979"><a href="#L-979"><span class="linenos"> 979</span></a>                <span class="p">],</span>  <span class="c1"># sum over the input dimension</span>
</span><span id="L-980"><a href="#L-980"><span class="linenos"> 980</span></a>            <span class="p">)</span>
</span><span id="L-981"><a href="#L-981"><span class="linenos"> 981</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-982"><a href="#L-982"><span class="linenos"> 982</span></a>            <span class="n">gradient_square</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">next_layer</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">**</span><span class="mi">2</span>
</span><span id="L-983"><a href="#L-983"><span class="linenos"> 983</span></a>            <span class="n">gradient_square_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
</span><span id="L-984"><a href="#L-984"><span class="linenos"> 984</span></a>                <span class="n">gradient_square</span><span class="p">,</span>
</span><span id="L-985"><a href="#L-985"><span class="linenos"> 985</span></a>                <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="L-986"><a href="#L-986"><span class="linenos"> 986</span></a>                    <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">gradient_square</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="L-987"><a href="#L-987"><span class="linenos"> 987</span></a>                <span class="p">],</span>  <span class="c1"># sum over the output dimension</span>
</span><span id="L-988"><a href="#L-988"><span class="linenos"> 988</span></a>            <span class="p">)</span>
</span><span id="L-989"><a href="#L-989"><span class="linenos"> 989</span></a>
</span><span id="L-990"><a href="#L-990"><span class="linenos"> 990</span></a>        <span class="n">activation_abs_batch_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
</span><span id="L-991"><a href="#L-991"><span class="linenos"> 991</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">activation</span><span class="p">),</span>
</span><span id="L-992"><a href="#L-992"><span class="linenos"> 992</span></a>            <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="L-993"><a href="#L-993"><span class="linenos"> 993</span></a>                <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">activation</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="L-994"><a href="#L-994"><span class="linenos"> 994</span></a>            <span class="p">],</span>  <span class="c1"># average the features over batch samples</span>
</span><span id="L-995"><a href="#L-995"><span class="linenos"> 995</span></a>        <span class="p">)</span>
</span><span id="L-996"><a href="#L-996"><span class="linenos"> 996</span></a>
</span><span id="L-997"><a href="#L-997"><span class="linenos"> 997</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">gradient_square_sum</span> <span class="o">*</span> <span class="n">activation_abs_batch_mean</span>
</span><span id="L-998"><a href="#L-998"><span class="linenos"> 998</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="L-999"><a href="#L-999"><span class="linenos"> 999</span></a>
</span><span id="L-1000"><a href="#L-1000"><span class="linenos">1000</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span><span id="L-1001"><a href="#L-1001"><span class="linenos">1001</span></a>
</span><span id="L-1002"><a href="#L-1002"><span class="linenos">1002</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_conductance_abs</span><span class="p">(</span>
</span><span id="L-1003"><a href="#L-1003"><span class="linenos">1003</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-1004"><a href="#L-1004"><span class="linenos">1004</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-1005"><a href="#L-1005"><span class="linenos">1005</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="L-1006"><a href="#L-1006"><span class="linenos">1006</span></a>        <span class="n">baselines</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="L-1007"><a href="#L-1007"><span class="linenos">1007</span></a>        <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-1008"><a href="#L-1008"><span class="linenos">1008</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-1009"><a href="#L-1009"><span class="linenos">1009</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-1010"><a href="#L-1010"><span class="linenos">1010</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-1011"><a href="#L-1011"><span class="linenos">1011</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of [conductance](https://openreview.net/forum?id=SylKoo0cKm). We implement this using [Layer Conductance](https://captum.ai/api/layer.html#layer-conductance) in Captum.</span>
</span><span id="L-1012"><a href="#L-1012"><span class="linenos">1012</span></a>
</span><span id="L-1013"><a href="#L-1013"><span class="linenos">1013</span></a><span class="sd">        **Args:**</span>
</span><span id="L-1014"><a href="#L-1014"><span class="linenos">1014</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="L-1015"><a href="#L-1015"><span class="linenos">1015</span></a><span class="sd">        - **input** (`Tensor` | `tuple[Tensor, ...]`): the input batch of the training step.</span>
</span><span id="L-1016"><a href="#L-1016"><span class="linenos">1016</span></a><span class="sd">        - **baselines** (`None` | `int` | `float` | `Tensor` | `tuple[int | float | Tensor, ...]`): starting point from which integral is computed in this method. Please refer to the [Captum documentation](https://captum.ai/api/layer.html#captum.attr.LayerConductance.attribute) for more details.</span>
</span><span id="L-1017"><a href="#L-1017"><span class="linenos">1017</span></a><span class="sd">        - **target** (`Tensor` | `None`): the target batch of the training step.</span>
</span><span id="L-1018"><a href="#L-1018"><span class="linenos">1018</span></a><span class="sd">        - **batch_idx** (`int`): the index of the current batch. This is an argument of the forward function during training.</span>
</span><span id="L-1019"><a href="#L-1019"><span class="linenos">1019</span></a><span class="sd">        - **num_batches** (`int`): the number of batches in the training step. This is an argument of the forward function during training.- **mask** (`Tensor`): the mask tensor of the layer. It has the same size as the feature tensor with size (number of units, ).</span>
</span><span id="L-1020"><a href="#L-1020"><span class="linenos">1020</span></a>
</span><span id="L-1021"><a href="#L-1021"><span class="linenos">1021</span></a><span class="sd">        **Returns:**</span>
</span><span id="L-1022"><a href="#L-1022"><span class="linenos">1022</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="L-1023"><a href="#L-1023"><span class="linenos">1023</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-1024"><a href="#L-1024"><span class="linenos">1024</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="L-1025"><a href="#L-1025"><span class="linenos">1025</span></a>
</span><span id="L-1026"><a href="#L-1026"><span class="linenos">1026</span></a>        <span class="c1"># initialize the Layer Conductance object</span>
</span><span id="L-1027"><a href="#L-1027"><span class="linenos">1027</span></a>        <span class="n">layer_conductance</span> <span class="o">=</span> <span class="n">LayerConductance</span><span class="p">(</span><span class="n">forward_func</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="n">layer</span><span class="p">)</span>
</span><span id="L-1028"><a href="#L-1028"><span class="linenos">1028</span></a>
</span><span id="L-1029"><a href="#L-1029"><span class="linenos">1029</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</span><span id="L-1030"><a href="#L-1030"><span class="linenos">1030</span></a>        <span class="c1"># calculate layer attribution of the step</span>
</span><span id="L-1031"><a href="#L-1031"><span class="linenos">1031</span></a>        <span class="n">attribution</span> <span class="o">=</span> <span class="n">layer_conductance</span><span class="o">.</span><span class="n">attribute</span><span class="p">(</span>
</span><span id="L-1032"><a href="#L-1032"><span class="linenos">1032</span></a>            <span class="n">inputs</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="L-1033"><a href="#L-1033"><span class="linenos">1033</span></a>            <span class="n">baselines</span><span class="o">=</span><span class="n">baselines</span><span class="p">,</span>
</span><span id="L-1034"><a href="#L-1034"><span class="linenos">1034</span></a>            <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="L-1035"><a href="#L-1035"><span class="linenos">1035</span></a>            <span class="n">additional_forward_args</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">num_batches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">),</span>
</span><span id="L-1036"><a href="#L-1036"><span class="linenos">1036</span></a>        <span class="p">)</span>
</span><span id="L-1037"><a href="#L-1037"><span class="linenos">1037</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</span><span id="L-1038"><a href="#L-1038"><span class="linenos">1038</span></a>
</span><span id="L-1039"><a href="#L-1039"><span class="linenos">1039</span></a>        <span class="n">attribution_abs_batch_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
</span><span id="L-1040"><a href="#L-1040"><span class="linenos">1040</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">attribution</span><span class="p">),</span>
</span><span id="L-1041"><a href="#L-1041"><span class="linenos">1041</span></a>            <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="L-1042"><a href="#L-1042"><span class="linenos">1042</span></a>                <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">attribution</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="L-1043"><a href="#L-1043"><span class="linenos">1043</span></a>            <span class="p">],</span>  <span class="c1"># average the features over batch samples</span>
</span><span id="L-1044"><a href="#L-1044"><span class="linenos">1044</span></a>        <span class="p">)</span>
</span><span id="L-1045"><a href="#L-1045"><span class="linenos">1045</span></a>
</span><span id="L-1046"><a href="#L-1046"><span class="linenos">1046</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">attribution_abs_batch_mean</span>
</span><span id="L-1047"><a href="#L-1047"><span class="linenos">1047</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="L-1048"><a href="#L-1048"><span class="linenos">1048</span></a>
</span><span id="L-1049"><a href="#L-1049"><span class="linenos">1049</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span><span id="L-1050"><a href="#L-1050"><span class="linenos">1050</span></a>
</span><span id="L-1051"><a href="#L-1051"><span class="linenos">1051</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_internal_influence_abs</span><span class="p">(</span>
</span><span id="L-1052"><a href="#L-1052"><span class="linenos">1052</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-1053"><a href="#L-1053"><span class="linenos">1053</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-1054"><a href="#L-1054"><span class="linenos">1054</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="L-1055"><a href="#L-1055"><span class="linenos">1055</span></a>        <span class="n">baselines</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="L-1056"><a href="#L-1056"><span class="linenos">1056</span></a>        <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-1057"><a href="#L-1057"><span class="linenos">1057</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-1058"><a href="#L-1058"><span class="linenos">1058</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-1059"><a href="#L-1059"><span class="linenos">1059</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-1060"><a href="#L-1060"><span class="linenos">1060</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of [internal influence](https://openreview.net/forum?id=SJPpHzW0-). We implement this using [Internal Influence](https://captum.ai/api/layer.html#internal-influence) in Captum.</span>
</span><span id="L-1061"><a href="#L-1061"><span class="linenos">1061</span></a>
</span><span id="L-1062"><a href="#L-1062"><span class="linenos">1062</span></a><span class="sd">        **Args:**</span>
</span><span id="L-1063"><a href="#L-1063"><span class="linenos">1063</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="L-1064"><a href="#L-1064"><span class="linenos">1064</span></a><span class="sd">        - **input** (`Tensor` | `tuple[Tensor, ...]`): the input batch of the training step.</span>
</span><span id="L-1065"><a href="#L-1065"><span class="linenos">1065</span></a><span class="sd">        - **baselines** (`None` | `int` | `float` | `Tensor` | `tuple[int | float | Tensor, ...]`): starting point from which integral is computed in this method. Please refer to the [Captum documentation](https://captum.ai/api/layer.html#captum.attr.InternalInfluence.attribute) for more details.</span>
</span><span id="L-1066"><a href="#L-1066"><span class="linenos">1066</span></a><span class="sd">        - **target** (`Tensor` | `None`): the target batch of the training step.</span>
</span><span id="L-1067"><a href="#L-1067"><span class="linenos">1067</span></a><span class="sd">        - **batch_idx** (`int`): the index of the current batch. This is an argument of the forward function during training.</span>
</span><span id="L-1068"><a href="#L-1068"><span class="linenos">1068</span></a><span class="sd">        - **num_batches** (`int`): the number of batches in the training step. This is an argument of the forward function during training.</span>
</span><span id="L-1069"><a href="#L-1069"><span class="linenos">1069</span></a>
</span><span id="L-1070"><a href="#L-1070"><span class="linenos">1070</span></a><span class="sd">        **Returns:**</span>
</span><span id="L-1071"><a href="#L-1071"><span class="linenos">1071</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="L-1072"><a href="#L-1072"><span class="linenos">1072</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-1073"><a href="#L-1073"><span class="linenos">1073</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="L-1074"><a href="#L-1074"><span class="linenos">1074</span></a>
</span><span id="L-1075"><a href="#L-1075"><span class="linenos">1075</span></a>        <span class="c1"># initialize the Internal Influence object</span>
</span><span id="L-1076"><a href="#L-1076"><span class="linenos">1076</span></a>        <span class="n">internal_influence</span> <span class="o">=</span> <span class="n">InternalInfluence</span><span class="p">(</span><span class="n">forward_func</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="n">layer</span><span class="p">)</span>
</span><span id="L-1077"><a href="#L-1077"><span class="linenos">1077</span></a>
</span><span id="L-1078"><a href="#L-1078"><span class="linenos">1078</span></a>        <span class="c1"># convert the target to long type to avoid error</span>
</span><span id="L-1079"><a href="#L-1079"><span class="linenos">1079</span></a>        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">long</span><span class="p">()</span> <span class="k">if</span> <span class="n">target</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="L-1080"><a href="#L-1080"><span class="linenos">1080</span></a>
</span><span id="L-1081"><a href="#L-1081"><span class="linenos">1081</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</span><span id="L-1082"><a href="#L-1082"><span class="linenos">1082</span></a>        <span class="c1"># calculate layer attribution of the step</span>
</span><span id="L-1083"><a href="#L-1083"><span class="linenos">1083</span></a>        <span class="n">attribution</span> <span class="o">=</span> <span class="n">internal_influence</span><span class="o">.</span><span class="n">attribute</span><span class="p">(</span>
</span><span id="L-1084"><a href="#L-1084"><span class="linenos">1084</span></a>            <span class="n">inputs</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="L-1085"><a href="#L-1085"><span class="linenos">1085</span></a>            <span class="n">baselines</span><span class="o">=</span><span class="n">baselines</span><span class="p">,</span>
</span><span id="L-1086"><a href="#L-1086"><span class="linenos">1086</span></a>            <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="L-1087"><a href="#L-1087"><span class="linenos">1087</span></a>            <span class="n">additional_forward_args</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">num_batches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">),</span>
</span><span id="L-1088"><a href="#L-1088"><span class="linenos">1088</span></a>            <span class="n">n_steps</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>  <span class="c1"># set 10 instead of default 50 to accelerate the computation</span>
</span><span id="L-1089"><a href="#L-1089"><span class="linenos">1089</span></a>        <span class="p">)</span>
</span><span id="L-1090"><a href="#L-1090"><span class="linenos">1090</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</span><span id="L-1091"><a href="#L-1091"><span class="linenos">1091</span></a>
</span><span id="L-1092"><a href="#L-1092"><span class="linenos">1092</span></a>        <span class="n">attribution_abs_batch_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
</span><span id="L-1093"><a href="#L-1093"><span class="linenos">1093</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">attribution</span><span class="p">),</span>
</span><span id="L-1094"><a href="#L-1094"><span class="linenos">1094</span></a>            <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="L-1095"><a href="#L-1095"><span class="linenos">1095</span></a>                <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">attribution</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="L-1096"><a href="#L-1096"><span class="linenos">1096</span></a>            <span class="p">],</span>  <span class="c1"># average the features over batch samples</span>
</span><span id="L-1097"><a href="#L-1097"><span class="linenos">1097</span></a>        <span class="p">)</span>
</span><span id="L-1098"><a href="#L-1098"><span class="linenos">1098</span></a>
</span><span id="L-1099"><a href="#L-1099"><span class="linenos">1099</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">attribution_abs_batch_mean</span>
</span><span id="L-1100"><a href="#L-1100"><span class="linenos">1100</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="L-1101"><a href="#L-1101"><span class="linenos">1101</span></a>
</span><span id="L-1102"><a href="#L-1102"><span class="linenos">1102</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span><span id="L-1103"><a href="#L-1103"><span class="linenos">1103</span></a>
</span><span id="L-1104"><a href="#L-1104"><span class="linenos">1104</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_gradcam_abs</span><span class="p">(</span>
</span><span id="L-1105"><a href="#L-1105"><span class="linenos">1105</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-1106"><a href="#L-1106"><span class="linenos">1106</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-1107"><a href="#L-1107"><span class="linenos">1107</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="L-1108"><a href="#L-1108"><span class="linenos">1108</span></a>        <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-1109"><a href="#L-1109"><span class="linenos">1109</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-1110"><a href="#L-1110"><span class="linenos">1110</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-1111"><a href="#L-1111"><span class="linenos">1111</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-1112"><a href="#L-1112"><span class="linenos">1112</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of [Grad-CAM](https://openreview.net/forum?id=SJPpHzW0-). We implement this using [Layer Grad-CAM](https://captum.ai/api/layer.html#gradcam) in Captum.</span>
</span><span id="L-1113"><a href="#L-1113"><span class="linenos">1113</span></a>
</span><span id="L-1114"><a href="#L-1114"><span class="linenos">1114</span></a><span class="sd">        **Args:**</span>
</span><span id="L-1115"><a href="#L-1115"><span class="linenos">1115</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="L-1116"><a href="#L-1116"><span class="linenos">1116</span></a><span class="sd">        - **input** (`Tensor` | `tuple[Tensor, ...]`): the input batch of the training step.</span>
</span><span id="L-1117"><a href="#L-1117"><span class="linenos">1117</span></a><span class="sd">        - **target** (`Tensor` | `None`): the target batch of the training step.</span>
</span><span id="L-1118"><a href="#L-1118"><span class="linenos">1118</span></a><span class="sd">        - **batch_idx** (`int`): the index of the current batch. This is an argument of the forward function during training.</span>
</span><span id="L-1119"><a href="#L-1119"><span class="linenos">1119</span></a><span class="sd">        - **num_batches** (`int`): the number of batches in the training step. This is an argument of the forward function during training.</span>
</span><span id="L-1120"><a href="#L-1120"><span class="linenos">1120</span></a>
</span><span id="L-1121"><a href="#L-1121"><span class="linenos">1121</span></a><span class="sd">        **Returns:**</span>
</span><span id="L-1122"><a href="#L-1122"><span class="linenos">1122</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="L-1123"><a href="#L-1123"><span class="linenos">1123</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-1124"><a href="#L-1124"><span class="linenos">1124</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="L-1125"><a href="#L-1125"><span class="linenos">1125</span></a>
</span><span id="L-1126"><a href="#L-1126"><span class="linenos">1126</span></a>        <span class="c1"># initialize the GradCAM object</span>
</span><span id="L-1127"><a href="#L-1127"><span class="linenos">1127</span></a>        <span class="n">gradcam</span> <span class="o">=</span> <span class="n">LayerGradCam</span><span class="p">(</span><span class="n">forward_func</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="n">layer</span><span class="p">)</span>
</span><span id="L-1128"><a href="#L-1128"><span class="linenos">1128</span></a>
</span><span id="L-1129"><a href="#L-1129"><span class="linenos">1129</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</span><span id="L-1130"><a href="#L-1130"><span class="linenos">1130</span></a>        <span class="c1"># calculate layer attribution of the step</span>
</span><span id="L-1131"><a href="#L-1131"><span class="linenos">1131</span></a>        <span class="n">attribution</span> <span class="o">=</span> <span class="n">gradcam</span><span class="o">.</span><span class="n">attribute</span><span class="p">(</span>
</span><span id="L-1132"><a href="#L-1132"><span class="linenos">1132</span></a>            <span class="n">inputs</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="L-1133"><a href="#L-1133"><span class="linenos">1133</span></a>            <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="L-1134"><a href="#L-1134"><span class="linenos">1134</span></a>            <span class="n">additional_forward_args</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">num_batches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">),</span>
</span><span id="L-1135"><a href="#L-1135"><span class="linenos">1135</span></a>        <span class="p">)</span>
</span><span id="L-1136"><a href="#L-1136"><span class="linenos">1136</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</span><span id="L-1137"><a href="#L-1137"><span class="linenos">1137</span></a>
</span><span id="L-1138"><a href="#L-1138"><span class="linenos">1138</span></a>        <span class="n">attribution_abs_batch_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
</span><span id="L-1139"><a href="#L-1139"><span class="linenos">1139</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">attribution</span><span class="p">),</span>
</span><span id="L-1140"><a href="#L-1140"><span class="linenos">1140</span></a>            <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="L-1141"><a href="#L-1141"><span class="linenos">1141</span></a>                <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">attribution</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="L-1142"><a href="#L-1142"><span class="linenos">1142</span></a>            <span class="p">],</span>  <span class="c1"># average the features over batch samples</span>
</span><span id="L-1143"><a href="#L-1143"><span class="linenos">1143</span></a>        <span class="p">)</span>
</span><span id="L-1144"><a href="#L-1144"><span class="linenos">1144</span></a>
</span><span id="L-1145"><a href="#L-1145"><span class="linenos">1145</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">attribution_abs_batch_mean</span>
</span><span id="L-1146"><a href="#L-1146"><span class="linenos">1146</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="L-1147"><a href="#L-1147"><span class="linenos">1147</span></a>
</span><span id="L-1148"><a href="#L-1148"><span class="linenos">1148</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span><span id="L-1149"><a href="#L-1149"><span class="linenos">1149</span></a>
</span><span id="L-1150"><a href="#L-1150"><span class="linenos">1150</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_deeplift_abs</span><span class="p">(</span>
</span><span id="L-1151"><a href="#L-1151"><span class="linenos">1151</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-1152"><a href="#L-1152"><span class="linenos">1152</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-1153"><a href="#L-1153"><span class="linenos">1153</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="L-1154"><a href="#L-1154"><span class="linenos">1154</span></a>        <span class="n">baselines</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="L-1155"><a href="#L-1155"><span class="linenos">1155</span></a>        <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-1156"><a href="#L-1156"><span class="linenos">1156</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-1157"><a href="#L-1157"><span class="linenos">1157</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-1158"><a href="#L-1158"><span class="linenos">1158</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-1159"><a href="#L-1159"><span class="linenos">1159</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of [DeepLift](https://proceedings.mlr.press/v70/shrikumar17a/shrikumar17a.pdf). We implement this using [Layer DeepLift](https://captum.ai/api/layer.html#layer-deeplift) in Captum.</span>
</span><span id="L-1160"><a href="#L-1160"><span class="linenos">1160</span></a>
</span><span id="L-1161"><a href="#L-1161"><span class="linenos">1161</span></a><span class="sd">        **Args:**</span>
</span><span id="L-1162"><a href="#L-1162"><span class="linenos">1162</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="L-1163"><a href="#L-1163"><span class="linenos">1163</span></a><span class="sd">        - **input** (`Tensor` | `tuple[Tensor, ...]`): the input batch of the training step.</span>
</span><span id="L-1164"><a href="#L-1164"><span class="linenos">1164</span></a><span class="sd">        - **baselines** (`None` | `int` | `float` | `Tensor` | `tuple[int | float | Tensor, ...]`): baselines define reference samples that are compared with the inputs. Please refer to the [Captum documentation](https://captum.ai/api/layer.html#captum.attr.LayerDeepLift.attribute) for more details.</span>
</span><span id="L-1165"><a href="#L-1165"><span class="linenos">1165</span></a><span class="sd">        - **target** (`Tensor` | `None`): the target batch of the training step.</span>
</span><span id="L-1166"><a href="#L-1166"><span class="linenos">1166</span></a><span class="sd">        - **batch_idx** (`int`): the index of the current batch. This is an argument of the forward function during training.</span>
</span><span id="L-1167"><a href="#L-1167"><span class="linenos">1167</span></a><span class="sd">        - **num_batches** (`int`): the number of batches in the training step. This is an argument of the forward function during training.</span>
</span><span id="L-1168"><a href="#L-1168"><span class="linenos">1168</span></a>
</span><span id="L-1169"><a href="#L-1169"><span class="linenos">1169</span></a><span class="sd">        **Returns:**</span>
</span><span id="L-1170"><a href="#L-1170"><span class="linenos">1170</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="L-1171"><a href="#L-1171"><span class="linenos">1171</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-1172"><a href="#L-1172"><span class="linenos">1172</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="L-1173"><a href="#L-1173"><span class="linenos">1173</span></a>
</span><span id="L-1174"><a href="#L-1174"><span class="linenos">1174</span></a>        <span class="c1"># initialize the Layer DeepLift object</span>
</span><span id="L-1175"><a href="#L-1175"><span class="linenos">1175</span></a>        <span class="n">layer_deeplift</span> <span class="o">=</span> <span class="n">LayerDeepLift</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="n">layer</span><span class="p">)</span>
</span><span id="L-1176"><a href="#L-1176"><span class="linenos">1176</span></a>
</span><span id="L-1177"><a href="#L-1177"><span class="linenos">1177</span></a>        <span class="c1"># convert the target to long type to avoid error</span>
</span><span id="L-1178"><a href="#L-1178"><span class="linenos">1178</span></a>        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">long</span><span class="p">()</span> <span class="k">if</span> <span class="n">target</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="L-1179"><a href="#L-1179"><span class="linenos">1179</span></a>
</span><span id="L-1180"><a href="#L-1180"><span class="linenos">1180</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</span><span id="L-1181"><a href="#L-1181"><span class="linenos">1181</span></a>        <span class="c1"># calculate layer attribution of the step</span>
</span><span id="L-1182"><a href="#L-1182"><span class="linenos">1182</span></a>        <span class="n">attribution</span> <span class="o">=</span> <span class="n">layer_deeplift</span><span class="o">.</span><span class="n">attribute</span><span class="p">(</span>
</span><span id="L-1183"><a href="#L-1183"><span class="linenos">1183</span></a>            <span class="n">inputs</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="L-1184"><a href="#L-1184"><span class="linenos">1184</span></a>            <span class="n">baselines</span><span class="o">=</span><span class="n">baselines</span><span class="p">,</span>
</span><span id="L-1185"><a href="#L-1185"><span class="linenos">1185</span></a>            <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="L-1186"><a href="#L-1186"><span class="linenos">1186</span></a>            <span class="n">additional_forward_args</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">num_batches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">),</span>
</span><span id="L-1187"><a href="#L-1187"><span class="linenos">1187</span></a>        <span class="p">)</span>
</span><span id="L-1188"><a href="#L-1188"><span class="linenos">1188</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</span><span id="L-1189"><a href="#L-1189"><span class="linenos">1189</span></a>
</span><span id="L-1190"><a href="#L-1190"><span class="linenos">1190</span></a>        <span class="n">attribution_abs_batch_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
</span><span id="L-1191"><a href="#L-1191"><span class="linenos">1191</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">attribution</span><span class="p">),</span>
</span><span id="L-1192"><a href="#L-1192"><span class="linenos">1192</span></a>            <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="L-1193"><a href="#L-1193"><span class="linenos">1193</span></a>                <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">attribution</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="L-1194"><a href="#L-1194"><span class="linenos">1194</span></a>            <span class="p">],</span>  <span class="c1"># average the features over batch samples</span>
</span><span id="L-1195"><a href="#L-1195"><span class="linenos">1195</span></a>        <span class="p">)</span>
</span><span id="L-1196"><a href="#L-1196"><span class="linenos">1196</span></a>
</span><span id="L-1197"><a href="#L-1197"><span class="linenos">1197</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">attribution_abs_batch_mean</span>
</span><span id="L-1198"><a href="#L-1198"><span class="linenos">1198</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="L-1199"><a href="#L-1199"><span class="linenos">1199</span></a>
</span><span id="L-1200"><a href="#L-1200"><span class="linenos">1200</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span><span id="L-1201"><a href="#L-1201"><span class="linenos">1201</span></a>
</span><span id="L-1202"><a href="#L-1202"><span class="linenos">1202</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_deepliftshap_abs</span><span class="p">(</span>
</span><span id="L-1203"><a href="#L-1203"><span class="linenos">1203</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-1204"><a href="#L-1204"><span class="linenos">1204</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-1205"><a href="#L-1205"><span class="linenos">1205</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="L-1206"><a href="#L-1206"><span class="linenos">1206</span></a>        <span class="n">baselines</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="L-1207"><a href="#L-1207"><span class="linenos">1207</span></a>        <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-1208"><a href="#L-1208"><span class="linenos">1208</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-1209"><a href="#L-1209"><span class="linenos">1209</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-1210"><a href="#L-1210"><span class="linenos">1210</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-1211"><a href="#L-1211"><span class="linenos">1211</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of [DeepLift SHAP](https://proceedings.neurips.cc/paper_files/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf). We implement this using [Layer DeepLiftShap](https://captum.ai/api/layer.html#layer-deepliftshap) in Captum.</span>
</span><span id="L-1212"><a href="#L-1212"><span class="linenos">1212</span></a>
</span><span id="L-1213"><a href="#L-1213"><span class="linenos">1213</span></a><span class="sd">        **Args:**</span>
</span><span id="L-1214"><a href="#L-1214"><span class="linenos">1214</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="L-1215"><a href="#L-1215"><span class="linenos">1215</span></a><span class="sd">        - **input** (`Tensor` | `tuple[Tensor, ...]`): the input batch of the training step.</span>
</span><span id="L-1216"><a href="#L-1216"><span class="linenos">1216</span></a><span class="sd">        - **baselines** (`None` | `int` | `float` | `Tensor` | `tuple[int | float | Tensor, ...]`): baselines define reference samples that are compared with the inputs. Please refer to the [Captum documentation](https://captum.ai/api/layer.html#captum.attr.LayerDeepLiftShap.attribute) for more details.</span>
</span><span id="L-1217"><a href="#L-1217"><span class="linenos">1217</span></a><span class="sd">        - **target** (`Tensor` | `None`): the target batch of the training step.</span>
</span><span id="L-1218"><a href="#L-1218"><span class="linenos">1218</span></a><span class="sd">        - **batch_idx** (`int`): the index of the current batch. This is an argument of the forward function during training.</span>
</span><span id="L-1219"><a href="#L-1219"><span class="linenos">1219</span></a><span class="sd">        - **num_batches** (`int`): the number of batches in the training step. This is an argument of the forward function during training.</span>
</span><span id="L-1220"><a href="#L-1220"><span class="linenos">1220</span></a>
</span><span id="L-1221"><a href="#L-1221"><span class="linenos">1221</span></a><span class="sd">        **Returns:**</span>
</span><span id="L-1222"><a href="#L-1222"><span class="linenos">1222</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="L-1223"><a href="#L-1223"><span class="linenos">1223</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-1224"><a href="#L-1224"><span class="linenos">1224</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="L-1225"><a href="#L-1225"><span class="linenos">1225</span></a>
</span><span id="L-1226"><a href="#L-1226"><span class="linenos">1226</span></a>        <span class="c1"># initialize the Layer DeepLiftShap object</span>
</span><span id="L-1227"><a href="#L-1227"><span class="linenos">1227</span></a>        <span class="n">layer_deepliftshap</span> <span class="o">=</span> <span class="n">LayerDeepLiftShap</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="n">layer</span><span class="p">)</span>
</span><span id="L-1228"><a href="#L-1228"><span class="linenos">1228</span></a>
</span><span id="L-1229"><a href="#L-1229"><span class="linenos">1229</span></a>        <span class="c1"># convert the target to long type to avoid error</span>
</span><span id="L-1230"><a href="#L-1230"><span class="linenos">1230</span></a>        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">long</span><span class="p">()</span> <span class="k">if</span> <span class="n">target</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="L-1231"><a href="#L-1231"><span class="linenos">1231</span></a>
</span><span id="L-1232"><a href="#L-1232"><span class="linenos">1232</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</span><span id="L-1233"><a href="#L-1233"><span class="linenos">1233</span></a>        <span class="c1"># calculate layer attribution of the step</span>
</span><span id="L-1234"><a href="#L-1234"><span class="linenos">1234</span></a>        <span class="n">attribution</span> <span class="o">=</span> <span class="n">layer_deepliftshap</span><span class="o">.</span><span class="n">attribute</span><span class="p">(</span>
</span><span id="L-1235"><a href="#L-1235"><span class="linenos">1235</span></a>            <span class="n">inputs</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="L-1236"><a href="#L-1236"><span class="linenos">1236</span></a>            <span class="n">baselines</span><span class="o">=</span><span class="n">baselines</span><span class="p">,</span>
</span><span id="L-1237"><a href="#L-1237"><span class="linenos">1237</span></a>            <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="L-1238"><a href="#L-1238"><span class="linenos">1238</span></a>            <span class="n">additional_forward_args</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">num_batches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">),</span>
</span><span id="L-1239"><a href="#L-1239"><span class="linenos">1239</span></a>        <span class="p">)</span>
</span><span id="L-1240"><a href="#L-1240"><span class="linenos">1240</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</span><span id="L-1241"><a href="#L-1241"><span class="linenos">1241</span></a>
</span><span id="L-1242"><a href="#L-1242"><span class="linenos">1242</span></a>        <span class="n">attribution_abs_batch_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
</span><span id="L-1243"><a href="#L-1243"><span class="linenos">1243</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">attribution</span><span class="p">),</span>
</span><span id="L-1244"><a href="#L-1244"><span class="linenos">1244</span></a>            <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="L-1245"><a href="#L-1245"><span class="linenos">1245</span></a>                <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">attribution</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="L-1246"><a href="#L-1246"><span class="linenos">1246</span></a>            <span class="p">],</span>  <span class="c1"># average the features over batch samples</span>
</span><span id="L-1247"><a href="#L-1247"><span class="linenos">1247</span></a>        <span class="p">)</span>
</span><span id="L-1248"><a href="#L-1248"><span class="linenos">1248</span></a>
</span><span id="L-1249"><a href="#L-1249"><span class="linenos">1249</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">attribution_abs_batch_mean</span>
</span><span id="L-1250"><a href="#L-1250"><span class="linenos">1250</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="L-1251"><a href="#L-1251"><span class="linenos">1251</span></a>
</span><span id="L-1252"><a href="#L-1252"><span class="linenos">1252</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span><span id="L-1253"><a href="#L-1253"><span class="linenos">1253</span></a>
</span><span id="L-1254"><a href="#L-1254"><span class="linenos">1254</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_gradientshap_abs</span><span class="p">(</span>
</span><span id="L-1255"><a href="#L-1255"><span class="linenos">1255</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-1256"><a href="#L-1256"><span class="linenos">1256</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-1257"><a href="#L-1257"><span class="linenos">1257</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="L-1258"><a href="#L-1258"><span class="linenos">1258</span></a>        <span class="n">baselines</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="L-1259"><a href="#L-1259"><span class="linenos">1259</span></a>        <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-1260"><a href="#L-1260"><span class="linenos">1260</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-1261"><a href="#L-1261"><span class="linenos">1261</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-1262"><a href="#L-1262"><span class="linenos">1262</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-1263"><a href="#L-1263"><span class="linenos">1263</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of gradient SHAP. We implement this using [Layer GradientShap](https://captum.ai/api/layer.html#layer-gradientshap) in Captum.</span>
</span><span id="L-1264"><a href="#L-1264"><span class="linenos">1264</span></a>
</span><span id="L-1265"><a href="#L-1265"><span class="linenos">1265</span></a><span class="sd">        **Args:**</span>
</span><span id="L-1266"><a href="#L-1266"><span class="linenos">1266</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="L-1267"><a href="#L-1267"><span class="linenos">1267</span></a><span class="sd">        - **input** (`Tensor` | `tuple[Tensor, ...]`): the input batch of the training step.</span>
</span><span id="L-1268"><a href="#L-1268"><span class="linenos">1268</span></a><span class="sd">        - **baselines** (`None` | `int` | `float` | `Tensor` | `tuple[int | float | Tensor, ...]`): starting point from which expectation is computed. Please refer to the [Captum documentation](https://captum.ai/api/layer.html#captum.attr.LayerGradientShap.attribute) for more details. If `None`, the baselines are set to zero.</span>
</span><span id="L-1269"><a href="#L-1269"><span class="linenos">1269</span></a><span class="sd">        - **target** (`Tensor` | `None`): the target batch of the training step.</span>
</span><span id="L-1270"><a href="#L-1270"><span class="linenos">1270</span></a><span class="sd">        - **batch_idx** (`int`): the index of the current batch. This is an argument of the forward function during training.</span>
</span><span id="L-1271"><a href="#L-1271"><span class="linenos">1271</span></a><span class="sd">        - **num_batches** (`int`): the number of batches in the training step. This is an argument of the forward function during training.</span>
</span><span id="L-1272"><a href="#L-1272"><span class="linenos">1272</span></a>
</span><span id="L-1273"><a href="#L-1273"><span class="linenos">1273</span></a><span class="sd">        **Returns:**</span>
</span><span id="L-1274"><a href="#L-1274"><span class="linenos">1274</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="L-1275"><a href="#L-1275"><span class="linenos">1275</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-1276"><a href="#L-1276"><span class="linenos">1276</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="L-1277"><a href="#L-1277"><span class="linenos">1277</span></a>
</span><span id="L-1278"><a href="#L-1278"><span class="linenos">1278</span></a>        <span class="k">if</span> <span class="n">baselines</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-1279"><a href="#L-1279"><span class="linenos">1279</span></a>            <span class="n">baselines</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span>
</span><span id="L-1280"><a href="#L-1280"><span class="linenos">1280</span></a>                <span class="nb">input</span>
</span><span id="L-1281"><a href="#L-1281"><span class="linenos">1281</span></a>            <span class="p">)</span>  <span class="c1"># baselines are mandatory for GradientShap API. We explicitly set them to zero</span>
</span><span id="L-1282"><a href="#L-1282"><span class="linenos">1282</span></a>
</span><span id="L-1283"><a href="#L-1283"><span class="linenos">1283</span></a>        <span class="c1"># initialize the Layer GradientShap object</span>
</span><span id="L-1284"><a href="#L-1284"><span class="linenos">1284</span></a>        <span class="n">layer_gradientshap</span> <span class="o">=</span> <span class="n">LayerGradientShap</span><span class="p">(</span><span class="n">forward_func</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="n">layer</span><span class="p">)</span>
</span><span id="L-1285"><a href="#L-1285"><span class="linenos">1285</span></a>
</span><span id="L-1286"><a href="#L-1286"><span class="linenos">1286</span></a>        <span class="c1"># convert the target to long type to avoid error</span>
</span><span id="L-1287"><a href="#L-1287"><span class="linenos">1287</span></a>        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">long</span><span class="p">()</span> <span class="k">if</span> <span class="n">target</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="L-1288"><a href="#L-1288"><span class="linenos">1288</span></a>
</span><span id="L-1289"><a href="#L-1289"><span class="linenos">1289</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</span><span id="L-1290"><a href="#L-1290"><span class="linenos">1290</span></a>        <span class="c1"># calculate layer attribution of the step</span>
</span><span id="L-1291"><a href="#L-1291"><span class="linenos">1291</span></a>        <span class="n">attribution</span> <span class="o">=</span> <span class="n">layer_gradientshap</span><span class="o">.</span><span class="n">attribute</span><span class="p">(</span>
</span><span id="L-1292"><a href="#L-1292"><span class="linenos">1292</span></a>            <span class="n">inputs</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="L-1293"><a href="#L-1293"><span class="linenos">1293</span></a>            <span class="n">baselines</span><span class="o">=</span><span class="n">baselines</span><span class="p">,</span>
</span><span id="L-1294"><a href="#L-1294"><span class="linenos">1294</span></a>            <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="L-1295"><a href="#L-1295"><span class="linenos">1295</span></a>            <span class="n">additional_forward_args</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">num_batches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">),</span>
</span><span id="L-1296"><a href="#L-1296"><span class="linenos">1296</span></a>        <span class="p">)</span>
</span><span id="L-1297"><a href="#L-1297"><span class="linenos">1297</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</span><span id="L-1298"><a href="#L-1298"><span class="linenos">1298</span></a>
</span><span id="L-1299"><a href="#L-1299"><span class="linenos">1299</span></a>        <span class="n">attribution_abs_batch_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
</span><span id="L-1300"><a href="#L-1300"><span class="linenos">1300</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">attribution</span><span class="p">),</span>
</span><span id="L-1301"><a href="#L-1301"><span class="linenos">1301</span></a>            <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="L-1302"><a href="#L-1302"><span class="linenos">1302</span></a>                <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">attribution</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="L-1303"><a href="#L-1303"><span class="linenos">1303</span></a>            <span class="p">],</span>  <span class="c1"># average the features over batch samples</span>
</span><span id="L-1304"><a href="#L-1304"><span class="linenos">1304</span></a>        <span class="p">)</span>
</span><span id="L-1305"><a href="#L-1305"><span class="linenos">1305</span></a>
</span><span id="L-1306"><a href="#L-1306"><span class="linenos">1306</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">attribution_abs_batch_mean</span>
</span><span id="L-1307"><a href="#L-1307"><span class="linenos">1307</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="L-1308"><a href="#L-1308"><span class="linenos">1308</span></a>
</span><span id="L-1309"><a href="#L-1309"><span class="linenos">1309</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span><span id="L-1310"><a href="#L-1310"><span class="linenos">1310</span></a>
</span><span id="L-1311"><a href="#L-1311"><span class="linenos">1311</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_integrated_gradients_abs</span><span class="p">(</span>
</span><span id="L-1312"><a href="#L-1312"><span class="linenos">1312</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-1313"><a href="#L-1313"><span class="linenos">1313</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-1314"><a href="#L-1314"><span class="linenos">1314</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="L-1315"><a href="#L-1315"><span class="linenos">1315</span></a>        <span class="n">baselines</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="L-1316"><a href="#L-1316"><span class="linenos">1316</span></a>        <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-1317"><a href="#L-1317"><span class="linenos">1317</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-1318"><a href="#L-1318"><span class="linenos">1318</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-1319"><a href="#L-1319"><span class="linenos">1319</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-1320"><a href="#L-1320"><span class="linenos">1320</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of [integrated gradients](https://proceedings.mlr.press/v70/sundararajan17a/sundararajan17a.pdf). We implement this using [Layer Integrated Gradients](https://captum.ai/api/layer.html#layer-integrated-gradients) in Captum.</span>
</span><span id="L-1321"><a href="#L-1321"><span class="linenos">1321</span></a>
</span><span id="L-1322"><a href="#L-1322"><span class="linenos">1322</span></a><span class="sd">        **Args:**</span>
</span><span id="L-1323"><a href="#L-1323"><span class="linenos">1323</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="L-1324"><a href="#L-1324"><span class="linenos">1324</span></a><span class="sd">        - **input** (`Tensor` | `tuple[Tensor, ...]`): the input batch of the training step.</span>
</span><span id="L-1325"><a href="#L-1325"><span class="linenos">1325</span></a><span class="sd">        - **baselines** (`None` | `int` | `float` | `Tensor` | `tuple[int | float | Tensor, ...]`): starting point from which integral is computed. Please refer to the [Captum documentation](https://captum.ai/api/layer.html#captum.attr.LayerIntegratedGradients.attribute) for more details.</span>
</span><span id="L-1326"><a href="#L-1326"><span class="linenos">1326</span></a><span class="sd">        - **target** (`Tensor` | `None`): the target batch of the training step.</span>
</span><span id="L-1327"><a href="#L-1327"><span class="linenos">1327</span></a><span class="sd">        - **batch_idx** (`int`): the index of the current batch. This is an argument of the forward function during training.</span>
</span><span id="L-1328"><a href="#L-1328"><span class="linenos">1328</span></a><span class="sd">        - **num_batches** (`int`): the number of batches in the training step. This is an argument of the forward function during training.</span>
</span><span id="L-1329"><a href="#L-1329"><span class="linenos">1329</span></a>
</span><span id="L-1330"><a href="#L-1330"><span class="linenos">1330</span></a><span class="sd">        **Returns:**</span>
</span><span id="L-1331"><a href="#L-1331"><span class="linenos">1331</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="L-1332"><a href="#L-1332"><span class="linenos">1332</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-1333"><a href="#L-1333"><span class="linenos">1333</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="L-1334"><a href="#L-1334"><span class="linenos">1334</span></a>
</span><span id="L-1335"><a href="#L-1335"><span class="linenos">1335</span></a>        <span class="c1"># initialize the Layer Integrated Gradients object</span>
</span><span id="L-1336"><a href="#L-1336"><span class="linenos">1336</span></a>        <span class="n">layer_integrated_gradients</span> <span class="o">=</span> <span class="n">LayerIntegratedGradients</span><span class="p">(</span>
</span><span id="L-1337"><a href="#L-1337"><span class="linenos">1337</span></a>            <span class="n">forward_func</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="n">layer</span>
</span><span id="L-1338"><a href="#L-1338"><span class="linenos">1338</span></a>        <span class="p">)</span>
</span><span id="L-1339"><a href="#L-1339"><span class="linenos">1339</span></a>
</span><span id="L-1340"><a href="#L-1340"><span class="linenos">1340</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</span><span id="L-1341"><a href="#L-1341"><span class="linenos">1341</span></a>        <span class="c1"># calculate layer attribution of the step</span>
</span><span id="L-1342"><a href="#L-1342"><span class="linenos">1342</span></a>        <span class="n">attribution</span> <span class="o">=</span> <span class="n">layer_integrated_gradients</span><span class="o">.</span><span class="n">attribute</span><span class="p">(</span>
</span><span id="L-1343"><a href="#L-1343"><span class="linenos">1343</span></a>            <span class="n">inputs</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="L-1344"><a href="#L-1344"><span class="linenos">1344</span></a>            <span class="n">baselines</span><span class="o">=</span><span class="n">baselines</span><span class="p">,</span>
</span><span id="L-1345"><a href="#L-1345"><span class="linenos">1345</span></a>            <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="L-1346"><a href="#L-1346"><span class="linenos">1346</span></a>            <span class="n">additional_forward_args</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">num_batches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">),</span>
</span><span id="L-1347"><a href="#L-1347"><span class="linenos">1347</span></a>        <span class="p">)</span>
</span><span id="L-1348"><a href="#L-1348"><span class="linenos">1348</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</span><span id="L-1349"><a href="#L-1349"><span class="linenos">1349</span></a>
</span><span id="L-1350"><a href="#L-1350"><span class="linenos">1350</span></a>        <span class="n">attribution_abs_batch_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
</span><span id="L-1351"><a href="#L-1351"><span class="linenos">1351</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">attribution</span><span class="p">),</span>
</span><span id="L-1352"><a href="#L-1352"><span class="linenos">1352</span></a>            <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="L-1353"><a href="#L-1353"><span class="linenos">1353</span></a>                <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">attribution</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="L-1354"><a href="#L-1354"><span class="linenos">1354</span></a>            <span class="p">],</span>  <span class="c1"># average the features over batch samples</span>
</span><span id="L-1355"><a href="#L-1355"><span class="linenos">1355</span></a>        <span class="p">)</span>
</span><span id="L-1356"><a href="#L-1356"><span class="linenos">1356</span></a>
</span><span id="L-1357"><a href="#L-1357"><span class="linenos">1357</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">attribution_abs_batch_mean</span>
</span><span id="L-1358"><a href="#L-1358"><span class="linenos">1358</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="L-1359"><a href="#L-1359"><span class="linenos">1359</span></a>
</span><span id="L-1360"><a href="#L-1360"><span class="linenos">1360</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span><span id="L-1361"><a href="#L-1361"><span class="linenos">1361</span></a>
</span><span id="L-1362"><a href="#L-1362"><span class="linenos">1362</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_feature_ablation_abs</span><span class="p">(</span>
</span><span id="L-1363"><a href="#L-1363"><span class="linenos">1363</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-1364"><a href="#L-1364"><span class="linenos">1364</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-1365"><a href="#L-1365"><span class="linenos">1365</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="L-1366"><a href="#L-1366"><span class="linenos">1366</span></a>        <span class="n">layer_baselines</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="L-1367"><a href="#L-1367"><span class="linenos">1367</span></a>        <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-1368"><a href="#L-1368"><span class="linenos">1368</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-1369"><a href="#L-1369"><span class="linenos">1369</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-1370"><a href="#L-1370"><span class="linenos">1370</span></a>        <span class="n">if_captum</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-1371"><a href="#L-1371"><span class="linenos">1371</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-1372"><a href="#L-1372"><span class="linenos">1372</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of [feature ablation](https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53) attribution. We implement this using [Layer Feature Ablation](https://captum.ai/api/layer.html#layer-feature-ablation) in Captum.</span>
</span><span id="L-1373"><a href="#L-1373"><span class="linenos">1373</span></a>
</span><span id="L-1374"><a href="#L-1374"><span class="linenos">1374</span></a><span class="sd">        **Args:**</span>
</span><span id="L-1375"><a href="#L-1375"><span class="linenos">1375</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="L-1376"><a href="#L-1376"><span class="linenos">1376</span></a><span class="sd">        - **input** (`Tensor` | `tuple[Tensor, ...]`): the input batch of the training step.</span>
</span><span id="L-1377"><a href="#L-1377"><span class="linenos">1377</span></a><span class="sd">        - **layer_baselines** (`None` | `int` | `float` | `Tensor` | `tuple[int | float | Tensor, ...]`): reference values which replace each layer input / output value when ablated. Please refer to the [Captum documentation](https://captum.ai/api/layer.html#captum.attr.LayerFeatureAblation.attribute) for more details.</span>
</span><span id="L-1378"><a href="#L-1378"><span class="linenos">1378</span></a><span class="sd">        - **target** (`Tensor` | `None`): the target batch of the training step.</span>
</span><span id="L-1379"><a href="#L-1379"><span class="linenos">1379</span></a><span class="sd">        - **batch_idx** (`int`): the index of the current batch. This is an argument of the forward function during training.</span>
</span><span id="L-1380"><a href="#L-1380"><span class="linenos">1380</span></a><span class="sd">        - **num_batches** (`int`): the number of batches in the training step. This is an argument of the forward function during training.</span>
</span><span id="L-1381"><a href="#L-1381"><span class="linenos">1381</span></a><span class="sd">        - **if_captum** (`bool`): whether to use Captum or not. If `True`, we use Captum to calculate the feature ablation. If `False`, we use our implementation. Default is `False`, because our implementation is much faster.</span>
</span><span id="L-1382"><a href="#L-1382"><span class="linenos">1382</span></a>
</span><span id="L-1383"><a href="#L-1383"><span class="linenos">1383</span></a><span class="sd">        **Returns:**</span>
</span><span id="L-1384"><a href="#L-1384"><span class="linenos">1384</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="L-1385"><a href="#L-1385"><span class="linenos">1385</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-1386"><a href="#L-1386"><span class="linenos">1386</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="L-1387"><a href="#L-1387"><span class="linenos">1387</span></a>
</span><span id="L-1388"><a href="#L-1388"><span class="linenos">1388</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="n">if_captum</span><span class="p">:</span>
</span><span id="L-1389"><a href="#L-1389"><span class="linenos">1389</span></a>            <span class="c1"># 1. Baseline logits (take first element of forward output)</span>
</span><span id="L-1390"><a href="#L-1390"><span class="linenos">1390</span></a>            <span class="n">baseline_out</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
</span><span id="L-1391"><a href="#L-1391"><span class="linenos">1391</span></a>                <span class="nb">input</span><span class="p">,</span> <span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">num_batches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span>
</span><span id="L-1392"><a href="#L-1392"><span class="linenos">1392</span></a>            <span class="p">)</span>
</span><span id="L-1393"><a href="#L-1393"><span class="linenos">1393</span></a>            <span class="k">if</span> <span class="n">target</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-1394"><a href="#L-1394"><span class="linenos">1394</span></a>                <span class="n">baseline_scores</span> <span class="o">=</span> <span class="n">baseline_out</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-1395"><a href="#L-1395"><span class="linenos">1395</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="L-1396"><a href="#L-1396"><span class="linenos">1396</span></a>                <span class="n">baseline_scores</span> <span class="o">=</span> <span class="n">baseline_out</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-1397"><a href="#L-1397"><span class="linenos">1397</span></a>
</span><span id="L-1398"><a href="#L-1398"><span class="linenos">1398</span></a>            <span class="c1"># 2. Capture layer’s output shape</span>
</span><span id="L-1399"><a href="#L-1399"><span class="linenos">1399</span></a>            <span class="n">activs</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="L-1400"><a href="#L-1400"><span class="linenos">1400</span></a>            <span class="n">handle</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">register_forward_hook</span><span class="p">(</span>
</span><span id="L-1401"><a href="#L-1401"><span class="linenos">1401</span></a>                <span class="k">lambda</span> <span class="n">module</span><span class="p">,</span> <span class="n">inp</span><span class="p">,</span> <span class="n">out</span><span class="p">:</span> <span class="n">activs</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;output&quot;</span><span class="p">,</span> <span class="n">out</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
</span><span id="L-1402"><a href="#L-1402"><span class="linenos">1402</span></a>            <span class="p">)</span>
</span><span id="L-1403"><a href="#L-1403"><span class="linenos">1403</span></a>            <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">num_batches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">)</span>
</span><span id="L-1404"><a href="#L-1404"><span class="linenos">1404</span></a>            <span class="n">handle</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span>
</span><span id="L-1405"><a href="#L-1405"><span class="linenos">1405</span></a>            <span class="n">layer_output</span> <span class="o">=</span> <span class="n">activs</span><span class="p">[</span><span class="s2">&quot;output&quot;</span><span class="p">]</span>  <span class="c1"># shape (B, F, ...)</span>
</span><span id="L-1406"><a href="#L-1406"><span class="linenos">1406</span></a>
</span><span id="L-1407"><a href="#L-1407"><span class="linenos">1407</span></a>            <span class="c1"># 3. Build baseline tensor matching that shape</span>
</span><span id="L-1408"><a href="#L-1408"><span class="linenos">1408</span></a>            <span class="k">if</span> <span class="n">layer_baselines</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-1409"><a href="#L-1409"><span class="linenos">1409</span></a>                <span class="n">baseline_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">layer_output</span><span class="p">)</span>
</span><span id="L-1410"><a href="#L-1410"><span class="linenos">1410</span></a>            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer_baselines</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">)):</span>
</span><span id="L-1411"><a href="#L-1411"><span class="linenos">1411</span></a>                <span class="n">baseline_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">layer_output</span><span class="p">,</span> <span class="n">layer_baselines</span><span class="p">)</span>
</span><span id="L-1412"><a href="#L-1412"><span class="linenos">1412</span></a>            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer_baselines</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
</span><span id="L-1413"><a href="#L-1413"><span class="linenos">1413</span></a>                <span class="k">if</span> <span class="n">layer_baselines</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">layer_output</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
</span><span id="L-1414"><a href="#L-1414"><span class="linenos">1414</span></a>                    <span class="n">baseline_tensor</span> <span class="o">=</span> <span class="n">layer_baselines</span>
</span><span id="L-1415"><a href="#L-1415"><span class="linenos">1415</span></a>                <span class="k">elif</span> <span class="n">layer_baselines</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">layer_output</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
</span><span id="L-1416"><a href="#L-1416"><span class="linenos">1416</span></a>                    <span class="n">baseline_tensor</span> <span class="o">=</span> <span class="n">layer_baselines</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span>
</span><span id="L-1417"><a href="#L-1417"><span class="linenos">1417</span></a>                        <span class="n">layer_output</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">*</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">layer_baselines</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
</span><span id="L-1418"><a href="#L-1418"><span class="linenos">1418</span></a>                    <span class="p">)</span>
</span><span id="L-1419"><a href="#L-1419"><span class="linenos">1419</span></a>                <span class="k">else</span><span class="p">:</span>
</span><span id="L-1420"><a href="#L-1420"><span class="linenos">1420</span></a>                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
</span><span id="L-1421"><a href="#L-1421"><span class="linenos">1421</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="L-1422"><a href="#L-1422"><span class="linenos">1422</span></a>                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
</span><span id="L-1423"><a href="#L-1423"><span class="linenos">1423</span></a>
</span><span id="L-1424"><a href="#L-1424"><span class="linenos">1424</span></a>            <span class="n">B</span><span class="p">,</span> <span class="n">F</span> <span class="o">=</span> <span class="n">layer_output</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">layer_output</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-1425"><a href="#L-1425"><span class="linenos">1425</span></a>
</span><span id="L-1426"><a href="#L-1426"><span class="linenos">1426</span></a>            <span class="c1"># 4. Create a “mega-batch” replicating the input F times</span>
</span><span id="L-1427"><a href="#L-1427"><span class="linenos">1427</span></a>            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
</span><span id="L-1428"><a href="#L-1428"><span class="linenos">1428</span></a>                <span class="n">mega_inputs</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span>
</span><span id="L-1429"><a href="#L-1429"><span class="linenos">1429</span></a>                    <span class="n">t</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">F</span><span class="p">,</span> <span class="o">*</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">t</span><span class="o">.</span><span class="n">ndim</span><span class="p">))</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="n">t</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
</span><span id="L-1430"><a href="#L-1430"><span class="linenos">1430</span></a>                    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">input</span>
</span><span id="L-1431"><a href="#L-1431"><span class="linenos">1431</span></a>                <span class="p">)</span>
</span><span id="L-1432"><a href="#L-1432"><span class="linenos">1432</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="L-1433"><a href="#L-1433"><span class="linenos">1433</span></a>                <span class="n">mega_inputs</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-1434"><a href="#L-1434"><span class="linenos">1434</span></a>                    <span class="nb">input</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="L-1435"><a href="#L-1435"><span class="linenos">1435</span></a>                    <span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">F</span><span class="p">,</span> <span class="o">*</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">input</span><span class="o">.</span><span class="n">ndim</span><span class="p">))</span>
</span><span id="L-1436"><a href="#L-1436"><span class="linenos">1436</span></a>                    <span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
</span><span id="L-1437"><a href="#L-1437"><span class="linenos">1437</span></a>                <span class="p">)</span>
</span><span id="L-1438"><a href="#L-1438"><span class="linenos">1438</span></a>
</span><span id="L-1439"><a href="#L-1439"><span class="linenos">1439</span></a>            <span class="c1"># 5. Equally replicate the baseline tensor</span>
</span><span id="L-1440"><a href="#L-1440"><span class="linenos">1440</span></a>            <span class="n">mega_baseline</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-1441"><a href="#L-1441"><span class="linenos">1441</span></a>                <span class="n">baseline_tensor</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="L-1442"><a href="#L-1442"><span class="linenos">1442</span></a>                <span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">F</span><span class="p">,</span> <span class="o">*</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">baseline_tensor</span><span class="o">.</span><span class="n">ndim</span><span class="p">))</span>
</span><span id="L-1443"><a href="#L-1443"><span class="linenos">1443</span></a>                <span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="n">baseline_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
</span><span id="L-1444"><a href="#L-1444"><span class="linenos">1444</span></a>            <span class="p">)</span>
</span><span id="L-1445"><a href="#L-1445"><span class="linenos">1445</span></a>
</span><span id="L-1446"><a href="#L-1446"><span class="linenos">1446</span></a>            <span class="c1"># 6. Precompute vectorized indices</span>
</span><span id="L-1447"><a href="#L-1447"><span class="linenos">1447</span></a>            <span class="n">device</span> <span class="o">=</span> <span class="n">layer_output</span><span class="o">.</span><span class="n">device</span>
</span><span id="L-1448"><a href="#L-1448"><span class="linenos">1448</span></a>            <span class="n">positions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">F</span> <span class="o">*</span> <span class="n">B</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># [0,1,...,F*B-1]</span>
</span><span id="L-1449"><a href="#L-1449"><span class="linenos">1449</span></a>            <span class="n">feat_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">F</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span>
</span><span id="L-1450"><a href="#L-1450"><span class="linenos">1450</span></a>                <span class="n">B</span>
</span><span id="L-1451"><a href="#L-1451"><span class="linenos">1451</span></a>            <span class="p">)</span>  <span class="c1"># [0,0,...,1,1,...,F-1]</span>
</span><span id="L-1452"><a href="#L-1452"><span class="linenos">1452</span></a>
</span><span id="L-1453"><a href="#L-1453"><span class="linenos">1453</span></a>            <span class="c1"># 7. One hook to zero out each channel slice across the mega-batch</span>
</span><span id="L-1454"><a href="#L-1454"><span class="linenos">1454</span></a>            <span class="k">def</span><span class="w"> </span><span class="nf">mega_ablate_hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">inp</span><span class="p">,</span> <span class="n">out</span><span class="p">):</span>
</span><span id="L-1455"><a href="#L-1455"><span class="linenos">1455</span></a>                <span class="n">out_mod</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</span><span id="L-1456"><a href="#L-1456"><span class="linenos">1456</span></a>                <span class="c1"># for each sample in mega-batch, zero its corresponding channel</span>
</span><span id="L-1457"><a href="#L-1457"><span class="linenos">1457</span></a>                <span class="n">out_mod</span><span class="p">[</span><span class="n">positions</span><span class="p">,</span> <span class="n">feat_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">mega_baseline</span><span class="p">[</span><span class="n">positions</span><span class="p">,</span> <span class="n">feat_idx</span><span class="p">]</span>
</span><span id="L-1458"><a href="#L-1458"><span class="linenos">1458</span></a>                <span class="k">return</span> <span class="n">out_mod</span>
</span><span id="L-1459"><a href="#L-1459"><span class="linenos">1459</span></a>
</span><span id="L-1460"><a href="#L-1460"><span class="linenos">1460</span></a>            <span class="n">h</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">register_forward_hook</span><span class="p">(</span><span class="n">mega_ablate_hook</span><span class="p">)</span>
</span><span id="L-1461"><a href="#L-1461"><span class="linenos">1461</span></a>            <span class="n">out_all</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
</span><span id="L-1462"><a href="#L-1462"><span class="linenos">1462</span></a>                <span class="n">mega_inputs</span><span class="p">,</span> <span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">num_batches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span>
</span><span id="L-1463"><a href="#L-1463"><span class="linenos">1463</span></a>            <span class="p">)</span>
</span><span id="L-1464"><a href="#L-1464"><span class="linenos">1464</span></a>            <span class="n">h</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span>
</span><span id="L-1465"><a href="#L-1465"><span class="linenos">1465</span></a>
</span><span id="L-1466"><a href="#L-1466"><span class="linenos">1466</span></a>            <span class="c1"># 8. Recover scores, reshape [F*B] → [F, B], diff &amp; mean</span>
</span><span id="L-1467"><a href="#L-1467"><span class="linenos">1467</span></a>            <span class="k">if</span> <span class="n">target</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-1468"><a href="#L-1468"><span class="linenos">1468</span></a>                <span class="n">tgt_flat</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">F</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-1469"><a href="#L-1469"><span class="linenos">1469</span></a>                <span class="n">scores_all</span> <span class="o">=</span> <span class="n">out_all</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">tgt_flat</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-1470"><a href="#L-1470"><span class="linenos">1470</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="L-1471"><a href="#L-1471"><span class="linenos">1471</span></a>                <span class="n">scores_all</span> <span class="o">=</span> <span class="n">out_all</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-1472"><a href="#L-1472"><span class="linenos">1472</span></a>
</span><span id="L-1473"><a href="#L-1473"><span class="linenos">1473</span></a>            <span class="n">scores_all</span> <span class="o">=</span> <span class="n">scores_all</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">F</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>
</span><span id="L-1474"><a href="#L-1474"><span class="linenos">1474</span></a>            <span class="n">diffs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">baseline_scores</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="n">scores_all</span><span class="p">)</span>
</span><span id="L-1475"><a href="#L-1475"><span class="linenos">1475</span></a>            <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">diffs</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>  <span class="c1"># [F]</span>
</span><span id="L-1476"><a href="#L-1476"><span class="linenos">1476</span></a>
</span><span id="L-1477"><a href="#L-1477"><span class="linenos">1477</span></a>            <span class="k">return</span> <span class="n">importance_step_layer</span>
</span><span id="L-1478"><a href="#L-1478"><span class="linenos">1478</span></a>
</span><span id="L-1479"><a href="#L-1479"><span class="linenos">1479</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-1480"><a href="#L-1480"><span class="linenos">1480</span></a>            <span class="c1"># initialize the Layer Feature Ablation object</span>
</span><span id="L-1481"><a href="#L-1481"><span class="linenos">1481</span></a>            <span class="n">layer_feature_ablation</span> <span class="o">=</span> <span class="n">LayerFeatureAblation</span><span class="p">(</span>
</span><span id="L-1482"><a href="#L-1482"><span class="linenos">1482</span></a>                <span class="n">forward_func</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="n">layer</span>
</span><span id="L-1483"><a href="#L-1483"><span class="linenos">1483</span></a>            <span class="p">)</span>
</span><span id="L-1484"><a href="#L-1484"><span class="linenos">1484</span></a>
</span><span id="L-1485"><a href="#L-1485"><span class="linenos">1485</span></a>            <span class="c1"># calculate layer attribution of the step</span>
</span><span id="L-1486"><a href="#L-1486"><span class="linenos">1486</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</span><span id="L-1487"><a href="#L-1487"><span class="linenos">1487</span></a>            <span class="n">attribution</span> <span class="o">=</span> <span class="n">layer_feature_ablation</span><span class="o">.</span><span class="n">attribute</span><span class="p">(</span>
</span><span id="L-1488"><a href="#L-1488"><span class="linenos">1488</span></a>                <span class="n">inputs</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="L-1489"><a href="#L-1489"><span class="linenos">1489</span></a>                <span class="n">layer_baselines</span><span class="o">=</span><span class="n">layer_baselines</span><span class="p">,</span>
</span><span id="L-1490"><a href="#L-1490"><span class="linenos">1490</span></a>                <span class="c1"># target=target, # disable target to enable perturbations_per_eval</span>
</span><span id="L-1491"><a href="#L-1491"><span class="linenos">1491</span></a>                <span class="n">additional_forward_args</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">num_batches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">),</span>
</span><span id="L-1492"><a href="#L-1492"><span class="linenos">1492</span></a>                <span class="n">perturbations_per_eval</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>  <span class="c1"># to accelerate the computation</span>
</span><span id="L-1493"><a href="#L-1493"><span class="linenos">1493</span></a>            <span class="p">)</span>
</span><span id="L-1494"><a href="#L-1494"><span class="linenos">1494</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</span><span id="L-1495"><a href="#L-1495"><span class="linenos">1495</span></a>
</span><span id="L-1496"><a href="#L-1496"><span class="linenos">1496</span></a>            <span class="n">attribution_abs_batch_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
</span><span id="L-1497"><a href="#L-1497"><span class="linenos">1497</span></a>                <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">attribution</span><span class="p">),</span>
</span><span id="L-1498"><a href="#L-1498"><span class="linenos">1498</span></a>                <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="L-1499"><a href="#L-1499"><span class="linenos">1499</span></a>                    <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">attribution</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="L-1500"><a href="#L-1500"><span class="linenos">1500</span></a>                <span class="p">],</span>  <span class="c1"># average the features over batch samples</span>
</span><span id="L-1501"><a href="#L-1501"><span class="linenos">1501</span></a>            <span class="p">)</span>
</span><span id="L-1502"><a href="#L-1502"><span class="linenos">1502</span></a>
</span><span id="L-1503"><a href="#L-1503"><span class="linenos">1503</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">attribution_abs_batch_mean</span>
</span><span id="L-1504"><a href="#L-1504"><span class="linenos">1504</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="L-1505"><a href="#L-1505"><span class="linenos">1505</span></a>
</span><span id="L-1506"><a href="#L-1506"><span class="linenos">1506</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span><span id="L-1507"><a href="#L-1507"><span class="linenos">1507</span></a>
</span><span id="L-1508"><a href="#L-1508"><span class="linenos">1508</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_lrp_abs</span><span class="p">(</span>
</span><span id="L-1509"><a href="#L-1509"><span class="linenos">1509</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-1510"><a href="#L-1510"><span class="linenos">1510</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-1511"><a href="#L-1511"><span class="linenos">1511</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="L-1512"><a href="#L-1512"><span class="linenos">1512</span></a>        <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-1513"><a href="#L-1513"><span class="linenos">1513</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-1514"><a href="#L-1514"><span class="linenos">1514</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-1515"><a href="#L-1515"><span class="linenos">1515</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-1516"><a href="#L-1516"><span class="linenos">1516</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of [LRP](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140). We implement this using [Layer LRP](https://captum.ai/api/layer.html#layer-lrp) in Captum.</span>
</span><span id="L-1517"><a href="#L-1517"><span class="linenos">1517</span></a>
</span><span id="L-1518"><a href="#L-1518"><span class="linenos">1518</span></a><span class="sd">        **Args:**</span>
</span><span id="L-1519"><a href="#L-1519"><span class="linenos">1519</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="L-1520"><a href="#L-1520"><span class="linenos">1520</span></a><span class="sd">        - **input** (`Tensor` | `tuple[Tensor, ...]`): the input batch of the training step.</span>
</span><span id="L-1521"><a href="#L-1521"><span class="linenos">1521</span></a><span class="sd">        - **target** (`Tensor` | `None`): the target batch of the training step.</span>
</span><span id="L-1522"><a href="#L-1522"><span class="linenos">1522</span></a><span class="sd">        - **batch_idx** (`int`): the index of the current batch. This is an argument of the forward function during training.</span>
</span><span id="L-1523"><a href="#L-1523"><span class="linenos">1523</span></a><span class="sd">        - **num_batches** (`int`): the number of batches in the training step. This is an argument of the forward function during training.</span>
</span><span id="L-1524"><a href="#L-1524"><span class="linenos">1524</span></a>
</span><span id="L-1525"><a href="#L-1525"><span class="linenos">1525</span></a><span class="sd">        **Returns:**</span>
</span><span id="L-1526"><a href="#L-1526"><span class="linenos">1526</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="L-1527"><a href="#L-1527"><span class="linenos">1527</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-1528"><a href="#L-1528"><span class="linenos">1528</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="L-1529"><a href="#L-1529"><span class="linenos">1529</span></a>
</span><span id="L-1530"><a href="#L-1530"><span class="linenos">1530</span></a>        <span class="c1"># initialize the Layer LRP object</span>
</span><span id="L-1531"><a href="#L-1531"><span class="linenos">1531</span></a>        <span class="n">layer_lrp</span> <span class="o">=</span> <span class="n">LayerLRP</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="n">layer</span><span class="p">)</span>
</span><span id="L-1532"><a href="#L-1532"><span class="linenos">1532</span></a>
</span><span id="L-1533"><a href="#L-1533"><span class="linenos">1533</span></a>        <span class="c1"># set model to evaluation mode to prevent updating the model parameters</span>
</span><span id="L-1534"><a href="#L-1534"><span class="linenos">1534</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</span><span id="L-1535"><a href="#L-1535"><span class="linenos">1535</span></a>
</span><span id="L-1536"><a href="#L-1536"><span class="linenos">1536</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</span><span id="L-1537"><a href="#L-1537"><span class="linenos">1537</span></a>        <span class="c1"># calculate layer attribution of the step</span>
</span><span id="L-1538"><a href="#L-1538"><span class="linenos">1538</span></a>        <span class="n">attribution</span> <span class="o">=</span> <span class="n">layer_lrp</span><span class="o">.</span><span class="n">attribute</span><span class="p">(</span>
</span><span id="L-1539"><a href="#L-1539"><span class="linenos">1539</span></a>            <span class="n">inputs</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="L-1540"><a href="#L-1540"><span class="linenos">1540</span></a>            <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="L-1541"><a href="#L-1541"><span class="linenos">1541</span></a>            <span class="n">additional_forward_args</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">num_batches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">),</span>
</span><span id="L-1542"><a href="#L-1542"><span class="linenos">1542</span></a>        <span class="p">)</span>
</span><span id="L-1543"><a href="#L-1543"><span class="linenos">1543</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</span><span id="L-1544"><a href="#L-1544"><span class="linenos">1544</span></a>
</span><span id="L-1545"><a href="#L-1545"><span class="linenos">1545</span></a>        <span class="n">attribution_abs_batch_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
</span><span id="L-1546"><a href="#L-1546"><span class="linenos">1546</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">attribution</span><span class="p">),</span>
</span><span id="L-1547"><a href="#L-1547"><span class="linenos">1547</span></a>            <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="L-1548"><a href="#L-1548"><span class="linenos">1548</span></a>                <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">attribution</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="L-1549"><a href="#L-1549"><span class="linenos">1549</span></a>            <span class="p">],</span>  <span class="c1"># average the features over batch samples</span>
</span><span id="L-1550"><a href="#L-1550"><span class="linenos">1550</span></a>        <span class="p">)</span>
</span><span id="L-1551"><a href="#L-1551"><span class="linenos">1551</span></a>
</span><span id="L-1552"><a href="#L-1552"><span class="linenos">1552</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">attribution_abs_batch_mean</span>
</span><span id="L-1553"><a href="#L-1553"><span class="linenos">1553</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="L-1554"><a href="#L-1554"><span class="linenos">1554</span></a>
</span><span id="L-1555"><a href="#L-1555"><span class="linenos">1555</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span><span id="L-1556"><a href="#L-1556"><span class="linenos">1556</span></a>
</span><span id="L-1557"><a href="#L-1557"><span class="linenos">1557</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_cbp_adaptive_contribution</span><span class="p">(</span>
</span><span id="L-1558"><a href="#L-1558"><span class="linenos">1558</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-1559"><a href="#L-1559"><span class="linenos">1559</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-1560"><a href="#L-1560"><span class="linenos">1560</span></a>        <span class="n">activation</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-1561"><a href="#L-1561"><span class="linenos">1561</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-1562"><a href="#L-1562"><span class="linenos">1562</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the sum of absolute values of layer output weights multiplied by absolute values of activation, then divided by the reciprocal of sum of absolute values of layer input weights. It is equal to the adaptive contribution utility in [CBP](https://www.nature.com/articles/s41586-024-07711-7).</span>
</span><span id="L-1563"><a href="#L-1563"><span class="linenos">1563</span></a>
</span><span id="L-1564"><a href="#L-1564"><span class="linenos">1564</span></a><span class="sd">        **Args:**</span>
</span><span id="L-1565"><a href="#L-1565"><span class="linenos">1565</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="L-1566"><a href="#L-1566"><span class="linenos">1566</span></a><span class="sd">        - **activation** (`Tensor`): the activation tensor of the layer. It has the same size of (number of units, ).</span>
</span><span id="L-1567"><a href="#L-1567"><span class="linenos">1567</span></a>
</span><span id="L-1568"><a href="#L-1568"><span class="linenos">1568</span></a><span class="sd">        **Returns:**</span>
</span><span id="L-1569"><a href="#L-1569"><span class="linenos">1569</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="L-1570"><a href="#L-1570"><span class="linenos">1570</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-1571"><a href="#L-1571"><span class="linenos">1571</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="L-1572"><a href="#L-1572"><span class="linenos">1572</span></a>
</span><span id="L-1573"><a href="#L-1573"><span class="linenos">1573</span></a>        <span class="n">input_weight_abs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span><span id="L-1574"><a href="#L-1574"><span class="linenos">1574</span></a>        <span class="n">input_weight_abs_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
</span><span id="L-1575"><a href="#L-1575"><span class="linenos">1575</span></a>            <span class="n">input_weight_abs</span><span class="p">,</span>
</span><span id="L-1576"><a href="#L-1576"><span class="linenos">1576</span></a>            <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="L-1577"><a href="#L-1577"><span class="linenos">1577</span></a>                <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">input_weight_abs</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">0</span>
</span><span id="L-1578"><a href="#L-1578"><span class="linenos">1578</span></a>            <span class="p">],</span>  <span class="c1"># sum over the input dimension</span>
</span><span id="L-1579"><a href="#L-1579"><span class="linenos">1579</span></a>        <span class="p">)</span>
</span><span id="L-1580"><a href="#L-1580"><span class="linenos">1580</span></a>        <span class="n">input_weight_abs_sum_reciprocal</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">reciprocal</span><span class="p">(</span><span class="n">input_weight_abs_sum</span><span class="p">)</span>
</span><span id="L-1581"><a href="#L-1581"><span class="linenos">1581</span></a>
</span><span id="L-1582"><a href="#L-1582"><span class="linenos">1582</span></a>        <span class="n">output_weight_abs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">next_layer</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span><span id="L-1583"><a href="#L-1583"><span class="linenos">1583</span></a>        <span class="n">output_weight_abs_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
</span><span id="L-1584"><a href="#L-1584"><span class="linenos">1584</span></a>            <span class="n">output_weight_abs</span><span class="p">,</span>
</span><span id="L-1585"><a href="#L-1585"><span class="linenos">1585</span></a>            <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="L-1586"><a href="#L-1586"><span class="linenos">1586</span></a>                <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">output_weight_abs</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="L-1587"><a href="#L-1587"><span class="linenos">1587</span></a>            <span class="p">],</span>  <span class="c1"># sum over the output dimension</span>
</span><span id="L-1588"><a href="#L-1588"><span class="linenos">1588</span></a>        <span class="p">)</span>
</span><span id="L-1589"><a href="#L-1589"><span class="linenos">1589</span></a>
</span><span id="L-1590"><a href="#L-1590"><span class="linenos">1590</span></a>        <span class="n">activation_abs_batch_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
</span><span id="L-1591"><a href="#L-1591"><span class="linenos">1591</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">activation</span><span class="p">),</span>
</span><span id="L-1592"><a href="#L-1592"><span class="linenos">1592</span></a>            <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="L-1593"><a href="#L-1593"><span class="linenos">1593</span></a>                <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">activation</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="L-1594"><a href="#L-1594"><span class="linenos">1594</span></a>            <span class="p">],</span>  <span class="c1"># average the features over batch samples</span>
</span><span id="L-1595"><a href="#L-1595"><span class="linenos">1595</span></a>        <span class="p">)</span>
</span><span id="L-1596"><a href="#L-1596"><span class="linenos">1596</span></a>
</span><span id="L-1597"><a href="#L-1597"><span class="linenos">1597</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-1598"><a href="#L-1598"><span class="linenos">1598</span></a>            <span class="n">output_weight_abs_sum</span>
</span><span id="L-1599"><a href="#L-1599"><span class="linenos">1599</span></a>            <span class="o">*</span> <span class="n">activation_abs_batch_mean</span>
</span><span id="L-1600"><a href="#L-1600"><span class="linenos">1600</span></a>            <span class="o">*</span> <span class="n">input_weight_abs_sum_reciprocal</span>
</span><span id="L-1601"><a href="#L-1601"><span class="linenos">1601</span></a>        <span class="p">)</span>
</span><span id="L-1602"><a href="#L-1602"><span class="linenos">1602</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="L-1603"><a href="#L-1603"><span class="linenos">1603</span></a>
</span><span id="L-1604"><a href="#L-1604"><span class="linenos">1604</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span></pre></div>


            </section>
                <section id="FGAdaHAT">
                            <input id="FGAdaHAT-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">FGAdaHAT</span><wbr>(<span class="base"><a href="adahat.html#AdaHAT">clarena.cl_algorithms.adahat.AdaHAT</a></span>):

                <label class="view-source-button" for="FGAdaHAT-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FGAdaHAT"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FGAdaHAT-37"><a href="#FGAdaHAT-37"><span class="linenos">  37</span></a><span class="k">class</span><span class="w"> </span><span class="nc">FGAdaHAT</span><span class="p">(</span><span class="n">AdaHAT</span><span class="p">):</span>
</span><span id="FGAdaHAT-38"><a href="#FGAdaHAT-38"><span class="linenos">  38</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;FG-AdaHAT (Fine-Grained Adaptive Hard Attention to the Task) algorithm.</span>
</span><span id="FGAdaHAT-39"><a href="#FGAdaHAT-39"><span class="linenos">  39</span></a>
</span><span id="FGAdaHAT-40"><a href="#FGAdaHAT-40"><span class="linenos">  40</span></a><span class="sd">    An architecture-based continual learning approach that improves [AdaHAT (Adaptive Hard Attention to the Task)](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9) by introducing fine-grained neuron-wise importance measures guiding the adaptive adjustment mechanism in AdaHAT.</span>
</span><span id="FGAdaHAT-41"><a href="#FGAdaHAT-41"><span class="linenos">  41</span></a>
</span><span id="FGAdaHAT-42"><a href="#FGAdaHAT-42"><span class="linenos">  42</span></a><span class="sd">    We implement FG-AdaHAT as a subclass of AdaHAT, as it reuses AdaHAT&#39;s summative mask and other components.</span>
</span><span id="FGAdaHAT-43"><a href="#FGAdaHAT-43"><span class="linenos">  43</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT-44"><a href="#FGAdaHAT-44"><span class="linenos">  44</span></a>
</span><span id="FGAdaHAT-45"><a href="#FGAdaHAT-45"><span class="linenos">  45</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="FGAdaHAT-46"><a href="#FGAdaHAT-46"><span class="linenos">  46</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="FGAdaHAT-47"><a href="#FGAdaHAT-47"><span class="linenos">  47</span></a>        <span class="n">backbone</span><span class="p">:</span> <span class="n">HATMaskBackbone</span><span class="p">,</span>
</span><span id="FGAdaHAT-48"><a href="#FGAdaHAT-48"><span class="linenos">  48</span></a>        <span class="n">heads</span><span class="p">:</span> <span class="n">HeadsTIL</span><span class="p">,</span>
</span><span id="FGAdaHAT-49"><a href="#FGAdaHAT-49"><span class="linenos">  49</span></a>        <span class="n">adjustment_intensity</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="FGAdaHAT-50"><a href="#FGAdaHAT-50"><span class="linenos">  50</span></a>        <span class="n">importance_type</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT-51"><a href="#FGAdaHAT-51"><span class="linenos">  51</span></a>        <span class="n">importance_summing_strategy</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT-52"><a href="#FGAdaHAT-52"><span class="linenos">  52</span></a>        <span class="n">importance_scheduler_type</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT-53"><a href="#FGAdaHAT-53"><span class="linenos">  53</span></a>        <span class="n">neuron_to_weight_importance_aggregation_mode</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT-54"><a href="#FGAdaHAT-54"><span class="linenos">  54</span></a>        <span class="n">s_max</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="FGAdaHAT-55"><a href="#FGAdaHAT-55"><span class="linenos">  55</span></a>        <span class="n">clamp_threshold</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="FGAdaHAT-56"><a href="#FGAdaHAT-56"><span class="linenos">  56</span></a>        <span class="n">mask_sparsity_reg_factor</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="FGAdaHAT-57"><a href="#FGAdaHAT-57"><span class="linenos">  57</span></a>        <span class="n">mask_sparsity_reg_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;original&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT-58"><a href="#FGAdaHAT-58"><span class="linenos">  58</span></a>        <span class="n">base_importance</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
</span><span id="FGAdaHAT-59"><a href="#FGAdaHAT-59"><span class="linenos">  59</span></a>        <span class="n">base_mask_sparsity_reg</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
</span><span id="FGAdaHAT-60"><a href="#FGAdaHAT-60"><span class="linenos">  60</span></a>        <span class="n">base_linear</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
</span><span id="FGAdaHAT-61"><a href="#FGAdaHAT-61"><span class="linenos">  61</span></a>        <span class="n">filter_by_cumulative_mask</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="FGAdaHAT-62"><a href="#FGAdaHAT-62"><span class="linenos">  62</span></a>        <span class="n">filter_unmasked_importance</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="FGAdaHAT-63"><a href="#FGAdaHAT-63"><span class="linenos">  63</span></a>        <span class="n">step_multiply_training_mask</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="FGAdaHAT-64"><a href="#FGAdaHAT-64"><span class="linenos">  64</span></a>        <span class="n">task_embedding_init_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;N01&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT-65"><a href="#FGAdaHAT-65"><span class="linenos">  65</span></a>        <span class="n">importance_summing_strategy_linear_step</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FGAdaHAT-66"><a href="#FGAdaHAT-66"><span class="linenos">  66</span></a>        <span class="n">importance_summing_strategy_exponential_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FGAdaHAT-67"><a href="#FGAdaHAT-67"><span class="linenos">  67</span></a>        <span class="n">importance_summing_strategy_log_base</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FGAdaHAT-68"><a href="#FGAdaHAT-68"><span class="linenos">  68</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FGAdaHAT-69"><a href="#FGAdaHAT-69"><span class="linenos">  69</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialize the FG-AdaHAT algorithm with the network.</span>
</span><span id="FGAdaHAT-70"><a href="#FGAdaHAT-70"><span class="linenos">  70</span></a>
</span><span id="FGAdaHAT-71"><a href="#FGAdaHAT-71"><span class="linenos">  71</span></a><span class="sd">        **Args:**</span>
</span><span id="FGAdaHAT-72"><a href="#FGAdaHAT-72"><span class="linenos">  72</span></a><span class="sd">        - **backbone** (`HATMaskBackbone`): must be a backbone network with the HAT mask mechanism.</span>
</span><span id="FGAdaHAT-73"><a href="#FGAdaHAT-73"><span class="linenos">  73</span></a><span class="sd">        - **heads** (`HeadsTIL`): output heads. FG-AdaHAT supports only TIL (Task-Incremental Learning).</span>
</span><span id="FGAdaHAT-74"><a href="#FGAdaHAT-74"><span class="linenos">  74</span></a><span class="sd">        - **adjustment_intensity** (`float`): hyperparameter, controls the overall intensity of gradient adjustment (the $\alpha$ in the paper).</span>
</span><span id="FGAdaHAT-75"><a href="#FGAdaHAT-75"><span class="linenos">  75</span></a><span class="sd">        - **importance_type** (`str`): the type of neuron-wise importance, must be one of:</span>
</span><span id="FGAdaHAT-76"><a href="#FGAdaHAT-76"><span class="linenos">  76</span></a><span class="sd">            1. &#39;input_weight_abs_sum&#39;: sum of absolute input weights;</span>
</span><span id="FGAdaHAT-77"><a href="#FGAdaHAT-77"><span class="linenos">  77</span></a><span class="sd">            2. &#39;output_weight_abs_sum&#39;: sum of absolute output weights;</span>
</span><span id="FGAdaHAT-78"><a href="#FGAdaHAT-78"><span class="linenos">  78</span></a><span class="sd">            3. &#39;input_weight_gradient_abs_sum&#39;: sum of absolute gradients of the input weights (Input Gradients (IG) in the paper);</span>
</span><span id="FGAdaHAT-79"><a href="#FGAdaHAT-79"><span class="linenos">  79</span></a><span class="sd">            4. &#39;output_weight_gradient_abs_sum&#39;: sum of absolute gradients of the output weights (Output Gradients (OG) in the paper);</span>
</span><span id="FGAdaHAT-80"><a href="#FGAdaHAT-80"><span class="linenos">  80</span></a><span class="sd">            5. &#39;activation_abs&#39;: absolute activation;</span>
</span><span id="FGAdaHAT-81"><a href="#FGAdaHAT-81"><span class="linenos">  81</span></a><span class="sd">            6. &#39;input_weight_abs_sum_x_activation_abs&#39;: sum of absolute input weights multiplied by absolute activation (Input Contribution Utility (ICU) in the paper);</span>
</span><span id="FGAdaHAT-82"><a href="#FGAdaHAT-82"><span class="linenos">  82</span></a><span class="sd">            7. &#39;output_weight_abs_sum_x_activation_abs&#39;: sum of absolute output weights multiplied by absolute activation (Contribution Utility (CU) in the paper);</span>
</span><span id="FGAdaHAT-83"><a href="#FGAdaHAT-83"><span class="linenos">  83</span></a><span class="sd">            8. &#39;gradient_x_activation_abs&#39;: absolute gradient (the saliency) multiplied by activation;</span>
</span><span id="FGAdaHAT-84"><a href="#FGAdaHAT-84"><span class="linenos">  84</span></a><span class="sd">            9. &#39;input_weight_gradient_square_sum&#39;: sum of squared gradients of the input weights;</span>
</span><span id="FGAdaHAT-85"><a href="#FGAdaHAT-85"><span class="linenos">  85</span></a><span class="sd">            10. &#39;output_weight_gradient_square_sum&#39;: sum of squared gradients of the output weights;</span>
</span><span id="FGAdaHAT-86"><a href="#FGAdaHAT-86"><span class="linenos">  86</span></a><span class="sd">            11. &#39;input_weight_gradient_square_sum_x_activation_abs&#39;: sum of squared gradients of the input weights multiplied by absolute activation (Activation Fisher Information (AFI) in the paper);</span>
</span><span id="FGAdaHAT-87"><a href="#FGAdaHAT-87"><span class="linenos">  87</span></a><span class="sd">            12. &#39;output_weight_gradient_square_sum_x_activation_abs&#39;: sum of squared gradients of the output weights multiplied by absolute activation;</span>
</span><span id="FGAdaHAT-88"><a href="#FGAdaHAT-88"><span class="linenos">  88</span></a><span class="sd">            13. &#39;conductance_abs&#39;: absolute layer conductance;</span>
</span><span id="FGAdaHAT-89"><a href="#FGAdaHAT-89"><span class="linenos">  89</span></a><span class="sd">            14. &#39;internal_influence_abs&#39;: absolute internal influence (Internal Influence (II) in the paper);</span>
</span><span id="FGAdaHAT-90"><a href="#FGAdaHAT-90"><span class="linenos">  90</span></a><span class="sd">            15. &#39;gradcam_abs&#39;: absolute Grad-CAM;</span>
</span><span id="FGAdaHAT-91"><a href="#FGAdaHAT-91"><span class="linenos">  91</span></a><span class="sd">            16. &#39;deeplift_abs&#39;: absolute DeepLIFT (DeepLIFT (DL) in the paper);</span>
</span><span id="FGAdaHAT-92"><a href="#FGAdaHAT-92"><span class="linenos">  92</span></a><span class="sd">            17. &#39;deepliftshap_abs&#39;: absolute DeepLIFT-SHAP;</span>
</span><span id="FGAdaHAT-93"><a href="#FGAdaHAT-93"><span class="linenos">  93</span></a><span class="sd">            18. &#39;gradientshap_abs&#39;: absolute Gradient-SHAP (Gradient SHAP (GS) in the paper);</span>
</span><span id="FGAdaHAT-94"><a href="#FGAdaHAT-94"><span class="linenos">  94</span></a><span class="sd">            19. &#39;integrated_gradients_abs&#39;: absolute Integrated Gradients;</span>
</span><span id="FGAdaHAT-95"><a href="#FGAdaHAT-95"><span class="linenos">  95</span></a><span class="sd">            20. &#39;feature_ablation_abs&#39;: absolute Feature Ablation (Feature Ablation (FA) in the paper);</span>
</span><span id="FGAdaHAT-96"><a href="#FGAdaHAT-96"><span class="linenos">  96</span></a><span class="sd">            21. &#39;lrp_abs&#39;: absolute Layer-wise Relevance Propagation (LRP);</span>
</span><span id="FGAdaHAT-97"><a href="#FGAdaHAT-97"><span class="linenos">  97</span></a><span class="sd">            22. &#39;cbp_adaptation&#39;: the adaptation function in [Continual Backpropagation (CBP)](https://www.nature.com/articles/s41586-024-07711-7);</span>
</span><span id="FGAdaHAT-98"><a href="#FGAdaHAT-98"><span class="linenos">  98</span></a><span class="sd">            23. &#39;cbp_adaptive_contribution&#39;: the adaptive contribution function in [Continual Backpropagation (CBP)](https://www.nature.com/articles/s41586-024-07711-7);</span>
</span><span id="FGAdaHAT-99"><a href="#FGAdaHAT-99"><span class="linenos">  99</span></a><span class="sd">        - **importance_summing_strategy** (`str`): the strategy to sum neuron-wise importance for previous tasks, must be one of:</span>
</span><span id="FGAdaHAT-100"><a href="#FGAdaHAT-100"><span class="linenos"> 100</span></a><span class="sd">            1. &#39;add_latest&#39;: add the latest neuron-wise importance to the summative importance;</span>
</span><span id="FGAdaHAT-101"><a href="#FGAdaHAT-101"><span class="linenos"> 101</span></a><span class="sd">            2. &#39;add_all&#39;: add all previous neuron-wise importance (including the latest) to the summative importance;</span>
</span><span id="FGAdaHAT-102"><a href="#FGAdaHAT-102"><span class="linenos"> 102</span></a><span class="sd">            3. &#39;add_average&#39;: add the average of all previous neuron-wise importance (including the latest) to the summative importance;</span>
</span><span id="FGAdaHAT-103"><a href="#FGAdaHAT-103"><span class="linenos"> 103</span></a><span class="sd">            4. &#39;linear_decrease&#39;: weigh the previous neuron-wise importance by a linear factor that decreases with the task ID;</span>
</span><span id="FGAdaHAT-104"><a href="#FGAdaHAT-104"><span class="linenos"> 104</span></a><span class="sd">            5. &#39;quadratic_decrease&#39;: weigh the previous neuron-wise importance that decreases quadratically with the task ID;</span>
</span><span id="FGAdaHAT-105"><a href="#FGAdaHAT-105"><span class="linenos"> 105</span></a><span class="sd">            6. &#39;cubic_decrease&#39;: weigh the previous neuron-wise importance that decreases cubically with the task ID;</span>
</span><span id="FGAdaHAT-106"><a href="#FGAdaHAT-106"><span class="linenos"> 106</span></a><span class="sd">            7. &#39;exponential_decrease&#39;: weigh the previous neuron-wise importance by an exponential factor that decreases with the task ID;</span>
</span><span id="FGAdaHAT-107"><a href="#FGAdaHAT-107"><span class="linenos"> 107</span></a><span class="sd">            8. &#39;log_decrease&#39;: weigh the previous neuron-wise importance by a logarithmic factor that decreases with the task ID;</span>
</span><span id="FGAdaHAT-108"><a href="#FGAdaHAT-108"><span class="linenos"> 108</span></a><span class="sd">            9. &#39;factorial_decrease&#39;: weigh the previous neuron-wise importance that decreases factorially with the task ID;</span>
</span><span id="FGAdaHAT-109"><a href="#FGAdaHAT-109"><span class="linenos"> 109</span></a><span class="sd">        - **importance_scheduler_type** (`str`): the scheduler for importance, i.e., the factor $c^t$ multiplied to parameter importance. Must be one of:</span>
</span><span id="FGAdaHAT-110"><a href="#FGAdaHAT-110"><span class="linenos"> 110</span></a><span class="sd">            1. &#39;linear_sparsity_reg&#39;: $c^t = (t+b_L) \cdot [R(M^t, M^{&lt;t}) + b_R]$, where $R(M^t, M^{&lt;t})$ is the mask sparsity regularization betwwen the current task and previous tasks, $b_L$ is the base linear factor (see argument `base_linear`), and $b_R$ is the base mask sparsity regularization factor (see argument `base_mask_sparsity_reg`);</span>
</span><span id="FGAdaHAT-111"><a href="#FGAdaHAT-111"><span class="linenos"> 111</span></a><span class="sd">            2. &#39;sparsity_reg&#39;: $c^t = [R(M^t, M^{&lt;t}) + b_R]$;</span>
</span><span id="FGAdaHAT-112"><a href="#FGAdaHAT-112"><span class="linenos"> 112</span></a><span class="sd">            3. &#39;summative_mask_sparsity_reg&#39;: $c^t_{l,ij} = \left(\min \left(m^{&lt;t, \text{sum}}_{l,i}, m^{&lt;t, \text{sum}}_{l-1,j}\right)+b_L\right) \cdot [R(M^t, M^{&lt;t}) + b_R]$.</span>
</span><span id="FGAdaHAT-113"><a href="#FGAdaHAT-113"><span class="linenos"> 113</span></a><span class="sd">        - **neuron_to_weight_importance_aggregation_mode** (`str`): aggregation mode from neuron-wise to weight-wise importance ($\text{Agg}(\cdot)$ in the paper), must be one of:</span>
</span><span id="FGAdaHAT-114"><a href="#FGAdaHAT-114"><span class="linenos"> 114</span></a><span class="sd">            1. &#39;min&#39;: take the minimum of neuron-wise importance for each weight;</span>
</span><span id="FGAdaHAT-115"><a href="#FGAdaHAT-115"><span class="linenos"> 115</span></a><span class="sd">            2. &#39;max&#39;: take the maximum of neuron-wise importance for each weight;</span>
</span><span id="FGAdaHAT-116"><a href="#FGAdaHAT-116"><span class="linenos"> 116</span></a><span class="sd">            3. &#39;mean&#39;: take the mean of neuron-wise importance for each weight.</span>
</span><span id="FGAdaHAT-117"><a href="#FGAdaHAT-117"><span class="linenos"> 117</span></a><span class="sd">        - **s_max** (`float`): hyperparameter, the maximum scaling factor in the gate function. See Sec. 2.4 &quot;Hard Attention Training&quot; in the [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="FGAdaHAT-118"><a href="#FGAdaHAT-118"><span class="linenos"> 118</span></a><span class="sd">        - **clamp_threshold** (`float`): the threshold for task embedding gradient compensation. See Sec. 2.5 &quot;Embedding Gradient Compensation&quot; in the [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="FGAdaHAT-119"><a href="#FGAdaHAT-119"><span class="linenos"> 119</span></a><span class="sd">        - **mask_sparsity_reg_factor** (`float`): hyperparameter, the regularization factor for mask sparsity.</span>
</span><span id="FGAdaHAT-120"><a href="#FGAdaHAT-120"><span class="linenos"> 120</span></a><span class="sd">        - **mask_sparsity_reg_mode** (`str`): the mode of mask sparsity regularization, must be one of:</span>
</span><span id="FGAdaHAT-121"><a href="#FGAdaHAT-121"><span class="linenos"> 121</span></a><span class="sd">            1. &#39;original&#39; (default): the original mask sparsity regularization in the [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="FGAdaHAT-122"><a href="#FGAdaHAT-122"><span class="linenos"> 122</span></a><span class="sd">            2. &#39;cross&#39;: the cross version of mask sparsity regularization.</span>
</span><span id="FGAdaHAT-123"><a href="#FGAdaHAT-123"><span class="linenos"> 123</span></a><span class="sd">        - **base_importance** (`float`): base value added to importance ($b_I$ in the paper). Default: 0.01.</span>
</span><span id="FGAdaHAT-124"><a href="#FGAdaHAT-124"><span class="linenos"> 124</span></a><span class="sd">        - **base_mask_sparsity_reg** (`float`): base value added to mask sparsity regularization factor in the importance scheduler ($b_R$ in the paper). Default: 0.1.</span>
</span><span id="FGAdaHAT-125"><a href="#FGAdaHAT-125"><span class="linenos"> 125</span></a><span class="sd">        - **base_linear** (`float`): base value added to the linear factor in the importance scheduler ($b_L$ in the paper). Default: 10.</span>
</span><span id="FGAdaHAT-126"><a href="#FGAdaHAT-126"><span class="linenos"> 126</span></a><span class="sd">        - **filter_by_cumulative_mask** (`bool`): whether to multiply the cumulative mask to the importance when calculating adjustment rate. Default: False.</span>
</span><span id="FGAdaHAT-127"><a href="#FGAdaHAT-127"><span class="linenos"> 127</span></a><span class="sd">        - **filter_unmasked_importance** (`bool`): whether to filter unmasked importance values (set to 0) at the end of task training. Default: False.</span>
</span><span id="FGAdaHAT-128"><a href="#FGAdaHAT-128"><span class="linenos"> 128</span></a><span class="sd">        - **step_multiply_training_mask** (`bool`): whether to multiply the training mask to the importance at each training step. Default: True.</span>
</span><span id="FGAdaHAT-129"><a href="#FGAdaHAT-129"><span class="linenos"> 129</span></a><span class="sd">        - **task_embedding_init_mode** (`str`): the initialization mode for task embeddings, must be one of:</span>
</span><span id="FGAdaHAT-130"><a href="#FGAdaHAT-130"><span class="linenos"> 130</span></a><span class="sd">            1. &#39;N01&#39; (default): standard normal distribution $N(0, 1)$.</span>
</span><span id="FGAdaHAT-131"><a href="#FGAdaHAT-131"><span class="linenos"> 131</span></a><span class="sd">            2. &#39;U-11&#39;: uniform distribution $U(-1, 1)$.</span>
</span><span id="FGAdaHAT-132"><a href="#FGAdaHAT-132"><span class="linenos"> 132</span></a><span class="sd">            3. &#39;U01&#39;: uniform distribution $U(0, 1)$.</span>
</span><span id="FGAdaHAT-133"><a href="#FGAdaHAT-133"><span class="linenos"> 133</span></a><span class="sd">            4. &#39;U-10&#39;: uniform distribution $U(-1, 0)$.</span>
</span><span id="FGAdaHAT-134"><a href="#FGAdaHAT-134"><span class="linenos"> 134</span></a><span class="sd">            5. &#39;last&#39;: inherit the task embedding from the last task.</span>
</span><span id="FGAdaHAT-135"><a href="#FGAdaHAT-135"><span class="linenos"> 135</span></a><span class="sd">        - **importance_summing_strategy_linear_step** (`float` | `None`): linear step for the importance summing strategy (used when `importance_summing_strategy` is &#39;linear_decrease&#39;). Must be &gt; 0.</span>
</span><span id="FGAdaHAT-136"><a href="#FGAdaHAT-136"><span class="linenos"> 136</span></a><span class="sd">        - **importance_summing_strategy_exponential_rate** (`float` | `None`): exponential rate for the importance summing strategy (used when `importance_summing_strategy` is &#39;exponential_decrease&#39;). Must be &gt; 1.</span>
</span><span id="FGAdaHAT-137"><a href="#FGAdaHAT-137"><span class="linenos"> 137</span></a><span class="sd">        - **importance_summing_strategy_log_base** (`float` | `None`): base for the logarithm in the importance summing strategy (used when `importance_summing_strategy` is &#39;log_decrease&#39;). Must be &gt; 1.</span>
</span><span id="FGAdaHAT-138"><a href="#FGAdaHAT-138"><span class="linenos"> 138</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT-139"><a href="#FGAdaHAT-139"><span class="linenos"> 139</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="FGAdaHAT-140"><a href="#FGAdaHAT-140"><span class="linenos"> 140</span></a>            <span class="n">backbone</span><span class="o">=</span><span class="n">backbone</span><span class="p">,</span>
</span><span id="FGAdaHAT-141"><a href="#FGAdaHAT-141"><span class="linenos"> 141</span></a>            <span class="n">heads</span><span class="o">=</span><span class="n">heads</span><span class="p">,</span>
</span><span id="FGAdaHAT-142"><a href="#FGAdaHAT-142"><span class="linenos"> 142</span></a>            <span class="n">adjustment_mode</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="c1"># use the own adjustment mechanism of FG-AdaHAT</span>
</span><span id="FGAdaHAT-143"><a href="#FGAdaHAT-143"><span class="linenos"> 143</span></a>            <span class="n">adjustment_intensity</span><span class="o">=</span><span class="n">adjustment_intensity</span><span class="p">,</span>
</span><span id="FGAdaHAT-144"><a href="#FGAdaHAT-144"><span class="linenos"> 144</span></a>            <span class="n">s_max</span><span class="o">=</span><span class="n">s_max</span><span class="p">,</span>
</span><span id="FGAdaHAT-145"><a href="#FGAdaHAT-145"><span class="linenos"> 145</span></a>            <span class="n">clamp_threshold</span><span class="o">=</span><span class="n">clamp_threshold</span><span class="p">,</span>
</span><span id="FGAdaHAT-146"><a href="#FGAdaHAT-146"><span class="linenos"> 146</span></a>            <span class="n">mask_sparsity_reg_factor</span><span class="o">=</span><span class="n">mask_sparsity_reg_factor</span><span class="p">,</span>
</span><span id="FGAdaHAT-147"><a href="#FGAdaHAT-147"><span class="linenos"> 147</span></a>            <span class="n">mask_sparsity_reg_mode</span><span class="o">=</span><span class="n">mask_sparsity_reg_mode</span><span class="p">,</span>
</span><span id="FGAdaHAT-148"><a href="#FGAdaHAT-148"><span class="linenos"> 148</span></a>            <span class="n">task_embedding_init_mode</span><span class="o">=</span><span class="n">task_embedding_init_mode</span><span class="p">,</span>
</span><span id="FGAdaHAT-149"><a href="#FGAdaHAT-149"><span class="linenos"> 149</span></a>            <span class="n">epsilon</span><span class="o">=</span><span class="n">base_mask_sparsity_reg</span><span class="p">,</span>  <span class="c1"># the epsilon is now the base mask sparsity regularization factor</span>
</span><span id="FGAdaHAT-150"><a href="#FGAdaHAT-150"><span class="linenos"> 150</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT-151"><a href="#FGAdaHAT-151"><span class="linenos"> 151</span></a>
</span><span id="FGAdaHAT-152"><a href="#FGAdaHAT-152"><span class="linenos"> 152</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">importance_type</span>
</span><span id="FGAdaHAT-153"><a href="#FGAdaHAT-153"><span class="linenos"> 153</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The type of the neuron-wise importance added to AdaHAT importance.&quot;&quot;&quot;</span>
</span><span id="FGAdaHAT-154"><a href="#FGAdaHAT-154"><span class="linenos"> 154</span></a>
</span><span id="FGAdaHAT-155"><a href="#FGAdaHAT-155"><span class="linenos"> 155</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">importance_scheduler_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">importance_scheduler_type</span>
</span><span id="FGAdaHAT-156"><a href="#FGAdaHAT-156"><span class="linenos"> 156</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the type of the importance scheduler.&quot;&quot;&quot;</span>
</span><span id="FGAdaHAT-157"><a href="#FGAdaHAT-157"><span class="linenos"> 157</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">neuron_to_weight_importance_aggregation_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FGAdaHAT-158"><a href="#FGAdaHAT-158"><span class="linenos"> 158</span></a>            <span class="n">neuron_to_weight_importance_aggregation_mode</span>
</span><span id="FGAdaHAT-159"><a href="#FGAdaHAT-159"><span class="linenos"> 159</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT-160"><a href="#FGAdaHAT-160"><span class="linenos"> 160</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The mode of aggregation from neuron-wise to weight-wise importance. &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT-161"><a href="#FGAdaHAT-161"><span class="linenos"> 161</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">filter_by_cumulative_mask</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">filter_by_cumulative_mask</span>
</span><span id="FGAdaHAT-162"><a href="#FGAdaHAT-162"><span class="linenos"> 162</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The flag to filter importance by the cumulative mask when calculating the adjustment rate.&quot;&quot;&quot;</span>
</span><span id="FGAdaHAT-163"><a href="#FGAdaHAT-163"><span class="linenos"> 163</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">filter_unmasked_importance</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">filter_unmasked_importance</span>
</span><span id="FGAdaHAT-164"><a href="#FGAdaHAT-164"><span class="linenos"> 164</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The flag to filter unmasked importance values (set them to 0) at the end of task training.&quot;&quot;&quot;</span>
</span><span id="FGAdaHAT-165"><a href="#FGAdaHAT-165"><span class="linenos"> 165</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">step_multiply_training_mask</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">step_multiply_training_mask</span>
</span><span id="FGAdaHAT-166"><a href="#FGAdaHAT-166"><span class="linenos"> 166</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The flag to multiply the training mask to the importance at each training step.&quot;&quot;&quot;</span>
</span><span id="FGAdaHAT-167"><a href="#FGAdaHAT-167"><span class="linenos"> 167</span></a>
</span><span id="FGAdaHAT-168"><a href="#FGAdaHAT-168"><span class="linenos"> 168</span></a>        <span class="c1"># importance summing strategy</span>
</span><span id="FGAdaHAT-169"><a href="#FGAdaHAT-169"><span class="linenos"> 169</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">importance_summing_strategy</span>
</span><span id="FGAdaHAT-170"><a href="#FGAdaHAT-170"><span class="linenos"> 170</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The strategy to sum the neuron-wise importance for previous tasks.&quot;&quot;&quot;</span>
</span><span id="FGAdaHAT-171"><a href="#FGAdaHAT-171"><span class="linenos"> 171</span></a>        <span class="k">if</span> <span class="n">importance_summing_strategy_linear_step</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FGAdaHAT-172"><a href="#FGAdaHAT-172"><span class="linenos"> 172</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy_linear_step</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FGAdaHAT-173"><a href="#FGAdaHAT-173"><span class="linenos"> 173</span></a>                <span class="n">importance_summing_strategy_linear_step</span>
</span><span id="FGAdaHAT-174"><a href="#FGAdaHAT-174"><span class="linenos"> 174</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT-175"><a href="#FGAdaHAT-175"><span class="linenos"> 175</span></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The linear step for the importance summing strategy (only when `importance_summing_strategy` is &#39;linear_decrease&#39;).&quot;&quot;&quot;</span>
</span><span id="FGAdaHAT-176"><a href="#FGAdaHAT-176"><span class="linenos"> 176</span></a>        <span class="k">if</span> <span class="n">importance_summing_strategy_exponential_rate</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FGAdaHAT-177"><a href="#FGAdaHAT-177"><span class="linenos"> 177</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy_exponential_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FGAdaHAT-178"><a href="#FGAdaHAT-178"><span class="linenos"> 178</span></a>                <span class="n">importance_summing_strategy_exponential_rate</span>
</span><span id="FGAdaHAT-179"><a href="#FGAdaHAT-179"><span class="linenos"> 179</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT-180"><a href="#FGAdaHAT-180"><span class="linenos"> 180</span></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The exponential rate for the importance summing strategy (only when `importance_summing_strategy` is &#39;exponential_decrease&#39;). &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT-181"><a href="#FGAdaHAT-181"><span class="linenos"> 181</span></a>        <span class="k">if</span> <span class="n">importance_summing_strategy_log_base</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FGAdaHAT-182"><a href="#FGAdaHAT-182"><span class="linenos"> 182</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy_log_base</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FGAdaHAT-183"><a href="#FGAdaHAT-183"><span class="linenos"> 183</span></a>                <span class="n">importance_summing_strategy_log_base</span>
</span><span id="FGAdaHAT-184"><a href="#FGAdaHAT-184"><span class="linenos"> 184</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT-185"><a href="#FGAdaHAT-185"><span class="linenos"> 185</span></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The base for the logarithm in the importance summing strategy (only when `importance_summing_strategy` is &#39;log_decrease&#39;). &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT-186"><a href="#FGAdaHAT-186"><span class="linenos"> 186</span></a>
</span><span id="FGAdaHAT-187"><a href="#FGAdaHAT-187"><span class="linenos"> 187</span></a>        <span class="c1"># base values</span>
</span><span id="FGAdaHAT-188"><a href="#FGAdaHAT-188"><span class="linenos"> 188</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">base_importance</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">base_importance</span>
</span><span id="FGAdaHAT-189"><a href="#FGAdaHAT-189"><span class="linenos"> 189</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The base value added to the importance to avoid zero. &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT-190"><a href="#FGAdaHAT-190"><span class="linenos"> 190</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">base_mask_sparsity_reg</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">base_mask_sparsity_reg</span>
</span><span id="FGAdaHAT-191"><a href="#FGAdaHAT-191"><span class="linenos"> 191</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The base value added to the mask sparsity regularization to avoid zero. &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT-192"><a href="#FGAdaHAT-192"><span class="linenos"> 192</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">base_linear</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">base_linear</span>
</span><span id="FGAdaHAT-193"><a href="#FGAdaHAT-193"><span class="linenos"> 193</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The base value added to the linear layer to avoid zero. &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT-194"><a href="#FGAdaHAT-194"><span class="linenos"> 194</span></a>
</span><span id="FGAdaHAT-195"><a href="#FGAdaHAT-195"><span class="linenos"> 195</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">importances</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="FGAdaHAT-196"><a href="#FGAdaHAT-196"><span class="linenos"> 196</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The min-max scaled ($[0, 1]$) neuron-wise importance of units. It is $I^{\tau}_{l}$ in the paper. Keys are task IDs and values are the corresponding importance tensors. Each importance tensor is a dict where keys are layer names and values are the importance tensor for the layer. The utility tensor is the same size as the feature tensor with size (number of units, ). &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT-197"><a href="#FGAdaHAT-197"><span class="linenos"> 197</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">summative_importance_for_previous_tasks</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="FGAdaHAT-198"><a href="#FGAdaHAT-198"><span class="linenos"> 198</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The summative neuron-wise importance values of units for previous tasks before the current task `self.task_id`. See $I^{&lt;t}_{l}$ in the paper. Keys are layer names and values are the summative importance tensor for the layer. The summative importance tensor has the same size as the feature tensor with size (number of units, ). &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT-199"><a href="#FGAdaHAT-199"><span class="linenos"> 199</span></a>
</span><span id="FGAdaHAT-200"><a href="#FGAdaHAT-200"><span class="linenos"> 200</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">num_steps_t</span><span class="p">:</span> <span class="nb">int</span>
</span><span id="FGAdaHAT-201"><a href="#FGAdaHAT-201"><span class="linenos"> 201</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the number of training steps for the current task `self.task_id`.&quot;&quot;&quot;</span>
</span><span id="FGAdaHAT-202"><a href="#FGAdaHAT-202"><span class="linenos"> 202</span></a>        <span class="c1"># set manual optimization</span>
</span><span id="FGAdaHAT-203"><a href="#FGAdaHAT-203"><span class="linenos"> 203</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">automatic_optimization</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="FGAdaHAT-204"><a href="#FGAdaHAT-204"><span class="linenos"> 204</span></a>
</span><span id="FGAdaHAT-205"><a href="#FGAdaHAT-205"><span class="linenos"> 205</span></a>        <span class="n">FGAdaHAT</span><span class="o">.</span><span class="n">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</span><span id="FGAdaHAT-206"><a href="#FGAdaHAT-206"><span class="linenos"> 206</span></a>
</span><span id="FGAdaHAT-207"><a href="#FGAdaHAT-207"><span class="linenos"> 207</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FGAdaHAT-208"><a href="#FGAdaHAT-208"><span class="linenos"> 208</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Check the sanity of the arguments.&quot;&quot;&quot;</span>
</span><span id="FGAdaHAT-209"><a href="#FGAdaHAT-209"><span class="linenos"> 209</span></a>
</span><span id="FGAdaHAT-210"><a href="#FGAdaHAT-210"><span class="linenos"> 210</span></a>        <span class="c1"># check importance type</span>
</span><span id="FGAdaHAT-211"><a href="#FGAdaHAT-211"><span class="linenos"> 211</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span>
</span><span id="FGAdaHAT-212"><a href="#FGAdaHAT-212"><span class="linenos"> 212</span></a>            <span class="s2">&quot;input_weight_abs_sum&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT-213"><a href="#FGAdaHAT-213"><span class="linenos"> 213</span></a>            <span class="s2">&quot;output_weight_abs_sum&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT-214"><a href="#FGAdaHAT-214"><span class="linenos"> 214</span></a>            <span class="s2">&quot;input_weight_gradient_abs_sum&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT-215"><a href="#FGAdaHAT-215"><span class="linenos"> 215</span></a>            <span class="s2">&quot;output_weight_gradient_abs_sum&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT-216"><a href="#FGAdaHAT-216"><span class="linenos"> 216</span></a>            <span class="s2">&quot;activation_abs&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT-217"><a href="#FGAdaHAT-217"><span class="linenos"> 217</span></a>            <span class="s2">&quot;input_weight_abs_sum_x_activation_abs&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT-218"><a href="#FGAdaHAT-218"><span class="linenos"> 218</span></a>            <span class="s2">&quot;output_weight_abs_sum_x_activation_abs&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT-219"><a href="#FGAdaHAT-219"><span class="linenos"> 219</span></a>            <span class="s2">&quot;gradient_x_activation_abs&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT-220"><a href="#FGAdaHAT-220"><span class="linenos"> 220</span></a>            <span class="s2">&quot;input_weight_gradient_square_sum&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT-221"><a href="#FGAdaHAT-221"><span class="linenos"> 221</span></a>            <span class="s2">&quot;output_weight_gradient_square_sum&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT-222"><a href="#FGAdaHAT-222"><span class="linenos"> 222</span></a>            <span class="s2">&quot;input_weight_gradient_square_sum_x_activation_abs&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT-223"><a href="#FGAdaHAT-223"><span class="linenos"> 223</span></a>            <span class="s2">&quot;output_weight_gradient_square_sum_x_activation_abs&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT-224"><a href="#FGAdaHAT-224"><span class="linenos"> 224</span></a>            <span class="s2">&quot;conductance_abs&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT-225"><a href="#FGAdaHAT-225"><span class="linenos"> 225</span></a>            <span class="s2">&quot;internal_influence_abs&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT-226"><a href="#FGAdaHAT-226"><span class="linenos"> 226</span></a>            <span class="s2">&quot;gradcam_abs&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT-227"><a href="#FGAdaHAT-227"><span class="linenos"> 227</span></a>            <span class="s2">&quot;deeplift_abs&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT-228"><a href="#FGAdaHAT-228"><span class="linenos"> 228</span></a>            <span class="s2">&quot;deepliftshap_abs&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT-229"><a href="#FGAdaHAT-229"><span class="linenos"> 229</span></a>            <span class="s2">&quot;gradientshap_abs&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT-230"><a href="#FGAdaHAT-230"><span class="linenos"> 230</span></a>            <span class="s2">&quot;integrated_gradients_abs&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT-231"><a href="#FGAdaHAT-231"><span class="linenos"> 231</span></a>            <span class="s2">&quot;feature_ablation_abs&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT-232"><a href="#FGAdaHAT-232"><span class="linenos"> 232</span></a>            <span class="s2">&quot;lrp_abs&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT-233"><a href="#FGAdaHAT-233"><span class="linenos"> 233</span></a>            <span class="s2">&quot;cbp_adaptation&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT-234"><a href="#FGAdaHAT-234"><span class="linenos"> 234</span></a>            <span class="s2">&quot;cbp_adaptive_contribution&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT-235"><a href="#FGAdaHAT-235"><span class="linenos"> 235</span></a>        <span class="p">]:</span>
</span><span id="FGAdaHAT-236"><a href="#FGAdaHAT-236"><span class="linenos"> 236</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="FGAdaHAT-237"><a href="#FGAdaHAT-237"><span class="linenos"> 237</span></a>                <span class="sa">f</span><span class="s2">&quot;importance_type must be one of the predefined types, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="FGAdaHAT-238"><a href="#FGAdaHAT-238"><span class="linenos"> 238</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT-239"><a href="#FGAdaHAT-239"><span class="linenos"> 239</span></a>
</span><span id="FGAdaHAT-240"><a href="#FGAdaHAT-240"><span class="linenos"> 240</span></a>        <span class="c1"># check importance summing strategy</span>
</span><span id="FGAdaHAT-241"><a href="#FGAdaHAT-241"><span class="linenos"> 241</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span>
</span><span id="FGAdaHAT-242"><a href="#FGAdaHAT-242"><span class="linenos"> 242</span></a>            <span class="s2">&quot;add_latest&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT-243"><a href="#FGAdaHAT-243"><span class="linenos"> 243</span></a>            <span class="s2">&quot;add_all&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT-244"><a href="#FGAdaHAT-244"><span class="linenos"> 244</span></a>            <span class="s2">&quot;add_average&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT-245"><a href="#FGAdaHAT-245"><span class="linenos"> 245</span></a>            <span class="s2">&quot;linear_decrease&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT-246"><a href="#FGAdaHAT-246"><span class="linenos"> 246</span></a>            <span class="s2">&quot;quadratic_decrease&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT-247"><a href="#FGAdaHAT-247"><span class="linenos"> 247</span></a>            <span class="s2">&quot;cubic_decrease&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT-248"><a href="#FGAdaHAT-248"><span class="linenos"> 248</span></a>            <span class="s2">&quot;exponential_decrease&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT-249"><a href="#FGAdaHAT-249"><span class="linenos"> 249</span></a>            <span class="s2">&quot;log_decrease&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT-250"><a href="#FGAdaHAT-250"><span class="linenos"> 250</span></a>            <span class="s2">&quot;factorial_decrease&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT-251"><a href="#FGAdaHAT-251"><span class="linenos"> 251</span></a>        <span class="p">]:</span>
</span><span id="FGAdaHAT-252"><a href="#FGAdaHAT-252"><span class="linenos"> 252</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="FGAdaHAT-253"><a href="#FGAdaHAT-253"><span class="linenos"> 253</span></a>                <span class="sa">f</span><span class="s2">&quot;importance_summing_strategy must be one of the predefined strategies, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="FGAdaHAT-254"><a href="#FGAdaHAT-254"><span class="linenos"> 254</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT-255"><a href="#FGAdaHAT-255"><span class="linenos"> 255</span></a>
</span><span id="FGAdaHAT-256"><a href="#FGAdaHAT-256"><span class="linenos"> 256</span></a>        <span class="c1"># check importance scheduler type</span>
</span><span id="FGAdaHAT-257"><a href="#FGAdaHAT-257"><span class="linenos"> 257</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_scheduler_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span>
</span><span id="FGAdaHAT-258"><a href="#FGAdaHAT-258"><span class="linenos"> 258</span></a>            <span class="s2">&quot;linear_sparsity_reg&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT-259"><a href="#FGAdaHAT-259"><span class="linenos"> 259</span></a>            <span class="s2">&quot;sparsity_reg&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT-260"><a href="#FGAdaHAT-260"><span class="linenos"> 260</span></a>            <span class="s2">&quot;summative_mask_sparsity_reg&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT-261"><a href="#FGAdaHAT-261"><span class="linenos"> 261</span></a>        <span class="p">]:</span>
</span><span id="FGAdaHAT-262"><a href="#FGAdaHAT-262"><span class="linenos"> 262</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="FGAdaHAT-263"><a href="#FGAdaHAT-263"><span class="linenos"> 263</span></a>                <span class="sa">f</span><span class="s2">&quot;importance_scheduler_type must be one of the predefined types, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">importance_scheduler_type</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="FGAdaHAT-264"><a href="#FGAdaHAT-264"><span class="linenos"> 264</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT-265"><a href="#FGAdaHAT-265"><span class="linenos"> 265</span></a>
</span><span id="FGAdaHAT-266"><a href="#FGAdaHAT-266"><span class="linenos"> 266</span></a>        <span class="c1"># check neuron to weight importance aggregation mode</span>
</span><span id="FGAdaHAT-267"><a href="#FGAdaHAT-267"><span class="linenos"> 267</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">neuron_to_weight_importance_aggregation_mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span>
</span><span id="FGAdaHAT-268"><a href="#FGAdaHAT-268"><span class="linenos"> 268</span></a>            <span class="s2">&quot;min&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT-269"><a href="#FGAdaHAT-269"><span class="linenos"> 269</span></a>            <span class="s2">&quot;max&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT-270"><a href="#FGAdaHAT-270"><span class="linenos"> 270</span></a>            <span class="s2">&quot;mean&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT-271"><a href="#FGAdaHAT-271"><span class="linenos"> 271</span></a>        <span class="p">]:</span>
</span><span id="FGAdaHAT-272"><a href="#FGAdaHAT-272"><span class="linenos"> 272</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="FGAdaHAT-273"><a href="#FGAdaHAT-273"><span class="linenos"> 273</span></a>                <span class="sa">f</span><span class="s2">&quot;neuron_to_weight_importance_aggregation_mode must be one of the predefined modes, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">neuron_to_weight_importance_aggregation_mode</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="FGAdaHAT-274"><a href="#FGAdaHAT-274"><span class="linenos"> 274</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT-275"><a href="#FGAdaHAT-275"><span class="linenos"> 275</span></a>
</span><span id="FGAdaHAT-276"><a href="#FGAdaHAT-276"><span class="linenos"> 276</span></a>        <span class="c1"># check base values</span>
</span><span id="FGAdaHAT-277"><a href="#FGAdaHAT-277"><span class="linenos"> 277</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_importance</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="FGAdaHAT-278"><a href="#FGAdaHAT-278"><span class="linenos"> 278</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="FGAdaHAT-279"><a href="#FGAdaHAT-279"><span class="linenos"> 279</span></a>                <span class="sa">f</span><span class="s2">&quot;base_importance must be &gt;= 0, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">base_importance</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="FGAdaHAT-280"><a href="#FGAdaHAT-280"><span class="linenos"> 280</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT-281"><a href="#FGAdaHAT-281"><span class="linenos"> 281</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_mask_sparsity_reg</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="FGAdaHAT-282"><a href="#FGAdaHAT-282"><span class="linenos"> 282</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="FGAdaHAT-283"><a href="#FGAdaHAT-283"><span class="linenos"> 283</span></a>                <span class="sa">f</span><span class="s2">&quot;base_mask_sparsity_reg must be &gt; 0, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">base_mask_sparsity_reg</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="FGAdaHAT-284"><a href="#FGAdaHAT-284"><span class="linenos"> 284</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT-285"><a href="#FGAdaHAT-285"><span class="linenos"> 285</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_linear</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="FGAdaHAT-286"><a href="#FGAdaHAT-286"><span class="linenos"> 286</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;base_linear must be &gt; 0, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">base_linear</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="FGAdaHAT-287"><a href="#FGAdaHAT-287"><span class="linenos"> 287</span></a>
</span><span id="FGAdaHAT-288"><a href="#FGAdaHAT-288"><span class="linenos"> 288</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_start</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FGAdaHAT-289"><a href="#FGAdaHAT-289"><span class="linenos"> 289</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialize neuron importance accumulation variable for each layer as zeros, in addition to AdaHAT&#39;s summative mask initialization.&quot;&quot;&quot;</span>
</span><span id="FGAdaHAT-290"><a href="#FGAdaHAT-290"><span class="linenos"> 290</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">on_train_start</span><span class="p">()</span>
</span><span id="FGAdaHAT-291"><a href="#FGAdaHAT-291"><span class="linenos"> 291</span></a>
</span><span id="FGAdaHAT-292"><a href="#FGAdaHAT-292"><span class="linenos"> 292</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">importances</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FGAdaHAT-293"><a href="#FGAdaHAT-293"><span class="linenos"> 293</span></a>            <span class="p">{}</span>
</span><span id="FGAdaHAT-294"><a href="#FGAdaHAT-294"><span class="linenos"> 294</span></a>        <span class="p">)</span>  <span class="c1"># initialize the importance for the current task</span>
</span><span id="FGAdaHAT-295"><a href="#FGAdaHAT-295"><span class="linenos"> 295</span></a>
</span><span id="FGAdaHAT-296"><a href="#FGAdaHAT-296"><span class="linenos"> 296</span></a>        <span class="c1"># initialize the neuron importance at the beginning of each task. This should not be called in `__init__()` method because `self.device` is not available at that time.</span>
</span><span id="FGAdaHAT-297"><a href="#FGAdaHAT-297"><span class="linenos"> 297</span></a>        <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="FGAdaHAT-298"><a href="#FGAdaHAT-298"><span class="linenos"> 298</span></a>            <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span>
</span><span id="FGAdaHAT-299"><a href="#FGAdaHAT-299"><span class="linenos"> 299</span></a>                <span class="n">layer_name</span>
</span><span id="FGAdaHAT-300"><a href="#FGAdaHAT-300"><span class="linenos"> 300</span></a>            <span class="p">)</span>  <span class="c1"># get the layer by its name</span>
</span><span id="FGAdaHAT-301"><a href="#FGAdaHAT-301"><span class="linenos"> 301</span></a>            <span class="n">num_units</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="FGAdaHAT-302"><a href="#FGAdaHAT-302"><span class="linenos"> 302</span></a>
</span><span id="FGAdaHAT-303"><a href="#FGAdaHAT-303"><span class="linenos"> 303</span></a>            <span class="c1"># initialize the accumulated importance at the beginning of each task</span>
</span><span id="FGAdaHAT-304"><a href="#FGAdaHAT-304"><span class="linenos"> 304</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">importances</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">][</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_units</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
</span><span id="FGAdaHAT-305"><a href="#FGAdaHAT-305"><span class="linenos"> 305</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
</span><span id="FGAdaHAT-306"><a href="#FGAdaHAT-306"><span class="linenos"> 306</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT-307"><a href="#FGAdaHAT-307"><span class="linenos"> 307</span></a>
</span><span id="FGAdaHAT-308"><a href="#FGAdaHAT-308"><span class="linenos"> 308</span></a>            <span class="c1"># reset the number of steps counter for the current task</span>
</span><span id="FGAdaHAT-309"><a href="#FGAdaHAT-309"><span class="linenos"> 309</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">num_steps_t</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="FGAdaHAT-310"><a href="#FGAdaHAT-310"><span class="linenos"> 310</span></a>
</span><span id="FGAdaHAT-311"><a href="#FGAdaHAT-311"><span class="linenos"> 311</span></a>            <span class="c1"># initialize the summative neuron-wise importance at the beginning of the first task</span>
</span><span id="FGAdaHAT-312"><a href="#FGAdaHAT-312"><span class="linenos"> 312</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="FGAdaHAT-313"><a href="#FGAdaHAT-313"><span class="linenos"> 313</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">summative_importance_for_previous_tasks</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
</span><span id="FGAdaHAT-314"><a href="#FGAdaHAT-314"><span class="linenos"> 314</span></a>                    <span class="n">num_units</span>
</span><span id="FGAdaHAT-315"><a href="#FGAdaHAT-315"><span class="linenos"> 315</span></a>                <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
</span><span id="FGAdaHAT-316"><a href="#FGAdaHAT-316"><span class="linenos"> 316</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
</span><span id="FGAdaHAT-317"><a href="#FGAdaHAT-317"><span class="linenos"> 317</span></a>                <span class="p">)</span>  <span class="c1"># the summative neuron-wise importance for previous tasks $I^{&lt;t}_{l}$ is initialized as zeros mask when $t=1$</span>
</span><span id="FGAdaHAT-318"><a href="#FGAdaHAT-318"><span class="linenos"> 318</span></a>
</span><span id="FGAdaHAT-319"><a href="#FGAdaHAT-319"><span class="linenos"> 319</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">clip_grad_by_adjustment</span><span class="p">(</span>
</span><span id="FGAdaHAT-320"><a href="#FGAdaHAT-320"><span class="linenos"> 320</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="FGAdaHAT-321"><a href="#FGAdaHAT-321"><span class="linenos"> 321</span></a>        <span class="n">network_sparsity</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span>
</span><span id="FGAdaHAT-322"><a href="#FGAdaHAT-322"><span class="linenos"> 322</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="n">Tensor</span><span class="p">]:</span>
</span><span id="FGAdaHAT-323"><a href="#FGAdaHAT-323"><span class="linenos"> 323</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Clip the gradients by the adjustment rate. See Eq. (1) in the paper.</span>
</span><span id="FGAdaHAT-324"><a href="#FGAdaHAT-324"><span class="linenos"> 324</span></a>
</span><span id="FGAdaHAT-325"><a href="#FGAdaHAT-325"><span class="linenos"> 325</span></a><span class="sd">        Note that because the task embedding fully covers every layer in the backbone network, no parameters are left out of this system. This applies not only to parameters between layers with task embeddings, but also to those before the first layer. We design it separately in the code.</span>
</span><span id="FGAdaHAT-326"><a href="#FGAdaHAT-326"><span class="linenos"> 326</span></a>
</span><span id="FGAdaHAT-327"><a href="#FGAdaHAT-327"><span class="linenos"> 327</span></a><span class="sd">        Network capacity is measured alongside this method. Network capacity is defined as the average adjustment rate over all parameters. See Sec. 4.1 in the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="FGAdaHAT-328"><a href="#FGAdaHAT-328"><span class="linenos"> 328</span></a>
</span><span id="FGAdaHAT-329"><a href="#FGAdaHAT-329"><span class="linenos"> 329</span></a><span class="sd">        **Args:**</span>
</span><span id="FGAdaHAT-330"><a href="#FGAdaHAT-330"><span class="linenos"> 330</span></a><span class="sd">        - **network_sparsity** (`dict[str, Tensor]`): the network sparsity (i.e., mask sparsity loss of each layer) for the current task. Keys are layer names and values are the network sparsity values. It is used to calculate the adjustment rate for gradients. In FG-AdaHAT, it is used to construct the importance scheduler.</span>
</span><span id="FGAdaHAT-331"><a href="#FGAdaHAT-331"><span class="linenos"> 331</span></a>
</span><span id="FGAdaHAT-332"><a href="#FGAdaHAT-332"><span class="linenos"> 332</span></a><span class="sd">        **Returns:**</span>
</span><span id="FGAdaHAT-333"><a href="#FGAdaHAT-333"><span class="linenos"> 333</span></a><span class="sd">        - **adjustment_rate_weight** (`dict[str, Tensor]`): the adjustment rate for weights. Keys (`str`) are layer names and values (`Tensor`) are the adjustment rate tensors.</span>
</span><span id="FGAdaHAT-334"><a href="#FGAdaHAT-334"><span class="linenos"> 334</span></a><span class="sd">        - **adjustment_rate_bias** (`dict[str, Tensor]`): the adjustment rate for biases. Keys (`str`) are layer names and values (`Tensor`) are the adjustment rate tensors.</span>
</span><span id="FGAdaHAT-335"><a href="#FGAdaHAT-335"><span class="linenos"> 335</span></a><span class="sd">        - **capacity** (`Tensor`): the calculated network capacity.</span>
</span><span id="FGAdaHAT-336"><a href="#FGAdaHAT-336"><span class="linenos"> 336</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT-337"><a href="#FGAdaHAT-337"><span class="linenos"> 337</span></a>
</span><span id="FGAdaHAT-338"><a href="#FGAdaHAT-338"><span class="linenos"> 338</span></a>        <span class="c1"># initialize network capacity metric</span>
</span><span id="FGAdaHAT-339"><a href="#FGAdaHAT-339"><span class="linenos"> 339</span></a>        <span class="n">capacity</span> <span class="o">=</span> <span class="n">HATNetworkCapacityMetric</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="FGAdaHAT-340"><a href="#FGAdaHAT-340"><span class="linenos"> 340</span></a>        <span class="n">adjustment_rate_weight</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="FGAdaHAT-341"><a href="#FGAdaHAT-341"><span class="linenos"> 341</span></a>        <span class="n">adjustment_rate_bias</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="FGAdaHAT-342"><a href="#FGAdaHAT-342"><span class="linenos"> 342</span></a>
</span><span id="FGAdaHAT-343"><a href="#FGAdaHAT-343"><span class="linenos"> 343</span></a>        <span class="c1"># calculate the adjustment rate for gradients of the parameters, both weights and biases (if they exist). See Eq. (2) in the paper</span>
</span><span id="FGAdaHAT-344"><a href="#FGAdaHAT-344"><span class="linenos"> 344</span></a>        <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="FGAdaHAT-345"><a href="#FGAdaHAT-345"><span class="linenos"> 345</span></a>
</span><span id="FGAdaHAT-346"><a href="#FGAdaHAT-346"><span class="linenos"> 346</span></a>            <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span>
</span><span id="FGAdaHAT-347"><a href="#FGAdaHAT-347"><span class="linenos"> 347</span></a>                <span class="n">layer_name</span>
</span><span id="FGAdaHAT-348"><a href="#FGAdaHAT-348"><span class="linenos"> 348</span></a>            <span class="p">)</span>  <span class="c1"># get the layer by its name</span>
</span><span id="FGAdaHAT-349"><a href="#FGAdaHAT-349"><span class="linenos"> 349</span></a>
</span><span id="FGAdaHAT-350"><a href="#FGAdaHAT-350"><span class="linenos"> 350</span></a>            <span class="c1"># placeholder for the adjustment rate to avoid the error of using it before assignment</span>
</span><span id="FGAdaHAT-351"><a href="#FGAdaHAT-351"><span class="linenos"> 351</span></a>            <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="FGAdaHAT-352"><a href="#FGAdaHAT-352"><span class="linenos"> 352</span></a>            <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="FGAdaHAT-353"><a href="#FGAdaHAT-353"><span class="linenos"> 353</span></a>
</span><span id="FGAdaHAT-354"><a href="#FGAdaHAT-354"><span class="linenos"> 354</span></a>            <span class="c1"># aggregate the neuron-wise importance to weight-wise importance. Note that the neuron-wise importance has already been min-max scaled to $[0, 1]$ in the `on_train_batch_end()` method, added the base value, and filtered by the mask</span>
</span><span id="FGAdaHAT-355"><a href="#FGAdaHAT-355"><span class="linenos"> 355</span></a>            <span class="n">weight_importance</span><span class="p">,</span> <span class="n">bias_importance</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FGAdaHAT-356"><a href="#FGAdaHAT-356"><span class="linenos"> 356</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_measure_parameter_wise</span><span class="p">(</span>
</span><span id="FGAdaHAT-357"><a href="#FGAdaHAT-357"><span class="linenos"> 357</span></a>                    <span class="n">unit_wise_measure</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">summative_importance_for_previous_tasks</span><span class="p">,</span>
</span><span id="FGAdaHAT-358"><a href="#FGAdaHAT-358"><span class="linenos"> 358</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="FGAdaHAT-359"><a href="#FGAdaHAT-359"><span class="linenos"> 359</span></a>                    <span class="n">aggregation_mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">neuron_to_weight_importance_aggregation_mode</span><span class="p">,</span>
</span><span id="FGAdaHAT-360"><a href="#FGAdaHAT-360"><span class="linenos"> 360</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT-361"><a href="#FGAdaHAT-361"><span class="linenos"> 361</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT-362"><a href="#FGAdaHAT-362"><span class="linenos"> 362</span></a>
</span><span id="FGAdaHAT-363"><a href="#FGAdaHAT-363"><span class="linenos"> 363</span></a>            <span class="n">weight_mask</span><span class="p">,</span> <span class="n">bias_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_measure_parameter_wise</span><span class="p">(</span>
</span><span id="FGAdaHAT-364"><a href="#FGAdaHAT-364"><span class="linenos"> 364</span></a>                <span class="n">unit_wise_measure</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">cumulative_mask_for_previous_tasks</span><span class="p">,</span>
</span><span id="FGAdaHAT-365"><a href="#FGAdaHAT-365"><span class="linenos"> 365</span></a>                <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="FGAdaHAT-366"><a href="#FGAdaHAT-366"><span class="linenos"> 366</span></a>                <span class="n">aggregation_mode</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT-367"><a href="#FGAdaHAT-367"><span class="linenos"> 367</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT-368"><a href="#FGAdaHAT-368"><span class="linenos"> 368</span></a>
</span><span id="FGAdaHAT-369"><a href="#FGAdaHAT-369"><span class="linenos"> 369</span></a>            <span class="c1"># filter the weight importance by the cumulative mask</span>
</span><span id="FGAdaHAT-370"><a href="#FGAdaHAT-370"><span class="linenos"> 370</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">filter_by_cumulative_mask</span><span class="p">:</span>
</span><span id="FGAdaHAT-371"><a href="#FGAdaHAT-371"><span class="linenos"> 371</span></a>                <span class="n">weight_importance</span> <span class="o">=</span> <span class="n">weight_importance</span> <span class="o">*</span> <span class="n">weight_mask</span>
</span><span id="FGAdaHAT-372"><a href="#FGAdaHAT-372"><span class="linenos"> 372</span></a>                <span class="n">bias_importance</span> <span class="o">=</span> <span class="n">bias_importance</span> <span class="o">*</span> <span class="n">bias_mask</span>
</span><span id="FGAdaHAT-373"><a href="#FGAdaHAT-373"><span class="linenos"> 373</span></a>
</span><span id="FGAdaHAT-374"><a href="#FGAdaHAT-374"><span class="linenos"> 374</span></a>            <span class="n">network_sparsity_layer</span> <span class="o">=</span> <span class="n">network_sparsity</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="FGAdaHAT-375"><a href="#FGAdaHAT-375"><span class="linenos"> 375</span></a>
</span><span id="FGAdaHAT-376"><a href="#FGAdaHAT-376"><span class="linenos"> 376</span></a>            <span class="c1"># calculate importance scheduler (the factor of importance). See Eq. (3) in the paper</span>
</span><span id="FGAdaHAT-377"><a href="#FGAdaHAT-377"><span class="linenos"> 377</span></a>            <span class="n">factor</span> <span class="o">=</span> <span class="n">network_sparsity_layer</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_mask_sparsity_reg</span>
</span><span id="FGAdaHAT-378"><a href="#FGAdaHAT-378"><span class="linenos"> 378</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_scheduler_type</span> <span class="o">==</span> <span class="s2">&quot;linear_sparsity_reg&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT-379"><a href="#FGAdaHAT-379"><span class="linenos"> 379</span></a>                <span class="n">factor</span> <span class="o">=</span> <span class="n">factor</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_linear</span><span class="p">)</span>
</span><span id="FGAdaHAT-380"><a href="#FGAdaHAT-380"><span class="linenos"> 380</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_scheduler_type</span> <span class="o">==</span> <span class="s2">&quot;sparsity_reg&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT-381"><a href="#FGAdaHAT-381"><span class="linenos"> 381</span></a>                <span class="k">pass</span>
</span><span id="FGAdaHAT-382"><a href="#FGAdaHAT-382"><span class="linenos"> 382</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_scheduler_type</span> <span class="o">==</span> <span class="s2">&quot;summative_mask_sparsity_reg&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT-383"><a href="#FGAdaHAT-383"><span class="linenos"> 383</span></a>                <span class="n">factor</span> <span class="o">=</span> <span class="n">factor</span> <span class="o">*</span> <span class="p">(</span>
</span><span id="FGAdaHAT-384"><a href="#FGAdaHAT-384"><span class="linenos"> 384</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">summative_mask_for_previous_tasks</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_linear</span>
</span><span id="FGAdaHAT-385"><a href="#FGAdaHAT-385"><span class="linenos"> 385</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT-386"><a href="#FGAdaHAT-386"><span class="linenos"> 386</span></a>
</span><span id="FGAdaHAT-387"><a href="#FGAdaHAT-387"><span class="linenos"> 387</span></a>            <span class="c1"># calculate the adjustment rate</span>
</span><span id="FGAdaHAT-388"><a href="#FGAdaHAT-388"><span class="linenos"> 388</span></a>            <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span>
</span><span id="FGAdaHAT-389"><a href="#FGAdaHAT-389"><span class="linenos"> 389</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span><span class="p">,</span>
</span><span id="FGAdaHAT-390"><a href="#FGAdaHAT-390"><span class="linenos"> 390</span></a>                <span class="p">(</span><span class="n">factor</span> <span class="o">*</span> <span class="n">weight_importance</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span><span class="p">),</span>
</span><span id="FGAdaHAT-391"><a href="#FGAdaHAT-391"><span class="linenos"> 391</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT-392"><a href="#FGAdaHAT-392"><span class="linenos"> 392</span></a>
</span><span id="FGAdaHAT-393"><a href="#FGAdaHAT-393"><span class="linenos"> 393</span></a>            <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span>
</span><span id="FGAdaHAT-394"><a href="#FGAdaHAT-394"><span class="linenos"> 394</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span><span class="p">,</span>
</span><span id="FGAdaHAT-395"><a href="#FGAdaHAT-395"><span class="linenos"> 395</span></a>                <span class="p">(</span><span class="n">factor</span> <span class="o">*</span> <span class="n">bias_importance</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span><span class="p">),</span>
</span><span id="FGAdaHAT-396"><a href="#FGAdaHAT-396"><span class="linenos"> 396</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT-397"><a href="#FGAdaHAT-397"><span class="linenos"> 397</span></a>
</span><span id="FGAdaHAT-398"><a href="#FGAdaHAT-398"><span class="linenos"> 398</span></a>            <span class="c1"># apply the adjustment rate to the gradients</span>
</span><span id="FGAdaHAT-399"><a href="#FGAdaHAT-399"><span class="linenos"> 399</span></a>            <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">adjustment_rate_weight_layer</span>
</span><span id="FGAdaHAT-400"><a href="#FGAdaHAT-400"><span class="linenos"> 400</span></a>            <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FGAdaHAT-401"><a href="#FGAdaHAT-401"><span class="linenos"> 401</span></a>                <span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">adjustment_rate_bias_layer</span>
</span><span id="FGAdaHAT-402"><a href="#FGAdaHAT-402"><span class="linenos"> 402</span></a>
</span><span id="FGAdaHAT-403"><a href="#FGAdaHAT-403"><span class="linenos"> 403</span></a>            <span class="c1"># store the adjustment rate for logging</span>
</span><span id="FGAdaHAT-404"><a href="#FGAdaHAT-404"><span class="linenos"> 404</span></a>            <span class="n">adjustment_rate_weight</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">adjustment_rate_weight_layer</span>
</span><span id="FGAdaHAT-405"><a href="#FGAdaHAT-405"><span class="linenos"> 405</span></a>            <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FGAdaHAT-406"><a href="#FGAdaHAT-406"><span class="linenos"> 406</span></a>                <span class="n">adjustment_rate_bias</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">adjustment_rate_bias_layer</span>
</span><span id="FGAdaHAT-407"><a href="#FGAdaHAT-407"><span class="linenos"> 407</span></a>
</span><span id="FGAdaHAT-408"><a href="#FGAdaHAT-408"><span class="linenos"> 408</span></a>            <span class="c1"># update network capacity metric</span>
</span><span id="FGAdaHAT-409"><a href="#FGAdaHAT-409"><span class="linenos"> 409</span></a>            <span class="n">capacity</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">adjustment_rate_weight_layer</span><span class="p">,</span> <span class="n">adjustment_rate_bias_layer</span><span class="p">)</span>
</span><span id="FGAdaHAT-410"><a href="#FGAdaHAT-410"><span class="linenos"> 410</span></a>
</span><span id="FGAdaHAT-411"><a href="#FGAdaHAT-411"><span class="linenos"> 411</span></a>        <span class="k">return</span> <span class="n">adjustment_rate_weight</span><span class="p">,</span> <span class="n">adjustment_rate_bias</span><span class="p">,</span> <span class="n">capacity</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
</span><span id="FGAdaHAT-412"><a href="#FGAdaHAT-412"><span class="linenos"> 412</span></a>
</span><span id="FGAdaHAT-413"><a href="#FGAdaHAT-413"><span class="linenos"> 413</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_batch_end</span><span class="p">(</span>
</span><span id="FGAdaHAT-414"><a href="#FGAdaHAT-414"><span class="linenos"> 414</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span>
</span><span id="FGAdaHAT-415"><a href="#FGAdaHAT-415"><span class="linenos"> 415</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FGAdaHAT-416"><a href="#FGAdaHAT-416"><span class="linenos"> 416</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Calculate the step-wise importance, update the accumulated importance and number of steps counter after each training step.</span>
</span><span id="FGAdaHAT-417"><a href="#FGAdaHAT-417"><span class="linenos"> 417</span></a>
</span><span id="FGAdaHAT-418"><a href="#FGAdaHAT-418"><span class="linenos"> 418</span></a><span class="sd">        **Args:**</span>
</span><span id="FGAdaHAT-419"><a href="#FGAdaHAT-419"><span class="linenos"> 419</span></a><span class="sd">        - **outputs** (`dict[str, Any]`): outputs of the training step (returns of `training_step()` in `CLAlgorithm`).</span>
</span><span id="FGAdaHAT-420"><a href="#FGAdaHAT-420"><span class="linenos"> 420</span></a><span class="sd">        - **batch** (`Any`): training data batch.</span>
</span><span id="FGAdaHAT-421"><a href="#FGAdaHAT-421"><span class="linenos"> 421</span></a><span class="sd">        - **batch_idx** (`int`): index of the current batch (for mask figure file name).</span>
</span><span id="FGAdaHAT-422"><a href="#FGAdaHAT-422"><span class="linenos"> 422</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT-423"><a href="#FGAdaHAT-423"><span class="linenos"> 423</span></a>
</span><span id="FGAdaHAT-424"><a href="#FGAdaHAT-424"><span class="linenos"> 424</span></a>        <span class="c1"># get potential useful information from training batch</span>
</span><span id="FGAdaHAT-425"><a href="#FGAdaHAT-425"><span class="linenos"> 425</span></a>        <span class="n">activations</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;activations&quot;</span><span class="p">]</span>
</span><span id="FGAdaHAT-426"><a href="#FGAdaHAT-426"><span class="linenos"> 426</span></a>        <span class="nb">input</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">]</span>
</span><span id="FGAdaHAT-427"><a href="#FGAdaHAT-427"><span class="linenos"> 427</span></a>        <span class="n">target</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;target&quot;</span><span class="p">]</span>
</span><span id="FGAdaHAT-428"><a href="#FGAdaHAT-428"><span class="linenos"> 428</span></a>        <span class="n">mask</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;mask&quot;</span><span class="p">]</span>
</span><span id="FGAdaHAT-429"><a href="#FGAdaHAT-429"><span class="linenos"> 429</span></a>        <span class="n">num_batches</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">num_training_batches</span>
</span><span id="FGAdaHAT-430"><a href="#FGAdaHAT-430"><span class="linenos"> 430</span></a>
</span><span id="FGAdaHAT-431"><a href="#FGAdaHAT-431"><span class="linenos"> 431</span></a>        <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="FGAdaHAT-432"><a href="#FGAdaHAT-432"><span class="linenos"> 432</span></a>            <span class="c1"># layer-wise operation</span>
</span><span id="FGAdaHAT-433"><a href="#FGAdaHAT-433"><span class="linenos"> 433</span></a>
</span><span id="FGAdaHAT-434"><a href="#FGAdaHAT-434"><span class="linenos"> 434</span></a>            <span class="n">activation</span> <span class="o">=</span> <span class="n">activations</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="FGAdaHAT-435"><a href="#FGAdaHAT-435"><span class="linenos"> 435</span></a>
</span><span id="FGAdaHAT-436"><a href="#FGAdaHAT-436"><span class="linenos"> 436</span></a>            <span class="c1"># calculate neuron-wise importance of the training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper.</span>
</span><span id="FGAdaHAT-437"><a href="#FGAdaHAT-437"><span class="linenos"> 437</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;input_weight_abs_sum&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT-438"><a href="#FGAdaHAT-438"><span class="linenos"> 438</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_weight_abs_sum</span><span class="p">(</span>
</span><span id="FGAdaHAT-439"><a href="#FGAdaHAT-439"><span class="linenos"> 439</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="FGAdaHAT-440"><a href="#FGAdaHAT-440"><span class="linenos"> 440</span></a>                    <span class="n">if_output_weight</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="FGAdaHAT-441"><a href="#FGAdaHAT-441"><span class="linenos"> 441</span></a>                    <span class="n">reciprocal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="FGAdaHAT-442"><a href="#FGAdaHAT-442"><span class="linenos"> 442</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT-443"><a href="#FGAdaHAT-443"><span class="linenos"> 443</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;output_weight_abs_sum&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT-444"><a href="#FGAdaHAT-444"><span class="linenos"> 444</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_weight_abs_sum</span><span class="p">(</span>
</span><span id="FGAdaHAT-445"><a href="#FGAdaHAT-445"><span class="linenos"> 445</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="FGAdaHAT-446"><a href="#FGAdaHAT-446"><span class="linenos"> 446</span></a>                    <span class="n">if_output_weight</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="FGAdaHAT-447"><a href="#FGAdaHAT-447"><span class="linenos"> 447</span></a>                    <span class="n">reciprocal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="FGAdaHAT-448"><a href="#FGAdaHAT-448"><span class="linenos"> 448</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT-449"><a href="#FGAdaHAT-449"><span class="linenos"> 449</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;input_weight_gradient_abs_sum&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT-450"><a href="#FGAdaHAT-450"><span class="linenos"> 450</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FGAdaHAT-451"><a href="#FGAdaHAT-451"><span class="linenos"> 451</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_weight_gradient_abs_sum</span><span class="p">(</span>
</span><span id="FGAdaHAT-452"><a href="#FGAdaHAT-452"><span class="linenos"> 452</span></a>                        <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span> <span class="n">if_output_weight</span><span class="o">=</span><span class="kc">False</span>
</span><span id="FGAdaHAT-453"><a href="#FGAdaHAT-453"><span class="linenos"> 453</span></a>                    <span class="p">)</span>
</span><span id="FGAdaHAT-454"><a href="#FGAdaHAT-454"><span class="linenos"> 454</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT-455"><a href="#FGAdaHAT-455"><span class="linenos"> 455</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;output_weight_gradient_abs_sum&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT-456"><a href="#FGAdaHAT-456"><span class="linenos"> 456</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FGAdaHAT-457"><a href="#FGAdaHAT-457"><span class="linenos"> 457</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_weight_gradient_abs_sum</span><span class="p">(</span>
</span><span id="FGAdaHAT-458"><a href="#FGAdaHAT-458"><span class="linenos"> 458</span></a>                        <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span> <span class="n">if_output_weight</span><span class="o">=</span><span class="kc">True</span>
</span><span id="FGAdaHAT-459"><a href="#FGAdaHAT-459"><span class="linenos"> 459</span></a>                    <span class="p">)</span>
</span><span id="FGAdaHAT-460"><a href="#FGAdaHAT-460"><span class="linenos"> 460</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT-461"><a href="#FGAdaHAT-461"><span class="linenos"> 461</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;activation_abs&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT-462"><a href="#FGAdaHAT-462"><span class="linenos"> 462</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_activation_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT-463"><a href="#FGAdaHAT-463"><span class="linenos"> 463</span></a>                    <span class="n">activation</span><span class="o">=</span><span class="n">activation</span>
</span><span id="FGAdaHAT-464"><a href="#FGAdaHAT-464"><span class="linenos"> 464</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT-465"><a href="#FGAdaHAT-465"><span class="linenos"> 465</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;input_weight_abs_sum_x_activation_abs&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT-466"><a href="#FGAdaHAT-466"><span class="linenos"> 466</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FGAdaHAT-467"><a href="#FGAdaHAT-467"><span class="linenos"> 467</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_weight_abs_sum_x_activation_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT-468"><a href="#FGAdaHAT-468"><span class="linenos"> 468</span></a>                        <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="FGAdaHAT-469"><a href="#FGAdaHAT-469"><span class="linenos"> 469</span></a>                        <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
</span><span id="FGAdaHAT-470"><a href="#FGAdaHAT-470"><span class="linenos"> 470</span></a>                        <span class="n">if_output_weight</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="FGAdaHAT-471"><a href="#FGAdaHAT-471"><span class="linenos"> 471</span></a>                    <span class="p">)</span>
</span><span id="FGAdaHAT-472"><a href="#FGAdaHAT-472"><span class="linenos"> 472</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT-473"><a href="#FGAdaHAT-473"><span class="linenos"> 473</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;output_weight_abs_sum_x_activation_abs&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT-474"><a href="#FGAdaHAT-474"><span class="linenos"> 474</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FGAdaHAT-475"><a href="#FGAdaHAT-475"><span class="linenos"> 475</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_weight_abs_sum_x_activation_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT-476"><a href="#FGAdaHAT-476"><span class="linenos"> 476</span></a>                        <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="FGAdaHAT-477"><a href="#FGAdaHAT-477"><span class="linenos"> 477</span></a>                        <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
</span><span id="FGAdaHAT-478"><a href="#FGAdaHAT-478"><span class="linenos"> 478</span></a>                        <span class="n">if_output_weight</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="FGAdaHAT-479"><a href="#FGAdaHAT-479"><span class="linenos"> 479</span></a>                    <span class="p">)</span>
</span><span id="FGAdaHAT-480"><a href="#FGAdaHAT-480"><span class="linenos"> 480</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT-481"><a href="#FGAdaHAT-481"><span class="linenos"> 481</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;gradient_x_activation_abs&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT-482"><a href="#FGAdaHAT-482"><span class="linenos"> 482</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FGAdaHAT-483"><a href="#FGAdaHAT-483"><span class="linenos"> 483</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_gradient_x_activation_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT-484"><a href="#FGAdaHAT-484"><span class="linenos"> 484</span></a>                        <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="FGAdaHAT-485"><a href="#FGAdaHAT-485"><span class="linenos"> 485</span></a>                        <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="FGAdaHAT-486"><a href="#FGAdaHAT-486"><span class="linenos"> 486</span></a>                        <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="FGAdaHAT-487"><a href="#FGAdaHAT-487"><span class="linenos"> 487</span></a>                        <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="FGAdaHAT-488"><a href="#FGAdaHAT-488"><span class="linenos"> 488</span></a>                        <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="FGAdaHAT-489"><a href="#FGAdaHAT-489"><span class="linenos"> 489</span></a>                    <span class="p">)</span>
</span><span id="FGAdaHAT-490"><a href="#FGAdaHAT-490"><span class="linenos"> 490</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT-491"><a href="#FGAdaHAT-491"><span class="linenos"> 491</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;input_weight_gradient_square_sum&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT-492"><a href="#FGAdaHAT-492"><span class="linenos"> 492</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FGAdaHAT-493"><a href="#FGAdaHAT-493"><span class="linenos"> 493</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_weight_gradient_square_sum</span><span class="p">(</span>
</span><span id="FGAdaHAT-494"><a href="#FGAdaHAT-494"><span class="linenos"> 494</span></a>                        <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="FGAdaHAT-495"><a href="#FGAdaHAT-495"><span class="linenos"> 495</span></a>                        <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
</span><span id="FGAdaHAT-496"><a href="#FGAdaHAT-496"><span class="linenos"> 496</span></a>                        <span class="n">if_output_weight</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="FGAdaHAT-497"><a href="#FGAdaHAT-497"><span class="linenos"> 497</span></a>                    <span class="p">)</span>
</span><span id="FGAdaHAT-498"><a href="#FGAdaHAT-498"><span class="linenos"> 498</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT-499"><a href="#FGAdaHAT-499"><span class="linenos"> 499</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;output_weight_gradient_square_sum&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT-500"><a href="#FGAdaHAT-500"><span class="linenos"> 500</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FGAdaHAT-501"><a href="#FGAdaHAT-501"><span class="linenos"> 501</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_weight_gradient_square_sum</span><span class="p">(</span>
</span><span id="FGAdaHAT-502"><a href="#FGAdaHAT-502"><span class="linenos"> 502</span></a>                        <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="FGAdaHAT-503"><a href="#FGAdaHAT-503"><span class="linenos"> 503</span></a>                        <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
</span><span id="FGAdaHAT-504"><a href="#FGAdaHAT-504"><span class="linenos"> 504</span></a>                        <span class="n">if_output_weight</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="FGAdaHAT-505"><a href="#FGAdaHAT-505"><span class="linenos"> 505</span></a>                    <span class="p">)</span>
</span><span id="FGAdaHAT-506"><a href="#FGAdaHAT-506"><span class="linenos"> 506</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT-507"><a href="#FGAdaHAT-507"><span class="linenos"> 507</span></a>            <span class="k">elif</span> <span class="p">(</span>
</span><span id="FGAdaHAT-508"><a href="#FGAdaHAT-508"><span class="linenos"> 508</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span>
</span><span id="FGAdaHAT-509"><a href="#FGAdaHAT-509"><span class="linenos"> 509</span></a>                <span class="o">==</span> <span class="s2">&quot;input_weight_gradient_square_sum_x_activation_abs&quot;</span>
</span><span id="FGAdaHAT-510"><a href="#FGAdaHAT-510"><span class="linenos"> 510</span></a>            <span class="p">):</span>
</span><span id="FGAdaHAT-511"><a href="#FGAdaHAT-511"><span class="linenos"> 511</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_weight_gradient_square_sum_x_activation_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT-512"><a href="#FGAdaHAT-512"><span class="linenos"> 512</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="FGAdaHAT-513"><a href="#FGAdaHAT-513"><span class="linenos"> 513</span></a>                    <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
</span><span id="FGAdaHAT-514"><a href="#FGAdaHAT-514"><span class="linenos"> 514</span></a>                    <span class="n">if_output_weight</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="FGAdaHAT-515"><a href="#FGAdaHAT-515"><span class="linenos"> 515</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT-516"><a href="#FGAdaHAT-516"><span class="linenos"> 516</span></a>            <span class="k">elif</span> <span class="p">(</span>
</span><span id="FGAdaHAT-517"><a href="#FGAdaHAT-517"><span class="linenos"> 517</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span>
</span><span id="FGAdaHAT-518"><a href="#FGAdaHAT-518"><span class="linenos"> 518</span></a>                <span class="o">==</span> <span class="s2">&quot;output_weight_gradient_square_sum_x_activation_abs&quot;</span>
</span><span id="FGAdaHAT-519"><a href="#FGAdaHAT-519"><span class="linenos"> 519</span></a>            <span class="p">):</span>
</span><span id="FGAdaHAT-520"><a href="#FGAdaHAT-520"><span class="linenos"> 520</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_weight_gradient_square_sum_x_activation_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT-521"><a href="#FGAdaHAT-521"><span class="linenos"> 521</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="FGAdaHAT-522"><a href="#FGAdaHAT-522"><span class="linenos"> 522</span></a>                    <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
</span><span id="FGAdaHAT-523"><a href="#FGAdaHAT-523"><span class="linenos"> 523</span></a>                    <span class="n">if_output_weight</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="FGAdaHAT-524"><a href="#FGAdaHAT-524"><span class="linenos"> 524</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT-525"><a href="#FGAdaHAT-525"><span class="linenos"> 525</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;conductance_abs&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT-526"><a href="#FGAdaHAT-526"><span class="linenos"> 526</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_conductance_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT-527"><a href="#FGAdaHAT-527"><span class="linenos"> 527</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="FGAdaHAT-528"><a href="#FGAdaHAT-528"><span class="linenos"> 528</span></a>                    <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="FGAdaHAT-529"><a href="#FGAdaHAT-529"><span class="linenos"> 529</span></a>                    <span class="n">baselines</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="FGAdaHAT-530"><a href="#FGAdaHAT-530"><span class="linenos"> 530</span></a>                    <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="FGAdaHAT-531"><a href="#FGAdaHAT-531"><span class="linenos"> 531</span></a>                    <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="FGAdaHAT-532"><a href="#FGAdaHAT-532"><span class="linenos"> 532</span></a>                    <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="FGAdaHAT-533"><a href="#FGAdaHAT-533"><span class="linenos"> 533</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT-534"><a href="#FGAdaHAT-534"><span class="linenos"> 534</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;internal_influence_abs&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT-535"><a href="#FGAdaHAT-535"><span class="linenos"> 535</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_internal_influence_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT-536"><a href="#FGAdaHAT-536"><span class="linenos"> 536</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="FGAdaHAT-537"><a href="#FGAdaHAT-537"><span class="linenos"> 537</span></a>                    <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="FGAdaHAT-538"><a href="#FGAdaHAT-538"><span class="linenos"> 538</span></a>                    <span class="n">baselines</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="FGAdaHAT-539"><a href="#FGAdaHAT-539"><span class="linenos"> 539</span></a>                    <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="FGAdaHAT-540"><a href="#FGAdaHAT-540"><span class="linenos"> 540</span></a>                    <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="FGAdaHAT-541"><a href="#FGAdaHAT-541"><span class="linenos"> 541</span></a>                    <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="FGAdaHAT-542"><a href="#FGAdaHAT-542"><span class="linenos"> 542</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT-543"><a href="#FGAdaHAT-543"><span class="linenos"> 543</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;gradcam_abs&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT-544"><a href="#FGAdaHAT-544"><span class="linenos"> 544</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_gradcam_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT-545"><a href="#FGAdaHAT-545"><span class="linenos"> 545</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="FGAdaHAT-546"><a href="#FGAdaHAT-546"><span class="linenos"> 546</span></a>                    <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="FGAdaHAT-547"><a href="#FGAdaHAT-547"><span class="linenos"> 547</span></a>                    <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="FGAdaHAT-548"><a href="#FGAdaHAT-548"><span class="linenos"> 548</span></a>                    <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="FGAdaHAT-549"><a href="#FGAdaHAT-549"><span class="linenos"> 549</span></a>                    <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="FGAdaHAT-550"><a href="#FGAdaHAT-550"><span class="linenos"> 550</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT-551"><a href="#FGAdaHAT-551"><span class="linenos"> 551</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;deeplift_abs&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT-552"><a href="#FGAdaHAT-552"><span class="linenos"> 552</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_deeplift_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT-553"><a href="#FGAdaHAT-553"><span class="linenos"> 553</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="FGAdaHAT-554"><a href="#FGAdaHAT-554"><span class="linenos"> 554</span></a>                    <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="FGAdaHAT-555"><a href="#FGAdaHAT-555"><span class="linenos"> 555</span></a>                    <span class="n">baselines</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="FGAdaHAT-556"><a href="#FGAdaHAT-556"><span class="linenos"> 556</span></a>                    <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="FGAdaHAT-557"><a href="#FGAdaHAT-557"><span class="linenos"> 557</span></a>                    <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="FGAdaHAT-558"><a href="#FGAdaHAT-558"><span class="linenos"> 558</span></a>                    <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="FGAdaHAT-559"><a href="#FGAdaHAT-559"><span class="linenos"> 559</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT-560"><a href="#FGAdaHAT-560"><span class="linenos"> 560</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;deepliftshap_abs&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT-561"><a href="#FGAdaHAT-561"><span class="linenos"> 561</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_deepliftshap_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT-562"><a href="#FGAdaHAT-562"><span class="linenos"> 562</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="FGAdaHAT-563"><a href="#FGAdaHAT-563"><span class="linenos"> 563</span></a>                    <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="FGAdaHAT-564"><a href="#FGAdaHAT-564"><span class="linenos"> 564</span></a>                    <span class="n">baselines</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="FGAdaHAT-565"><a href="#FGAdaHAT-565"><span class="linenos"> 565</span></a>                    <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="FGAdaHAT-566"><a href="#FGAdaHAT-566"><span class="linenos"> 566</span></a>                    <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="FGAdaHAT-567"><a href="#FGAdaHAT-567"><span class="linenos"> 567</span></a>                    <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="FGAdaHAT-568"><a href="#FGAdaHAT-568"><span class="linenos"> 568</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT-569"><a href="#FGAdaHAT-569"><span class="linenos"> 569</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;gradientshap_abs&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT-570"><a href="#FGAdaHAT-570"><span class="linenos"> 570</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_gradientshap_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT-571"><a href="#FGAdaHAT-571"><span class="linenos"> 571</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="FGAdaHAT-572"><a href="#FGAdaHAT-572"><span class="linenos"> 572</span></a>                    <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="FGAdaHAT-573"><a href="#FGAdaHAT-573"><span class="linenos"> 573</span></a>                    <span class="n">baselines</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="FGAdaHAT-574"><a href="#FGAdaHAT-574"><span class="linenos"> 574</span></a>                    <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="FGAdaHAT-575"><a href="#FGAdaHAT-575"><span class="linenos"> 575</span></a>                    <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="FGAdaHAT-576"><a href="#FGAdaHAT-576"><span class="linenos"> 576</span></a>                    <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="FGAdaHAT-577"><a href="#FGAdaHAT-577"><span class="linenos"> 577</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT-578"><a href="#FGAdaHAT-578"><span class="linenos"> 578</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;integrated_gradients_abs&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT-579"><a href="#FGAdaHAT-579"><span class="linenos"> 579</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FGAdaHAT-580"><a href="#FGAdaHAT-580"><span class="linenos"> 580</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_integrated_gradients_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT-581"><a href="#FGAdaHAT-581"><span class="linenos"> 581</span></a>                        <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="FGAdaHAT-582"><a href="#FGAdaHAT-582"><span class="linenos"> 582</span></a>                        <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="FGAdaHAT-583"><a href="#FGAdaHAT-583"><span class="linenos"> 583</span></a>                        <span class="n">baselines</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="FGAdaHAT-584"><a href="#FGAdaHAT-584"><span class="linenos"> 584</span></a>                        <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="FGAdaHAT-585"><a href="#FGAdaHAT-585"><span class="linenos"> 585</span></a>                        <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="FGAdaHAT-586"><a href="#FGAdaHAT-586"><span class="linenos"> 586</span></a>                        <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="FGAdaHAT-587"><a href="#FGAdaHAT-587"><span class="linenos"> 587</span></a>                    <span class="p">)</span>
</span><span id="FGAdaHAT-588"><a href="#FGAdaHAT-588"><span class="linenos"> 588</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT-589"><a href="#FGAdaHAT-589"><span class="linenos"> 589</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;feature_ablation_abs&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT-590"><a href="#FGAdaHAT-590"><span class="linenos"> 590</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_feature_ablation_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT-591"><a href="#FGAdaHAT-591"><span class="linenos"> 591</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="FGAdaHAT-592"><a href="#FGAdaHAT-592"><span class="linenos"> 592</span></a>                    <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="FGAdaHAT-593"><a href="#FGAdaHAT-593"><span class="linenos"> 593</span></a>                    <span class="n">layer_baselines</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="FGAdaHAT-594"><a href="#FGAdaHAT-594"><span class="linenos"> 594</span></a>                    <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="FGAdaHAT-595"><a href="#FGAdaHAT-595"><span class="linenos"> 595</span></a>                    <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="FGAdaHAT-596"><a href="#FGAdaHAT-596"><span class="linenos"> 596</span></a>                    <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="FGAdaHAT-597"><a href="#FGAdaHAT-597"><span class="linenos"> 597</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT-598"><a href="#FGAdaHAT-598"><span class="linenos"> 598</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;lrp_abs&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT-599"><a href="#FGAdaHAT-599"><span class="linenos"> 599</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_lrp_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT-600"><a href="#FGAdaHAT-600"><span class="linenos"> 600</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="FGAdaHAT-601"><a href="#FGAdaHAT-601"><span class="linenos"> 601</span></a>                    <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="FGAdaHAT-602"><a href="#FGAdaHAT-602"><span class="linenos"> 602</span></a>                    <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="FGAdaHAT-603"><a href="#FGAdaHAT-603"><span class="linenos"> 603</span></a>                    <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="FGAdaHAT-604"><a href="#FGAdaHAT-604"><span class="linenos"> 604</span></a>                    <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="FGAdaHAT-605"><a href="#FGAdaHAT-605"><span class="linenos"> 605</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT-606"><a href="#FGAdaHAT-606"><span class="linenos"> 606</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;cbp_adaptation&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT-607"><a href="#FGAdaHAT-607"><span class="linenos"> 607</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_weight_abs_sum</span><span class="p">(</span>
</span><span id="FGAdaHAT-608"><a href="#FGAdaHAT-608"><span class="linenos"> 608</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="FGAdaHAT-609"><a href="#FGAdaHAT-609"><span class="linenos"> 609</span></a>                    <span class="n">if_output_weight</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="FGAdaHAT-610"><a href="#FGAdaHAT-610"><span class="linenos"> 610</span></a>                    <span class="n">reciprocal</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="FGAdaHAT-611"><a href="#FGAdaHAT-611"><span class="linenos"> 611</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT-612"><a href="#FGAdaHAT-612"><span class="linenos"> 612</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;cbp_adaptive_contribution&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT-613"><a href="#FGAdaHAT-613"><span class="linenos"> 613</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FGAdaHAT-614"><a href="#FGAdaHAT-614"><span class="linenos"> 614</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_cbp_adaptive_contribution</span><span class="p">(</span>
</span><span id="FGAdaHAT-615"><a href="#FGAdaHAT-615"><span class="linenos"> 615</span></a>                        <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="FGAdaHAT-616"><a href="#FGAdaHAT-616"><span class="linenos"> 616</span></a>                        <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
</span><span id="FGAdaHAT-617"><a href="#FGAdaHAT-617"><span class="linenos"> 617</span></a>                    <span class="p">)</span>
</span><span id="FGAdaHAT-618"><a href="#FGAdaHAT-618"><span class="linenos"> 618</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT-619"><a href="#FGAdaHAT-619"><span class="linenos"> 619</span></a>
</span><span id="FGAdaHAT-620"><a href="#FGAdaHAT-620"><span class="linenos"> 620</span></a>            <span class="n">importance_step</span> <span class="o">=</span> <span class="n">min_max_normalize</span><span class="p">(</span>
</span><span id="FGAdaHAT-621"><a href="#FGAdaHAT-621"><span class="linenos"> 621</span></a>                <span class="n">importance_step</span>
</span><span id="FGAdaHAT-622"><a href="#FGAdaHAT-622"><span class="linenos"> 622</span></a>            <span class="p">)</span>  <span class="c1"># min-max scaling the utility to $[0, 1]$. See Eq. (5) in the paper</span>
</span><span id="FGAdaHAT-623"><a href="#FGAdaHAT-623"><span class="linenos"> 623</span></a>
</span><span id="FGAdaHAT-624"><a href="#FGAdaHAT-624"><span class="linenos"> 624</span></a>            <span class="c1"># multiply the importance by the training mask. See Eq. (6) in the paper</span>
</span><span id="FGAdaHAT-625"><a href="#FGAdaHAT-625"><span class="linenos"> 625</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_multiply_training_mask</span><span class="p">:</span>
</span><span id="FGAdaHAT-626"><a href="#FGAdaHAT-626"><span class="linenos"> 626</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="n">importance_step</span> <span class="o">*</span> <span class="n">mask</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="FGAdaHAT-627"><a href="#FGAdaHAT-627"><span class="linenos"> 627</span></a>
</span><span id="FGAdaHAT-628"><a href="#FGAdaHAT-628"><span class="linenos"> 628</span></a>            <span class="c1"># update accumulated importance</span>
</span><span id="FGAdaHAT-629"><a href="#FGAdaHAT-629"><span class="linenos"> 629</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">importances</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">][</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FGAdaHAT-630"><a href="#FGAdaHAT-630"><span class="linenos"> 630</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">importances</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">][</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">+</span> <span class="n">importance_step</span>
</span><span id="FGAdaHAT-631"><a href="#FGAdaHAT-631"><span class="linenos"> 631</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT-632"><a href="#FGAdaHAT-632"><span class="linenos"> 632</span></a>
</span><span id="FGAdaHAT-633"><a href="#FGAdaHAT-633"><span class="linenos"> 633</span></a>        <span class="c1"># update number of steps counter</span>
</span><span id="FGAdaHAT-634"><a href="#FGAdaHAT-634"><span class="linenos"> 634</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">num_steps_t</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span id="FGAdaHAT-635"><a href="#FGAdaHAT-635"><span class="linenos"> 635</span></a>
</span><span id="FGAdaHAT-636"><a href="#FGAdaHAT-636"><span class="linenos"> 636</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_end</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FGAdaHAT-637"><a href="#FGAdaHAT-637"><span class="linenos"> 637</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Additionally calculate neuron-wise importance for previous tasks at the end of training each task.&quot;&quot;&quot;</span>
</span><span id="FGAdaHAT-638"><a href="#FGAdaHAT-638"><span class="linenos"> 638</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">on_train_end</span><span class="p">()</span>  <span class="c1"># store the mask and update cumulative and summative masks</span>
</span><span id="FGAdaHAT-639"><a href="#FGAdaHAT-639"><span class="linenos"> 639</span></a>
</span><span id="FGAdaHAT-640"><a href="#FGAdaHAT-640"><span class="linenos"> 640</span></a>        <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="FGAdaHAT-641"><a href="#FGAdaHAT-641"><span class="linenos"> 641</span></a>
</span><span id="FGAdaHAT-642"><a href="#FGAdaHAT-642"><span class="linenos"> 642</span></a>            <span class="c1"># average the neuron-wise step importance. See Eq. (4) in the paper</span>
</span><span id="FGAdaHAT-643"><a href="#FGAdaHAT-643"><span class="linenos"> 643</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">importances</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">][</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FGAdaHAT-644"><a href="#FGAdaHAT-644"><span class="linenos"> 644</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">importances</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">][</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="FGAdaHAT-645"><a href="#FGAdaHAT-645"><span class="linenos"> 645</span></a>            <span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_steps_t</span>
</span><span id="FGAdaHAT-646"><a href="#FGAdaHAT-646"><span class="linenos"> 646</span></a>
</span><span id="FGAdaHAT-647"><a href="#FGAdaHAT-647"><span class="linenos"> 647</span></a>            <span class="c1"># add the base importance. See Eq. (6) in the paper</span>
</span><span id="FGAdaHAT-648"><a href="#FGAdaHAT-648"><span class="linenos"> 648</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">importances</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">][</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FGAdaHAT-649"><a href="#FGAdaHAT-649"><span class="linenos"> 649</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">importances</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">][</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_importance</span>
</span><span id="FGAdaHAT-650"><a href="#FGAdaHAT-650"><span class="linenos"> 650</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT-651"><a href="#FGAdaHAT-651"><span class="linenos"> 651</span></a>
</span><span id="FGAdaHAT-652"><a href="#FGAdaHAT-652"><span class="linenos"> 652</span></a>            <span class="c1"># filter unmasked importance</span>
</span><span id="FGAdaHAT-653"><a href="#FGAdaHAT-653"><span class="linenos"> 653</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">filter_unmasked_importance</span><span class="p">:</span>
</span><span id="FGAdaHAT-654"><a href="#FGAdaHAT-654"><span class="linenos"> 654</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">importances</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">][</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FGAdaHAT-655"><a href="#FGAdaHAT-655"><span class="linenos"> 655</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">importances</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">][</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="FGAdaHAT-656"><a href="#FGAdaHAT-656"><span class="linenos"> 656</span></a>                    <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">masks</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">][</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="FGAdaHAT-657"><a href="#FGAdaHAT-657"><span class="linenos"> 657</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT-658"><a href="#FGAdaHAT-658"><span class="linenos"> 658</span></a>
</span><span id="FGAdaHAT-659"><a href="#FGAdaHAT-659"><span class="linenos"> 659</span></a>            <span class="c1"># calculate the summative neuron-wise importance for previous tasks. See Eq. (4) in the paper</span>
</span><span id="FGAdaHAT-660"><a href="#FGAdaHAT-660"><span class="linenos"> 660</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy</span> <span class="o">==</span> <span class="s2">&quot;add_latest&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT-661"><a href="#FGAdaHAT-661"><span class="linenos"> 661</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">summative_importance_for_previous_tasks</span><span class="p">[</span>
</span><span id="FGAdaHAT-662"><a href="#FGAdaHAT-662"><span class="linenos"> 662</span></a>                    <span class="n">layer_name</span>
</span><span id="FGAdaHAT-663"><a href="#FGAdaHAT-663"><span class="linenos"> 663</span></a>                <span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">importances</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">][</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="FGAdaHAT-664"><a href="#FGAdaHAT-664"><span class="linenos"> 664</span></a>
</span><span id="FGAdaHAT-665"><a href="#FGAdaHAT-665"><span class="linenos"> 665</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy</span> <span class="o">==</span> <span class="s2">&quot;add_all&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT-666"><a href="#FGAdaHAT-666"><span class="linenos"> 666</span></a>                <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
</span><span id="FGAdaHAT-667"><a href="#FGAdaHAT-667"><span class="linenos"> 667</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">summative_importance_for_previous_tasks</span><span class="p">[</span>
</span><span id="FGAdaHAT-668"><a href="#FGAdaHAT-668"><span class="linenos"> 668</span></a>                        <span class="n">layer_name</span>
</span><span id="FGAdaHAT-669"><a href="#FGAdaHAT-669"><span class="linenos"> 669</span></a>                    <span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">importances</span><span class="p">[</span><span class="n">t</span><span class="p">][</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="FGAdaHAT-670"><a href="#FGAdaHAT-670"><span class="linenos"> 670</span></a>
</span><span id="FGAdaHAT-671"><a href="#FGAdaHAT-671"><span class="linenos"> 671</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy</span> <span class="o">==</span> <span class="s2">&quot;add_average&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT-672"><a href="#FGAdaHAT-672"><span class="linenos"> 672</span></a>                <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
</span><span id="FGAdaHAT-673"><a href="#FGAdaHAT-673"><span class="linenos"> 673</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">summative_importance_for_previous_tasks</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span>
</span><span id="FGAdaHAT-674"><a href="#FGAdaHAT-674"><span class="linenos"> 674</span></a>                        <span class="bp">self</span><span class="o">.</span><span class="n">importances</span><span class="p">[</span><span class="n">t</span><span class="p">][</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span>
</span><span id="FGAdaHAT-675"><a href="#FGAdaHAT-675"><span class="linenos"> 675</span></a>                    <span class="p">)</span>
</span><span id="FGAdaHAT-676"><a href="#FGAdaHAT-676"><span class="linenos"> 676</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="FGAdaHAT-677"><a href="#FGAdaHAT-677"><span class="linenos"> 677</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">summative_importance_for_previous_tasks</span><span class="p">[</span>
</span><span id="FGAdaHAT-678"><a href="#FGAdaHAT-678"><span class="linenos"> 678</span></a>                    <span class="n">layer_name</span>
</span><span id="FGAdaHAT-679"><a href="#FGAdaHAT-679"><span class="linenos"> 679</span></a>                <span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span>
</span><span id="FGAdaHAT-680"><a href="#FGAdaHAT-680"><span class="linenos"> 680</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">summative_importance_for_previous_tasks</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="FGAdaHAT-681"><a href="#FGAdaHAT-681"><span class="linenos"> 681</span></a>                <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
</span><span id="FGAdaHAT-682"><a href="#FGAdaHAT-682"><span class="linenos"> 682</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
</span><span id="FGAdaHAT-683"><a href="#FGAdaHAT-683"><span class="linenos"> 683</span></a>                <span class="p">)</span>  <span class="c1"># starting adding from 0</span>
</span><span id="FGAdaHAT-684"><a href="#FGAdaHAT-684"><span class="linenos"> 684</span></a>
</span><span id="FGAdaHAT-685"><a href="#FGAdaHAT-685"><span class="linenos"> 685</span></a>                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy</span> <span class="o">==</span> <span class="s2">&quot;linear_decrease&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT-686"><a href="#FGAdaHAT-686"><span class="linenos"> 686</span></a>                    <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy_linear_step</span>
</span><span id="FGAdaHAT-687"><a href="#FGAdaHAT-687"><span class="linenos"> 687</span></a>                    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
</span><span id="FGAdaHAT-688"><a href="#FGAdaHAT-688"><span class="linenos"> 688</span></a>                        <span class="n">w_t</span> <span class="o">=</span> <span class="n">s</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">-</span> <span class="n">t</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="FGAdaHAT-689"><a href="#FGAdaHAT-689"><span class="linenos"> 689</span></a>
</span><span id="FGAdaHAT-690"><a href="#FGAdaHAT-690"><span class="linenos"> 690</span></a>                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy</span> <span class="o">==</span> <span class="s2">&quot;quadratic_decrease&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT-691"><a href="#FGAdaHAT-691"><span class="linenos"> 691</span></a>                    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
</span><span id="FGAdaHAT-692"><a href="#FGAdaHAT-692"><span class="linenos"> 692</span></a>                        <span class="n">w_t</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">-</span> <span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
</span><span id="FGAdaHAT-693"><a href="#FGAdaHAT-693"><span class="linenos"> 693</span></a>                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy</span> <span class="o">==</span> <span class="s2">&quot;cubic_decrease&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT-694"><a href="#FGAdaHAT-694"><span class="linenos"> 694</span></a>                    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
</span><span id="FGAdaHAT-695"><a href="#FGAdaHAT-695"><span class="linenos"> 695</span></a>                        <span class="n">w_t</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">-</span> <span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">3</span>
</span><span id="FGAdaHAT-696"><a href="#FGAdaHAT-696"><span class="linenos"> 696</span></a>                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy</span> <span class="o">==</span> <span class="s2">&quot;exponential_decrease&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT-697"><a href="#FGAdaHAT-697"><span class="linenos"> 697</span></a>                    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
</span><span id="FGAdaHAT-698"><a href="#FGAdaHAT-698"><span class="linenos"> 698</span></a>                        <span class="n">r</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy_exponential_rate</span>
</span><span id="FGAdaHAT-699"><a href="#FGAdaHAT-699"><span class="linenos"> 699</span></a>
</span><span id="FGAdaHAT-700"><a href="#FGAdaHAT-700"><span class="linenos"> 700</span></a>                        <span class="n">w_t</span> <span class="o">=</span> <span class="n">r</span> <span class="o">**</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">-</span> <span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="FGAdaHAT-701"><a href="#FGAdaHAT-701"><span class="linenos"> 701</span></a>                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy</span> <span class="o">==</span> <span class="s2">&quot;log_decrease&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT-702"><a href="#FGAdaHAT-702"><span class="linenos"> 702</span></a>                    <span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy_log_base</span>
</span><span id="FGAdaHAT-703"><a href="#FGAdaHAT-703"><span class="linenos"> 703</span></a>                    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
</span><span id="FGAdaHAT-704"><a href="#FGAdaHAT-704"><span class="linenos"> 704</span></a>                        <span class="n">w_t</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">-</span> <span class="n">t</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="FGAdaHAT-705"><a href="#FGAdaHAT-705"><span class="linenos"> 705</span></a>                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy</span> <span class="o">==</span> <span class="s2">&quot;factorial_decrease&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT-706"><a href="#FGAdaHAT-706"><span class="linenos"> 706</span></a>                    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
</span><span id="FGAdaHAT-707"><a href="#FGAdaHAT-707"><span class="linenos"> 707</span></a>                        <span class="n">w_t</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">factorial</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">-</span> <span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="FGAdaHAT-708"><a href="#FGAdaHAT-708"><span class="linenos"> 708</span></a>                <span class="k">else</span><span class="p">:</span>
</span><span id="FGAdaHAT-709"><a href="#FGAdaHAT-709"><span class="linenos"> 709</span></a>                    <span class="k">raise</span> <span class="ne">ValueError</span>
</span><span id="FGAdaHAT-710"><a href="#FGAdaHAT-710"><span class="linenos"> 710</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">summative_importance_for_previous_tasks</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span>
</span><span id="FGAdaHAT-711"><a href="#FGAdaHAT-711"><span class="linenos"> 711</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">importances</span><span class="p">[</span><span class="n">t</span><span class="p">][</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">*</span> <span class="n">w_t</span>
</span><span id="FGAdaHAT-712"><a href="#FGAdaHAT-712"><span class="linenos"> 712</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT-713"><a href="#FGAdaHAT-713"><span class="linenos"> 713</span></a>
</span><span id="FGAdaHAT-714"><a href="#FGAdaHAT-714"><span class="linenos"> 714</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_weight_abs_sum</span><span class="p">(</span>
</span><span id="FGAdaHAT-715"><a href="#FGAdaHAT-715"><span class="linenos"> 715</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT-716"><a href="#FGAdaHAT-716"><span class="linenos"> 716</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT-717"><a href="#FGAdaHAT-717"><span class="linenos"> 717</span></a>        <span class="n">if_output_weight</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
</span><span id="FGAdaHAT-718"><a href="#FGAdaHAT-718"><span class="linenos"> 718</span></a>        <span class="n">reciprocal</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
</span><span id="FGAdaHAT-719"><a href="#FGAdaHAT-719"><span class="linenos"> 719</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="FGAdaHAT-720"><a href="#FGAdaHAT-720"><span class="linenos"> 720</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the sum of absolute values of layer input or output weights.</span>
</span><span id="FGAdaHAT-721"><a href="#FGAdaHAT-721"><span class="linenos"> 721</span></a>
</span><span id="FGAdaHAT-722"><a href="#FGAdaHAT-722"><span class="linenos"> 722</span></a><span class="sd">        **Args:**</span>
</span><span id="FGAdaHAT-723"><a href="#FGAdaHAT-723"><span class="linenos"> 723</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="FGAdaHAT-724"><a href="#FGAdaHAT-724"><span class="linenos"> 724</span></a><span class="sd">        - **if_output_weight** (`bool`): whether to use the output weights or input weights.</span>
</span><span id="FGAdaHAT-725"><a href="#FGAdaHAT-725"><span class="linenos"> 725</span></a><span class="sd">        - **reciprocal** (`bool`): whether to take reciprocal.</span>
</span><span id="FGAdaHAT-726"><a href="#FGAdaHAT-726"><span class="linenos"> 726</span></a>
</span><span id="FGAdaHAT-727"><a href="#FGAdaHAT-727"><span class="linenos"> 727</span></a><span class="sd">        **Returns:**</span>
</span><span id="FGAdaHAT-728"><a href="#FGAdaHAT-728"><span class="linenos"> 728</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="FGAdaHAT-729"><a href="#FGAdaHAT-729"><span class="linenos"> 729</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT-730"><a href="#FGAdaHAT-730"><span class="linenos"> 730</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="FGAdaHAT-731"><a href="#FGAdaHAT-731"><span class="linenos"> 731</span></a>
</span><span id="FGAdaHAT-732"><a href="#FGAdaHAT-732"><span class="linenos"> 732</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="n">if_output_weight</span><span class="p">:</span>
</span><span id="FGAdaHAT-733"><a href="#FGAdaHAT-733"><span class="linenos"> 733</span></a>            <span class="n">weight_abs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span><span id="FGAdaHAT-734"><a href="#FGAdaHAT-734"><span class="linenos"> 734</span></a>            <span class="n">weight_abs_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
</span><span id="FGAdaHAT-735"><a href="#FGAdaHAT-735"><span class="linenos"> 735</span></a>                <span class="n">weight_abs</span><span class="p">,</span>
</span><span id="FGAdaHAT-736"><a href="#FGAdaHAT-736"><span class="linenos"> 736</span></a>                <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT-737"><a href="#FGAdaHAT-737"><span class="linenos"> 737</span></a>                    <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">weight_abs</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">0</span>
</span><span id="FGAdaHAT-738"><a href="#FGAdaHAT-738"><span class="linenos"> 738</span></a>                <span class="p">],</span>  <span class="c1"># sum over the input dimension</span>
</span><span id="FGAdaHAT-739"><a href="#FGAdaHAT-739"><span class="linenos"> 739</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT-740"><a href="#FGAdaHAT-740"><span class="linenos"> 740</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="FGAdaHAT-741"><a href="#FGAdaHAT-741"><span class="linenos"> 741</span></a>            <span class="n">weight_abs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">next_layer</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span><span id="FGAdaHAT-742"><a href="#FGAdaHAT-742"><span class="linenos"> 742</span></a>            <span class="n">weight_abs_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
</span><span id="FGAdaHAT-743"><a href="#FGAdaHAT-743"><span class="linenos"> 743</span></a>                <span class="n">weight_abs</span><span class="p">,</span>
</span><span id="FGAdaHAT-744"><a href="#FGAdaHAT-744"><span class="linenos"> 744</span></a>                <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT-745"><a href="#FGAdaHAT-745"><span class="linenos"> 745</span></a>                    <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">weight_abs</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="FGAdaHAT-746"><a href="#FGAdaHAT-746"><span class="linenos"> 746</span></a>                <span class="p">],</span>  <span class="c1"># sum over the output dimension</span>
</span><span id="FGAdaHAT-747"><a href="#FGAdaHAT-747"><span class="linenos"> 747</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT-748"><a href="#FGAdaHAT-748"><span class="linenos"> 748</span></a>
</span><span id="FGAdaHAT-749"><a href="#FGAdaHAT-749"><span class="linenos"> 749</span></a>        <span class="k">if</span> <span class="n">reciprocal</span><span class="p">:</span>
</span><span id="FGAdaHAT-750"><a href="#FGAdaHAT-750"><span class="linenos"> 750</span></a>            <span class="n">weight_abs_sum_reciprocal</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">reciprocal</span><span class="p">(</span><span class="n">weight_abs_sum</span><span class="p">)</span>
</span><span id="FGAdaHAT-751"><a href="#FGAdaHAT-751"><span class="linenos"> 751</span></a>            <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">weight_abs_sum_reciprocal</span>
</span><span id="FGAdaHAT-752"><a href="#FGAdaHAT-752"><span class="linenos"> 752</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="FGAdaHAT-753"><a href="#FGAdaHAT-753"><span class="linenos"> 753</span></a>            <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">weight_abs_sum</span>
</span><span id="FGAdaHAT-754"><a href="#FGAdaHAT-754"><span class="linenos"> 754</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="FGAdaHAT-755"><a href="#FGAdaHAT-755"><span class="linenos"> 755</span></a>
</span><span id="FGAdaHAT-756"><a href="#FGAdaHAT-756"><span class="linenos"> 756</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span><span id="FGAdaHAT-757"><a href="#FGAdaHAT-757"><span class="linenos"> 757</span></a>
</span><span id="FGAdaHAT-758"><a href="#FGAdaHAT-758"><span class="linenos"> 758</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_weight_gradient_abs_sum</span><span class="p">(</span>
</span><span id="FGAdaHAT-759"><a href="#FGAdaHAT-759"><span class="linenos"> 759</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT-760"><a href="#FGAdaHAT-760"><span class="linenos"> 760</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT-761"><a href="#FGAdaHAT-761"><span class="linenos"> 761</span></a>        <span class="n">if_output_weight</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
</span><span id="FGAdaHAT-762"><a href="#FGAdaHAT-762"><span class="linenos"> 762</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="FGAdaHAT-763"><a href="#FGAdaHAT-763"><span class="linenos"> 763</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the sum of absolute values of gradients of the layer input or output weights.</span>
</span><span id="FGAdaHAT-764"><a href="#FGAdaHAT-764"><span class="linenos"> 764</span></a>
</span><span id="FGAdaHAT-765"><a href="#FGAdaHAT-765"><span class="linenos"> 765</span></a><span class="sd">        **Args:**</span>
</span><span id="FGAdaHAT-766"><a href="#FGAdaHAT-766"><span class="linenos"> 766</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="FGAdaHAT-767"><a href="#FGAdaHAT-767"><span class="linenos"> 767</span></a><span class="sd">        - **if_output_weight** (`bool`): whether to use the output weights or input weights.</span>
</span><span id="FGAdaHAT-768"><a href="#FGAdaHAT-768"><span class="linenos"> 768</span></a>
</span><span id="FGAdaHAT-769"><a href="#FGAdaHAT-769"><span class="linenos"> 769</span></a><span class="sd">        **Returns:**</span>
</span><span id="FGAdaHAT-770"><a href="#FGAdaHAT-770"><span class="linenos"> 770</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="FGAdaHAT-771"><a href="#FGAdaHAT-771"><span class="linenos"> 771</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT-772"><a href="#FGAdaHAT-772"><span class="linenos"> 772</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="FGAdaHAT-773"><a href="#FGAdaHAT-773"><span class="linenos"> 773</span></a>
</span><span id="FGAdaHAT-774"><a href="#FGAdaHAT-774"><span class="linenos"> 774</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="n">if_output_weight</span><span class="p">:</span>
</span><span id="FGAdaHAT-775"><a href="#FGAdaHAT-775"><span class="linenos"> 775</span></a>            <span class="n">gradient_abs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span><span id="FGAdaHAT-776"><a href="#FGAdaHAT-776"><span class="linenos"> 776</span></a>            <span class="n">gradient_abs_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
</span><span id="FGAdaHAT-777"><a href="#FGAdaHAT-777"><span class="linenos"> 777</span></a>                <span class="n">gradient_abs</span><span class="p">,</span>
</span><span id="FGAdaHAT-778"><a href="#FGAdaHAT-778"><span class="linenos"> 778</span></a>                <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT-779"><a href="#FGAdaHAT-779"><span class="linenos"> 779</span></a>                    <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">gradient_abs</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">0</span>
</span><span id="FGAdaHAT-780"><a href="#FGAdaHAT-780"><span class="linenos"> 780</span></a>                <span class="p">],</span>  <span class="c1"># sum over the input dimension</span>
</span><span id="FGAdaHAT-781"><a href="#FGAdaHAT-781"><span class="linenos"> 781</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT-782"><a href="#FGAdaHAT-782"><span class="linenos"> 782</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="FGAdaHAT-783"><a href="#FGAdaHAT-783"><span class="linenos"> 783</span></a>            <span class="n">gradient_abs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">next_layer</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span><span id="FGAdaHAT-784"><a href="#FGAdaHAT-784"><span class="linenos"> 784</span></a>            <span class="n">gradient_abs_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
</span><span id="FGAdaHAT-785"><a href="#FGAdaHAT-785"><span class="linenos"> 785</span></a>                <span class="n">gradient_abs</span><span class="p">,</span>
</span><span id="FGAdaHAT-786"><a href="#FGAdaHAT-786"><span class="linenos"> 786</span></a>                <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT-787"><a href="#FGAdaHAT-787"><span class="linenos"> 787</span></a>                    <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">gradient_abs</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="FGAdaHAT-788"><a href="#FGAdaHAT-788"><span class="linenos"> 788</span></a>                <span class="p">],</span>  <span class="c1"># sum over the output dimension</span>
</span><span id="FGAdaHAT-789"><a href="#FGAdaHAT-789"><span class="linenos"> 789</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT-790"><a href="#FGAdaHAT-790"><span class="linenos"> 790</span></a>
</span><span id="FGAdaHAT-791"><a href="#FGAdaHAT-791"><span class="linenos"> 791</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">gradient_abs_sum</span>
</span><span id="FGAdaHAT-792"><a href="#FGAdaHAT-792"><span class="linenos"> 792</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="FGAdaHAT-793"><a href="#FGAdaHAT-793"><span class="linenos"> 793</span></a>
</span><span id="FGAdaHAT-794"><a href="#FGAdaHAT-794"><span class="linenos"> 794</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span><span id="FGAdaHAT-795"><a href="#FGAdaHAT-795"><span class="linenos"> 795</span></a>
</span><span id="FGAdaHAT-796"><a href="#FGAdaHAT-796"><span class="linenos"> 796</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_activation_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT-797"><a href="#FGAdaHAT-797"><span class="linenos"> 797</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT-798"><a href="#FGAdaHAT-798"><span class="linenos"> 798</span></a>        <span class="n">activation</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="FGAdaHAT-799"><a href="#FGAdaHAT-799"><span class="linenos"> 799</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="FGAdaHAT-800"><a href="#FGAdaHAT-800"><span class="linenos"> 800</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute value of activation of the layer. This is our own implementation of [Layer Activation](https://captum.ai/api/layer.html#layer-activation) in Captum.</span>
</span><span id="FGAdaHAT-801"><a href="#FGAdaHAT-801"><span class="linenos"> 801</span></a>
</span><span id="FGAdaHAT-802"><a href="#FGAdaHAT-802"><span class="linenos"> 802</span></a><span class="sd">        **Args:**</span>
</span><span id="FGAdaHAT-803"><a href="#FGAdaHAT-803"><span class="linenos"> 803</span></a><span class="sd">        - **activation** (`Tensor`): the activation tensor of the layer. It has the same size of (number of units, ).</span>
</span><span id="FGAdaHAT-804"><a href="#FGAdaHAT-804"><span class="linenos"> 804</span></a>
</span><span id="FGAdaHAT-805"><a href="#FGAdaHAT-805"><span class="linenos"> 805</span></a><span class="sd">        **Returns:**</span>
</span><span id="FGAdaHAT-806"><a href="#FGAdaHAT-806"><span class="linenos"> 806</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="FGAdaHAT-807"><a href="#FGAdaHAT-807"><span class="linenos"> 807</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT-808"><a href="#FGAdaHAT-808"><span class="linenos"> 808</span></a>        <span class="n">activation_abs_batch_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
</span><span id="FGAdaHAT-809"><a href="#FGAdaHAT-809"><span class="linenos"> 809</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">activation</span><span class="p">),</span>
</span><span id="FGAdaHAT-810"><a href="#FGAdaHAT-810"><span class="linenos"> 810</span></a>            <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT-811"><a href="#FGAdaHAT-811"><span class="linenos"> 811</span></a>                <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">activation</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="FGAdaHAT-812"><a href="#FGAdaHAT-812"><span class="linenos"> 812</span></a>            <span class="p">],</span>  <span class="c1"># average the features over batch samples</span>
</span><span id="FGAdaHAT-813"><a href="#FGAdaHAT-813"><span class="linenos"> 813</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT-814"><a href="#FGAdaHAT-814"><span class="linenos"> 814</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">activation_abs_batch_mean</span>
</span><span id="FGAdaHAT-815"><a href="#FGAdaHAT-815"><span class="linenos"> 815</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="FGAdaHAT-816"><a href="#FGAdaHAT-816"><span class="linenos"> 816</span></a>
</span><span id="FGAdaHAT-817"><a href="#FGAdaHAT-817"><span class="linenos"> 817</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span><span id="FGAdaHAT-818"><a href="#FGAdaHAT-818"><span class="linenos"> 818</span></a>
</span><span id="FGAdaHAT-819"><a href="#FGAdaHAT-819"><span class="linenos"> 819</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_weight_abs_sum_x_activation_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT-820"><a href="#FGAdaHAT-820"><span class="linenos"> 820</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT-821"><a href="#FGAdaHAT-821"><span class="linenos"> 821</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT-822"><a href="#FGAdaHAT-822"><span class="linenos"> 822</span></a>        <span class="n">activation</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="FGAdaHAT-823"><a href="#FGAdaHAT-823"><span class="linenos"> 823</span></a>        <span class="n">if_output_weight</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
</span><span id="FGAdaHAT-824"><a href="#FGAdaHAT-824"><span class="linenos"> 824</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="FGAdaHAT-825"><a href="#FGAdaHAT-825"><span class="linenos"> 825</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the sum of absolute values of layer input / output weights multiplied by absolute values of activation. The input weights version is equal to the contribution utility in [CBP](https://www.nature.com/articles/s41586-024-07711-7).</span>
</span><span id="FGAdaHAT-826"><a href="#FGAdaHAT-826"><span class="linenos"> 826</span></a>
</span><span id="FGAdaHAT-827"><a href="#FGAdaHAT-827"><span class="linenos"> 827</span></a><span class="sd">        **Args:**</span>
</span><span id="FGAdaHAT-828"><a href="#FGAdaHAT-828"><span class="linenos"> 828</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="FGAdaHAT-829"><a href="#FGAdaHAT-829"><span class="linenos"> 829</span></a><span class="sd">        - **activation** (`Tensor`): the activation tensor of the layer. It has the same size of (number of units, ).</span>
</span><span id="FGAdaHAT-830"><a href="#FGAdaHAT-830"><span class="linenos"> 830</span></a><span class="sd">        - **if_output_weight** (`bool`): whether to use the output weights or input weights.</span>
</span><span id="FGAdaHAT-831"><a href="#FGAdaHAT-831"><span class="linenos"> 831</span></a>
</span><span id="FGAdaHAT-832"><a href="#FGAdaHAT-832"><span class="linenos"> 832</span></a><span class="sd">        **Returns:**</span>
</span><span id="FGAdaHAT-833"><a href="#FGAdaHAT-833"><span class="linenos"> 833</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="FGAdaHAT-834"><a href="#FGAdaHAT-834"><span class="linenos"> 834</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT-835"><a href="#FGAdaHAT-835"><span class="linenos"> 835</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="FGAdaHAT-836"><a href="#FGAdaHAT-836"><span class="linenos"> 836</span></a>
</span><span id="FGAdaHAT-837"><a href="#FGAdaHAT-837"><span class="linenos"> 837</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="n">if_output_weight</span><span class="p">:</span>
</span><span id="FGAdaHAT-838"><a href="#FGAdaHAT-838"><span class="linenos"> 838</span></a>            <span class="n">weight_abs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span><span id="FGAdaHAT-839"><a href="#FGAdaHAT-839"><span class="linenos"> 839</span></a>            <span class="n">weight_abs_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
</span><span id="FGAdaHAT-840"><a href="#FGAdaHAT-840"><span class="linenos"> 840</span></a>                <span class="n">weight_abs</span><span class="p">,</span>
</span><span id="FGAdaHAT-841"><a href="#FGAdaHAT-841"><span class="linenos"> 841</span></a>                <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT-842"><a href="#FGAdaHAT-842"><span class="linenos"> 842</span></a>                    <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">weight_abs</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">0</span>
</span><span id="FGAdaHAT-843"><a href="#FGAdaHAT-843"><span class="linenos"> 843</span></a>                <span class="p">],</span>  <span class="c1"># sum over the input dimension</span>
</span><span id="FGAdaHAT-844"><a href="#FGAdaHAT-844"><span class="linenos"> 844</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT-845"><a href="#FGAdaHAT-845"><span class="linenos"> 845</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="FGAdaHAT-846"><a href="#FGAdaHAT-846"><span class="linenos"> 846</span></a>            <span class="n">weight_abs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">next_layer</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span><span id="FGAdaHAT-847"><a href="#FGAdaHAT-847"><span class="linenos"> 847</span></a>            <span class="n">weight_abs_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
</span><span id="FGAdaHAT-848"><a href="#FGAdaHAT-848"><span class="linenos"> 848</span></a>                <span class="n">weight_abs</span><span class="p">,</span>
</span><span id="FGAdaHAT-849"><a href="#FGAdaHAT-849"><span class="linenos"> 849</span></a>                <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT-850"><a href="#FGAdaHAT-850"><span class="linenos"> 850</span></a>                    <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">weight_abs</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="FGAdaHAT-851"><a href="#FGAdaHAT-851"><span class="linenos"> 851</span></a>                <span class="p">],</span>  <span class="c1"># sum over the output dimension</span>
</span><span id="FGAdaHAT-852"><a href="#FGAdaHAT-852"><span class="linenos"> 852</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT-853"><a href="#FGAdaHAT-853"><span class="linenos"> 853</span></a>
</span><span id="FGAdaHAT-854"><a href="#FGAdaHAT-854"><span class="linenos"> 854</span></a>        <span class="n">activation_abs_batch_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
</span><span id="FGAdaHAT-855"><a href="#FGAdaHAT-855"><span class="linenos"> 855</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">activation</span><span class="p">),</span>
</span><span id="FGAdaHAT-856"><a href="#FGAdaHAT-856"><span class="linenos"> 856</span></a>            <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT-857"><a href="#FGAdaHAT-857"><span class="linenos"> 857</span></a>                <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">activation</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="FGAdaHAT-858"><a href="#FGAdaHAT-858"><span class="linenos"> 858</span></a>            <span class="p">],</span>  <span class="c1"># average the features over batch samples</span>
</span><span id="FGAdaHAT-859"><a href="#FGAdaHAT-859"><span class="linenos"> 859</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT-860"><a href="#FGAdaHAT-860"><span class="linenos"> 860</span></a>
</span><span id="FGAdaHAT-861"><a href="#FGAdaHAT-861"><span class="linenos"> 861</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">weight_abs_sum</span> <span class="o">*</span> <span class="n">activation_abs_batch_mean</span>
</span><span id="FGAdaHAT-862"><a href="#FGAdaHAT-862"><span class="linenos"> 862</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="FGAdaHAT-863"><a href="#FGAdaHAT-863"><span class="linenos"> 863</span></a>
</span><span id="FGAdaHAT-864"><a href="#FGAdaHAT-864"><span class="linenos"> 864</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span><span id="FGAdaHAT-865"><a href="#FGAdaHAT-865"><span class="linenos"> 865</span></a>
</span><span id="FGAdaHAT-866"><a href="#FGAdaHAT-866"><span class="linenos"> 866</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_gradient_x_activation_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT-867"><a href="#FGAdaHAT-867"><span class="linenos"> 867</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT-868"><a href="#FGAdaHAT-868"><span class="linenos"> 868</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT-869"><a href="#FGAdaHAT-869"><span class="linenos"> 869</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="FGAdaHAT-870"><a href="#FGAdaHAT-870"><span class="linenos"> 870</span></a>        <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FGAdaHAT-871"><a href="#FGAdaHAT-871"><span class="linenos"> 871</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FGAdaHAT-872"><a href="#FGAdaHAT-872"><span class="linenos"> 872</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FGAdaHAT-873"><a href="#FGAdaHAT-873"><span class="linenos"> 873</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="FGAdaHAT-874"><a href="#FGAdaHAT-874"><span class="linenos"> 874</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of the gradient of layer activation multiplied by the activation. We implement this using [Layer Gradient X Activation](https://captum.ai/api/layer.html#layer-gradient-x-activation) in Captum.</span>
</span><span id="FGAdaHAT-875"><a href="#FGAdaHAT-875"><span class="linenos"> 875</span></a>
</span><span id="FGAdaHAT-876"><a href="#FGAdaHAT-876"><span class="linenos"> 876</span></a><span class="sd">        **Args:**</span>
</span><span id="FGAdaHAT-877"><a href="#FGAdaHAT-877"><span class="linenos"> 877</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="FGAdaHAT-878"><a href="#FGAdaHAT-878"><span class="linenos"> 878</span></a><span class="sd">        - **input** (`Tensor` | `tuple[Tensor, ...]`): the input batch of the training step.</span>
</span><span id="FGAdaHAT-879"><a href="#FGAdaHAT-879"><span class="linenos"> 879</span></a><span class="sd">        - **target** (`Tensor` | `None`): the target batch of the training step.</span>
</span><span id="FGAdaHAT-880"><a href="#FGAdaHAT-880"><span class="linenos"> 880</span></a><span class="sd">        - **batch_idx** (`int`): the index of the current batch. This is an argument of the forward function during training.</span>
</span><span id="FGAdaHAT-881"><a href="#FGAdaHAT-881"><span class="linenos"> 881</span></a><span class="sd">        - **num_batches** (`int`): the number of batches in the training step. This is an argument of the forward function during training.</span>
</span><span id="FGAdaHAT-882"><a href="#FGAdaHAT-882"><span class="linenos"> 882</span></a>
</span><span id="FGAdaHAT-883"><a href="#FGAdaHAT-883"><span class="linenos"> 883</span></a><span class="sd">        **Returns:**</span>
</span><span id="FGAdaHAT-884"><a href="#FGAdaHAT-884"><span class="linenos"> 884</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="FGAdaHAT-885"><a href="#FGAdaHAT-885"><span class="linenos"> 885</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT-886"><a href="#FGAdaHAT-886"><span class="linenos"> 886</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="FGAdaHAT-887"><a href="#FGAdaHAT-887"><span class="linenos"> 887</span></a>
</span><span id="FGAdaHAT-888"><a href="#FGAdaHAT-888"><span class="linenos"> 888</span></a>        <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
</span><span id="FGAdaHAT-889"><a href="#FGAdaHAT-889"><span class="linenos"> 889</span></a>
</span><span id="FGAdaHAT-890"><a href="#FGAdaHAT-890"><span class="linenos"> 890</span></a>        <span class="c1"># initialize the Layer Gradient X Activation object</span>
</span><span id="FGAdaHAT-891"><a href="#FGAdaHAT-891"><span class="linenos"> 891</span></a>        <span class="n">layer_gradient_x_activation</span> <span class="o">=</span> <span class="n">LayerGradientXActivation</span><span class="p">(</span>
</span><span id="FGAdaHAT-892"><a href="#FGAdaHAT-892"><span class="linenos"> 892</span></a>            <span class="n">forward_func</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="n">layer</span>
</span><span id="FGAdaHAT-893"><a href="#FGAdaHAT-893"><span class="linenos"> 893</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT-894"><a href="#FGAdaHAT-894"><span class="linenos"> 894</span></a>
</span><span id="FGAdaHAT-895"><a href="#FGAdaHAT-895"><span class="linenos"> 895</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</span><span id="FGAdaHAT-896"><a href="#FGAdaHAT-896"><span class="linenos"> 896</span></a>        <span class="c1"># calculate layer attribution of the step</span>
</span><span id="FGAdaHAT-897"><a href="#FGAdaHAT-897"><span class="linenos"> 897</span></a>        <span class="n">attribution</span> <span class="o">=</span> <span class="n">layer_gradient_x_activation</span><span class="o">.</span><span class="n">attribute</span><span class="p">(</span>
</span><span id="FGAdaHAT-898"><a href="#FGAdaHAT-898"><span class="linenos"> 898</span></a>            <span class="n">inputs</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="FGAdaHAT-899"><a href="#FGAdaHAT-899"><span class="linenos"> 899</span></a>            <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="FGAdaHAT-900"><a href="#FGAdaHAT-900"><span class="linenos"> 900</span></a>            <span class="n">additional_forward_args</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">num_batches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">),</span>
</span><span id="FGAdaHAT-901"><a href="#FGAdaHAT-901"><span class="linenos"> 901</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT-902"><a href="#FGAdaHAT-902"><span class="linenos"> 902</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</span><span id="FGAdaHAT-903"><a href="#FGAdaHAT-903"><span class="linenos"> 903</span></a>
</span><span id="FGAdaHAT-904"><a href="#FGAdaHAT-904"><span class="linenos"> 904</span></a>        <span class="n">attribution_abs_batch_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
</span><span id="FGAdaHAT-905"><a href="#FGAdaHAT-905"><span class="linenos"> 905</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">attribution</span><span class="p">),</span>
</span><span id="FGAdaHAT-906"><a href="#FGAdaHAT-906"><span class="linenos"> 906</span></a>            <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT-907"><a href="#FGAdaHAT-907"><span class="linenos"> 907</span></a>                <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">attribution</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="FGAdaHAT-908"><a href="#FGAdaHAT-908"><span class="linenos"> 908</span></a>            <span class="p">],</span>  <span class="c1"># average the features over batch samples</span>
</span><span id="FGAdaHAT-909"><a href="#FGAdaHAT-909"><span class="linenos"> 909</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT-910"><a href="#FGAdaHAT-910"><span class="linenos"> 910</span></a>
</span><span id="FGAdaHAT-911"><a href="#FGAdaHAT-911"><span class="linenos"> 911</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">attribution_abs_batch_mean</span>
</span><span id="FGAdaHAT-912"><a href="#FGAdaHAT-912"><span class="linenos"> 912</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="FGAdaHAT-913"><a href="#FGAdaHAT-913"><span class="linenos"> 913</span></a>
</span><span id="FGAdaHAT-914"><a href="#FGAdaHAT-914"><span class="linenos"> 914</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span><span id="FGAdaHAT-915"><a href="#FGAdaHAT-915"><span class="linenos"> 915</span></a>
</span><span id="FGAdaHAT-916"><a href="#FGAdaHAT-916"><span class="linenos"> 916</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_weight_gradient_square_sum</span><span class="p">(</span>
</span><span id="FGAdaHAT-917"><a href="#FGAdaHAT-917"><span class="linenos"> 917</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT-918"><a href="#FGAdaHAT-918"><span class="linenos"> 918</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT-919"><a href="#FGAdaHAT-919"><span class="linenos"> 919</span></a>        <span class="n">activation</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="FGAdaHAT-920"><a href="#FGAdaHAT-920"><span class="linenos"> 920</span></a>        <span class="n">if_output_weight</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
</span><span id="FGAdaHAT-921"><a href="#FGAdaHAT-921"><span class="linenos"> 921</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="FGAdaHAT-922"><a href="#FGAdaHAT-922"><span class="linenos"> 922</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the sum of layer weight gradient squares. The weight gradient square is equal to fisher information in [EWC](https://www.pnas.org/doi/10.1073/pnas.1611835114).</span>
</span><span id="FGAdaHAT-923"><a href="#FGAdaHAT-923"><span class="linenos"> 923</span></a>
</span><span id="FGAdaHAT-924"><a href="#FGAdaHAT-924"><span class="linenos"> 924</span></a><span class="sd">        **Args:**</span>
</span><span id="FGAdaHAT-925"><a href="#FGAdaHAT-925"><span class="linenos"> 925</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="FGAdaHAT-926"><a href="#FGAdaHAT-926"><span class="linenos"> 926</span></a><span class="sd">        - **activation** (`Tensor`): the activation tensor of the layer. It has the same size of (number of units, ).</span>
</span><span id="FGAdaHAT-927"><a href="#FGAdaHAT-927"><span class="linenos"> 927</span></a><span class="sd">        - **if_output_weight** (`bool`): whether to use the output weights or input weights.</span>
</span><span id="FGAdaHAT-928"><a href="#FGAdaHAT-928"><span class="linenos"> 928</span></a>
</span><span id="FGAdaHAT-929"><a href="#FGAdaHAT-929"><span class="linenos"> 929</span></a><span class="sd">        **Returns:**</span>
</span><span id="FGAdaHAT-930"><a href="#FGAdaHAT-930"><span class="linenos"> 930</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="FGAdaHAT-931"><a href="#FGAdaHAT-931"><span class="linenos"> 931</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT-932"><a href="#FGAdaHAT-932"><span class="linenos"> 932</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="FGAdaHAT-933"><a href="#FGAdaHAT-933"><span class="linenos"> 933</span></a>
</span><span id="FGAdaHAT-934"><a href="#FGAdaHAT-934"><span class="linenos"> 934</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="n">if_output_weight</span><span class="p">:</span>
</span><span id="FGAdaHAT-935"><a href="#FGAdaHAT-935"><span class="linenos"> 935</span></a>            <span class="n">gradient_square</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">**</span><span class="mi">2</span>
</span><span id="FGAdaHAT-936"><a href="#FGAdaHAT-936"><span class="linenos"> 936</span></a>            <span class="n">gradient_square_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
</span><span id="FGAdaHAT-937"><a href="#FGAdaHAT-937"><span class="linenos"> 937</span></a>                <span class="n">gradient_square</span><span class="p">,</span>
</span><span id="FGAdaHAT-938"><a href="#FGAdaHAT-938"><span class="linenos"> 938</span></a>                <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT-939"><a href="#FGAdaHAT-939"><span class="linenos"> 939</span></a>                    <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">gradient_square</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">0</span>
</span><span id="FGAdaHAT-940"><a href="#FGAdaHAT-940"><span class="linenos"> 940</span></a>                <span class="p">],</span>  <span class="c1"># sum over the input dimension</span>
</span><span id="FGAdaHAT-941"><a href="#FGAdaHAT-941"><span class="linenos"> 941</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT-942"><a href="#FGAdaHAT-942"><span class="linenos"> 942</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="FGAdaHAT-943"><a href="#FGAdaHAT-943"><span class="linenos"> 943</span></a>            <span class="n">gradient_square</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">next_layer</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">**</span><span class="mi">2</span>
</span><span id="FGAdaHAT-944"><a href="#FGAdaHAT-944"><span class="linenos"> 944</span></a>            <span class="n">gradient_square_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
</span><span id="FGAdaHAT-945"><a href="#FGAdaHAT-945"><span class="linenos"> 945</span></a>                <span class="n">gradient_square</span><span class="p">,</span>
</span><span id="FGAdaHAT-946"><a href="#FGAdaHAT-946"><span class="linenos"> 946</span></a>                <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT-947"><a href="#FGAdaHAT-947"><span class="linenos"> 947</span></a>                    <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">gradient_square</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="FGAdaHAT-948"><a href="#FGAdaHAT-948"><span class="linenos"> 948</span></a>                <span class="p">],</span>  <span class="c1"># sum over the output dimension</span>
</span><span id="FGAdaHAT-949"><a href="#FGAdaHAT-949"><span class="linenos"> 949</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT-950"><a href="#FGAdaHAT-950"><span class="linenos"> 950</span></a>
</span><span id="FGAdaHAT-951"><a href="#FGAdaHAT-951"><span class="linenos"> 951</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">gradient_square_sum</span>
</span><span id="FGAdaHAT-952"><a href="#FGAdaHAT-952"><span class="linenos"> 952</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="FGAdaHAT-953"><a href="#FGAdaHAT-953"><span class="linenos"> 953</span></a>
</span><span id="FGAdaHAT-954"><a href="#FGAdaHAT-954"><span class="linenos"> 954</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span><span id="FGAdaHAT-955"><a href="#FGAdaHAT-955"><span class="linenos"> 955</span></a>
</span><span id="FGAdaHAT-956"><a href="#FGAdaHAT-956"><span class="linenos"> 956</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_weight_gradient_square_sum_x_activation_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT-957"><a href="#FGAdaHAT-957"><span class="linenos"> 957</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT-958"><a href="#FGAdaHAT-958"><span class="linenos"> 958</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT-959"><a href="#FGAdaHAT-959"><span class="linenos"> 959</span></a>        <span class="n">activation</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="FGAdaHAT-960"><a href="#FGAdaHAT-960"><span class="linenos"> 960</span></a>        <span class="n">if_output_weight</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
</span><span id="FGAdaHAT-961"><a href="#FGAdaHAT-961"><span class="linenos"> 961</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="FGAdaHAT-962"><a href="#FGAdaHAT-962"><span class="linenos"> 962</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the sum of layer weight gradient squares multiplied by absolute values of activation. The weight gradient square is equal to fisher information in [EWC](https://www.pnas.org/doi/10.1073/pnas.1611835114).</span>
</span><span id="FGAdaHAT-963"><a href="#FGAdaHAT-963"><span class="linenos"> 963</span></a>
</span><span id="FGAdaHAT-964"><a href="#FGAdaHAT-964"><span class="linenos"> 964</span></a><span class="sd">        **Args:**</span>
</span><span id="FGAdaHAT-965"><a href="#FGAdaHAT-965"><span class="linenos"> 965</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="FGAdaHAT-966"><a href="#FGAdaHAT-966"><span class="linenos"> 966</span></a><span class="sd">        - **activation** (`Tensor`): the activation tensor of the layer. It has the same size of (number of units, ).</span>
</span><span id="FGAdaHAT-967"><a href="#FGAdaHAT-967"><span class="linenos"> 967</span></a><span class="sd">        - **if_output_weight** (`bool`): whether to use the output weights or input weights.</span>
</span><span id="FGAdaHAT-968"><a href="#FGAdaHAT-968"><span class="linenos"> 968</span></a>
</span><span id="FGAdaHAT-969"><a href="#FGAdaHAT-969"><span class="linenos"> 969</span></a><span class="sd">        **Returns:**</span>
</span><span id="FGAdaHAT-970"><a href="#FGAdaHAT-970"><span class="linenos"> 970</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="FGAdaHAT-971"><a href="#FGAdaHAT-971"><span class="linenos"> 971</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT-972"><a href="#FGAdaHAT-972"><span class="linenos"> 972</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="FGAdaHAT-973"><a href="#FGAdaHAT-973"><span class="linenos"> 973</span></a>
</span><span id="FGAdaHAT-974"><a href="#FGAdaHAT-974"><span class="linenos"> 974</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="n">if_output_weight</span><span class="p">:</span>
</span><span id="FGAdaHAT-975"><a href="#FGAdaHAT-975"><span class="linenos"> 975</span></a>            <span class="n">gradient_square</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">**</span><span class="mi">2</span>
</span><span id="FGAdaHAT-976"><a href="#FGAdaHAT-976"><span class="linenos"> 976</span></a>            <span class="n">gradient_square_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
</span><span id="FGAdaHAT-977"><a href="#FGAdaHAT-977"><span class="linenos"> 977</span></a>                <span class="n">gradient_square</span><span class="p">,</span>
</span><span id="FGAdaHAT-978"><a href="#FGAdaHAT-978"><span class="linenos"> 978</span></a>                <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT-979"><a href="#FGAdaHAT-979"><span class="linenos"> 979</span></a>                    <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">gradient_square</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">0</span>
</span><span id="FGAdaHAT-980"><a href="#FGAdaHAT-980"><span class="linenos"> 980</span></a>                <span class="p">],</span>  <span class="c1"># sum over the input dimension</span>
</span><span id="FGAdaHAT-981"><a href="#FGAdaHAT-981"><span class="linenos"> 981</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT-982"><a href="#FGAdaHAT-982"><span class="linenos"> 982</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="FGAdaHAT-983"><a href="#FGAdaHAT-983"><span class="linenos"> 983</span></a>            <span class="n">gradient_square</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">next_layer</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">**</span><span class="mi">2</span>
</span><span id="FGAdaHAT-984"><a href="#FGAdaHAT-984"><span class="linenos"> 984</span></a>            <span class="n">gradient_square_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
</span><span id="FGAdaHAT-985"><a href="#FGAdaHAT-985"><span class="linenos"> 985</span></a>                <span class="n">gradient_square</span><span class="p">,</span>
</span><span id="FGAdaHAT-986"><a href="#FGAdaHAT-986"><span class="linenos"> 986</span></a>                <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT-987"><a href="#FGAdaHAT-987"><span class="linenos"> 987</span></a>                    <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">gradient_square</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="FGAdaHAT-988"><a href="#FGAdaHAT-988"><span class="linenos"> 988</span></a>                <span class="p">],</span>  <span class="c1"># sum over the output dimension</span>
</span><span id="FGAdaHAT-989"><a href="#FGAdaHAT-989"><span class="linenos"> 989</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT-990"><a href="#FGAdaHAT-990"><span class="linenos"> 990</span></a>
</span><span id="FGAdaHAT-991"><a href="#FGAdaHAT-991"><span class="linenos"> 991</span></a>        <span class="n">activation_abs_batch_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
</span><span id="FGAdaHAT-992"><a href="#FGAdaHAT-992"><span class="linenos"> 992</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">activation</span><span class="p">),</span>
</span><span id="FGAdaHAT-993"><a href="#FGAdaHAT-993"><span class="linenos"> 993</span></a>            <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT-994"><a href="#FGAdaHAT-994"><span class="linenos"> 994</span></a>                <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">activation</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="FGAdaHAT-995"><a href="#FGAdaHAT-995"><span class="linenos"> 995</span></a>            <span class="p">],</span>  <span class="c1"># average the features over batch samples</span>
</span><span id="FGAdaHAT-996"><a href="#FGAdaHAT-996"><span class="linenos"> 996</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT-997"><a href="#FGAdaHAT-997"><span class="linenos"> 997</span></a>
</span><span id="FGAdaHAT-998"><a href="#FGAdaHAT-998"><span class="linenos"> 998</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">gradient_square_sum</span> <span class="o">*</span> <span class="n">activation_abs_batch_mean</span>
</span><span id="FGAdaHAT-999"><a href="#FGAdaHAT-999"><span class="linenos"> 999</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="FGAdaHAT-1000"><a href="#FGAdaHAT-1000"><span class="linenos">1000</span></a>
</span><span id="FGAdaHAT-1001"><a href="#FGAdaHAT-1001"><span class="linenos">1001</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span><span id="FGAdaHAT-1002"><a href="#FGAdaHAT-1002"><span class="linenos">1002</span></a>
</span><span id="FGAdaHAT-1003"><a href="#FGAdaHAT-1003"><span class="linenos">1003</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_conductance_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT-1004"><a href="#FGAdaHAT-1004"><span class="linenos">1004</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT-1005"><a href="#FGAdaHAT-1005"><span class="linenos">1005</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT-1006"><a href="#FGAdaHAT-1006"><span class="linenos">1006</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="FGAdaHAT-1007"><a href="#FGAdaHAT-1007"><span class="linenos">1007</span></a>        <span class="n">baselines</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="FGAdaHAT-1008"><a href="#FGAdaHAT-1008"><span class="linenos">1008</span></a>        <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FGAdaHAT-1009"><a href="#FGAdaHAT-1009"><span class="linenos">1009</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FGAdaHAT-1010"><a href="#FGAdaHAT-1010"><span class="linenos">1010</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FGAdaHAT-1011"><a href="#FGAdaHAT-1011"><span class="linenos">1011</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="FGAdaHAT-1012"><a href="#FGAdaHAT-1012"><span class="linenos">1012</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of [conductance](https://openreview.net/forum?id=SylKoo0cKm). We implement this using [Layer Conductance](https://captum.ai/api/layer.html#layer-conductance) in Captum.</span>
</span><span id="FGAdaHAT-1013"><a href="#FGAdaHAT-1013"><span class="linenos">1013</span></a>
</span><span id="FGAdaHAT-1014"><a href="#FGAdaHAT-1014"><span class="linenos">1014</span></a><span class="sd">        **Args:**</span>
</span><span id="FGAdaHAT-1015"><a href="#FGAdaHAT-1015"><span class="linenos">1015</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="FGAdaHAT-1016"><a href="#FGAdaHAT-1016"><span class="linenos">1016</span></a><span class="sd">        - **input** (`Tensor` | `tuple[Tensor, ...]`): the input batch of the training step.</span>
</span><span id="FGAdaHAT-1017"><a href="#FGAdaHAT-1017"><span class="linenos">1017</span></a><span class="sd">        - **baselines** (`None` | `int` | `float` | `Tensor` | `tuple[int | float | Tensor, ...]`): starting point from which integral is computed in this method. Please refer to the [Captum documentation](https://captum.ai/api/layer.html#captum.attr.LayerConductance.attribute) for more details.</span>
</span><span id="FGAdaHAT-1018"><a href="#FGAdaHAT-1018"><span class="linenos">1018</span></a><span class="sd">        - **target** (`Tensor` | `None`): the target batch of the training step.</span>
</span><span id="FGAdaHAT-1019"><a href="#FGAdaHAT-1019"><span class="linenos">1019</span></a><span class="sd">        - **batch_idx** (`int`): the index of the current batch. This is an argument of the forward function during training.</span>
</span><span id="FGAdaHAT-1020"><a href="#FGAdaHAT-1020"><span class="linenos">1020</span></a><span class="sd">        - **num_batches** (`int`): the number of batches in the training step. This is an argument of the forward function during training.- **mask** (`Tensor`): the mask tensor of the layer. It has the same size as the feature tensor with size (number of units, ).</span>
</span><span id="FGAdaHAT-1021"><a href="#FGAdaHAT-1021"><span class="linenos">1021</span></a>
</span><span id="FGAdaHAT-1022"><a href="#FGAdaHAT-1022"><span class="linenos">1022</span></a><span class="sd">        **Returns:**</span>
</span><span id="FGAdaHAT-1023"><a href="#FGAdaHAT-1023"><span class="linenos">1023</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="FGAdaHAT-1024"><a href="#FGAdaHAT-1024"><span class="linenos">1024</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT-1025"><a href="#FGAdaHAT-1025"><span class="linenos">1025</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="FGAdaHAT-1026"><a href="#FGAdaHAT-1026"><span class="linenos">1026</span></a>
</span><span id="FGAdaHAT-1027"><a href="#FGAdaHAT-1027"><span class="linenos">1027</span></a>        <span class="c1"># initialize the Layer Conductance object</span>
</span><span id="FGAdaHAT-1028"><a href="#FGAdaHAT-1028"><span class="linenos">1028</span></a>        <span class="n">layer_conductance</span> <span class="o">=</span> <span class="n">LayerConductance</span><span class="p">(</span><span class="n">forward_func</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="n">layer</span><span class="p">)</span>
</span><span id="FGAdaHAT-1029"><a href="#FGAdaHAT-1029"><span class="linenos">1029</span></a>
</span><span id="FGAdaHAT-1030"><a href="#FGAdaHAT-1030"><span class="linenos">1030</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</span><span id="FGAdaHAT-1031"><a href="#FGAdaHAT-1031"><span class="linenos">1031</span></a>        <span class="c1"># calculate layer attribution of the step</span>
</span><span id="FGAdaHAT-1032"><a href="#FGAdaHAT-1032"><span class="linenos">1032</span></a>        <span class="n">attribution</span> <span class="o">=</span> <span class="n">layer_conductance</span><span class="o">.</span><span class="n">attribute</span><span class="p">(</span>
</span><span id="FGAdaHAT-1033"><a href="#FGAdaHAT-1033"><span class="linenos">1033</span></a>            <span class="n">inputs</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="FGAdaHAT-1034"><a href="#FGAdaHAT-1034"><span class="linenos">1034</span></a>            <span class="n">baselines</span><span class="o">=</span><span class="n">baselines</span><span class="p">,</span>
</span><span id="FGAdaHAT-1035"><a href="#FGAdaHAT-1035"><span class="linenos">1035</span></a>            <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="FGAdaHAT-1036"><a href="#FGAdaHAT-1036"><span class="linenos">1036</span></a>            <span class="n">additional_forward_args</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">num_batches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">),</span>
</span><span id="FGAdaHAT-1037"><a href="#FGAdaHAT-1037"><span class="linenos">1037</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT-1038"><a href="#FGAdaHAT-1038"><span class="linenos">1038</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</span><span id="FGAdaHAT-1039"><a href="#FGAdaHAT-1039"><span class="linenos">1039</span></a>
</span><span id="FGAdaHAT-1040"><a href="#FGAdaHAT-1040"><span class="linenos">1040</span></a>        <span class="n">attribution_abs_batch_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
</span><span id="FGAdaHAT-1041"><a href="#FGAdaHAT-1041"><span class="linenos">1041</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">attribution</span><span class="p">),</span>
</span><span id="FGAdaHAT-1042"><a href="#FGAdaHAT-1042"><span class="linenos">1042</span></a>            <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT-1043"><a href="#FGAdaHAT-1043"><span class="linenos">1043</span></a>                <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">attribution</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="FGAdaHAT-1044"><a href="#FGAdaHAT-1044"><span class="linenos">1044</span></a>            <span class="p">],</span>  <span class="c1"># average the features over batch samples</span>
</span><span id="FGAdaHAT-1045"><a href="#FGAdaHAT-1045"><span class="linenos">1045</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT-1046"><a href="#FGAdaHAT-1046"><span class="linenos">1046</span></a>
</span><span id="FGAdaHAT-1047"><a href="#FGAdaHAT-1047"><span class="linenos">1047</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">attribution_abs_batch_mean</span>
</span><span id="FGAdaHAT-1048"><a href="#FGAdaHAT-1048"><span class="linenos">1048</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="FGAdaHAT-1049"><a href="#FGAdaHAT-1049"><span class="linenos">1049</span></a>
</span><span id="FGAdaHAT-1050"><a href="#FGAdaHAT-1050"><span class="linenos">1050</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span><span id="FGAdaHAT-1051"><a href="#FGAdaHAT-1051"><span class="linenos">1051</span></a>
</span><span id="FGAdaHAT-1052"><a href="#FGAdaHAT-1052"><span class="linenos">1052</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_internal_influence_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT-1053"><a href="#FGAdaHAT-1053"><span class="linenos">1053</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT-1054"><a href="#FGAdaHAT-1054"><span class="linenos">1054</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT-1055"><a href="#FGAdaHAT-1055"><span class="linenos">1055</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="FGAdaHAT-1056"><a href="#FGAdaHAT-1056"><span class="linenos">1056</span></a>        <span class="n">baselines</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="FGAdaHAT-1057"><a href="#FGAdaHAT-1057"><span class="linenos">1057</span></a>        <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FGAdaHAT-1058"><a href="#FGAdaHAT-1058"><span class="linenos">1058</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FGAdaHAT-1059"><a href="#FGAdaHAT-1059"><span class="linenos">1059</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FGAdaHAT-1060"><a href="#FGAdaHAT-1060"><span class="linenos">1060</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="FGAdaHAT-1061"><a href="#FGAdaHAT-1061"><span class="linenos">1061</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of [internal influence](https://openreview.net/forum?id=SJPpHzW0-). We implement this using [Internal Influence](https://captum.ai/api/layer.html#internal-influence) in Captum.</span>
</span><span id="FGAdaHAT-1062"><a href="#FGAdaHAT-1062"><span class="linenos">1062</span></a>
</span><span id="FGAdaHAT-1063"><a href="#FGAdaHAT-1063"><span class="linenos">1063</span></a><span class="sd">        **Args:**</span>
</span><span id="FGAdaHAT-1064"><a href="#FGAdaHAT-1064"><span class="linenos">1064</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="FGAdaHAT-1065"><a href="#FGAdaHAT-1065"><span class="linenos">1065</span></a><span class="sd">        - **input** (`Tensor` | `tuple[Tensor, ...]`): the input batch of the training step.</span>
</span><span id="FGAdaHAT-1066"><a href="#FGAdaHAT-1066"><span class="linenos">1066</span></a><span class="sd">        - **baselines** (`None` | `int` | `float` | `Tensor` | `tuple[int | float | Tensor, ...]`): starting point from which integral is computed in this method. Please refer to the [Captum documentation](https://captum.ai/api/layer.html#captum.attr.InternalInfluence.attribute) for more details.</span>
</span><span id="FGAdaHAT-1067"><a href="#FGAdaHAT-1067"><span class="linenos">1067</span></a><span class="sd">        - **target** (`Tensor` | `None`): the target batch of the training step.</span>
</span><span id="FGAdaHAT-1068"><a href="#FGAdaHAT-1068"><span class="linenos">1068</span></a><span class="sd">        - **batch_idx** (`int`): the index of the current batch. This is an argument of the forward function during training.</span>
</span><span id="FGAdaHAT-1069"><a href="#FGAdaHAT-1069"><span class="linenos">1069</span></a><span class="sd">        - **num_batches** (`int`): the number of batches in the training step. This is an argument of the forward function during training.</span>
</span><span id="FGAdaHAT-1070"><a href="#FGAdaHAT-1070"><span class="linenos">1070</span></a>
</span><span id="FGAdaHAT-1071"><a href="#FGAdaHAT-1071"><span class="linenos">1071</span></a><span class="sd">        **Returns:**</span>
</span><span id="FGAdaHAT-1072"><a href="#FGAdaHAT-1072"><span class="linenos">1072</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="FGAdaHAT-1073"><a href="#FGAdaHAT-1073"><span class="linenos">1073</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT-1074"><a href="#FGAdaHAT-1074"><span class="linenos">1074</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="FGAdaHAT-1075"><a href="#FGAdaHAT-1075"><span class="linenos">1075</span></a>
</span><span id="FGAdaHAT-1076"><a href="#FGAdaHAT-1076"><span class="linenos">1076</span></a>        <span class="c1"># initialize the Internal Influence object</span>
</span><span id="FGAdaHAT-1077"><a href="#FGAdaHAT-1077"><span class="linenos">1077</span></a>        <span class="n">internal_influence</span> <span class="o">=</span> <span class="n">InternalInfluence</span><span class="p">(</span><span class="n">forward_func</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="n">layer</span><span class="p">)</span>
</span><span id="FGAdaHAT-1078"><a href="#FGAdaHAT-1078"><span class="linenos">1078</span></a>
</span><span id="FGAdaHAT-1079"><a href="#FGAdaHAT-1079"><span class="linenos">1079</span></a>        <span class="c1"># convert the target to long type to avoid error</span>
</span><span id="FGAdaHAT-1080"><a href="#FGAdaHAT-1080"><span class="linenos">1080</span></a>        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">long</span><span class="p">()</span> <span class="k">if</span> <span class="n">target</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="FGAdaHAT-1081"><a href="#FGAdaHAT-1081"><span class="linenos">1081</span></a>
</span><span id="FGAdaHAT-1082"><a href="#FGAdaHAT-1082"><span class="linenos">1082</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</span><span id="FGAdaHAT-1083"><a href="#FGAdaHAT-1083"><span class="linenos">1083</span></a>        <span class="c1"># calculate layer attribution of the step</span>
</span><span id="FGAdaHAT-1084"><a href="#FGAdaHAT-1084"><span class="linenos">1084</span></a>        <span class="n">attribution</span> <span class="o">=</span> <span class="n">internal_influence</span><span class="o">.</span><span class="n">attribute</span><span class="p">(</span>
</span><span id="FGAdaHAT-1085"><a href="#FGAdaHAT-1085"><span class="linenos">1085</span></a>            <span class="n">inputs</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="FGAdaHAT-1086"><a href="#FGAdaHAT-1086"><span class="linenos">1086</span></a>            <span class="n">baselines</span><span class="o">=</span><span class="n">baselines</span><span class="p">,</span>
</span><span id="FGAdaHAT-1087"><a href="#FGAdaHAT-1087"><span class="linenos">1087</span></a>            <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="FGAdaHAT-1088"><a href="#FGAdaHAT-1088"><span class="linenos">1088</span></a>            <span class="n">additional_forward_args</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">num_batches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">),</span>
</span><span id="FGAdaHAT-1089"><a href="#FGAdaHAT-1089"><span class="linenos">1089</span></a>            <span class="n">n_steps</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>  <span class="c1"># set 10 instead of default 50 to accelerate the computation</span>
</span><span id="FGAdaHAT-1090"><a href="#FGAdaHAT-1090"><span class="linenos">1090</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT-1091"><a href="#FGAdaHAT-1091"><span class="linenos">1091</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</span><span id="FGAdaHAT-1092"><a href="#FGAdaHAT-1092"><span class="linenos">1092</span></a>
</span><span id="FGAdaHAT-1093"><a href="#FGAdaHAT-1093"><span class="linenos">1093</span></a>        <span class="n">attribution_abs_batch_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
</span><span id="FGAdaHAT-1094"><a href="#FGAdaHAT-1094"><span class="linenos">1094</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">attribution</span><span class="p">),</span>
</span><span id="FGAdaHAT-1095"><a href="#FGAdaHAT-1095"><span class="linenos">1095</span></a>            <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT-1096"><a href="#FGAdaHAT-1096"><span class="linenos">1096</span></a>                <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">attribution</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="FGAdaHAT-1097"><a href="#FGAdaHAT-1097"><span class="linenos">1097</span></a>            <span class="p">],</span>  <span class="c1"># average the features over batch samples</span>
</span><span id="FGAdaHAT-1098"><a href="#FGAdaHAT-1098"><span class="linenos">1098</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT-1099"><a href="#FGAdaHAT-1099"><span class="linenos">1099</span></a>
</span><span id="FGAdaHAT-1100"><a href="#FGAdaHAT-1100"><span class="linenos">1100</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">attribution_abs_batch_mean</span>
</span><span id="FGAdaHAT-1101"><a href="#FGAdaHAT-1101"><span class="linenos">1101</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="FGAdaHAT-1102"><a href="#FGAdaHAT-1102"><span class="linenos">1102</span></a>
</span><span id="FGAdaHAT-1103"><a href="#FGAdaHAT-1103"><span class="linenos">1103</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span><span id="FGAdaHAT-1104"><a href="#FGAdaHAT-1104"><span class="linenos">1104</span></a>
</span><span id="FGAdaHAT-1105"><a href="#FGAdaHAT-1105"><span class="linenos">1105</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_gradcam_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT-1106"><a href="#FGAdaHAT-1106"><span class="linenos">1106</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT-1107"><a href="#FGAdaHAT-1107"><span class="linenos">1107</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT-1108"><a href="#FGAdaHAT-1108"><span class="linenos">1108</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="FGAdaHAT-1109"><a href="#FGAdaHAT-1109"><span class="linenos">1109</span></a>        <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FGAdaHAT-1110"><a href="#FGAdaHAT-1110"><span class="linenos">1110</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FGAdaHAT-1111"><a href="#FGAdaHAT-1111"><span class="linenos">1111</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FGAdaHAT-1112"><a href="#FGAdaHAT-1112"><span class="linenos">1112</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="FGAdaHAT-1113"><a href="#FGAdaHAT-1113"><span class="linenos">1113</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of [Grad-CAM](https://openreview.net/forum?id=SJPpHzW0-). We implement this using [Layer Grad-CAM](https://captum.ai/api/layer.html#gradcam) in Captum.</span>
</span><span id="FGAdaHAT-1114"><a href="#FGAdaHAT-1114"><span class="linenos">1114</span></a>
</span><span id="FGAdaHAT-1115"><a href="#FGAdaHAT-1115"><span class="linenos">1115</span></a><span class="sd">        **Args:**</span>
</span><span id="FGAdaHAT-1116"><a href="#FGAdaHAT-1116"><span class="linenos">1116</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="FGAdaHAT-1117"><a href="#FGAdaHAT-1117"><span class="linenos">1117</span></a><span class="sd">        - **input** (`Tensor` | `tuple[Tensor, ...]`): the input batch of the training step.</span>
</span><span id="FGAdaHAT-1118"><a href="#FGAdaHAT-1118"><span class="linenos">1118</span></a><span class="sd">        - **target** (`Tensor` | `None`): the target batch of the training step.</span>
</span><span id="FGAdaHAT-1119"><a href="#FGAdaHAT-1119"><span class="linenos">1119</span></a><span class="sd">        - **batch_idx** (`int`): the index of the current batch. This is an argument of the forward function during training.</span>
</span><span id="FGAdaHAT-1120"><a href="#FGAdaHAT-1120"><span class="linenos">1120</span></a><span class="sd">        - **num_batches** (`int`): the number of batches in the training step. This is an argument of the forward function during training.</span>
</span><span id="FGAdaHAT-1121"><a href="#FGAdaHAT-1121"><span class="linenos">1121</span></a>
</span><span id="FGAdaHAT-1122"><a href="#FGAdaHAT-1122"><span class="linenos">1122</span></a><span class="sd">        **Returns:**</span>
</span><span id="FGAdaHAT-1123"><a href="#FGAdaHAT-1123"><span class="linenos">1123</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="FGAdaHAT-1124"><a href="#FGAdaHAT-1124"><span class="linenos">1124</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT-1125"><a href="#FGAdaHAT-1125"><span class="linenos">1125</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="FGAdaHAT-1126"><a href="#FGAdaHAT-1126"><span class="linenos">1126</span></a>
</span><span id="FGAdaHAT-1127"><a href="#FGAdaHAT-1127"><span class="linenos">1127</span></a>        <span class="c1"># initialize the GradCAM object</span>
</span><span id="FGAdaHAT-1128"><a href="#FGAdaHAT-1128"><span class="linenos">1128</span></a>        <span class="n">gradcam</span> <span class="o">=</span> <span class="n">LayerGradCam</span><span class="p">(</span><span class="n">forward_func</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="n">layer</span><span class="p">)</span>
</span><span id="FGAdaHAT-1129"><a href="#FGAdaHAT-1129"><span class="linenos">1129</span></a>
</span><span id="FGAdaHAT-1130"><a href="#FGAdaHAT-1130"><span class="linenos">1130</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</span><span id="FGAdaHAT-1131"><a href="#FGAdaHAT-1131"><span class="linenos">1131</span></a>        <span class="c1"># calculate layer attribution of the step</span>
</span><span id="FGAdaHAT-1132"><a href="#FGAdaHAT-1132"><span class="linenos">1132</span></a>        <span class="n">attribution</span> <span class="o">=</span> <span class="n">gradcam</span><span class="o">.</span><span class="n">attribute</span><span class="p">(</span>
</span><span id="FGAdaHAT-1133"><a href="#FGAdaHAT-1133"><span class="linenos">1133</span></a>            <span class="n">inputs</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="FGAdaHAT-1134"><a href="#FGAdaHAT-1134"><span class="linenos">1134</span></a>            <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="FGAdaHAT-1135"><a href="#FGAdaHAT-1135"><span class="linenos">1135</span></a>            <span class="n">additional_forward_args</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">num_batches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">),</span>
</span><span id="FGAdaHAT-1136"><a href="#FGAdaHAT-1136"><span class="linenos">1136</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT-1137"><a href="#FGAdaHAT-1137"><span class="linenos">1137</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</span><span id="FGAdaHAT-1138"><a href="#FGAdaHAT-1138"><span class="linenos">1138</span></a>
</span><span id="FGAdaHAT-1139"><a href="#FGAdaHAT-1139"><span class="linenos">1139</span></a>        <span class="n">attribution_abs_batch_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
</span><span id="FGAdaHAT-1140"><a href="#FGAdaHAT-1140"><span class="linenos">1140</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">attribution</span><span class="p">),</span>
</span><span id="FGAdaHAT-1141"><a href="#FGAdaHAT-1141"><span class="linenos">1141</span></a>            <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT-1142"><a href="#FGAdaHAT-1142"><span class="linenos">1142</span></a>                <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">attribution</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="FGAdaHAT-1143"><a href="#FGAdaHAT-1143"><span class="linenos">1143</span></a>            <span class="p">],</span>  <span class="c1"># average the features over batch samples</span>
</span><span id="FGAdaHAT-1144"><a href="#FGAdaHAT-1144"><span class="linenos">1144</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT-1145"><a href="#FGAdaHAT-1145"><span class="linenos">1145</span></a>
</span><span id="FGAdaHAT-1146"><a href="#FGAdaHAT-1146"><span class="linenos">1146</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">attribution_abs_batch_mean</span>
</span><span id="FGAdaHAT-1147"><a href="#FGAdaHAT-1147"><span class="linenos">1147</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="FGAdaHAT-1148"><a href="#FGAdaHAT-1148"><span class="linenos">1148</span></a>
</span><span id="FGAdaHAT-1149"><a href="#FGAdaHAT-1149"><span class="linenos">1149</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span><span id="FGAdaHAT-1150"><a href="#FGAdaHAT-1150"><span class="linenos">1150</span></a>
</span><span id="FGAdaHAT-1151"><a href="#FGAdaHAT-1151"><span class="linenos">1151</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_deeplift_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT-1152"><a href="#FGAdaHAT-1152"><span class="linenos">1152</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT-1153"><a href="#FGAdaHAT-1153"><span class="linenos">1153</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT-1154"><a href="#FGAdaHAT-1154"><span class="linenos">1154</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="FGAdaHAT-1155"><a href="#FGAdaHAT-1155"><span class="linenos">1155</span></a>        <span class="n">baselines</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="FGAdaHAT-1156"><a href="#FGAdaHAT-1156"><span class="linenos">1156</span></a>        <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FGAdaHAT-1157"><a href="#FGAdaHAT-1157"><span class="linenos">1157</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FGAdaHAT-1158"><a href="#FGAdaHAT-1158"><span class="linenos">1158</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FGAdaHAT-1159"><a href="#FGAdaHAT-1159"><span class="linenos">1159</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="FGAdaHAT-1160"><a href="#FGAdaHAT-1160"><span class="linenos">1160</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of [DeepLift](https://proceedings.mlr.press/v70/shrikumar17a/shrikumar17a.pdf). We implement this using [Layer DeepLift](https://captum.ai/api/layer.html#layer-deeplift) in Captum.</span>
</span><span id="FGAdaHAT-1161"><a href="#FGAdaHAT-1161"><span class="linenos">1161</span></a>
</span><span id="FGAdaHAT-1162"><a href="#FGAdaHAT-1162"><span class="linenos">1162</span></a><span class="sd">        **Args:**</span>
</span><span id="FGAdaHAT-1163"><a href="#FGAdaHAT-1163"><span class="linenos">1163</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="FGAdaHAT-1164"><a href="#FGAdaHAT-1164"><span class="linenos">1164</span></a><span class="sd">        - **input** (`Tensor` | `tuple[Tensor, ...]`): the input batch of the training step.</span>
</span><span id="FGAdaHAT-1165"><a href="#FGAdaHAT-1165"><span class="linenos">1165</span></a><span class="sd">        - **baselines** (`None` | `int` | `float` | `Tensor` | `tuple[int | float | Tensor, ...]`): baselines define reference samples that are compared with the inputs. Please refer to the [Captum documentation](https://captum.ai/api/layer.html#captum.attr.LayerDeepLift.attribute) for more details.</span>
</span><span id="FGAdaHAT-1166"><a href="#FGAdaHAT-1166"><span class="linenos">1166</span></a><span class="sd">        - **target** (`Tensor` | `None`): the target batch of the training step.</span>
</span><span id="FGAdaHAT-1167"><a href="#FGAdaHAT-1167"><span class="linenos">1167</span></a><span class="sd">        - **batch_idx** (`int`): the index of the current batch. This is an argument of the forward function during training.</span>
</span><span id="FGAdaHAT-1168"><a href="#FGAdaHAT-1168"><span class="linenos">1168</span></a><span class="sd">        - **num_batches** (`int`): the number of batches in the training step. This is an argument of the forward function during training.</span>
</span><span id="FGAdaHAT-1169"><a href="#FGAdaHAT-1169"><span class="linenos">1169</span></a>
</span><span id="FGAdaHAT-1170"><a href="#FGAdaHAT-1170"><span class="linenos">1170</span></a><span class="sd">        **Returns:**</span>
</span><span id="FGAdaHAT-1171"><a href="#FGAdaHAT-1171"><span class="linenos">1171</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="FGAdaHAT-1172"><a href="#FGAdaHAT-1172"><span class="linenos">1172</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT-1173"><a href="#FGAdaHAT-1173"><span class="linenos">1173</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="FGAdaHAT-1174"><a href="#FGAdaHAT-1174"><span class="linenos">1174</span></a>
</span><span id="FGAdaHAT-1175"><a href="#FGAdaHAT-1175"><span class="linenos">1175</span></a>        <span class="c1"># initialize the Layer DeepLift object</span>
</span><span id="FGAdaHAT-1176"><a href="#FGAdaHAT-1176"><span class="linenos">1176</span></a>        <span class="n">layer_deeplift</span> <span class="o">=</span> <span class="n">LayerDeepLift</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="n">layer</span><span class="p">)</span>
</span><span id="FGAdaHAT-1177"><a href="#FGAdaHAT-1177"><span class="linenos">1177</span></a>
</span><span id="FGAdaHAT-1178"><a href="#FGAdaHAT-1178"><span class="linenos">1178</span></a>        <span class="c1"># convert the target to long type to avoid error</span>
</span><span id="FGAdaHAT-1179"><a href="#FGAdaHAT-1179"><span class="linenos">1179</span></a>        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">long</span><span class="p">()</span> <span class="k">if</span> <span class="n">target</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="FGAdaHAT-1180"><a href="#FGAdaHAT-1180"><span class="linenos">1180</span></a>
</span><span id="FGAdaHAT-1181"><a href="#FGAdaHAT-1181"><span class="linenos">1181</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</span><span id="FGAdaHAT-1182"><a href="#FGAdaHAT-1182"><span class="linenos">1182</span></a>        <span class="c1"># calculate layer attribution of the step</span>
</span><span id="FGAdaHAT-1183"><a href="#FGAdaHAT-1183"><span class="linenos">1183</span></a>        <span class="n">attribution</span> <span class="o">=</span> <span class="n">layer_deeplift</span><span class="o">.</span><span class="n">attribute</span><span class="p">(</span>
</span><span id="FGAdaHAT-1184"><a href="#FGAdaHAT-1184"><span class="linenos">1184</span></a>            <span class="n">inputs</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="FGAdaHAT-1185"><a href="#FGAdaHAT-1185"><span class="linenos">1185</span></a>            <span class="n">baselines</span><span class="o">=</span><span class="n">baselines</span><span class="p">,</span>
</span><span id="FGAdaHAT-1186"><a href="#FGAdaHAT-1186"><span class="linenos">1186</span></a>            <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="FGAdaHAT-1187"><a href="#FGAdaHAT-1187"><span class="linenos">1187</span></a>            <span class="n">additional_forward_args</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">num_batches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">),</span>
</span><span id="FGAdaHAT-1188"><a href="#FGAdaHAT-1188"><span class="linenos">1188</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT-1189"><a href="#FGAdaHAT-1189"><span class="linenos">1189</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</span><span id="FGAdaHAT-1190"><a href="#FGAdaHAT-1190"><span class="linenos">1190</span></a>
</span><span id="FGAdaHAT-1191"><a href="#FGAdaHAT-1191"><span class="linenos">1191</span></a>        <span class="n">attribution_abs_batch_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
</span><span id="FGAdaHAT-1192"><a href="#FGAdaHAT-1192"><span class="linenos">1192</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">attribution</span><span class="p">),</span>
</span><span id="FGAdaHAT-1193"><a href="#FGAdaHAT-1193"><span class="linenos">1193</span></a>            <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT-1194"><a href="#FGAdaHAT-1194"><span class="linenos">1194</span></a>                <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">attribution</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="FGAdaHAT-1195"><a href="#FGAdaHAT-1195"><span class="linenos">1195</span></a>            <span class="p">],</span>  <span class="c1"># average the features over batch samples</span>
</span><span id="FGAdaHAT-1196"><a href="#FGAdaHAT-1196"><span class="linenos">1196</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT-1197"><a href="#FGAdaHAT-1197"><span class="linenos">1197</span></a>
</span><span id="FGAdaHAT-1198"><a href="#FGAdaHAT-1198"><span class="linenos">1198</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">attribution_abs_batch_mean</span>
</span><span id="FGAdaHAT-1199"><a href="#FGAdaHAT-1199"><span class="linenos">1199</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="FGAdaHAT-1200"><a href="#FGAdaHAT-1200"><span class="linenos">1200</span></a>
</span><span id="FGAdaHAT-1201"><a href="#FGAdaHAT-1201"><span class="linenos">1201</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span><span id="FGAdaHAT-1202"><a href="#FGAdaHAT-1202"><span class="linenos">1202</span></a>
</span><span id="FGAdaHAT-1203"><a href="#FGAdaHAT-1203"><span class="linenos">1203</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_deepliftshap_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT-1204"><a href="#FGAdaHAT-1204"><span class="linenos">1204</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT-1205"><a href="#FGAdaHAT-1205"><span class="linenos">1205</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT-1206"><a href="#FGAdaHAT-1206"><span class="linenos">1206</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="FGAdaHAT-1207"><a href="#FGAdaHAT-1207"><span class="linenos">1207</span></a>        <span class="n">baselines</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="FGAdaHAT-1208"><a href="#FGAdaHAT-1208"><span class="linenos">1208</span></a>        <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FGAdaHAT-1209"><a href="#FGAdaHAT-1209"><span class="linenos">1209</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FGAdaHAT-1210"><a href="#FGAdaHAT-1210"><span class="linenos">1210</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FGAdaHAT-1211"><a href="#FGAdaHAT-1211"><span class="linenos">1211</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="FGAdaHAT-1212"><a href="#FGAdaHAT-1212"><span class="linenos">1212</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of [DeepLift SHAP](https://proceedings.neurips.cc/paper_files/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf). We implement this using [Layer DeepLiftShap](https://captum.ai/api/layer.html#layer-deepliftshap) in Captum.</span>
</span><span id="FGAdaHAT-1213"><a href="#FGAdaHAT-1213"><span class="linenos">1213</span></a>
</span><span id="FGAdaHAT-1214"><a href="#FGAdaHAT-1214"><span class="linenos">1214</span></a><span class="sd">        **Args:**</span>
</span><span id="FGAdaHAT-1215"><a href="#FGAdaHAT-1215"><span class="linenos">1215</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="FGAdaHAT-1216"><a href="#FGAdaHAT-1216"><span class="linenos">1216</span></a><span class="sd">        - **input** (`Tensor` | `tuple[Tensor, ...]`): the input batch of the training step.</span>
</span><span id="FGAdaHAT-1217"><a href="#FGAdaHAT-1217"><span class="linenos">1217</span></a><span class="sd">        - **baselines** (`None` | `int` | `float` | `Tensor` | `tuple[int | float | Tensor, ...]`): baselines define reference samples that are compared with the inputs. Please refer to the [Captum documentation](https://captum.ai/api/layer.html#captum.attr.LayerDeepLiftShap.attribute) for more details.</span>
</span><span id="FGAdaHAT-1218"><a href="#FGAdaHAT-1218"><span class="linenos">1218</span></a><span class="sd">        - **target** (`Tensor` | `None`): the target batch of the training step.</span>
</span><span id="FGAdaHAT-1219"><a href="#FGAdaHAT-1219"><span class="linenos">1219</span></a><span class="sd">        - **batch_idx** (`int`): the index of the current batch. This is an argument of the forward function during training.</span>
</span><span id="FGAdaHAT-1220"><a href="#FGAdaHAT-1220"><span class="linenos">1220</span></a><span class="sd">        - **num_batches** (`int`): the number of batches in the training step. This is an argument of the forward function during training.</span>
</span><span id="FGAdaHAT-1221"><a href="#FGAdaHAT-1221"><span class="linenos">1221</span></a>
</span><span id="FGAdaHAT-1222"><a href="#FGAdaHAT-1222"><span class="linenos">1222</span></a><span class="sd">        **Returns:**</span>
</span><span id="FGAdaHAT-1223"><a href="#FGAdaHAT-1223"><span class="linenos">1223</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="FGAdaHAT-1224"><a href="#FGAdaHAT-1224"><span class="linenos">1224</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT-1225"><a href="#FGAdaHAT-1225"><span class="linenos">1225</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="FGAdaHAT-1226"><a href="#FGAdaHAT-1226"><span class="linenos">1226</span></a>
</span><span id="FGAdaHAT-1227"><a href="#FGAdaHAT-1227"><span class="linenos">1227</span></a>        <span class="c1"># initialize the Layer DeepLiftShap object</span>
</span><span id="FGAdaHAT-1228"><a href="#FGAdaHAT-1228"><span class="linenos">1228</span></a>        <span class="n">layer_deepliftshap</span> <span class="o">=</span> <span class="n">LayerDeepLiftShap</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="n">layer</span><span class="p">)</span>
</span><span id="FGAdaHAT-1229"><a href="#FGAdaHAT-1229"><span class="linenos">1229</span></a>
</span><span id="FGAdaHAT-1230"><a href="#FGAdaHAT-1230"><span class="linenos">1230</span></a>        <span class="c1"># convert the target to long type to avoid error</span>
</span><span id="FGAdaHAT-1231"><a href="#FGAdaHAT-1231"><span class="linenos">1231</span></a>        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">long</span><span class="p">()</span> <span class="k">if</span> <span class="n">target</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="FGAdaHAT-1232"><a href="#FGAdaHAT-1232"><span class="linenos">1232</span></a>
</span><span id="FGAdaHAT-1233"><a href="#FGAdaHAT-1233"><span class="linenos">1233</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</span><span id="FGAdaHAT-1234"><a href="#FGAdaHAT-1234"><span class="linenos">1234</span></a>        <span class="c1"># calculate layer attribution of the step</span>
</span><span id="FGAdaHAT-1235"><a href="#FGAdaHAT-1235"><span class="linenos">1235</span></a>        <span class="n">attribution</span> <span class="o">=</span> <span class="n">layer_deepliftshap</span><span class="o">.</span><span class="n">attribute</span><span class="p">(</span>
</span><span id="FGAdaHAT-1236"><a href="#FGAdaHAT-1236"><span class="linenos">1236</span></a>            <span class="n">inputs</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="FGAdaHAT-1237"><a href="#FGAdaHAT-1237"><span class="linenos">1237</span></a>            <span class="n">baselines</span><span class="o">=</span><span class="n">baselines</span><span class="p">,</span>
</span><span id="FGAdaHAT-1238"><a href="#FGAdaHAT-1238"><span class="linenos">1238</span></a>            <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="FGAdaHAT-1239"><a href="#FGAdaHAT-1239"><span class="linenos">1239</span></a>            <span class="n">additional_forward_args</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">num_batches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">),</span>
</span><span id="FGAdaHAT-1240"><a href="#FGAdaHAT-1240"><span class="linenos">1240</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT-1241"><a href="#FGAdaHAT-1241"><span class="linenos">1241</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</span><span id="FGAdaHAT-1242"><a href="#FGAdaHAT-1242"><span class="linenos">1242</span></a>
</span><span id="FGAdaHAT-1243"><a href="#FGAdaHAT-1243"><span class="linenos">1243</span></a>        <span class="n">attribution_abs_batch_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
</span><span id="FGAdaHAT-1244"><a href="#FGAdaHAT-1244"><span class="linenos">1244</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">attribution</span><span class="p">),</span>
</span><span id="FGAdaHAT-1245"><a href="#FGAdaHAT-1245"><span class="linenos">1245</span></a>            <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT-1246"><a href="#FGAdaHAT-1246"><span class="linenos">1246</span></a>                <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">attribution</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="FGAdaHAT-1247"><a href="#FGAdaHAT-1247"><span class="linenos">1247</span></a>            <span class="p">],</span>  <span class="c1"># average the features over batch samples</span>
</span><span id="FGAdaHAT-1248"><a href="#FGAdaHAT-1248"><span class="linenos">1248</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT-1249"><a href="#FGAdaHAT-1249"><span class="linenos">1249</span></a>
</span><span id="FGAdaHAT-1250"><a href="#FGAdaHAT-1250"><span class="linenos">1250</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">attribution_abs_batch_mean</span>
</span><span id="FGAdaHAT-1251"><a href="#FGAdaHAT-1251"><span class="linenos">1251</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="FGAdaHAT-1252"><a href="#FGAdaHAT-1252"><span class="linenos">1252</span></a>
</span><span id="FGAdaHAT-1253"><a href="#FGAdaHAT-1253"><span class="linenos">1253</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span><span id="FGAdaHAT-1254"><a href="#FGAdaHAT-1254"><span class="linenos">1254</span></a>
</span><span id="FGAdaHAT-1255"><a href="#FGAdaHAT-1255"><span class="linenos">1255</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_gradientshap_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT-1256"><a href="#FGAdaHAT-1256"><span class="linenos">1256</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT-1257"><a href="#FGAdaHAT-1257"><span class="linenos">1257</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT-1258"><a href="#FGAdaHAT-1258"><span class="linenos">1258</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="FGAdaHAT-1259"><a href="#FGAdaHAT-1259"><span class="linenos">1259</span></a>        <span class="n">baselines</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="FGAdaHAT-1260"><a href="#FGAdaHAT-1260"><span class="linenos">1260</span></a>        <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FGAdaHAT-1261"><a href="#FGAdaHAT-1261"><span class="linenos">1261</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FGAdaHAT-1262"><a href="#FGAdaHAT-1262"><span class="linenos">1262</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FGAdaHAT-1263"><a href="#FGAdaHAT-1263"><span class="linenos">1263</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="FGAdaHAT-1264"><a href="#FGAdaHAT-1264"><span class="linenos">1264</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of gradient SHAP. We implement this using [Layer GradientShap](https://captum.ai/api/layer.html#layer-gradientshap) in Captum.</span>
</span><span id="FGAdaHAT-1265"><a href="#FGAdaHAT-1265"><span class="linenos">1265</span></a>
</span><span id="FGAdaHAT-1266"><a href="#FGAdaHAT-1266"><span class="linenos">1266</span></a><span class="sd">        **Args:**</span>
</span><span id="FGAdaHAT-1267"><a href="#FGAdaHAT-1267"><span class="linenos">1267</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="FGAdaHAT-1268"><a href="#FGAdaHAT-1268"><span class="linenos">1268</span></a><span class="sd">        - **input** (`Tensor` | `tuple[Tensor, ...]`): the input batch of the training step.</span>
</span><span id="FGAdaHAT-1269"><a href="#FGAdaHAT-1269"><span class="linenos">1269</span></a><span class="sd">        - **baselines** (`None` | `int` | `float` | `Tensor` | `tuple[int | float | Tensor, ...]`): starting point from which expectation is computed. Please refer to the [Captum documentation](https://captum.ai/api/layer.html#captum.attr.LayerGradientShap.attribute) for more details. If `None`, the baselines are set to zero.</span>
</span><span id="FGAdaHAT-1270"><a href="#FGAdaHAT-1270"><span class="linenos">1270</span></a><span class="sd">        - **target** (`Tensor` | `None`): the target batch of the training step.</span>
</span><span id="FGAdaHAT-1271"><a href="#FGAdaHAT-1271"><span class="linenos">1271</span></a><span class="sd">        - **batch_idx** (`int`): the index of the current batch. This is an argument of the forward function during training.</span>
</span><span id="FGAdaHAT-1272"><a href="#FGAdaHAT-1272"><span class="linenos">1272</span></a><span class="sd">        - **num_batches** (`int`): the number of batches in the training step. This is an argument of the forward function during training.</span>
</span><span id="FGAdaHAT-1273"><a href="#FGAdaHAT-1273"><span class="linenos">1273</span></a>
</span><span id="FGAdaHAT-1274"><a href="#FGAdaHAT-1274"><span class="linenos">1274</span></a><span class="sd">        **Returns:**</span>
</span><span id="FGAdaHAT-1275"><a href="#FGAdaHAT-1275"><span class="linenos">1275</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="FGAdaHAT-1276"><a href="#FGAdaHAT-1276"><span class="linenos">1276</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT-1277"><a href="#FGAdaHAT-1277"><span class="linenos">1277</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="FGAdaHAT-1278"><a href="#FGAdaHAT-1278"><span class="linenos">1278</span></a>
</span><span id="FGAdaHAT-1279"><a href="#FGAdaHAT-1279"><span class="linenos">1279</span></a>        <span class="k">if</span> <span class="n">baselines</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FGAdaHAT-1280"><a href="#FGAdaHAT-1280"><span class="linenos">1280</span></a>            <span class="n">baselines</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span>
</span><span id="FGAdaHAT-1281"><a href="#FGAdaHAT-1281"><span class="linenos">1281</span></a>                <span class="nb">input</span>
</span><span id="FGAdaHAT-1282"><a href="#FGAdaHAT-1282"><span class="linenos">1282</span></a>            <span class="p">)</span>  <span class="c1"># baselines are mandatory for GradientShap API. We explicitly set them to zero</span>
</span><span id="FGAdaHAT-1283"><a href="#FGAdaHAT-1283"><span class="linenos">1283</span></a>
</span><span id="FGAdaHAT-1284"><a href="#FGAdaHAT-1284"><span class="linenos">1284</span></a>        <span class="c1"># initialize the Layer GradientShap object</span>
</span><span id="FGAdaHAT-1285"><a href="#FGAdaHAT-1285"><span class="linenos">1285</span></a>        <span class="n">layer_gradientshap</span> <span class="o">=</span> <span class="n">LayerGradientShap</span><span class="p">(</span><span class="n">forward_func</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="n">layer</span><span class="p">)</span>
</span><span id="FGAdaHAT-1286"><a href="#FGAdaHAT-1286"><span class="linenos">1286</span></a>
</span><span id="FGAdaHAT-1287"><a href="#FGAdaHAT-1287"><span class="linenos">1287</span></a>        <span class="c1"># convert the target to long type to avoid error</span>
</span><span id="FGAdaHAT-1288"><a href="#FGAdaHAT-1288"><span class="linenos">1288</span></a>        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">long</span><span class="p">()</span> <span class="k">if</span> <span class="n">target</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="FGAdaHAT-1289"><a href="#FGAdaHAT-1289"><span class="linenos">1289</span></a>
</span><span id="FGAdaHAT-1290"><a href="#FGAdaHAT-1290"><span class="linenos">1290</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</span><span id="FGAdaHAT-1291"><a href="#FGAdaHAT-1291"><span class="linenos">1291</span></a>        <span class="c1"># calculate layer attribution of the step</span>
</span><span id="FGAdaHAT-1292"><a href="#FGAdaHAT-1292"><span class="linenos">1292</span></a>        <span class="n">attribution</span> <span class="o">=</span> <span class="n">layer_gradientshap</span><span class="o">.</span><span class="n">attribute</span><span class="p">(</span>
</span><span id="FGAdaHAT-1293"><a href="#FGAdaHAT-1293"><span class="linenos">1293</span></a>            <span class="n">inputs</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="FGAdaHAT-1294"><a href="#FGAdaHAT-1294"><span class="linenos">1294</span></a>            <span class="n">baselines</span><span class="o">=</span><span class="n">baselines</span><span class="p">,</span>
</span><span id="FGAdaHAT-1295"><a href="#FGAdaHAT-1295"><span class="linenos">1295</span></a>            <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="FGAdaHAT-1296"><a href="#FGAdaHAT-1296"><span class="linenos">1296</span></a>            <span class="n">additional_forward_args</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">num_batches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">),</span>
</span><span id="FGAdaHAT-1297"><a href="#FGAdaHAT-1297"><span class="linenos">1297</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT-1298"><a href="#FGAdaHAT-1298"><span class="linenos">1298</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</span><span id="FGAdaHAT-1299"><a href="#FGAdaHAT-1299"><span class="linenos">1299</span></a>
</span><span id="FGAdaHAT-1300"><a href="#FGAdaHAT-1300"><span class="linenos">1300</span></a>        <span class="n">attribution_abs_batch_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
</span><span id="FGAdaHAT-1301"><a href="#FGAdaHAT-1301"><span class="linenos">1301</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">attribution</span><span class="p">),</span>
</span><span id="FGAdaHAT-1302"><a href="#FGAdaHAT-1302"><span class="linenos">1302</span></a>            <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT-1303"><a href="#FGAdaHAT-1303"><span class="linenos">1303</span></a>                <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">attribution</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="FGAdaHAT-1304"><a href="#FGAdaHAT-1304"><span class="linenos">1304</span></a>            <span class="p">],</span>  <span class="c1"># average the features over batch samples</span>
</span><span id="FGAdaHAT-1305"><a href="#FGAdaHAT-1305"><span class="linenos">1305</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT-1306"><a href="#FGAdaHAT-1306"><span class="linenos">1306</span></a>
</span><span id="FGAdaHAT-1307"><a href="#FGAdaHAT-1307"><span class="linenos">1307</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">attribution_abs_batch_mean</span>
</span><span id="FGAdaHAT-1308"><a href="#FGAdaHAT-1308"><span class="linenos">1308</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="FGAdaHAT-1309"><a href="#FGAdaHAT-1309"><span class="linenos">1309</span></a>
</span><span id="FGAdaHAT-1310"><a href="#FGAdaHAT-1310"><span class="linenos">1310</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span><span id="FGAdaHAT-1311"><a href="#FGAdaHAT-1311"><span class="linenos">1311</span></a>
</span><span id="FGAdaHAT-1312"><a href="#FGAdaHAT-1312"><span class="linenos">1312</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_integrated_gradients_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT-1313"><a href="#FGAdaHAT-1313"><span class="linenos">1313</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT-1314"><a href="#FGAdaHAT-1314"><span class="linenos">1314</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT-1315"><a href="#FGAdaHAT-1315"><span class="linenos">1315</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="FGAdaHAT-1316"><a href="#FGAdaHAT-1316"><span class="linenos">1316</span></a>        <span class="n">baselines</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="FGAdaHAT-1317"><a href="#FGAdaHAT-1317"><span class="linenos">1317</span></a>        <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FGAdaHAT-1318"><a href="#FGAdaHAT-1318"><span class="linenos">1318</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FGAdaHAT-1319"><a href="#FGAdaHAT-1319"><span class="linenos">1319</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FGAdaHAT-1320"><a href="#FGAdaHAT-1320"><span class="linenos">1320</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="FGAdaHAT-1321"><a href="#FGAdaHAT-1321"><span class="linenos">1321</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of [integrated gradients](https://proceedings.mlr.press/v70/sundararajan17a/sundararajan17a.pdf). We implement this using [Layer Integrated Gradients](https://captum.ai/api/layer.html#layer-integrated-gradients) in Captum.</span>
</span><span id="FGAdaHAT-1322"><a href="#FGAdaHAT-1322"><span class="linenos">1322</span></a>
</span><span id="FGAdaHAT-1323"><a href="#FGAdaHAT-1323"><span class="linenos">1323</span></a><span class="sd">        **Args:**</span>
</span><span id="FGAdaHAT-1324"><a href="#FGAdaHAT-1324"><span class="linenos">1324</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="FGAdaHAT-1325"><a href="#FGAdaHAT-1325"><span class="linenos">1325</span></a><span class="sd">        - **input** (`Tensor` | `tuple[Tensor, ...]`): the input batch of the training step.</span>
</span><span id="FGAdaHAT-1326"><a href="#FGAdaHAT-1326"><span class="linenos">1326</span></a><span class="sd">        - **baselines** (`None` | `int` | `float` | `Tensor` | `tuple[int | float | Tensor, ...]`): starting point from which integral is computed. Please refer to the [Captum documentation](https://captum.ai/api/layer.html#captum.attr.LayerIntegratedGradients.attribute) for more details.</span>
</span><span id="FGAdaHAT-1327"><a href="#FGAdaHAT-1327"><span class="linenos">1327</span></a><span class="sd">        - **target** (`Tensor` | `None`): the target batch of the training step.</span>
</span><span id="FGAdaHAT-1328"><a href="#FGAdaHAT-1328"><span class="linenos">1328</span></a><span class="sd">        - **batch_idx** (`int`): the index of the current batch. This is an argument of the forward function during training.</span>
</span><span id="FGAdaHAT-1329"><a href="#FGAdaHAT-1329"><span class="linenos">1329</span></a><span class="sd">        - **num_batches** (`int`): the number of batches in the training step. This is an argument of the forward function during training.</span>
</span><span id="FGAdaHAT-1330"><a href="#FGAdaHAT-1330"><span class="linenos">1330</span></a>
</span><span id="FGAdaHAT-1331"><a href="#FGAdaHAT-1331"><span class="linenos">1331</span></a><span class="sd">        **Returns:**</span>
</span><span id="FGAdaHAT-1332"><a href="#FGAdaHAT-1332"><span class="linenos">1332</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="FGAdaHAT-1333"><a href="#FGAdaHAT-1333"><span class="linenos">1333</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT-1334"><a href="#FGAdaHAT-1334"><span class="linenos">1334</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="FGAdaHAT-1335"><a href="#FGAdaHAT-1335"><span class="linenos">1335</span></a>
</span><span id="FGAdaHAT-1336"><a href="#FGAdaHAT-1336"><span class="linenos">1336</span></a>        <span class="c1"># initialize the Layer Integrated Gradients object</span>
</span><span id="FGAdaHAT-1337"><a href="#FGAdaHAT-1337"><span class="linenos">1337</span></a>        <span class="n">layer_integrated_gradients</span> <span class="o">=</span> <span class="n">LayerIntegratedGradients</span><span class="p">(</span>
</span><span id="FGAdaHAT-1338"><a href="#FGAdaHAT-1338"><span class="linenos">1338</span></a>            <span class="n">forward_func</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="n">layer</span>
</span><span id="FGAdaHAT-1339"><a href="#FGAdaHAT-1339"><span class="linenos">1339</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT-1340"><a href="#FGAdaHAT-1340"><span class="linenos">1340</span></a>
</span><span id="FGAdaHAT-1341"><a href="#FGAdaHAT-1341"><span class="linenos">1341</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</span><span id="FGAdaHAT-1342"><a href="#FGAdaHAT-1342"><span class="linenos">1342</span></a>        <span class="c1"># calculate layer attribution of the step</span>
</span><span id="FGAdaHAT-1343"><a href="#FGAdaHAT-1343"><span class="linenos">1343</span></a>        <span class="n">attribution</span> <span class="o">=</span> <span class="n">layer_integrated_gradients</span><span class="o">.</span><span class="n">attribute</span><span class="p">(</span>
</span><span id="FGAdaHAT-1344"><a href="#FGAdaHAT-1344"><span class="linenos">1344</span></a>            <span class="n">inputs</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="FGAdaHAT-1345"><a href="#FGAdaHAT-1345"><span class="linenos">1345</span></a>            <span class="n">baselines</span><span class="o">=</span><span class="n">baselines</span><span class="p">,</span>
</span><span id="FGAdaHAT-1346"><a href="#FGAdaHAT-1346"><span class="linenos">1346</span></a>            <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="FGAdaHAT-1347"><a href="#FGAdaHAT-1347"><span class="linenos">1347</span></a>            <span class="n">additional_forward_args</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">num_batches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">),</span>
</span><span id="FGAdaHAT-1348"><a href="#FGAdaHAT-1348"><span class="linenos">1348</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT-1349"><a href="#FGAdaHAT-1349"><span class="linenos">1349</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</span><span id="FGAdaHAT-1350"><a href="#FGAdaHAT-1350"><span class="linenos">1350</span></a>
</span><span id="FGAdaHAT-1351"><a href="#FGAdaHAT-1351"><span class="linenos">1351</span></a>        <span class="n">attribution_abs_batch_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
</span><span id="FGAdaHAT-1352"><a href="#FGAdaHAT-1352"><span class="linenos">1352</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">attribution</span><span class="p">),</span>
</span><span id="FGAdaHAT-1353"><a href="#FGAdaHAT-1353"><span class="linenos">1353</span></a>            <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT-1354"><a href="#FGAdaHAT-1354"><span class="linenos">1354</span></a>                <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">attribution</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="FGAdaHAT-1355"><a href="#FGAdaHAT-1355"><span class="linenos">1355</span></a>            <span class="p">],</span>  <span class="c1"># average the features over batch samples</span>
</span><span id="FGAdaHAT-1356"><a href="#FGAdaHAT-1356"><span class="linenos">1356</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT-1357"><a href="#FGAdaHAT-1357"><span class="linenos">1357</span></a>
</span><span id="FGAdaHAT-1358"><a href="#FGAdaHAT-1358"><span class="linenos">1358</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">attribution_abs_batch_mean</span>
</span><span id="FGAdaHAT-1359"><a href="#FGAdaHAT-1359"><span class="linenos">1359</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="FGAdaHAT-1360"><a href="#FGAdaHAT-1360"><span class="linenos">1360</span></a>
</span><span id="FGAdaHAT-1361"><a href="#FGAdaHAT-1361"><span class="linenos">1361</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span><span id="FGAdaHAT-1362"><a href="#FGAdaHAT-1362"><span class="linenos">1362</span></a>
</span><span id="FGAdaHAT-1363"><a href="#FGAdaHAT-1363"><span class="linenos">1363</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_feature_ablation_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT-1364"><a href="#FGAdaHAT-1364"><span class="linenos">1364</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT-1365"><a href="#FGAdaHAT-1365"><span class="linenos">1365</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT-1366"><a href="#FGAdaHAT-1366"><span class="linenos">1366</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="FGAdaHAT-1367"><a href="#FGAdaHAT-1367"><span class="linenos">1367</span></a>        <span class="n">layer_baselines</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="FGAdaHAT-1368"><a href="#FGAdaHAT-1368"><span class="linenos">1368</span></a>        <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FGAdaHAT-1369"><a href="#FGAdaHAT-1369"><span class="linenos">1369</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FGAdaHAT-1370"><a href="#FGAdaHAT-1370"><span class="linenos">1370</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FGAdaHAT-1371"><a href="#FGAdaHAT-1371"><span class="linenos">1371</span></a>        <span class="n">if_captum</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="FGAdaHAT-1372"><a href="#FGAdaHAT-1372"><span class="linenos">1372</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="FGAdaHAT-1373"><a href="#FGAdaHAT-1373"><span class="linenos">1373</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of [feature ablation](https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53) attribution. We implement this using [Layer Feature Ablation](https://captum.ai/api/layer.html#layer-feature-ablation) in Captum.</span>
</span><span id="FGAdaHAT-1374"><a href="#FGAdaHAT-1374"><span class="linenos">1374</span></a>
</span><span id="FGAdaHAT-1375"><a href="#FGAdaHAT-1375"><span class="linenos">1375</span></a><span class="sd">        **Args:**</span>
</span><span id="FGAdaHAT-1376"><a href="#FGAdaHAT-1376"><span class="linenos">1376</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="FGAdaHAT-1377"><a href="#FGAdaHAT-1377"><span class="linenos">1377</span></a><span class="sd">        - **input** (`Tensor` | `tuple[Tensor, ...]`): the input batch of the training step.</span>
</span><span id="FGAdaHAT-1378"><a href="#FGAdaHAT-1378"><span class="linenos">1378</span></a><span class="sd">        - **layer_baselines** (`None` | `int` | `float` | `Tensor` | `tuple[int | float | Tensor, ...]`): reference values which replace each layer input / output value when ablated. Please refer to the [Captum documentation](https://captum.ai/api/layer.html#captum.attr.LayerFeatureAblation.attribute) for more details.</span>
</span><span id="FGAdaHAT-1379"><a href="#FGAdaHAT-1379"><span class="linenos">1379</span></a><span class="sd">        - **target** (`Tensor` | `None`): the target batch of the training step.</span>
</span><span id="FGAdaHAT-1380"><a href="#FGAdaHAT-1380"><span class="linenos">1380</span></a><span class="sd">        - **batch_idx** (`int`): the index of the current batch. This is an argument of the forward function during training.</span>
</span><span id="FGAdaHAT-1381"><a href="#FGAdaHAT-1381"><span class="linenos">1381</span></a><span class="sd">        - **num_batches** (`int`): the number of batches in the training step. This is an argument of the forward function during training.</span>
</span><span id="FGAdaHAT-1382"><a href="#FGAdaHAT-1382"><span class="linenos">1382</span></a><span class="sd">        - **if_captum** (`bool`): whether to use Captum or not. If `True`, we use Captum to calculate the feature ablation. If `False`, we use our implementation. Default is `False`, because our implementation is much faster.</span>
</span><span id="FGAdaHAT-1383"><a href="#FGAdaHAT-1383"><span class="linenos">1383</span></a>
</span><span id="FGAdaHAT-1384"><a href="#FGAdaHAT-1384"><span class="linenos">1384</span></a><span class="sd">        **Returns:**</span>
</span><span id="FGAdaHAT-1385"><a href="#FGAdaHAT-1385"><span class="linenos">1385</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="FGAdaHAT-1386"><a href="#FGAdaHAT-1386"><span class="linenos">1386</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT-1387"><a href="#FGAdaHAT-1387"><span class="linenos">1387</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="FGAdaHAT-1388"><a href="#FGAdaHAT-1388"><span class="linenos">1388</span></a>
</span><span id="FGAdaHAT-1389"><a href="#FGAdaHAT-1389"><span class="linenos">1389</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="n">if_captum</span><span class="p">:</span>
</span><span id="FGAdaHAT-1390"><a href="#FGAdaHAT-1390"><span class="linenos">1390</span></a>            <span class="c1"># 1. Baseline logits (take first element of forward output)</span>
</span><span id="FGAdaHAT-1391"><a href="#FGAdaHAT-1391"><span class="linenos">1391</span></a>            <span class="n">baseline_out</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
</span><span id="FGAdaHAT-1392"><a href="#FGAdaHAT-1392"><span class="linenos">1392</span></a>                <span class="nb">input</span><span class="p">,</span> <span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">num_batches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span>
</span><span id="FGAdaHAT-1393"><a href="#FGAdaHAT-1393"><span class="linenos">1393</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT-1394"><a href="#FGAdaHAT-1394"><span class="linenos">1394</span></a>            <span class="k">if</span> <span class="n">target</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FGAdaHAT-1395"><a href="#FGAdaHAT-1395"><span class="linenos">1395</span></a>                <span class="n">baseline_scores</span> <span class="o">=</span> <span class="n">baseline_out</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span><span id="FGAdaHAT-1396"><a href="#FGAdaHAT-1396"><span class="linenos">1396</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="FGAdaHAT-1397"><a href="#FGAdaHAT-1397"><span class="linenos">1397</span></a>                <span class="n">baseline_scores</span> <span class="o">=</span> <span class="n">baseline_out</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="FGAdaHAT-1398"><a href="#FGAdaHAT-1398"><span class="linenos">1398</span></a>
</span><span id="FGAdaHAT-1399"><a href="#FGAdaHAT-1399"><span class="linenos">1399</span></a>            <span class="c1"># 2. Capture layer’s output shape</span>
</span><span id="FGAdaHAT-1400"><a href="#FGAdaHAT-1400"><span class="linenos">1400</span></a>            <span class="n">activs</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="FGAdaHAT-1401"><a href="#FGAdaHAT-1401"><span class="linenos">1401</span></a>            <span class="n">handle</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">register_forward_hook</span><span class="p">(</span>
</span><span id="FGAdaHAT-1402"><a href="#FGAdaHAT-1402"><span class="linenos">1402</span></a>                <span class="k">lambda</span> <span class="n">module</span><span class="p">,</span> <span class="n">inp</span><span class="p">,</span> <span class="n">out</span><span class="p">:</span> <span class="n">activs</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;output&quot;</span><span class="p">,</span> <span class="n">out</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
</span><span id="FGAdaHAT-1403"><a href="#FGAdaHAT-1403"><span class="linenos">1403</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT-1404"><a href="#FGAdaHAT-1404"><span class="linenos">1404</span></a>            <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">num_batches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">)</span>
</span><span id="FGAdaHAT-1405"><a href="#FGAdaHAT-1405"><span class="linenos">1405</span></a>            <span class="n">handle</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span>
</span><span id="FGAdaHAT-1406"><a href="#FGAdaHAT-1406"><span class="linenos">1406</span></a>            <span class="n">layer_output</span> <span class="o">=</span> <span class="n">activs</span><span class="p">[</span><span class="s2">&quot;output&quot;</span><span class="p">]</span>  <span class="c1"># shape (B, F, ...)</span>
</span><span id="FGAdaHAT-1407"><a href="#FGAdaHAT-1407"><span class="linenos">1407</span></a>
</span><span id="FGAdaHAT-1408"><a href="#FGAdaHAT-1408"><span class="linenos">1408</span></a>            <span class="c1"># 3. Build baseline tensor matching that shape</span>
</span><span id="FGAdaHAT-1409"><a href="#FGAdaHAT-1409"><span class="linenos">1409</span></a>            <span class="k">if</span> <span class="n">layer_baselines</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FGAdaHAT-1410"><a href="#FGAdaHAT-1410"><span class="linenos">1410</span></a>                <span class="n">baseline_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">layer_output</span><span class="p">)</span>
</span><span id="FGAdaHAT-1411"><a href="#FGAdaHAT-1411"><span class="linenos">1411</span></a>            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer_baselines</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">)):</span>
</span><span id="FGAdaHAT-1412"><a href="#FGAdaHAT-1412"><span class="linenos">1412</span></a>                <span class="n">baseline_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">layer_output</span><span class="p">,</span> <span class="n">layer_baselines</span><span class="p">)</span>
</span><span id="FGAdaHAT-1413"><a href="#FGAdaHAT-1413"><span class="linenos">1413</span></a>            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer_baselines</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
</span><span id="FGAdaHAT-1414"><a href="#FGAdaHAT-1414"><span class="linenos">1414</span></a>                <span class="k">if</span> <span class="n">layer_baselines</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">layer_output</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
</span><span id="FGAdaHAT-1415"><a href="#FGAdaHAT-1415"><span class="linenos">1415</span></a>                    <span class="n">baseline_tensor</span> <span class="o">=</span> <span class="n">layer_baselines</span>
</span><span id="FGAdaHAT-1416"><a href="#FGAdaHAT-1416"><span class="linenos">1416</span></a>                <span class="k">elif</span> <span class="n">layer_baselines</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">layer_output</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
</span><span id="FGAdaHAT-1417"><a href="#FGAdaHAT-1417"><span class="linenos">1417</span></a>                    <span class="n">baseline_tensor</span> <span class="o">=</span> <span class="n">layer_baselines</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span>
</span><span id="FGAdaHAT-1418"><a href="#FGAdaHAT-1418"><span class="linenos">1418</span></a>                        <span class="n">layer_output</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">*</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">layer_baselines</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
</span><span id="FGAdaHAT-1419"><a href="#FGAdaHAT-1419"><span class="linenos">1419</span></a>                    <span class="p">)</span>
</span><span id="FGAdaHAT-1420"><a href="#FGAdaHAT-1420"><span class="linenos">1420</span></a>                <span class="k">else</span><span class="p">:</span>
</span><span id="FGAdaHAT-1421"><a href="#FGAdaHAT-1421"><span class="linenos">1421</span></a>                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
</span><span id="FGAdaHAT-1422"><a href="#FGAdaHAT-1422"><span class="linenos">1422</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="FGAdaHAT-1423"><a href="#FGAdaHAT-1423"><span class="linenos">1423</span></a>                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
</span><span id="FGAdaHAT-1424"><a href="#FGAdaHAT-1424"><span class="linenos">1424</span></a>
</span><span id="FGAdaHAT-1425"><a href="#FGAdaHAT-1425"><span class="linenos">1425</span></a>            <span class="n">B</span><span class="p">,</span> <span class="n">F</span> <span class="o">=</span> <span class="n">layer_output</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">layer_output</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span><span id="FGAdaHAT-1426"><a href="#FGAdaHAT-1426"><span class="linenos">1426</span></a>
</span><span id="FGAdaHAT-1427"><a href="#FGAdaHAT-1427"><span class="linenos">1427</span></a>            <span class="c1"># 4. Create a “mega-batch” replicating the input F times</span>
</span><span id="FGAdaHAT-1428"><a href="#FGAdaHAT-1428"><span class="linenos">1428</span></a>            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
</span><span id="FGAdaHAT-1429"><a href="#FGAdaHAT-1429"><span class="linenos">1429</span></a>                <span class="n">mega_inputs</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span>
</span><span id="FGAdaHAT-1430"><a href="#FGAdaHAT-1430"><span class="linenos">1430</span></a>                    <span class="n">t</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">F</span><span class="p">,</span> <span class="o">*</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">t</span><span class="o">.</span><span class="n">ndim</span><span class="p">))</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="n">t</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
</span><span id="FGAdaHAT-1431"><a href="#FGAdaHAT-1431"><span class="linenos">1431</span></a>                    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">input</span>
</span><span id="FGAdaHAT-1432"><a href="#FGAdaHAT-1432"><span class="linenos">1432</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT-1433"><a href="#FGAdaHAT-1433"><span class="linenos">1433</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="FGAdaHAT-1434"><a href="#FGAdaHAT-1434"><span class="linenos">1434</span></a>                <span class="n">mega_inputs</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FGAdaHAT-1435"><a href="#FGAdaHAT-1435"><span class="linenos">1435</span></a>                    <span class="nb">input</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="FGAdaHAT-1436"><a href="#FGAdaHAT-1436"><span class="linenos">1436</span></a>                    <span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">F</span><span class="p">,</span> <span class="o">*</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">input</span><span class="o">.</span><span class="n">ndim</span><span class="p">))</span>
</span><span id="FGAdaHAT-1437"><a href="#FGAdaHAT-1437"><span class="linenos">1437</span></a>                    <span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
</span><span id="FGAdaHAT-1438"><a href="#FGAdaHAT-1438"><span class="linenos">1438</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT-1439"><a href="#FGAdaHAT-1439"><span class="linenos">1439</span></a>
</span><span id="FGAdaHAT-1440"><a href="#FGAdaHAT-1440"><span class="linenos">1440</span></a>            <span class="c1"># 5. Equally replicate the baseline tensor</span>
</span><span id="FGAdaHAT-1441"><a href="#FGAdaHAT-1441"><span class="linenos">1441</span></a>            <span class="n">mega_baseline</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FGAdaHAT-1442"><a href="#FGAdaHAT-1442"><span class="linenos">1442</span></a>                <span class="n">baseline_tensor</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="FGAdaHAT-1443"><a href="#FGAdaHAT-1443"><span class="linenos">1443</span></a>                <span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">F</span><span class="p">,</span> <span class="o">*</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">baseline_tensor</span><span class="o">.</span><span class="n">ndim</span><span class="p">))</span>
</span><span id="FGAdaHAT-1444"><a href="#FGAdaHAT-1444"><span class="linenos">1444</span></a>                <span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="n">baseline_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
</span><span id="FGAdaHAT-1445"><a href="#FGAdaHAT-1445"><span class="linenos">1445</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT-1446"><a href="#FGAdaHAT-1446"><span class="linenos">1446</span></a>
</span><span id="FGAdaHAT-1447"><a href="#FGAdaHAT-1447"><span class="linenos">1447</span></a>            <span class="c1"># 6. Precompute vectorized indices</span>
</span><span id="FGAdaHAT-1448"><a href="#FGAdaHAT-1448"><span class="linenos">1448</span></a>            <span class="n">device</span> <span class="o">=</span> <span class="n">layer_output</span><span class="o">.</span><span class="n">device</span>
</span><span id="FGAdaHAT-1449"><a href="#FGAdaHAT-1449"><span class="linenos">1449</span></a>            <span class="n">positions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">F</span> <span class="o">*</span> <span class="n">B</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># [0,1,...,F*B-1]</span>
</span><span id="FGAdaHAT-1450"><a href="#FGAdaHAT-1450"><span class="linenos">1450</span></a>            <span class="n">feat_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">F</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span>
</span><span id="FGAdaHAT-1451"><a href="#FGAdaHAT-1451"><span class="linenos">1451</span></a>                <span class="n">B</span>
</span><span id="FGAdaHAT-1452"><a href="#FGAdaHAT-1452"><span class="linenos">1452</span></a>            <span class="p">)</span>  <span class="c1"># [0,0,...,1,1,...,F-1]</span>
</span><span id="FGAdaHAT-1453"><a href="#FGAdaHAT-1453"><span class="linenos">1453</span></a>
</span><span id="FGAdaHAT-1454"><a href="#FGAdaHAT-1454"><span class="linenos">1454</span></a>            <span class="c1"># 7. One hook to zero out each channel slice across the mega-batch</span>
</span><span id="FGAdaHAT-1455"><a href="#FGAdaHAT-1455"><span class="linenos">1455</span></a>            <span class="k">def</span><span class="w"> </span><span class="nf">mega_ablate_hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">inp</span><span class="p">,</span> <span class="n">out</span><span class="p">):</span>
</span><span id="FGAdaHAT-1456"><a href="#FGAdaHAT-1456"><span class="linenos">1456</span></a>                <span class="n">out_mod</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</span><span id="FGAdaHAT-1457"><a href="#FGAdaHAT-1457"><span class="linenos">1457</span></a>                <span class="c1"># for each sample in mega-batch, zero its corresponding channel</span>
</span><span id="FGAdaHAT-1458"><a href="#FGAdaHAT-1458"><span class="linenos">1458</span></a>                <span class="n">out_mod</span><span class="p">[</span><span class="n">positions</span><span class="p">,</span> <span class="n">feat_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">mega_baseline</span><span class="p">[</span><span class="n">positions</span><span class="p">,</span> <span class="n">feat_idx</span><span class="p">]</span>
</span><span id="FGAdaHAT-1459"><a href="#FGAdaHAT-1459"><span class="linenos">1459</span></a>                <span class="k">return</span> <span class="n">out_mod</span>
</span><span id="FGAdaHAT-1460"><a href="#FGAdaHAT-1460"><span class="linenos">1460</span></a>
</span><span id="FGAdaHAT-1461"><a href="#FGAdaHAT-1461"><span class="linenos">1461</span></a>            <span class="n">h</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">register_forward_hook</span><span class="p">(</span><span class="n">mega_ablate_hook</span><span class="p">)</span>
</span><span id="FGAdaHAT-1462"><a href="#FGAdaHAT-1462"><span class="linenos">1462</span></a>            <span class="n">out_all</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
</span><span id="FGAdaHAT-1463"><a href="#FGAdaHAT-1463"><span class="linenos">1463</span></a>                <span class="n">mega_inputs</span><span class="p">,</span> <span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">num_batches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span>
</span><span id="FGAdaHAT-1464"><a href="#FGAdaHAT-1464"><span class="linenos">1464</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT-1465"><a href="#FGAdaHAT-1465"><span class="linenos">1465</span></a>            <span class="n">h</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span>
</span><span id="FGAdaHAT-1466"><a href="#FGAdaHAT-1466"><span class="linenos">1466</span></a>
</span><span id="FGAdaHAT-1467"><a href="#FGAdaHAT-1467"><span class="linenos">1467</span></a>            <span class="c1"># 8. Recover scores, reshape [F*B] → [F, B], diff &amp; mean</span>
</span><span id="FGAdaHAT-1468"><a href="#FGAdaHAT-1468"><span class="linenos">1468</span></a>            <span class="k">if</span> <span class="n">target</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FGAdaHAT-1469"><a href="#FGAdaHAT-1469"><span class="linenos">1469</span></a>                <span class="n">tgt_flat</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">F</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="FGAdaHAT-1470"><a href="#FGAdaHAT-1470"><span class="linenos">1470</span></a>                <span class="n">scores_all</span> <span class="o">=</span> <span class="n">out_all</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">tgt_flat</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span><span id="FGAdaHAT-1471"><a href="#FGAdaHAT-1471"><span class="linenos">1471</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="FGAdaHAT-1472"><a href="#FGAdaHAT-1472"><span class="linenos">1472</span></a>                <span class="n">scores_all</span> <span class="o">=</span> <span class="n">out_all</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="FGAdaHAT-1473"><a href="#FGAdaHAT-1473"><span class="linenos">1473</span></a>
</span><span id="FGAdaHAT-1474"><a href="#FGAdaHAT-1474"><span class="linenos">1474</span></a>            <span class="n">scores_all</span> <span class="o">=</span> <span class="n">scores_all</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">F</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>
</span><span id="FGAdaHAT-1475"><a href="#FGAdaHAT-1475"><span class="linenos">1475</span></a>            <span class="n">diffs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">baseline_scores</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="n">scores_all</span><span class="p">)</span>
</span><span id="FGAdaHAT-1476"><a href="#FGAdaHAT-1476"><span class="linenos">1476</span></a>            <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">diffs</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>  <span class="c1"># [F]</span>
</span><span id="FGAdaHAT-1477"><a href="#FGAdaHAT-1477"><span class="linenos">1477</span></a>
</span><span id="FGAdaHAT-1478"><a href="#FGAdaHAT-1478"><span class="linenos">1478</span></a>            <span class="k">return</span> <span class="n">importance_step_layer</span>
</span><span id="FGAdaHAT-1479"><a href="#FGAdaHAT-1479"><span class="linenos">1479</span></a>
</span><span id="FGAdaHAT-1480"><a href="#FGAdaHAT-1480"><span class="linenos">1480</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="FGAdaHAT-1481"><a href="#FGAdaHAT-1481"><span class="linenos">1481</span></a>            <span class="c1"># initialize the Layer Feature Ablation object</span>
</span><span id="FGAdaHAT-1482"><a href="#FGAdaHAT-1482"><span class="linenos">1482</span></a>            <span class="n">layer_feature_ablation</span> <span class="o">=</span> <span class="n">LayerFeatureAblation</span><span class="p">(</span>
</span><span id="FGAdaHAT-1483"><a href="#FGAdaHAT-1483"><span class="linenos">1483</span></a>                <span class="n">forward_func</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="n">layer</span>
</span><span id="FGAdaHAT-1484"><a href="#FGAdaHAT-1484"><span class="linenos">1484</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT-1485"><a href="#FGAdaHAT-1485"><span class="linenos">1485</span></a>
</span><span id="FGAdaHAT-1486"><a href="#FGAdaHAT-1486"><span class="linenos">1486</span></a>            <span class="c1"># calculate layer attribution of the step</span>
</span><span id="FGAdaHAT-1487"><a href="#FGAdaHAT-1487"><span class="linenos">1487</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</span><span id="FGAdaHAT-1488"><a href="#FGAdaHAT-1488"><span class="linenos">1488</span></a>            <span class="n">attribution</span> <span class="o">=</span> <span class="n">layer_feature_ablation</span><span class="o">.</span><span class="n">attribute</span><span class="p">(</span>
</span><span id="FGAdaHAT-1489"><a href="#FGAdaHAT-1489"><span class="linenos">1489</span></a>                <span class="n">inputs</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="FGAdaHAT-1490"><a href="#FGAdaHAT-1490"><span class="linenos">1490</span></a>                <span class="n">layer_baselines</span><span class="o">=</span><span class="n">layer_baselines</span><span class="p">,</span>
</span><span id="FGAdaHAT-1491"><a href="#FGAdaHAT-1491"><span class="linenos">1491</span></a>                <span class="c1"># target=target, # disable target to enable perturbations_per_eval</span>
</span><span id="FGAdaHAT-1492"><a href="#FGAdaHAT-1492"><span class="linenos">1492</span></a>                <span class="n">additional_forward_args</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">num_batches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">),</span>
</span><span id="FGAdaHAT-1493"><a href="#FGAdaHAT-1493"><span class="linenos">1493</span></a>                <span class="n">perturbations_per_eval</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>  <span class="c1"># to accelerate the computation</span>
</span><span id="FGAdaHAT-1494"><a href="#FGAdaHAT-1494"><span class="linenos">1494</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT-1495"><a href="#FGAdaHAT-1495"><span class="linenos">1495</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</span><span id="FGAdaHAT-1496"><a href="#FGAdaHAT-1496"><span class="linenos">1496</span></a>
</span><span id="FGAdaHAT-1497"><a href="#FGAdaHAT-1497"><span class="linenos">1497</span></a>            <span class="n">attribution_abs_batch_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
</span><span id="FGAdaHAT-1498"><a href="#FGAdaHAT-1498"><span class="linenos">1498</span></a>                <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">attribution</span><span class="p">),</span>
</span><span id="FGAdaHAT-1499"><a href="#FGAdaHAT-1499"><span class="linenos">1499</span></a>                <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT-1500"><a href="#FGAdaHAT-1500"><span class="linenos">1500</span></a>                    <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">attribution</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="FGAdaHAT-1501"><a href="#FGAdaHAT-1501"><span class="linenos">1501</span></a>                <span class="p">],</span>  <span class="c1"># average the features over batch samples</span>
</span><span id="FGAdaHAT-1502"><a href="#FGAdaHAT-1502"><span class="linenos">1502</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT-1503"><a href="#FGAdaHAT-1503"><span class="linenos">1503</span></a>
</span><span id="FGAdaHAT-1504"><a href="#FGAdaHAT-1504"><span class="linenos">1504</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">attribution_abs_batch_mean</span>
</span><span id="FGAdaHAT-1505"><a href="#FGAdaHAT-1505"><span class="linenos">1505</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="FGAdaHAT-1506"><a href="#FGAdaHAT-1506"><span class="linenos">1506</span></a>
</span><span id="FGAdaHAT-1507"><a href="#FGAdaHAT-1507"><span class="linenos">1507</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span><span id="FGAdaHAT-1508"><a href="#FGAdaHAT-1508"><span class="linenos">1508</span></a>
</span><span id="FGAdaHAT-1509"><a href="#FGAdaHAT-1509"><span class="linenos">1509</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_lrp_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT-1510"><a href="#FGAdaHAT-1510"><span class="linenos">1510</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT-1511"><a href="#FGAdaHAT-1511"><span class="linenos">1511</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT-1512"><a href="#FGAdaHAT-1512"><span class="linenos">1512</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="FGAdaHAT-1513"><a href="#FGAdaHAT-1513"><span class="linenos">1513</span></a>        <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FGAdaHAT-1514"><a href="#FGAdaHAT-1514"><span class="linenos">1514</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FGAdaHAT-1515"><a href="#FGAdaHAT-1515"><span class="linenos">1515</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FGAdaHAT-1516"><a href="#FGAdaHAT-1516"><span class="linenos">1516</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="FGAdaHAT-1517"><a href="#FGAdaHAT-1517"><span class="linenos">1517</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of [LRP](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140). We implement this using [Layer LRP](https://captum.ai/api/layer.html#layer-lrp) in Captum.</span>
</span><span id="FGAdaHAT-1518"><a href="#FGAdaHAT-1518"><span class="linenos">1518</span></a>
</span><span id="FGAdaHAT-1519"><a href="#FGAdaHAT-1519"><span class="linenos">1519</span></a><span class="sd">        **Args:**</span>
</span><span id="FGAdaHAT-1520"><a href="#FGAdaHAT-1520"><span class="linenos">1520</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="FGAdaHAT-1521"><a href="#FGAdaHAT-1521"><span class="linenos">1521</span></a><span class="sd">        - **input** (`Tensor` | `tuple[Tensor, ...]`): the input batch of the training step.</span>
</span><span id="FGAdaHAT-1522"><a href="#FGAdaHAT-1522"><span class="linenos">1522</span></a><span class="sd">        - **target** (`Tensor` | `None`): the target batch of the training step.</span>
</span><span id="FGAdaHAT-1523"><a href="#FGAdaHAT-1523"><span class="linenos">1523</span></a><span class="sd">        - **batch_idx** (`int`): the index of the current batch. This is an argument of the forward function during training.</span>
</span><span id="FGAdaHAT-1524"><a href="#FGAdaHAT-1524"><span class="linenos">1524</span></a><span class="sd">        - **num_batches** (`int`): the number of batches in the training step. This is an argument of the forward function during training.</span>
</span><span id="FGAdaHAT-1525"><a href="#FGAdaHAT-1525"><span class="linenos">1525</span></a>
</span><span id="FGAdaHAT-1526"><a href="#FGAdaHAT-1526"><span class="linenos">1526</span></a><span class="sd">        **Returns:**</span>
</span><span id="FGAdaHAT-1527"><a href="#FGAdaHAT-1527"><span class="linenos">1527</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="FGAdaHAT-1528"><a href="#FGAdaHAT-1528"><span class="linenos">1528</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT-1529"><a href="#FGAdaHAT-1529"><span class="linenos">1529</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="FGAdaHAT-1530"><a href="#FGAdaHAT-1530"><span class="linenos">1530</span></a>
</span><span id="FGAdaHAT-1531"><a href="#FGAdaHAT-1531"><span class="linenos">1531</span></a>        <span class="c1"># initialize the Layer LRP object</span>
</span><span id="FGAdaHAT-1532"><a href="#FGAdaHAT-1532"><span class="linenos">1532</span></a>        <span class="n">layer_lrp</span> <span class="o">=</span> <span class="n">LayerLRP</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="n">layer</span><span class="p">)</span>
</span><span id="FGAdaHAT-1533"><a href="#FGAdaHAT-1533"><span class="linenos">1533</span></a>
</span><span id="FGAdaHAT-1534"><a href="#FGAdaHAT-1534"><span class="linenos">1534</span></a>        <span class="c1"># set model to evaluation mode to prevent updating the model parameters</span>
</span><span id="FGAdaHAT-1535"><a href="#FGAdaHAT-1535"><span class="linenos">1535</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</span><span id="FGAdaHAT-1536"><a href="#FGAdaHAT-1536"><span class="linenos">1536</span></a>
</span><span id="FGAdaHAT-1537"><a href="#FGAdaHAT-1537"><span class="linenos">1537</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</span><span id="FGAdaHAT-1538"><a href="#FGAdaHAT-1538"><span class="linenos">1538</span></a>        <span class="c1"># calculate layer attribution of the step</span>
</span><span id="FGAdaHAT-1539"><a href="#FGAdaHAT-1539"><span class="linenos">1539</span></a>        <span class="n">attribution</span> <span class="o">=</span> <span class="n">layer_lrp</span><span class="o">.</span><span class="n">attribute</span><span class="p">(</span>
</span><span id="FGAdaHAT-1540"><a href="#FGAdaHAT-1540"><span class="linenos">1540</span></a>            <span class="n">inputs</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="FGAdaHAT-1541"><a href="#FGAdaHAT-1541"><span class="linenos">1541</span></a>            <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="FGAdaHAT-1542"><a href="#FGAdaHAT-1542"><span class="linenos">1542</span></a>            <span class="n">additional_forward_args</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">num_batches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">),</span>
</span><span id="FGAdaHAT-1543"><a href="#FGAdaHAT-1543"><span class="linenos">1543</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT-1544"><a href="#FGAdaHAT-1544"><span class="linenos">1544</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</span><span id="FGAdaHAT-1545"><a href="#FGAdaHAT-1545"><span class="linenos">1545</span></a>
</span><span id="FGAdaHAT-1546"><a href="#FGAdaHAT-1546"><span class="linenos">1546</span></a>        <span class="n">attribution_abs_batch_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
</span><span id="FGAdaHAT-1547"><a href="#FGAdaHAT-1547"><span class="linenos">1547</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">attribution</span><span class="p">),</span>
</span><span id="FGAdaHAT-1548"><a href="#FGAdaHAT-1548"><span class="linenos">1548</span></a>            <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT-1549"><a href="#FGAdaHAT-1549"><span class="linenos">1549</span></a>                <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">attribution</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="FGAdaHAT-1550"><a href="#FGAdaHAT-1550"><span class="linenos">1550</span></a>            <span class="p">],</span>  <span class="c1"># average the features over batch samples</span>
</span><span id="FGAdaHAT-1551"><a href="#FGAdaHAT-1551"><span class="linenos">1551</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT-1552"><a href="#FGAdaHAT-1552"><span class="linenos">1552</span></a>
</span><span id="FGAdaHAT-1553"><a href="#FGAdaHAT-1553"><span class="linenos">1553</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">attribution_abs_batch_mean</span>
</span><span id="FGAdaHAT-1554"><a href="#FGAdaHAT-1554"><span class="linenos">1554</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="FGAdaHAT-1555"><a href="#FGAdaHAT-1555"><span class="linenos">1555</span></a>
</span><span id="FGAdaHAT-1556"><a href="#FGAdaHAT-1556"><span class="linenos">1556</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span><span id="FGAdaHAT-1557"><a href="#FGAdaHAT-1557"><span class="linenos">1557</span></a>
</span><span id="FGAdaHAT-1558"><a href="#FGAdaHAT-1558"><span class="linenos">1558</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_cbp_adaptive_contribution</span><span class="p">(</span>
</span><span id="FGAdaHAT-1559"><a href="#FGAdaHAT-1559"><span class="linenos">1559</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT-1560"><a href="#FGAdaHAT-1560"><span class="linenos">1560</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT-1561"><a href="#FGAdaHAT-1561"><span class="linenos">1561</span></a>        <span class="n">activation</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="FGAdaHAT-1562"><a href="#FGAdaHAT-1562"><span class="linenos">1562</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="FGAdaHAT-1563"><a href="#FGAdaHAT-1563"><span class="linenos">1563</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the sum of absolute values of layer output weights multiplied by absolute values of activation, then divided by the reciprocal of sum of absolute values of layer input weights. It is equal to the adaptive contribution utility in [CBP](https://www.nature.com/articles/s41586-024-07711-7).</span>
</span><span id="FGAdaHAT-1564"><a href="#FGAdaHAT-1564"><span class="linenos">1564</span></a>
</span><span id="FGAdaHAT-1565"><a href="#FGAdaHAT-1565"><span class="linenos">1565</span></a><span class="sd">        **Args:**</span>
</span><span id="FGAdaHAT-1566"><a href="#FGAdaHAT-1566"><span class="linenos">1566</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="FGAdaHAT-1567"><a href="#FGAdaHAT-1567"><span class="linenos">1567</span></a><span class="sd">        - **activation** (`Tensor`): the activation tensor of the layer. It has the same size of (number of units, ).</span>
</span><span id="FGAdaHAT-1568"><a href="#FGAdaHAT-1568"><span class="linenos">1568</span></a>
</span><span id="FGAdaHAT-1569"><a href="#FGAdaHAT-1569"><span class="linenos">1569</span></a><span class="sd">        **Returns:**</span>
</span><span id="FGAdaHAT-1570"><a href="#FGAdaHAT-1570"><span class="linenos">1570</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="FGAdaHAT-1571"><a href="#FGAdaHAT-1571"><span class="linenos">1571</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT-1572"><a href="#FGAdaHAT-1572"><span class="linenos">1572</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="FGAdaHAT-1573"><a href="#FGAdaHAT-1573"><span class="linenos">1573</span></a>
</span><span id="FGAdaHAT-1574"><a href="#FGAdaHAT-1574"><span class="linenos">1574</span></a>        <span class="n">input_weight_abs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span><span id="FGAdaHAT-1575"><a href="#FGAdaHAT-1575"><span class="linenos">1575</span></a>        <span class="n">input_weight_abs_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
</span><span id="FGAdaHAT-1576"><a href="#FGAdaHAT-1576"><span class="linenos">1576</span></a>            <span class="n">input_weight_abs</span><span class="p">,</span>
</span><span id="FGAdaHAT-1577"><a href="#FGAdaHAT-1577"><span class="linenos">1577</span></a>            <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT-1578"><a href="#FGAdaHAT-1578"><span class="linenos">1578</span></a>                <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">input_weight_abs</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">0</span>
</span><span id="FGAdaHAT-1579"><a href="#FGAdaHAT-1579"><span class="linenos">1579</span></a>            <span class="p">],</span>  <span class="c1"># sum over the input dimension</span>
</span><span id="FGAdaHAT-1580"><a href="#FGAdaHAT-1580"><span class="linenos">1580</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT-1581"><a href="#FGAdaHAT-1581"><span class="linenos">1581</span></a>        <span class="n">input_weight_abs_sum_reciprocal</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">reciprocal</span><span class="p">(</span><span class="n">input_weight_abs_sum</span><span class="p">)</span>
</span><span id="FGAdaHAT-1582"><a href="#FGAdaHAT-1582"><span class="linenos">1582</span></a>
</span><span id="FGAdaHAT-1583"><a href="#FGAdaHAT-1583"><span class="linenos">1583</span></a>        <span class="n">output_weight_abs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">next_layer</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span><span id="FGAdaHAT-1584"><a href="#FGAdaHAT-1584"><span class="linenos">1584</span></a>        <span class="n">output_weight_abs_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
</span><span id="FGAdaHAT-1585"><a href="#FGAdaHAT-1585"><span class="linenos">1585</span></a>            <span class="n">output_weight_abs</span><span class="p">,</span>
</span><span id="FGAdaHAT-1586"><a href="#FGAdaHAT-1586"><span class="linenos">1586</span></a>            <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT-1587"><a href="#FGAdaHAT-1587"><span class="linenos">1587</span></a>                <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">output_weight_abs</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="FGAdaHAT-1588"><a href="#FGAdaHAT-1588"><span class="linenos">1588</span></a>            <span class="p">],</span>  <span class="c1"># sum over the output dimension</span>
</span><span id="FGAdaHAT-1589"><a href="#FGAdaHAT-1589"><span class="linenos">1589</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT-1590"><a href="#FGAdaHAT-1590"><span class="linenos">1590</span></a>
</span><span id="FGAdaHAT-1591"><a href="#FGAdaHAT-1591"><span class="linenos">1591</span></a>        <span class="n">activation_abs_batch_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
</span><span id="FGAdaHAT-1592"><a href="#FGAdaHAT-1592"><span class="linenos">1592</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">activation</span><span class="p">),</span>
</span><span id="FGAdaHAT-1593"><a href="#FGAdaHAT-1593"><span class="linenos">1593</span></a>            <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT-1594"><a href="#FGAdaHAT-1594"><span class="linenos">1594</span></a>                <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">activation</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="FGAdaHAT-1595"><a href="#FGAdaHAT-1595"><span class="linenos">1595</span></a>            <span class="p">],</span>  <span class="c1"># average the features over batch samples</span>
</span><span id="FGAdaHAT-1596"><a href="#FGAdaHAT-1596"><span class="linenos">1596</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT-1597"><a href="#FGAdaHAT-1597"><span class="linenos">1597</span></a>
</span><span id="FGAdaHAT-1598"><a href="#FGAdaHAT-1598"><span class="linenos">1598</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FGAdaHAT-1599"><a href="#FGAdaHAT-1599"><span class="linenos">1599</span></a>            <span class="n">output_weight_abs_sum</span>
</span><span id="FGAdaHAT-1600"><a href="#FGAdaHAT-1600"><span class="linenos">1600</span></a>            <span class="o">*</span> <span class="n">activation_abs_batch_mean</span>
</span><span id="FGAdaHAT-1601"><a href="#FGAdaHAT-1601"><span class="linenos">1601</span></a>            <span class="o">*</span> <span class="n">input_weight_abs_sum_reciprocal</span>
</span><span id="FGAdaHAT-1602"><a href="#FGAdaHAT-1602"><span class="linenos">1602</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT-1603"><a href="#FGAdaHAT-1603"><span class="linenos">1603</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="FGAdaHAT-1604"><a href="#FGAdaHAT-1604"><span class="linenos">1604</span></a>
</span><span id="FGAdaHAT-1605"><a href="#FGAdaHAT-1605"><span class="linenos">1605</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span></pre></div>


            <div class="docstring"><p>FG-AdaHAT (Fine-Grained Adaptive Hard Attention to the Task) algorithm.</p>

<p>An architecture-based continual learning approach that improves <a href="https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9">AdaHAT (Adaptive Hard Attention to the Task)</a> by introducing fine-grained neuron-wise importance measures guiding the adaptive adjustment mechanism in AdaHAT.</p>

<p>We implement FG-AdaHAT as a subclass of AdaHAT, as it reuses AdaHAT's summative mask and other components.</p>
</div>


                            <div id="FGAdaHAT.__init__" class="classattr">
                                        <input id="FGAdaHAT.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">FGAdaHAT</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">backbone</span><span class="p">:</span> <span class="n"><a href="../backbones.html#HATMaskBackbone">clarena.backbones.HATMaskBackbone</a></span>,</span><span class="param">	<span class="n">heads</span><span class="p">:</span> <span class="n"><a href="../heads.html#HeadsTIL">clarena.heads.HeadsTIL</a></span>,</span><span class="param">	<span class="n">adjustment_intensity</span><span class="p">:</span> <span class="nb">float</span>,</span><span class="param">	<span class="n">importance_type</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">importance_summing_strategy</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">importance_scheduler_type</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">neuron_to_weight_importance_aggregation_mode</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">s_max</span><span class="p">:</span> <span class="nb">float</span>,</span><span class="param">	<span class="n">clamp_threshold</span><span class="p">:</span> <span class="nb">float</span>,</span><span class="param">	<span class="n">mask_sparsity_reg_factor</span><span class="p">:</span> <span class="nb">float</span>,</span><span class="param">	<span class="n">mask_sparsity_reg_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;original&#39;</span>,</span><span class="param">	<span class="n">base_importance</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span>,</span><span class="param">	<span class="n">base_mask_sparsity_reg</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span>,</span><span class="param">	<span class="n">base_linear</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">10</span>,</span><span class="param">	<span class="n">filter_by_cumulative_mask</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>,</span><span class="param">	<span class="n">filter_unmasked_importance</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>,</span><span class="param">	<span class="n">step_multiply_training_mask</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>,</span><span class="param">	<span class="n">task_embedding_init_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;N01&#39;</span>,</span><span class="param">	<span class="n">importance_summing_strategy_linear_step</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">importance_summing_strategy_exponential_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">importance_summing_strategy_log_base</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span></span>)</span>

                <label class="view-source-button" for="FGAdaHAT.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FGAdaHAT.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FGAdaHAT.__init__-45"><a href="#FGAdaHAT.__init__-45"><span class="linenos"> 45</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="FGAdaHAT.__init__-46"><a href="#FGAdaHAT.__init__-46"><span class="linenos"> 46</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="FGAdaHAT.__init__-47"><a href="#FGAdaHAT.__init__-47"><span class="linenos"> 47</span></a>        <span class="n">backbone</span><span class="p">:</span> <span class="n">HATMaskBackbone</span><span class="p">,</span>
</span><span id="FGAdaHAT.__init__-48"><a href="#FGAdaHAT.__init__-48"><span class="linenos"> 48</span></a>        <span class="n">heads</span><span class="p">:</span> <span class="n">HeadsTIL</span><span class="p">,</span>
</span><span id="FGAdaHAT.__init__-49"><a href="#FGAdaHAT.__init__-49"><span class="linenos"> 49</span></a>        <span class="n">adjustment_intensity</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="FGAdaHAT.__init__-50"><a href="#FGAdaHAT.__init__-50"><span class="linenos"> 50</span></a>        <span class="n">importance_type</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT.__init__-51"><a href="#FGAdaHAT.__init__-51"><span class="linenos"> 51</span></a>        <span class="n">importance_summing_strategy</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT.__init__-52"><a href="#FGAdaHAT.__init__-52"><span class="linenos"> 52</span></a>        <span class="n">importance_scheduler_type</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT.__init__-53"><a href="#FGAdaHAT.__init__-53"><span class="linenos"> 53</span></a>        <span class="n">neuron_to_weight_importance_aggregation_mode</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT.__init__-54"><a href="#FGAdaHAT.__init__-54"><span class="linenos"> 54</span></a>        <span class="n">s_max</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="FGAdaHAT.__init__-55"><a href="#FGAdaHAT.__init__-55"><span class="linenos"> 55</span></a>        <span class="n">clamp_threshold</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="FGAdaHAT.__init__-56"><a href="#FGAdaHAT.__init__-56"><span class="linenos"> 56</span></a>        <span class="n">mask_sparsity_reg_factor</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="FGAdaHAT.__init__-57"><a href="#FGAdaHAT.__init__-57"><span class="linenos"> 57</span></a>        <span class="n">mask_sparsity_reg_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;original&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT.__init__-58"><a href="#FGAdaHAT.__init__-58"><span class="linenos"> 58</span></a>        <span class="n">base_importance</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
</span><span id="FGAdaHAT.__init__-59"><a href="#FGAdaHAT.__init__-59"><span class="linenos"> 59</span></a>        <span class="n">base_mask_sparsity_reg</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
</span><span id="FGAdaHAT.__init__-60"><a href="#FGAdaHAT.__init__-60"><span class="linenos"> 60</span></a>        <span class="n">base_linear</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
</span><span id="FGAdaHAT.__init__-61"><a href="#FGAdaHAT.__init__-61"><span class="linenos"> 61</span></a>        <span class="n">filter_by_cumulative_mask</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="FGAdaHAT.__init__-62"><a href="#FGAdaHAT.__init__-62"><span class="linenos"> 62</span></a>        <span class="n">filter_unmasked_importance</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="FGAdaHAT.__init__-63"><a href="#FGAdaHAT.__init__-63"><span class="linenos"> 63</span></a>        <span class="n">step_multiply_training_mask</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="FGAdaHAT.__init__-64"><a href="#FGAdaHAT.__init__-64"><span class="linenos"> 64</span></a>        <span class="n">task_embedding_init_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;N01&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT.__init__-65"><a href="#FGAdaHAT.__init__-65"><span class="linenos"> 65</span></a>        <span class="n">importance_summing_strategy_linear_step</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FGAdaHAT.__init__-66"><a href="#FGAdaHAT.__init__-66"><span class="linenos"> 66</span></a>        <span class="n">importance_summing_strategy_exponential_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FGAdaHAT.__init__-67"><a href="#FGAdaHAT.__init__-67"><span class="linenos"> 67</span></a>        <span class="n">importance_summing_strategy_log_base</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FGAdaHAT.__init__-68"><a href="#FGAdaHAT.__init__-68"><span class="linenos"> 68</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FGAdaHAT.__init__-69"><a href="#FGAdaHAT.__init__-69"><span class="linenos"> 69</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialize the FG-AdaHAT algorithm with the network.</span>
</span><span id="FGAdaHAT.__init__-70"><a href="#FGAdaHAT.__init__-70"><span class="linenos"> 70</span></a>
</span><span id="FGAdaHAT.__init__-71"><a href="#FGAdaHAT.__init__-71"><span class="linenos"> 71</span></a><span class="sd">        **Args:**</span>
</span><span id="FGAdaHAT.__init__-72"><a href="#FGAdaHAT.__init__-72"><span class="linenos"> 72</span></a><span class="sd">        - **backbone** (`HATMaskBackbone`): must be a backbone network with the HAT mask mechanism.</span>
</span><span id="FGAdaHAT.__init__-73"><a href="#FGAdaHAT.__init__-73"><span class="linenos"> 73</span></a><span class="sd">        - **heads** (`HeadsTIL`): output heads. FG-AdaHAT supports only TIL (Task-Incremental Learning).</span>
</span><span id="FGAdaHAT.__init__-74"><a href="#FGAdaHAT.__init__-74"><span class="linenos"> 74</span></a><span class="sd">        - **adjustment_intensity** (`float`): hyperparameter, controls the overall intensity of gradient adjustment (the $\alpha$ in the paper).</span>
</span><span id="FGAdaHAT.__init__-75"><a href="#FGAdaHAT.__init__-75"><span class="linenos"> 75</span></a><span class="sd">        - **importance_type** (`str`): the type of neuron-wise importance, must be one of:</span>
</span><span id="FGAdaHAT.__init__-76"><a href="#FGAdaHAT.__init__-76"><span class="linenos"> 76</span></a><span class="sd">            1. &#39;input_weight_abs_sum&#39;: sum of absolute input weights;</span>
</span><span id="FGAdaHAT.__init__-77"><a href="#FGAdaHAT.__init__-77"><span class="linenos"> 77</span></a><span class="sd">            2. &#39;output_weight_abs_sum&#39;: sum of absolute output weights;</span>
</span><span id="FGAdaHAT.__init__-78"><a href="#FGAdaHAT.__init__-78"><span class="linenos"> 78</span></a><span class="sd">            3. &#39;input_weight_gradient_abs_sum&#39;: sum of absolute gradients of the input weights (Input Gradients (IG) in the paper);</span>
</span><span id="FGAdaHAT.__init__-79"><a href="#FGAdaHAT.__init__-79"><span class="linenos"> 79</span></a><span class="sd">            4. &#39;output_weight_gradient_abs_sum&#39;: sum of absolute gradients of the output weights (Output Gradients (OG) in the paper);</span>
</span><span id="FGAdaHAT.__init__-80"><a href="#FGAdaHAT.__init__-80"><span class="linenos"> 80</span></a><span class="sd">            5. &#39;activation_abs&#39;: absolute activation;</span>
</span><span id="FGAdaHAT.__init__-81"><a href="#FGAdaHAT.__init__-81"><span class="linenos"> 81</span></a><span class="sd">            6. &#39;input_weight_abs_sum_x_activation_abs&#39;: sum of absolute input weights multiplied by absolute activation (Input Contribution Utility (ICU) in the paper);</span>
</span><span id="FGAdaHAT.__init__-82"><a href="#FGAdaHAT.__init__-82"><span class="linenos"> 82</span></a><span class="sd">            7. &#39;output_weight_abs_sum_x_activation_abs&#39;: sum of absolute output weights multiplied by absolute activation (Contribution Utility (CU) in the paper);</span>
</span><span id="FGAdaHAT.__init__-83"><a href="#FGAdaHAT.__init__-83"><span class="linenos"> 83</span></a><span class="sd">            8. &#39;gradient_x_activation_abs&#39;: absolute gradient (the saliency) multiplied by activation;</span>
</span><span id="FGAdaHAT.__init__-84"><a href="#FGAdaHAT.__init__-84"><span class="linenos"> 84</span></a><span class="sd">            9. &#39;input_weight_gradient_square_sum&#39;: sum of squared gradients of the input weights;</span>
</span><span id="FGAdaHAT.__init__-85"><a href="#FGAdaHAT.__init__-85"><span class="linenos"> 85</span></a><span class="sd">            10. &#39;output_weight_gradient_square_sum&#39;: sum of squared gradients of the output weights;</span>
</span><span id="FGAdaHAT.__init__-86"><a href="#FGAdaHAT.__init__-86"><span class="linenos"> 86</span></a><span class="sd">            11. &#39;input_weight_gradient_square_sum_x_activation_abs&#39;: sum of squared gradients of the input weights multiplied by absolute activation (Activation Fisher Information (AFI) in the paper);</span>
</span><span id="FGAdaHAT.__init__-87"><a href="#FGAdaHAT.__init__-87"><span class="linenos"> 87</span></a><span class="sd">            12. &#39;output_weight_gradient_square_sum_x_activation_abs&#39;: sum of squared gradients of the output weights multiplied by absolute activation;</span>
</span><span id="FGAdaHAT.__init__-88"><a href="#FGAdaHAT.__init__-88"><span class="linenos"> 88</span></a><span class="sd">            13. &#39;conductance_abs&#39;: absolute layer conductance;</span>
</span><span id="FGAdaHAT.__init__-89"><a href="#FGAdaHAT.__init__-89"><span class="linenos"> 89</span></a><span class="sd">            14. &#39;internal_influence_abs&#39;: absolute internal influence (Internal Influence (II) in the paper);</span>
</span><span id="FGAdaHAT.__init__-90"><a href="#FGAdaHAT.__init__-90"><span class="linenos"> 90</span></a><span class="sd">            15. &#39;gradcam_abs&#39;: absolute Grad-CAM;</span>
</span><span id="FGAdaHAT.__init__-91"><a href="#FGAdaHAT.__init__-91"><span class="linenos"> 91</span></a><span class="sd">            16. &#39;deeplift_abs&#39;: absolute DeepLIFT (DeepLIFT (DL) in the paper);</span>
</span><span id="FGAdaHAT.__init__-92"><a href="#FGAdaHAT.__init__-92"><span class="linenos"> 92</span></a><span class="sd">            17. &#39;deepliftshap_abs&#39;: absolute DeepLIFT-SHAP;</span>
</span><span id="FGAdaHAT.__init__-93"><a href="#FGAdaHAT.__init__-93"><span class="linenos"> 93</span></a><span class="sd">            18. &#39;gradientshap_abs&#39;: absolute Gradient-SHAP (Gradient SHAP (GS) in the paper);</span>
</span><span id="FGAdaHAT.__init__-94"><a href="#FGAdaHAT.__init__-94"><span class="linenos"> 94</span></a><span class="sd">            19. &#39;integrated_gradients_abs&#39;: absolute Integrated Gradients;</span>
</span><span id="FGAdaHAT.__init__-95"><a href="#FGAdaHAT.__init__-95"><span class="linenos"> 95</span></a><span class="sd">            20. &#39;feature_ablation_abs&#39;: absolute Feature Ablation (Feature Ablation (FA) in the paper);</span>
</span><span id="FGAdaHAT.__init__-96"><a href="#FGAdaHAT.__init__-96"><span class="linenos"> 96</span></a><span class="sd">            21. &#39;lrp_abs&#39;: absolute Layer-wise Relevance Propagation (LRP);</span>
</span><span id="FGAdaHAT.__init__-97"><a href="#FGAdaHAT.__init__-97"><span class="linenos"> 97</span></a><span class="sd">            22. &#39;cbp_adaptation&#39;: the adaptation function in [Continual Backpropagation (CBP)](https://www.nature.com/articles/s41586-024-07711-7);</span>
</span><span id="FGAdaHAT.__init__-98"><a href="#FGAdaHAT.__init__-98"><span class="linenos"> 98</span></a><span class="sd">            23. &#39;cbp_adaptive_contribution&#39;: the adaptive contribution function in [Continual Backpropagation (CBP)](https://www.nature.com/articles/s41586-024-07711-7);</span>
</span><span id="FGAdaHAT.__init__-99"><a href="#FGAdaHAT.__init__-99"><span class="linenos"> 99</span></a><span class="sd">        - **importance_summing_strategy** (`str`): the strategy to sum neuron-wise importance for previous tasks, must be one of:</span>
</span><span id="FGAdaHAT.__init__-100"><a href="#FGAdaHAT.__init__-100"><span class="linenos">100</span></a><span class="sd">            1. &#39;add_latest&#39;: add the latest neuron-wise importance to the summative importance;</span>
</span><span id="FGAdaHAT.__init__-101"><a href="#FGAdaHAT.__init__-101"><span class="linenos">101</span></a><span class="sd">            2. &#39;add_all&#39;: add all previous neuron-wise importance (including the latest) to the summative importance;</span>
</span><span id="FGAdaHAT.__init__-102"><a href="#FGAdaHAT.__init__-102"><span class="linenos">102</span></a><span class="sd">            3. &#39;add_average&#39;: add the average of all previous neuron-wise importance (including the latest) to the summative importance;</span>
</span><span id="FGAdaHAT.__init__-103"><a href="#FGAdaHAT.__init__-103"><span class="linenos">103</span></a><span class="sd">            4. &#39;linear_decrease&#39;: weigh the previous neuron-wise importance by a linear factor that decreases with the task ID;</span>
</span><span id="FGAdaHAT.__init__-104"><a href="#FGAdaHAT.__init__-104"><span class="linenos">104</span></a><span class="sd">            5. &#39;quadratic_decrease&#39;: weigh the previous neuron-wise importance that decreases quadratically with the task ID;</span>
</span><span id="FGAdaHAT.__init__-105"><a href="#FGAdaHAT.__init__-105"><span class="linenos">105</span></a><span class="sd">            6. &#39;cubic_decrease&#39;: weigh the previous neuron-wise importance that decreases cubically with the task ID;</span>
</span><span id="FGAdaHAT.__init__-106"><a href="#FGAdaHAT.__init__-106"><span class="linenos">106</span></a><span class="sd">            7. &#39;exponential_decrease&#39;: weigh the previous neuron-wise importance by an exponential factor that decreases with the task ID;</span>
</span><span id="FGAdaHAT.__init__-107"><a href="#FGAdaHAT.__init__-107"><span class="linenos">107</span></a><span class="sd">            8. &#39;log_decrease&#39;: weigh the previous neuron-wise importance by a logarithmic factor that decreases with the task ID;</span>
</span><span id="FGAdaHAT.__init__-108"><a href="#FGAdaHAT.__init__-108"><span class="linenos">108</span></a><span class="sd">            9. &#39;factorial_decrease&#39;: weigh the previous neuron-wise importance that decreases factorially with the task ID;</span>
</span><span id="FGAdaHAT.__init__-109"><a href="#FGAdaHAT.__init__-109"><span class="linenos">109</span></a><span class="sd">        - **importance_scheduler_type** (`str`): the scheduler for importance, i.e., the factor $c^t$ multiplied to parameter importance. Must be one of:</span>
</span><span id="FGAdaHAT.__init__-110"><a href="#FGAdaHAT.__init__-110"><span class="linenos">110</span></a><span class="sd">            1. &#39;linear_sparsity_reg&#39;: $c^t = (t+b_L) \cdot [R(M^t, M^{&lt;t}) + b_R]$, where $R(M^t, M^{&lt;t})$ is the mask sparsity regularization betwwen the current task and previous tasks, $b_L$ is the base linear factor (see argument `base_linear`), and $b_R$ is the base mask sparsity regularization factor (see argument `base_mask_sparsity_reg`);</span>
</span><span id="FGAdaHAT.__init__-111"><a href="#FGAdaHAT.__init__-111"><span class="linenos">111</span></a><span class="sd">            2. &#39;sparsity_reg&#39;: $c^t = [R(M^t, M^{&lt;t}) + b_R]$;</span>
</span><span id="FGAdaHAT.__init__-112"><a href="#FGAdaHAT.__init__-112"><span class="linenos">112</span></a><span class="sd">            3. &#39;summative_mask_sparsity_reg&#39;: $c^t_{l,ij} = \left(\min \left(m^{&lt;t, \text{sum}}_{l,i}, m^{&lt;t, \text{sum}}_{l-1,j}\right)+b_L\right) \cdot [R(M^t, M^{&lt;t}) + b_R]$.</span>
</span><span id="FGAdaHAT.__init__-113"><a href="#FGAdaHAT.__init__-113"><span class="linenos">113</span></a><span class="sd">        - **neuron_to_weight_importance_aggregation_mode** (`str`): aggregation mode from neuron-wise to weight-wise importance ($\text{Agg}(\cdot)$ in the paper), must be one of:</span>
</span><span id="FGAdaHAT.__init__-114"><a href="#FGAdaHAT.__init__-114"><span class="linenos">114</span></a><span class="sd">            1. &#39;min&#39;: take the minimum of neuron-wise importance for each weight;</span>
</span><span id="FGAdaHAT.__init__-115"><a href="#FGAdaHAT.__init__-115"><span class="linenos">115</span></a><span class="sd">            2. &#39;max&#39;: take the maximum of neuron-wise importance for each weight;</span>
</span><span id="FGAdaHAT.__init__-116"><a href="#FGAdaHAT.__init__-116"><span class="linenos">116</span></a><span class="sd">            3. &#39;mean&#39;: take the mean of neuron-wise importance for each weight.</span>
</span><span id="FGAdaHAT.__init__-117"><a href="#FGAdaHAT.__init__-117"><span class="linenos">117</span></a><span class="sd">        - **s_max** (`float`): hyperparameter, the maximum scaling factor in the gate function. See Sec. 2.4 &quot;Hard Attention Training&quot; in the [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="FGAdaHAT.__init__-118"><a href="#FGAdaHAT.__init__-118"><span class="linenos">118</span></a><span class="sd">        - **clamp_threshold** (`float`): the threshold for task embedding gradient compensation. See Sec. 2.5 &quot;Embedding Gradient Compensation&quot; in the [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="FGAdaHAT.__init__-119"><a href="#FGAdaHAT.__init__-119"><span class="linenos">119</span></a><span class="sd">        - **mask_sparsity_reg_factor** (`float`): hyperparameter, the regularization factor for mask sparsity.</span>
</span><span id="FGAdaHAT.__init__-120"><a href="#FGAdaHAT.__init__-120"><span class="linenos">120</span></a><span class="sd">        - **mask_sparsity_reg_mode** (`str`): the mode of mask sparsity regularization, must be one of:</span>
</span><span id="FGAdaHAT.__init__-121"><a href="#FGAdaHAT.__init__-121"><span class="linenos">121</span></a><span class="sd">            1. &#39;original&#39; (default): the original mask sparsity regularization in the [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="FGAdaHAT.__init__-122"><a href="#FGAdaHAT.__init__-122"><span class="linenos">122</span></a><span class="sd">            2. &#39;cross&#39;: the cross version of mask sparsity regularization.</span>
</span><span id="FGAdaHAT.__init__-123"><a href="#FGAdaHAT.__init__-123"><span class="linenos">123</span></a><span class="sd">        - **base_importance** (`float`): base value added to importance ($b_I$ in the paper). Default: 0.01.</span>
</span><span id="FGAdaHAT.__init__-124"><a href="#FGAdaHAT.__init__-124"><span class="linenos">124</span></a><span class="sd">        - **base_mask_sparsity_reg** (`float`): base value added to mask sparsity regularization factor in the importance scheduler ($b_R$ in the paper). Default: 0.1.</span>
</span><span id="FGAdaHAT.__init__-125"><a href="#FGAdaHAT.__init__-125"><span class="linenos">125</span></a><span class="sd">        - **base_linear** (`float`): base value added to the linear factor in the importance scheduler ($b_L$ in the paper). Default: 10.</span>
</span><span id="FGAdaHAT.__init__-126"><a href="#FGAdaHAT.__init__-126"><span class="linenos">126</span></a><span class="sd">        - **filter_by_cumulative_mask** (`bool`): whether to multiply the cumulative mask to the importance when calculating adjustment rate. Default: False.</span>
</span><span id="FGAdaHAT.__init__-127"><a href="#FGAdaHAT.__init__-127"><span class="linenos">127</span></a><span class="sd">        - **filter_unmasked_importance** (`bool`): whether to filter unmasked importance values (set to 0) at the end of task training. Default: False.</span>
</span><span id="FGAdaHAT.__init__-128"><a href="#FGAdaHAT.__init__-128"><span class="linenos">128</span></a><span class="sd">        - **step_multiply_training_mask** (`bool`): whether to multiply the training mask to the importance at each training step. Default: True.</span>
</span><span id="FGAdaHAT.__init__-129"><a href="#FGAdaHAT.__init__-129"><span class="linenos">129</span></a><span class="sd">        - **task_embedding_init_mode** (`str`): the initialization mode for task embeddings, must be one of:</span>
</span><span id="FGAdaHAT.__init__-130"><a href="#FGAdaHAT.__init__-130"><span class="linenos">130</span></a><span class="sd">            1. &#39;N01&#39; (default): standard normal distribution $N(0, 1)$.</span>
</span><span id="FGAdaHAT.__init__-131"><a href="#FGAdaHAT.__init__-131"><span class="linenos">131</span></a><span class="sd">            2. &#39;U-11&#39;: uniform distribution $U(-1, 1)$.</span>
</span><span id="FGAdaHAT.__init__-132"><a href="#FGAdaHAT.__init__-132"><span class="linenos">132</span></a><span class="sd">            3. &#39;U01&#39;: uniform distribution $U(0, 1)$.</span>
</span><span id="FGAdaHAT.__init__-133"><a href="#FGAdaHAT.__init__-133"><span class="linenos">133</span></a><span class="sd">            4. &#39;U-10&#39;: uniform distribution $U(-1, 0)$.</span>
</span><span id="FGAdaHAT.__init__-134"><a href="#FGAdaHAT.__init__-134"><span class="linenos">134</span></a><span class="sd">            5. &#39;last&#39;: inherit the task embedding from the last task.</span>
</span><span id="FGAdaHAT.__init__-135"><a href="#FGAdaHAT.__init__-135"><span class="linenos">135</span></a><span class="sd">        - **importance_summing_strategy_linear_step** (`float` | `None`): linear step for the importance summing strategy (used when `importance_summing_strategy` is &#39;linear_decrease&#39;). Must be &gt; 0.</span>
</span><span id="FGAdaHAT.__init__-136"><a href="#FGAdaHAT.__init__-136"><span class="linenos">136</span></a><span class="sd">        - **importance_summing_strategy_exponential_rate** (`float` | `None`): exponential rate for the importance summing strategy (used when `importance_summing_strategy` is &#39;exponential_decrease&#39;). Must be &gt; 1.</span>
</span><span id="FGAdaHAT.__init__-137"><a href="#FGAdaHAT.__init__-137"><span class="linenos">137</span></a><span class="sd">        - **importance_summing_strategy_log_base** (`float` | `None`): base for the logarithm in the importance summing strategy (used when `importance_summing_strategy` is &#39;log_decrease&#39;). Must be &gt; 1.</span>
</span><span id="FGAdaHAT.__init__-138"><a href="#FGAdaHAT.__init__-138"><span class="linenos">138</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT.__init__-139"><a href="#FGAdaHAT.__init__-139"><span class="linenos">139</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="FGAdaHAT.__init__-140"><a href="#FGAdaHAT.__init__-140"><span class="linenos">140</span></a>            <span class="n">backbone</span><span class="o">=</span><span class="n">backbone</span><span class="p">,</span>
</span><span id="FGAdaHAT.__init__-141"><a href="#FGAdaHAT.__init__-141"><span class="linenos">141</span></a>            <span class="n">heads</span><span class="o">=</span><span class="n">heads</span><span class="p">,</span>
</span><span id="FGAdaHAT.__init__-142"><a href="#FGAdaHAT.__init__-142"><span class="linenos">142</span></a>            <span class="n">adjustment_mode</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="c1"># use the own adjustment mechanism of FG-AdaHAT</span>
</span><span id="FGAdaHAT.__init__-143"><a href="#FGAdaHAT.__init__-143"><span class="linenos">143</span></a>            <span class="n">adjustment_intensity</span><span class="o">=</span><span class="n">adjustment_intensity</span><span class="p">,</span>
</span><span id="FGAdaHAT.__init__-144"><a href="#FGAdaHAT.__init__-144"><span class="linenos">144</span></a>            <span class="n">s_max</span><span class="o">=</span><span class="n">s_max</span><span class="p">,</span>
</span><span id="FGAdaHAT.__init__-145"><a href="#FGAdaHAT.__init__-145"><span class="linenos">145</span></a>            <span class="n">clamp_threshold</span><span class="o">=</span><span class="n">clamp_threshold</span><span class="p">,</span>
</span><span id="FGAdaHAT.__init__-146"><a href="#FGAdaHAT.__init__-146"><span class="linenos">146</span></a>            <span class="n">mask_sparsity_reg_factor</span><span class="o">=</span><span class="n">mask_sparsity_reg_factor</span><span class="p">,</span>
</span><span id="FGAdaHAT.__init__-147"><a href="#FGAdaHAT.__init__-147"><span class="linenos">147</span></a>            <span class="n">mask_sparsity_reg_mode</span><span class="o">=</span><span class="n">mask_sparsity_reg_mode</span><span class="p">,</span>
</span><span id="FGAdaHAT.__init__-148"><a href="#FGAdaHAT.__init__-148"><span class="linenos">148</span></a>            <span class="n">task_embedding_init_mode</span><span class="o">=</span><span class="n">task_embedding_init_mode</span><span class="p">,</span>
</span><span id="FGAdaHAT.__init__-149"><a href="#FGAdaHAT.__init__-149"><span class="linenos">149</span></a>            <span class="n">epsilon</span><span class="o">=</span><span class="n">base_mask_sparsity_reg</span><span class="p">,</span>  <span class="c1"># the epsilon is now the base mask sparsity regularization factor</span>
</span><span id="FGAdaHAT.__init__-150"><a href="#FGAdaHAT.__init__-150"><span class="linenos">150</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT.__init__-151"><a href="#FGAdaHAT.__init__-151"><span class="linenos">151</span></a>
</span><span id="FGAdaHAT.__init__-152"><a href="#FGAdaHAT.__init__-152"><span class="linenos">152</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">importance_type</span>
</span><span id="FGAdaHAT.__init__-153"><a href="#FGAdaHAT.__init__-153"><span class="linenos">153</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The type of the neuron-wise importance added to AdaHAT importance.&quot;&quot;&quot;</span>
</span><span id="FGAdaHAT.__init__-154"><a href="#FGAdaHAT.__init__-154"><span class="linenos">154</span></a>
</span><span id="FGAdaHAT.__init__-155"><a href="#FGAdaHAT.__init__-155"><span class="linenos">155</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">importance_scheduler_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">importance_scheduler_type</span>
</span><span id="FGAdaHAT.__init__-156"><a href="#FGAdaHAT.__init__-156"><span class="linenos">156</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the type of the importance scheduler.&quot;&quot;&quot;</span>
</span><span id="FGAdaHAT.__init__-157"><a href="#FGAdaHAT.__init__-157"><span class="linenos">157</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">neuron_to_weight_importance_aggregation_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FGAdaHAT.__init__-158"><a href="#FGAdaHAT.__init__-158"><span class="linenos">158</span></a>            <span class="n">neuron_to_weight_importance_aggregation_mode</span>
</span><span id="FGAdaHAT.__init__-159"><a href="#FGAdaHAT.__init__-159"><span class="linenos">159</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT.__init__-160"><a href="#FGAdaHAT.__init__-160"><span class="linenos">160</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The mode of aggregation from neuron-wise to weight-wise importance. &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT.__init__-161"><a href="#FGAdaHAT.__init__-161"><span class="linenos">161</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">filter_by_cumulative_mask</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">filter_by_cumulative_mask</span>
</span><span id="FGAdaHAT.__init__-162"><a href="#FGAdaHAT.__init__-162"><span class="linenos">162</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The flag to filter importance by the cumulative mask when calculating the adjustment rate.&quot;&quot;&quot;</span>
</span><span id="FGAdaHAT.__init__-163"><a href="#FGAdaHAT.__init__-163"><span class="linenos">163</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">filter_unmasked_importance</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">filter_unmasked_importance</span>
</span><span id="FGAdaHAT.__init__-164"><a href="#FGAdaHAT.__init__-164"><span class="linenos">164</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The flag to filter unmasked importance values (set them to 0) at the end of task training.&quot;&quot;&quot;</span>
</span><span id="FGAdaHAT.__init__-165"><a href="#FGAdaHAT.__init__-165"><span class="linenos">165</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">step_multiply_training_mask</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">step_multiply_training_mask</span>
</span><span id="FGAdaHAT.__init__-166"><a href="#FGAdaHAT.__init__-166"><span class="linenos">166</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The flag to multiply the training mask to the importance at each training step.&quot;&quot;&quot;</span>
</span><span id="FGAdaHAT.__init__-167"><a href="#FGAdaHAT.__init__-167"><span class="linenos">167</span></a>
</span><span id="FGAdaHAT.__init__-168"><a href="#FGAdaHAT.__init__-168"><span class="linenos">168</span></a>        <span class="c1"># importance summing strategy</span>
</span><span id="FGAdaHAT.__init__-169"><a href="#FGAdaHAT.__init__-169"><span class="linenos">169</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">importance_summing_strategy</span>
</span><span id="FGAdaHAT.__init__-170"><a href="#FGAdaHAT.__init__-170"><span class="linenos">170</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The strategy to sum the neuron-wise importance for previous tasks.&quot;&quot;&quot;</span>
</span><span id="FGAdaHAT.__init__-171"><a href="#FGAdaHAT.__init__-171"><span class="linenos">171</span></a>        <span class="k">if</span> <span class="n">importance_summing_strategy_linear_step</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FGAdaHAT.__init__-172"><a href="#FGAdaHAT.__init__-172"><span class="linenos">172</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy_linear_step</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FGAdaHAT.__init__-173"><a href="#FGAdaHAT.__init__-173"><span class="linenos">173</span></a>                <span class="n">importance_summing_strategy_linear_step</span>
</span><span id="FGAdaHAT.__init__-174"><a href="#FGAdaHAT.__init__-174"><span class="linenos">174</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT.__init__-175"><a href="#FGAdaHAT.__init__-175"><span class="linenos">175</span></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The linear step for the importance summing strategy (only when `importance_summing_strategy` is &#39;linear_decrease&#39;).&quot;&quot;&quot;</span>
</span><span id="FGAdaHAT.__init__-176"><a href="#FGAdaHAT.__init__-176"><span class="linenos">176</span></a>        <span class="k">if</span> <span class="n">importance_summing_strategy_exponential_rate</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FGAdaHAT.__init__-177"><a href="#FGAdaHAT.__init__-177"><span class="linenos">177</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy_exponential_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FGAdaHAT.__init__-178"><a href="#FGAdaHAT.__init__-178"><span class="linenos">178</span></a>                <span class="n">importance_summing_strategy_exponential_rate</span>
</span><span id="FGAdaHAT.__init__-179"><a href="#FGAdaHAT.__init__-179"><span class="linenos">179</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT.__init__-180"><a href="#FGAdaHAT.__init__-180"><span class="linenos">180</span></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The exponential rate for the importance summing strategy (only when `importance_summing_strategy` is &#39;exponential_decrease&#39;). &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT.__init__-181"><a href="#FGAdaHAT.__init__-181"><span class="linenos">181</span></a>        <span class="k">if</span> <span class="n">importance_summing_strategy_log_base</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FGAdaHAT.__init__-182"><a href="#FGAdaHAT.__init__-182"><span class="linenos">182</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy_log_base</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FGAdaHAT.__init__-183"><a href="#FGAdaHAT.__init__-183"><span class="linenos">183</span></a>                <span class="n">importance_summing_strategy_log_base</span>
</span><span id="FGAdaHAT.__init__-184"><a href="#FGAdaHAT.__init__-184"><span class="linenos">184</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT.__init__-185"><a href="#FGAdaHAT.__init__-185"><span class="linenos">185</span></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The base for the logarithm in the importance summing strategy (only when `importance_summing_strategy` is &#39;log_decrease&#39;). &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT.__init__-186"><a href="#FGAdaHAT.__init__-186"><span class="linenos">186</span></a>
</span><span id="FGAdaHAT.__init__-187"><a href="#FGAdaHAT.__init__-187"><span class="linenos">187</span></a>        <span class="c1"># base values</span>
</span><span id="FGAdaHAT.__init__-188"><a href="#FGAdaHAT.__init__-188"><span class="linenos">188</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">base_importance</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">base_importance</span>
</span><span id="FGAdaHAT.__init__-189"><a href="#FGAdaHAT.__init__-189"><span class="linenos">189</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The base value added to the importance to avoid zero. &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT.__init__-190"><a href="#FGAdaHAT.__init__-190"><span class="linenos">190</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">base_mask_sparsity_reg</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">base_mask_sparsity_reg</span>
</span><span id="FGAdaHAT.__init__-191"><a href="#FGAdaHAT.__init__-191"><span class="linenos">191</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The base value added to the mask sparsity regularization to avoid zero. &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT.__init__-192"><a href="#FGAdaHAT.__init__-192"><span class="linenos">192</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">base_linear</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">base_linear</span>
</span><span id="FGAdaHAT.__init__-193"><a href="#FGAdaHAT.__init__-193"><span class="linenos">193</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The base value added to the linear layer to avoid zero. &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT.__init__-194"><a href="#FGAdaHAT.__init__-194"><span class="linenos">194</span></a>
</span><span id="FGAdaHAT.__init__-195"><a href="#FGAdaHAT.__init__-195"><span class="linenos">195</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">importances</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="FGAdaHAT.__init__-196"><a href="#FGAdaHAT.__init__-196"><span class="linenos">196</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The min-max scaled ($[0, 1]$) neuron-wise importance of units. It is $I^{\tau}_{l}$ in the paper. Keys are task IDs and values are the corresponding importance tensors. Each importance tensor is a dict where keys are layer names and values are the importance tensor for the layer. The utility tensor is the same size as the feature tensor with size (number of units, ). &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT.__init__-197"><a href="#FGAdaHAT.__init__-197"><span class="linenos">197</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">summative_importance_for_previous_tasks</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="FGAdaHAT.__init__-198"><a href="#FGAdaHAT.__init__-198"><span class="linenos">198</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The summative neuron-wise importance values of units for previous tasks before the current task `self.task_id`. See $I^{&lt;t}_{l}$ in the paper. Keys are layer names and values are the summative importance tensor for the layer. The summative importance tensor has the same size as the feature tensor with size (number of units, ). &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT.__init__-199"><a href="#FGAdaHAT.__init__-199"><span class="linenos">199</span></a>
</span><span id="FGAdaHAT.__init__-200"><a href="#FGAdaHAT.__init__-200"><span class="linenos">200</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">num_steps_t</span><span class="p">:</span> <span class="nb">int</span>
</span><span id="FGAdaHAT.__init__-201"><a href="#FGAdaHAT.__init__-201"><span class="linenos">201</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the number of training steps for the current task `self.task_id`.&quot;&quot;&quot;</span>
</span><span id="FGAdaHAT.__init__-202"><a href="#FGAdaHAT.__init__-202"><span class="linenos">202</span></a>        <span class="c1"># set manual optimization</span>
</span><span id="FGAdaHAT.__init__-203"><a href="#FGAdaHAT.__init__-203"><span class="linenos">203</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">automatic_optimization</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="FGAdaHAT.__init__-204"><a href="#FGAdaHAT.__init__-204"><span class="linenos">204</span></a>
</span><span id="FGAdaHAT.__init__-205"><a href="#FGAdaHAT.__init__-205"><span class="linenos">205</span></a>        <span class="n">FGAdaHAT</span><span class="o">.</span><span class="n">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Initialize the FG-AdaHAT algorithm with the network.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>backbone</strong> (<code>HATMaskBackbone</code>): must be a backbone network with the HAT mask mechanism.</li>
<li><strong>heads</strong> (<code>HeadsTIL</code>): output heads. FG-AdaHAT supports only TIL (Task-Incremental Learning).</li>
<li><strong>adjustment_intensity</strong> (<code><a href="#FGAdaHAT.float">float</a></code>): hyperparameter, controls the overall intensity of gradient adjustment (the $\alpha$ in the paper).</li>
<li><strong>importance_type</strong> (<code>str</code>): the type of neuron-wise importance, must be one of:
<ol>
<li>'input_weight_abs_sum': sum of absolute input weights;</li>
<li>'output_weight_abs_sum': sum of absolute output weights;</li>
<li>'input_weight_gradient_abs_sum': sum of absolute gradients of the input weights (Input Gradients (IG) in the paper);</li>
<li>'output_weight_gradient_abs_sum': sum of absolute gradients of the output weights (Output Gradients (OG) in the paper);</li>
<li>'activation_abs': absolute activation;</li>
<li>'input_weight_abs_sum_x_activation_abs': sum of absolute input weights multiplied by absolute activation (Input Contribution Utility (ICU) in the paper);</li>
<li>'output_weight_abs_sum_x_activation_abs': sum of absolute output weights multiplied by absolute activation (Contribution Utility (CU) in the paper);</li>
<li>'gradient_x_activation_abs': absolute gradient (the saliency) multiplied by activation;</li>
<li>'input_weight_gradient_square_sum': sum of squared gradients of the input weights;</li>
<li>'output_weight_gradient_square_sum': sum of squared gradients of the output weights;</li>
<li>'input_weight_gradient_square_sum_x_activation_abs': sum of squared gradients of the input weights multiplied by absolute activation (Activation Fisher Information (AFI) in the paper);</li>
<li>'output_weight_gradient_square_sum_x_activation_abs': sum of squared gradients of the output weights multiplied by absolute activation;</li>
<li>'conductance_abs': absolute layer conductance;</li>
<li>'internal_influence_abs': absolute internal influence (Internal Influence (II) in the paper);</li>
<li>'gradcam_abs': absolute Grad-CAM;</li>
<li>'deeplift_abs': absolute DeepLIFT (DeepLIFT (DL) in the paper);</li>
<li>'deepliftshap_abs': absolute DeepLIFT-SHAP;</li>
<li>'gradientshap_abs': absolute Gradient-SHAP (Gradient SHAP (GS) in the paper);</li>
<li>'integrated_gradients_abs': absolute Integrated Gradients;</li>
<li>'feature_ablation_abs': absolute Feature Ablation (Feature Ablation (FA) in the paper);</li>
<li>'lrp_abs': absolute Layer-wise Relevance Propagation (LRP);</li>
<li>'cbp_adaptation': the adaptation function in <a href="https://www.nature.com/articles/s41586-024-07711-7">Continual Backpropagation (CBP)</a>;</li>
<li>'cbp_adaptive_contribution': the adaptive contribution function in <a href="https://www.nature.com/articles/s41586-024-07711-7">Continual Backpropagation (CBP)</a>;</li>
</ol></li>
<li><strong>importance_summing_strategy</strong> (<code>str</code>): the strategy to sum neuron-wise importance for previous tasks, must be one of:
<ol>
<li>'add_latest': add the latest neuron-wise importance to the summative importance;</li>
<li>'add_all': add all previous neuron-wise importance (including the latest) to the summative importance;</li>
<li>'add_average': add the average of all previous neuron-wise importance (including the latest) to the summative importance;</li>
<li>'linear_decrease': weigh the previous neuron-wise importance by a linear factor that decreases with the task ID;</li>
<li>'quadratic_decrease': weigh the previous neuron-wise importance that decreases quadratically with the task ID;</li>
<li>'cubic_decrease': weigh the previous neuron-wise importance that decreases cubically with the task ID;</li>
<li>'exponential_decrease': weigh the previous neuron-wise importance by an exponential factor that decreases with the task ID;</li>
<li>'log_decrease': weigh the previous neuron-wise importance by a logarithmic factor that decreases with the task ID;</li>
<li>'factorial_decrease': weigh the previous neuron-wise importance that decreases factorially with the task ID;</li>
</ol></li>
<li><strong>importance_scheduler_type</strong> (<code>str</code>): the scheduler for importance, i.e., the factor $c^t$ multiplied to parameter importance. Must be one of:
<ol>
<li>'linear_sparsity_reg': $c^t = (t+b_L) \cdot [R(M^t, M^{<t}) + b_R]$, where $R(M^t, M^{<t})$ is the mask sparsity regularization betwwen the current task and previous tasks, $b_L$ is the base linear factor (see argument <code><a href="#FGAdaHAT.base_linear">base_linear</a></code>), and $b_R$ is the base mask sparsity regularization factor (see argument <code><a href="#FGAdaHAT.base_mask_sparsity_reg">base_mask_sparsity_reg</a></code>);</li>
<li>'sparsity_reg': $c^t = [R(M^t, M^{<t}) + b_R]$;</li>
<li>'summative_mask_sparsity_reg': $c^t_{l,ij} = \left(\min \left(m^{<t, \text{sum}}_{l,i}, m^{<t, \text{sum}}_{l-1,j}\right)+b_L\right) \cdot [R(M^t, M^{<t}) + b_R]$.</li>
</ol></li>
<li><strong>neuron_to_weight_importance_aggregation_mode</strong> (<code>str</code>): aggregation mode from neuron-wise to weight-wise importance ($\text{Agg}(\cdot)$ in the paper), must be one of:
<ol>
<li>'min': take the minimum of neuron-wise importance for each weight;</li>
<li>'max': take the maximum of neuron-wise importance for each weight;</li>
<li>'mean': take the mean of neuron-wise importance for each weight.</li>
</ol></li>
<li><strong>s_max</strong> (<code><a href="#FGAdaHAT.float">float</a></code>): hyperparameter, the maximum scaling factor in the gate function. See Sec. 2.4 "Hard Attention Training" in the <a href="http://proceedings.mlr.press/v80/serra18a">HAT paper</a>.</li>
<li><strong>clamp_threshold</strong> (<code><a href="#FGAdaHAT.float">float</a></code>): the threshold for task embedding gradient compensation. See Sec. 2.5 "Embedding Gradient Compensation" in the <a href="http://proceedings.mlr.press/v80/serra18a">HAT paper</a>.</li>
<li><strong>mask_sparsity_reg_factor</strong> (<code><a href="#FGAdaHAT.float">float</a></code>): hyperparameter, the regularization factor for mask sparsity.</li>
<li><strong>mask_sparsity_reg_mode</strong> (<code>str</code>): the mode of mask sparsity regularization, must be one of:
<ol>
<li>'original' (default): the original mask sparsity regularization in the <a href="http://proceedings.mlr.press/v80/serra18a">HAT paper</a>.</li>
<li>'cross': the cross version of mask sparsity regularization.</li>
</ol></li>
<li><strong>base_importance</strong> (<code><a href="#FGAdaHAT.float">float</a></code>): base value added to importance ($b_I$ in the paper). Default: 0.01.</li>
<li><strong>base_mask_sparsity_reg</strong> (<code><a href="#FGAdaHAT.float">float</a></code>): base value added to mask sparsity regularization factor in the importance scheduler ($b_R$ in the paper). Default: 0.1.</li>
<li><strong>base_linear</strong> (<code><a href="#FGAdaHAT.float">float</a></code>): base value added to the linear factor in the importance scheduler ($b_L$ in the paper). Default: 10.</li>
<li><strong>filter_by_cumulative_mask</strong> (<code>bool</code>): whether to multiply the cumulative mask to the importance when calculating adjustment rate. Default: False.</li>
<li><strong>filter_unmasked_importance</strong> (<code>bool</code>): whether to filter unmasked importance values (set to 0) at the end of task training. Default: False.</li>
<li><strong>step_multiply_training_mask</strong> (<code>bool</code>): whether to multiply the training mask to the importance at each training step. Default: True.</li>
<li><strong>task_embedding_init_mode</strong> (<code>str</code>): the initialization mode for task embeddings, must be one of:
<ol>
<li>'N01' (default): standard normal distribution $N(0, 1)$.</li>
<li>'U-11': uniform distribution $U(-1, 1)$.</li>
<li>'U01': uniform distribution $U(0, 1)$.</li>
<li>'U-10': uniform distribution $U(-1, 0)$.</li>
<li>'last': inherit the task embedding from the last task.</li>
</ol></li>
<li><strong>importance_summing_strategy_linear_step</strong> (<code><a href="#FGAdaHAT.float">float</a></code> | <code>None</code>): linear step for the importance summing strategy (used when <code><a href="#FGAdaHAT.importance_summing_strategy">importance_summing_strategy</a></code> is 'linear_decrease'). Must be &gt; 0.</li>
<li><strong>importance_summing_strategy_exponential_rate</strong> (<code><a href="#FGAdaHAT.float">float</a></code> | <code>None</code>): exponential rate for the importance summing strategy (used when <code><a href="#FGAdaHAT.importance_summing_strategy">importance_summing_strategy</a></code> is 'exponential_decrease'). Must be &gt; 1.</li>
<li><strong>importance_summing_strategy_log_base</strong> (<code><a href="#FGAdaHAT.float">float</a></code> | <code>None</code>): base for the logarithm in the importance summing strategy (used when <code><a href="#FGAdaHAT.importance_summing_strategy">importance_summing_strategy</a></code> is 'log_decrease'). Must be &gt; 1.</li>
</ul>
</div>


                            </div>
                            <div id="FGAdaHAT.importance_type" class="classattr">
                                <div class="attr variable">
            <span class="name">importance_type</span><span class="annotation">: str | None</span>

        
    </div>
    <a class="headerlink" href="#FGAdaHAT.importance_type"></a>
    
            <div class="docstring"><p>The type of the neuron-wise importance added to AdaHAT importance.</p>
</div>


                            </div>
                            <div id="FGAdaHAT.importance_scheduler_type" class="classattr">
                                <div class="attr variable">
            <span class="name">importance_scheduler_type</span><span class="annotation">: str</span>

        
    </div>
    <a class="headerlink" href="#FGAdaHAT.importance_scheduler_type"></a>
    
            <div class="docstring"><p>Store the type of the importance scheduler.</p>
</div>


                            </div>
                            <div id="FGAdaHAT.neuron_to_weight_importance_aggregation_mode" class="classattr">
                                <div class="attr variable">
            <span class="name">neuron_to_weight_importance_aggregation_mode</span><span class="annotation">: str</span>

        
    </div>
    <a class="headerlink" href="#FGAdaHAT.neuron_to_weight_importance_aggregation_mode"></a>
    
            <div class="docstring"><p>The mode of aggregation from neuron-wise to weight-wise importance.</p>
</div>


                            </div>
                            <div id="FGAdaHAT.filter_by_cumulative_mask" class="classattr">
                                <div class="attr variable">
            <span class="name">filter_by_cumulative_mask</span><span class="annotation">: bool</span>

        
    </div>
    <a class="headerlink" href="#FGAdaHAT.filter_by_cumulative_mask"></a>
    
            <div class="docstring"><p>The flag to filter importance by the cumulative mask when calculating the adjustment rate.</p>
</div>


                            </div>
                            <div id="FGAdaHAT.filter_unmasked_importance" class="classattr">
                                <div class="attr variable">
            <span class="name">filter_unmasked_importance</span><span class="annotation">: bool</span>

        
    </div>
    <a class="headerlink" href="#FGAdaHAT.filter_unmasked_importance"></a>
    
            <div class="docstring"><p>The flag to filter unmasked importance values (set them to 0) at the end of task training.</p>
</div>


                            </div>
                            <div id="FGAdaHAT.step_multiply_training_mask" class="classattr">
                                <div class="attr variable">
            <span class="name">step_multiply_training_mask</span><span class="annotation">: bool</span>

        
    </div>
    <a class="headerlink" href="#FGAdaHAT.step_multiply_training_mask"></a>
    
            <div class="docstring"><p>The flag to multiply the training mask to the importance at each training step.</p>
</div>


                            </div>
                            <div id="FGAdaHAT.importance_summing_strategy" class="classattr">
                                <div class="attr variable">
            <span class="name">importance_summing_strategy</span><span class="annotation">: str</span>

        
    </div>
    <a class="headerlink" href="#FGAdaHAT.importance_summing_strategy"></a>
    
            <div class="docstring"><p>The strategy to sum the neuron-wise importance for previous tasks.</p>
</div>


                            </div>
                            <div id="FGAdaHAT.base_importance" class="classattr">
                                <div class="attr variable">
            <span class="name">base_importance</span><span class="annotation">: float</span>

        
    </div>
    <a class="headerlink" href="#FGAdaHAT.base_importance"></a>
    
            <div class="docstring"><p>The base value added to the importance to avoid zero.</p>
</div>


                            </div>
                            <div id="FGAdaHAT.base_mask_sparsity_reg" class="classattr">
                                <div class="attr variable">
            <span class="name">base_mask_sparsity_reg</span><span class="annotation">: float</span>

        
    </div>
    <a class="headerlink" href="#FGAdaHAT.base_mask_sparsity_reg"></a>
    
            <div class="docstring"><p>The base value added to the mask sparsity regularization to avoid zero.</p>
</div>


                            </div>
                            <div id="FGAdaHAT.base_linear" class="classattr">
                                <div class="attr variable">
            <span class="name">base_linear</span><span class="annotation">: float</span>

        
    </div>
    <a class="headerlink" href="#FGAdaHAT.base_linear"></a>
    
            <div class="docstring"><p>The base value added to the linear layer to avoid zero.</p>
</div>


                            </div>
                            <div id="FGAdaHAT.importances" class="classattr">
                                <div class="attr variable">
            <span class="name">importances</span><span class="annotation">: dict[int, dict[str, torch.Tensor]]</span>

        
    </div>
    <a class="headerlink" href="#FGAdaHAT.importances"></a>
    
            <div class="docstring"><p>The min-max scaled ($[0, 1]$) neuron-wise importance of units. It is $I^{\tau}_{l}$ in the paper. Keys are task IDs and values are the corresponding importance tensors. Each importance tensor is a dict where keys are layer names and values are the importance tensor for the layer. The utility tensor is the same size as the feature tensor with size (number of units, ).</p>
</div>


                            </div>
                            <div id="FGAdaHAT.summative_importance_for_previous_tasks" class="classattr">
                                <div class="attr variable">
            <span class="name">summative_importance_for_previous_tasks</span><span class="annotation">: dict[str, torch.Tensor]</span>

        
    </div>
    <a class="headerlink" href="#FGAdaHAT.summative_importance_for_previous_tasks"></a>
    
            <div class="docstring"><p>The summative neuron-wise importance values of units for previous tasks before the current task <code>self.task_id</code>. See $I^{<t}_{l}$ in the paper. Keys are layer names and values are the summative importance tensor for the layer. The summative importance tensor has the same size as the feature tensor with size (number of units, ).</p>
</div>


                            </div>
                            <div id="FGAdaHAT.num_steps_t" class="classattr">
                                <div class="attr variable">
            <span class="name">num_steps_t</span><span class="annotation">: int</span>

        
    </div>
    <a class="headerlink" href="#FGAdaHAT.num_steps_t"></a>
    
            <div class="docstring"><p>Store the number of training steps for the current task <code>self.task_id</code>.</p>
</div>


                            </div>
                            <div id="FGAdaHAT.automatic_optimization" class="classattr">
                                        <input id="FGAdaHAT.automatic_optimization-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr variable">
            <span class="name">automatic_optimization</span><span class="annotation">: bool</span>

                <label class="view-source-button" for="FGAdaHAT.automatic_optimization-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FGAdaHAT.automatic_optimization"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FGAdaHAT.automatic_optimization-290"><a href="#FGAdaHAT.automatic_optimization-290"><span class="linenos">290</span></a>    <span class="nd">@property</span>
</span><span id="FGAdaHAT.automatic_optimization-291"><a href="#FGAdaHAT.automatic_optimization-291"><span class="linenos">291</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">automatic_optimization</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
</span><span id="FGAdaHAT.automatic_optimization-292"><a href="#FGAdaHAT.automatic_optimization-292"><span class="linenos">292</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``.&quot;&quot;&quot;</span>
</span><span id="FGAdaHAT.automatic_optimization-293"><a href="#FGAdaHAT.automatic_optimization-293"><span class="linenos">293</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_automatic_optimization</span>
</span></pre></div>


            <div class="docstring"><p>If set to <code>False</code> you are responsible for calling <code>.backward()</code>, <code>.step()</code>, <code>.zero_grad()</code>.</p>
</div>


                            </div>
                            <div id="FGAdaHAT.sanity_check" class="classattr">
                                        <input id="FGAdaHAT.sanity_check-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">sanity_check</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span></span><span class="return-annotation">) -> <span class="kc">None</span>:</span></span>

                <label class="view-source-button" for="FGAdaHAT.sanity_check-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FGAdaHAT.sanity_check"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FGAdaHAT.sanity_check-207"><a href="#FGAdaHAT.sanity_check-207"><span class="linenos">207</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FGAdaHAT.sanity_check-208"><a href="#FGAdaHAT.sanity_check-208"><span class="linenos">208</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Check the sanity of the arguments.&quot;&quot;&quot;</span>
</span><span id="FGAdaHAT.sanity_check-209"><a href="#FGAdaHAT.sanity_check-209"><span class="linenos">209</span></a>
</span><span id="FGAdaHAT.sanity_check-210"><a href="#FGAdaHAT.sanity_check-210"><span class="linenos">210</span></a>        <span class="c1"># check importance type</span>
</span><span id="FGAdaHAT.sanity_check-211"><a href="#FGAdaHAT.sanity_check-211"><span class="linenos">211</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span>
</span><span id="FGAdaHAT.sanity_check-212"><a href="#FGAdaHAT.sanity_check-212"><span class="linenos">212</span></a>            <span class="s2">&quot;input_weight_abs_sum&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT.sanity_check-213"><a href="#FGAdaHAT.sanity_check-213"><span class="linenos">213</span></a>            <span class="s2">&quot;output_weight_abs_sum&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT.sanity_check-214"><a href="#FGAdaHAT.sanity_check-214"><span class="linenos">214</span></a>            <span class="s2">&quot;input_weight_gradient_abs_sum&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT.sanity_check-215"><a href="#FGAdaHAT.sanity_check-215"><span class="linenos">215</span></a>            <span class="s2">&quot;output_weight_gradient_abs_sum&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT.sanity_check-216"><a href="#FGAdaHAT.sanity_check-216"><span class="linenos">216</span></a>            <span class="s2">&quot;activation_abs&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT.sanity_check-217"><a href="#FGAdaHAT.sanity_check-217"><span class="linenos">217</span></a>            <span class="s2">&quot;input_weight_abs_sum_x_activation_abs&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT.sanity_check-218"><a href="#FGAdaHAT.sanity_check-218"><span class="linenos">218</span></a>            <span class="s2">&quot;output_weight_abs_sum_x_activation_abs&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT.sanity_check-219"><a href="#FGAdaHAT.sanity_check-219"><span class="linenos">219</span></a>            <span class="s2">&quot;gradient_x_activation_abs&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT.sanity_check-220"><a href="#FGAdaHAT.sanity_check-220"><span class="linenos">220</span></a>            <span class="s2">&quot;input_weight_gradient_square_sum&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT.sanity_check-221"><a href="#FGAdaHAT.sanity_check-221"><span class="linenos">221</span></a>            <span class="s2">&quot;output_weight_gradient_square_sum&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT.sanity_check-222"><a href="#FGAdaHAT.sanity_check-222"><span class="linenos">222</span></a>            <span class="s2">&quot;input_weight_gradient_square_sum_x_activation_abs&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT.sanity_check-223"><a href="#FGAdaHAT.sanity_check-223"><span class="linenos">223</span></a>            <span class="s2">&quot;output_weight_gradient_square_sum_x_activation_abs&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT.sanity_check-224"><a href="#FGAdaHAT.sanity_check-224"><span class="linenos">224</span></a>            <span class="s2">&quot;conductance_abs&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT.sanity_check-225"><a href="#FGAdaHAT.sanity_check-225"><span class="linenos">225</span></a>            <span class="s2">&quot;internal_influence_abs&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT.sanity_check-226"><a href="#FGAdaHAT.sanity_check-226"><span class="linenos">226</span></a>            <span class="s2">&quot;gradcam_abs&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT.sanity_check-227"><a href="#FGAdaHAT.sanity_check-227"><span class="linenos">227</span></a>            <span class="s2">&quot;deeplift_abs&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT.sanity_check-228"><a href="#FGAdaHAT.sanity_check-228"><span class="linenos">228</span></a>            <span class="s2">&quot;deepliftshap_abs&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT.sanity_check-229"><a href="#FGAdaHAT.sanity_check-229"><span class="linenos">229</span></a>            <span class="s2">&quot;gradientshap_abs&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT.sanity_check-230"><a href="#FGAdaHAT.sanity_check-230"><span class="linenos">230</span></a>            <span class="s2">&quot;integrated_gradients_abs&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT.sanity_check-231"><a href="#FGAdaHAT.sanity_check-231"><span class="linenos">231</span></a>            <span class="s2">&quot;feature_ablation_abs&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT.sanity_check-232"><a href="#FGAdaHAT.sanity_check-232"><span class="linenos">232</span></a>            <span class="s2">&quot;lrp_abs&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT.sanity_check-233"><a href="#FGAdaHAT.sanity_check-233"><span class="linenos">233</span></a>            <span class="s2">&quot;cbp_adaptation&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT.sanity_check-234"><a href="#FGAdaHAT.sanity_check-234"><span class="linenos">234</span></a>            <span class="s2">&quot;cbp_adaptive_contribution&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT.sanity_check-235"><a href="#FGAdaHAT.sanity_check-235"><span class="linenos">235</span></a>        <span class="p">]:</span>
</span><span id="FGAdaHAT.sanity_check-236"><a href="#FGAdaHAT.sanity_check-236"><span class="linenos">236</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="FGAdaHAT.sanity_check-237"><a href="#FGAdaHAT.sanity_check-237"><span class="linenos">237</span></a>                <span class="sa">f</span><span class="s2">&quot;importance_type must be one of the predefined types, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="FGAdaHAT.sanity_check-238"><a href="#FGAdaHAT.sanity_check-238"><span class="linenos">238</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT.sanity_check-239"><a href="#FGAdaHAT.sanity_check-239"><span class="linenos">239</span></a>
</span><span id="FGAdaHAT.sanity_check-240"><a href="#FGAdaHAT.sanity_check-240"><span class="linenos">240</span></a>        <span class="c1"># check importance summing strategy</span>
</span><span id="FGAdaHAT.sanity_check-241"><a href="#FGAdaHAT.sanity_check-241"><span class="linenos">241</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span>
</span><span id="FGAdaHAT.sanity_check-242"><a href="#FGAdaHAT.sanity_check-242"><span class="linenos">242</span></a>            <span class="s2">&quot;add_latest&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT.sanity_check-243"><a href="#FGAdaHAT.sanity_check-243"><span class="linenos">243</span></a>            <span class="s2">&quot;add_all&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT.sanity_check-244"><a href="#FGAdaHAT.sanity_check-244"><span class="linenos">244</span></a>            <span class="s2">&quot;add_average&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT.sanity_check-245"><a href="#FGAdaHAT.sanity_check-245"><span class="linenos">245</span></a>            <span class="s2">&quot;linear_decrease&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT.sanity_check-246"><a href="#FGAdaHAT.sanity_check-246"><span class="linenos">246</span></a>            <span class="s2">&quot;quadratic_decrease&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT.sanity_check-247"><a href="#FGAdaHAT.sanity_check-247"><span class="linenos">247</span></a>            <span class="s2">&quot;cubic_decrease&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT.sanity_check-248"><a href="#FGAdaHAT.sanity_check-248"><span class="linenos">248</span></a>            <span class="s2">&quot;exponential_decrease&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT.sanity_check-249"><a href="#FGAdaHAT.sanity_check-249"><span class="linenos">249</span></a>            <span class="s2">&quot;log_decrease&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT.sanity_check-250"><a href="#FGAdaHAT.sanity_check-250"><span class="linenos">250</span></a>            <span class="s2">&quot;factorial_decrease&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT.sanity_check-251"><a href="#FGAdaHAT.sanity_check-251"><span class="linenos">251</span></a>        <span class="p">]:</span>
</span><span id="FGAdaHAT.sanity_check-252"><a href="#FGAdaHAT.sanity_check-252"><span class="linenos">252</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="FGAdaHAT.sanity_check-253"><a href="#FGAdaHAT.sanity_check-253"><span class="linenos">253</span></a>                <span class="sa">f</span><span class="s2">&quot;importance_summing_strategy must be one of the predefined strategies, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="FGAdaHAT.sanity_check-254"><a href="#FGAdaHAT.sanity_check-254"><span class="linenos">254</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT.sanity_check-255"><a href="#FGAdaHAT.sanity_check-255"><span class="linenos">255</span></a>
</span><span id="FGAdaHAT.sanity_check-256"><a href="#FGAdaHAT.sanity_check-256"><span class="linenos">256</span></a>        <span class="c1"># check importance scheduler type</span>
</span><span id="FGAdaHAT.sanity_check-257"><a href="#FGAdaHAT.sanity_check-257"><span class="linenos">257</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_scheduler_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span>
</span><span id="FGAdaHAT.sanity_check-258"><a href="#FGAdaHAT.sanity_check-258"><span class="linenos">258</span></a>            <span class="s2">&quot;linear_sparsity_reg&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT.sanity_check-259"><a href="#FGAdaHAT.sanity_check-259"><span class="linenos">259</span></a>            <span class="s2">&quot;sparsity_reg&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT.sanity_check-260"><a href="#FGAdaHAT.sanity_check-260"><span class="linenos">260</span></a>            <span class="s2">&quot;summative_mask_sparsity_reg&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT.sanity_check-261"><a href="#FGAdaHAT.sanity_check-261"><span class="linenos">261</span></a>        <span class="p">]:</span>
</span><span id="FGAdaHAT.sanity_check-262"><a href="#FGAdaHAT.sanity_check-262"><span class="linenos">262</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="FGAdaHAT.sanity_check-263"><a href="#FGAdaHAT.sanity_check-263"><span class="linenos">263</span></a>                <span class="sa">f</span><span class="s2">&quot;importance_scheduler_type must be one of the predefined types, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">importance_scheduler_type</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="FGAdaHAT.sanity_check-264"><a href="#FGAdaHAT.sanity_check-264"><span class="linenos">264</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT.sanity_check-265"><a href="#FGAdaHAT.sanity_check-265"><span class="linenos">265</span></a>
</span><span id="FGAdaHAT.sanity_check-266"><a href="#FGAdaHAT.sanity_check-266"><span class="linenos">266</span></a>        <span class="c1"># check neuron to weight importance aggregation mode</span>
</span><span id="FGAdaHAT.sanity_check-267"><a href="#FGAdaHAT.sanity_check-267"><span class="linenos">267</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">neuron_to_weight_importance_aggregation_mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span>
</span><span id="FGAdaHAT.sanity_check-268"><a href="#FGAdaHAT.sanity_check-268"><span class="linenos">268</span></a>            <span class="s2">&quot;min&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT.sanity_check-269"><a href="#FGAdaHAT.sanity_check-269"><span class="linenos">269</span></a>            <span class="s2">&quot;max&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT.sanity_check-270"><a href="#FGAdaHAT.sanity_check-270"><span class="linenos">270</span></a>            <span class="s2">&quot;mean&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT.sanity_check-271"><a href="#FGAdaHAT.sanity_check-271"><span class="linenos">271</span></a>        <span class="p">]:</span>
</span><span id="FGAdaHAT.sanity_check-272"><a href="#FGAdaHAT.sanity_check-272"><span class="linenos">272</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="FGAdaHAT.sanity_check-273"><a href="#FGAdaHAT.sanity_check-273"><span class="linenos">273</span></a>                <span class="sa">f</span><span class="s2">&quot;neuron_to_weight_importance_aggregation_mode must be one of the predefined modes, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">neuron_to_weight_importance_aggregation_mode</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="FGAdaHAT.sanity_check-274"><a href="#FGAdaHAT.sanity_check-274"><span class="linenos">274</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT.sanity_check-275"><a href="#FGAdaHAT.sanity_check-275"><span class="linenos">275</span></a>
</span><span id="FGAdaHAT.sanity_check-276"><a href="#FGAdaHAT.sanity_check-276"><span class="linenos">276</span></a>        <span class="c1"># check base values</span>
</span><span id="FGAdaHAT.sanity_check-277"><a href="#FGAdaHAT.sanity_check-277"><span class="linenos">277</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_importance</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="FGAdaHAT.sanity_check-278"><a href="#FGAdaHAT.sanity_check-278"><span class="linenos">278</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="FGAdaHAT.sanity_check-279"><a href="#FGAdaHAT.sanity_check-279"><span class="linenos">279</span></a>                <span class="sa">f</span><span class="s2">&quot;base_importance must be &gt;= 0, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">base_importance</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="FGAdaHAT.sanity_check-280"><a href="#FGAdaHAT.sanity_check-280"><span class="linenos">280</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT.sanity_check-281"><a href="#FGAdaHAT.sanity_check-281"><span class="linenos">281</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_mask_sparsity_reg</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="FGAdaHAT.sanity_check-282"><a href="#FGAdaHAT.sanity_check-282"><span class="linenos">282</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="FGAdaHAT.sanity_check-283"><a href="#FGAdaHAT.sanity_check-283"><span class="linenos">283</span></a>                <span class="sa">f</span><span class="s2">&quot;base_mask_sparsity_reg must be &gt; 0, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">base_mask_sparsity_reg</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="FGAdaHAT.sanity_check-284"><a href="#FGAdaHAT.sanity_check-284"><span class="linenos">284</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT.sanity_check-285"><a href="#FGAdaHAT.sanity_check-285"><span class="linenos">285</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_linear</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="FGAdaHAT.sanity_check-286"><a href="#FGAdaHAT.sanity_check-286"><span class="linenos">286</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;base_linear must be &gt; 0, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">base_linear</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Check the sanity of the arguments.</p>
</div>


                            </div>
                            <div id="FGAdaHAT.on_train_start" class="classattr">
                                        <input id="FGAdaHAT.on_train_start-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">on_train_start</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span></span><span class="return-annotation">) -> <span class="kc">None</span>:</span></span>

                <label class="view-source-button" for="FGAdaHAT.on_train_start-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FGAdaHAT.on_train_start"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FGAdaHAT.on_train_start-288"><a href="#FGAdaHAT.on_train_start-288"><span class="linenos">288</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_start</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FGAdaHAT.on_train_start-289"><a href="#FGAdaHAT.on_train_start-289"><span class="linenos">289</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialize neuron importance accumulation variable for each layer as zeros, in addition to AdaHAT&#39;s summative mask initialization.&quot;&quot;&quot;</span>
</span><span id="FGAdaHAT.on_train_start-290"><a href="#FGAdaHAT.on_train_start-290"><span class="linenos">290</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">on_train_start</span><span class="p">()</span>
</span><span id="FGAdaHAT.on_train_start-291"><a href="#FGAdaHAT.on_train_start-291"><span class="linenos">291</span></a>
</span><span id="FGAdaHAT.on_train_start-292"><a href="#FGAdaHAT.on_train_start-292"><span class="linenos">292</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">importances</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FGAdaHAT.on_train_start-293"><a href="#FGAdaHAT.on_train_start-293"><span class="linenos">293</span></a>            <span class="p">{}</span>
</span><span id="FGAdaHAT.on_train_start-294"><a href="#FGAdaHAT.on_train_start-294"><span class="linenos">294</span></a>        <span class="p">)</span>  <span class="c1"># initialize the importance for the current task</span>
</span><span id="FGAdaHAT.on_train_start-295"><a href="#FGAdaHAT.on_train_start-295"><span class="linenos">295</span></a>
</span><span id="FGAdaHAT.on_train_start-296"><a href="#FGAdaHAT.on_train_start-296"><span class="linenos">296</span></a>        <span class="c1"># initialize the neuron importance at the beginning of each task. This should not be called in `__init__()` method because `self.device` is not available at that time.</span>
</span><span id="FGAdaHAT.on_train_start-297"><a href="#FGAdaHAT.on_train_start-297"><span class="linenos">297</span></a>        <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="FGAdaHAT.on_train_start-298"><a href="#FGAdaHAT.on_train_start-298"><span class="linenos">298</span></a>            <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span>
</span><span id="FGAdaHAT.on_train_start-299"><a href="#FGAdaHAT.on_train_start-299"><span class="linenos">299</span></a>                <span class="n">layer_name</span>
</span><span id="FGAdaHAT.on_train_start-300"><a href="#FGAdaHAT.on_train_start-300"><span class="linenos">300</span></a>            <span class="p">)</span>  <span class="c1"># get the layer by its name</span>
</span><span id="FGAdaHAT.on_train_start-301"><a href="#FGAdaHAT.on_train_start-301"><span class="linenos">301</span></a>            <span class="n">num_units</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="FGAdaHAT.on_train_start-302"><a href="#FGAdaHAT.on_train_start-302"><span class="linenos">302</span></a>
</span><span id="FGAdaHAT.on_train_start-303"><a href="#FGAdaHAT.on_train_start-303"><span class="linenos">303</span></a>            <span class="c1"># initialize the accumulated importance at the beginning of each task</span>
</span><span id="FGAdaHAT.on_train_start-304"><a href="#FGAdaHAT.on_train_start-304"><span class="linenos">304</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">importances</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">][</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_units</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
</span><span id="FGAdaHAT.on_train_start-305"><a href="#FGAdaHAT.on_train_start-305"><span class="linenos">305</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
</span><span id="FGAdaHAT.on_train_start-306"><a href="#FGAdaHAT.on_train_start-306"><span class="linenos">306</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT.on_train_start-307"><a href="#FGAdaHAT.on_train_start-307"><span class="linenos">307</span></a>
</span><span id="FGAdaHAT.on_train_start-308"><a href="#FGAdaHAT.on_train_start-308"><span class="linenos">308</span></a>            <span class="c1"># reset the number of steps counter for the current task</span>
</span><span id="FGAdaHAT.on_train_start-309"><a href="#FGAdaHAT.on_train_start-309"><span class="linenos">309</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">num_steps_t</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="FGAdaHAT.on_train_start-310"><a href="#FGAdaHAT.on_train_start-310"><span class="linenos">310</span></a>
</span><span id="FGAdaHAT.on_train_start-311"><a href="#FGAdaHAT.on_train_start-311"><span class="linenos">311</span></a>            <span class="c1"># initialize the summative neuron-wise importance at the beginning of the first task</span>
</span><span id="FGAdaHAT.on_train_start-312"><a href="#FGAdaHAT.on_train_start-312"><span class="linenos">312</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="FGAdaHAT.on_train_start-313"><a href="#FGAdaHAT.on_train_start-313"><span class="linenos">313</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">summative_importance_for_previous_tasks</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
</span><span id="FGAdaHAT.on_train_start-314"><a href="#FGAdaHAT.on_train_start-314"><span class="linenos">314</span></a>                    <span class="n">num_units</span>
</span><span id="FGAdaHAT.on_train_start-315"><a href="#FGAdaHAT.on_train_start-315"><span class="linenos">315</span></a>                <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
</span><span id="FGAdaHAT.on_train_start-316"><a href="#FGAdaHAT.on_train_start-316"><span class="linenos">316</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
</span><span id="FGAdaHAT.on_train_start-317"><a href="#FGAdaHAT.on_train_start-317"><span class="linenos">317</span></a>                <span class="p">)</span>  <span class="c1"># the summative neuron-wise importance for previous tasks $I^{&lt;t}_{l}$ is initialized as zeros mask when $t=1$</span>
</span></pre></div>


            <div class="docstring"><p>Initialize neuron importance accumulation variable for each layer as zeros, in addition to AdaHAT's summative mask initialization.</p>
</div>


                            </div>
                            <div id="FGAdaHAT.clip_grad_by_adjustment" class="classattr">
                                        <input id="FGAdaHAT.clip_grad_by_adjustment-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">clip_grad_by_adjustment</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">network_sparsity</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span></span><span class="return-annotation">) -> <span class="nb">tuple</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>:</span></span>

                <label class="view-source-button" for="FGAdaHAT.clip_grad_by_adjustment-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FGAdaHAT.clip_grad_by_adjustment"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FGAdaHAT.clip_grad_by_adjustment-319"><a href="#FGAdaHAT.clip_grad_by_adjustment-319"><span class="linenos">319</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">clip_grad_by_adjustment</span><span class="p">(</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-320"><a href="#FGAdaHAT.clip_grad_by_adjustment-320"><span class="linenos">320</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-321"><a href="#FGAdaHAT.clip_grad_by_adjustment-321"><span class="linenos">321</span></a>        <span class="n">network_sparsity</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-322"><a href="#FGAdaHAT.clip_grad_by_adjustment-322"><span class="linenos">322</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="n">Tensor</span><span class="p">]:</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-323"><a href="#FGAdaHAT.clip_grad_by_adjustment-323"><span class="linenos">323</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Clip the gradients by the adjustment rate. See Eq. (1) in the paper.</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-324"><a href="#FGAdaHAT.clip_grad_by_adjustment-324"><span class="linenos">324</span></a>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-325"><a href="#FGAdaHAT.clip_grad_by_adjustment-325"><span class="linenos">325</span></a><span class="sd">        Note that because the task embedding fully covers every layer in the backbone network, no parameters are left out of this system. This applies not only to parameters between layers with task embeddings, but also to those before the first layer. We design it separately in the code.</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-326"><a href="#FGAdaHAT.clip_grad_by_adjustment-326"><span class="linenos">326</span></a>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-327"><a href="#FGAdaHAT.clip_grad_by_adjustment-327"><span class="linenos">327</span></a><span class="sd">        Network capacity is measured alongside this method. Network capacity is defined as the average adjustment rate over all parameters. See Sec. 4.1 in the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-328"><a href="#FGAdaHAT.clip_grad_by_adjustment-328"><span class="linenos">328</span></a>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-329"><a href="#FGAdaHAT.clip_grad_by_adjustment-329"><span class="linenos">329</span></a><span class="sd">        **Args:**</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-330"><a href="#FGAdaHAT.clip_grad_by_adjustment-330"><span class="linenos">330</span></a><span class="sd">        - **network_sparsity** (`dict[str, Tensor]`): the network sparsity (i.e., mask sparsity loss of each layer) for the current task. Keys are layer names and values are the network sparsity values. It is used to calculate the adjustment rate for gradients. In FG-AdaHAT, it is used to construct the importance scheduler.</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-331"><a href="#FGAdaHAT.clip_grad_by_adjustment-331"><span class="linenos">331</span></a>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-332"><a href="#FGAdaHAT.clip_grad_by_adjustment-332"><span class="linenos">332</span></a><span class="sd">        **Returns:**</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-333"><a href="#FGAdaHAT.clip_grad_by_adjustment-333"><span class="linenos">333</span></a><span class="sd">        - **adjustment_rate_weight** (`dict[str, Tensor]`): the adjustment rate for weights. Keys (`str`) are layer names and values (`Tensor`) are the adjustment rate tensors.</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-334"><a href="#FGAdaHAT.clip_grad_by_adjustment-334"><span class="linenos">334</span></a><span class="sd">        - **adjustment_rate_bias** (`dict[str, Tensor]`): the adjustment rate for biases. Keys (`str`) are layer names and values (`Tensor`) are the adjustment rate tensors.</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-335"><a href="#FGAdaHAT.clip_grad_by_adjustment-335"><span class="linenos">335</span></a><span class="sd">        - **capacity** (`Tensor`): the calculated network capacity.</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-336"><a href="#FGAdaHAT.clip_grad_by_adjustment-336"><span class="linenos">336</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-337"><a href="#FGAdaHAT.clip_grad_by_adjustment-337"><span class="linenos">337</span></a>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-338"><a href="#FGAdaHAT.clip_grad_by_adjustment-338"><span class="linenos">338</span></a>        <span class="c1"># initialize network capacity metric</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-339"><a href="#FGAdaHAT.clip_grad_by_adjustment-339"><span class="linenos">339</span></a>        <span class="n">capacity</span> <span class="o">=</span> <span class="n">HATNetworkCapacityMetric</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-340"><a href="#FGAdaHAT.clip_grad_by_adjustment-340"><span class="linenos">340</span></a>        <span class="n">adjustment_rate_weight</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-341"><a href="#FGAdaHAT.clip_grad_by_adjustment-341"><span class="linenos">341</span></a>        <span class="n">adjustment_rate_bias</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-342"><a href="#FGAdaHAT.clip_grad_by_adjustment-342"><span class="linenos">342</span></a>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-343"><a href="#FGAdaHAT.clip_grad_by_adjustment-343"><span class="linenos">343</span></a>        <span class="c1"># calculate the adjustment rate for gradients of the parameters, both weights and biases (if they exist). See Eq. (2) in the paper</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-344"><a href="#FGAdaHAT.clip_grad_by_adjustment-344"><span class="linenos">344</span></a>        <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-345"><a href="#FGAdaHAT.clip_grad_by_adjustment-345"><span class="linenos">345</span></a>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-346"><a href="#FGAdaHAT.clip_grad_by_adjustment-346"><span class="linenos">346</span></a>            <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-347"><a href="#FGAdaHAT.clip_grad_by_adjustment-347"><span class="linenos">347</span></a>                <span class="n">layer_name</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-348"><a href="#FGAdaHAT.clip_grad_by_adjustment-348"><span class="linenos">348</span></a>            <span class="p">)</span>  <span class="c1"># get the layer by its name</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-349"><a href="#FGAdaHAT.clip_grad_by_adjustment-349"><span class="linenos">349</span></a>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-350"><a href="#FGAdaHAT.clip_grad_by_adjustment-350"><span class="linenos">350</span></a>            <span class="c1"># placeholder for the adjustment rate to avoid the error of using it before assignment</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-351"><a href="#FGAdaHAT.clip_grad_by_adjustment-351"><span class="linenos">351</span></a>            <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-352"><a href="#FGAdaHAT.clip_grad_by_adjustment-352"><span class="linenos">352</span></a>            <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-353"><a href="#FGAdaHAT.clip_grad_by_adjustment-353"><span class="linenos">353</span></a>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-354"><a href="#FGAdaHAT.clip_grad_by_adjustment-354"><span class="linenos">354</span></a>            <span class="c1"># aggregate the neuron-wise importance to weight-wise importance. Note that the neuron-wise importance has already been min-max scaled to $[0, 1]$ in the `on_train_batch_end()` method, added the base value, and filtered by the mask</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-355"><a href="#FGAdaHAT.clip_grad_by_adjustment-355"><span class="linenos">355</span></a>            <span class="n">weight_importance</span><span class="p">,</span> <span class="n">bias_importance</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-356"><a href="#FGAdaHAT.clip_grad_by_adjustment-356"><span class="linenos">356</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_measure_parameter_wise</span><span class="p">(</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-357"><a href="#FGAdaHAT.clip_grad_by_adjustment-357"><span class="linenos">357</span></a>                    <span class="n">unit_wise_measure</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">summative_importance_for_previous_tasks</span><span class="p">,</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-358"><a href="#FGAdaHAT.clip_grad_by_adjustment-358"><span class="linenos">358</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-359"><a href="#FGAdaHAT.clip_grad_by_adjustment-359"><span class="linenos">359</span></a>                    <span class="n">aggregation_mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">neuron_to_weight_importance_aggregation_mode</span><span class="p">,</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-360"><a href="#FGAdaHAT.clip_grad_by_adjustment-360"><span class="linenos">360</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-361"><a href="#FGAdaHAT.clip_grad_by_adjustment-361"><span class="linenos">361</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-362"><a href="#FGAdaHAT.clip_grad_by_adjustment-362"><span class="linenos">362</span></a>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-363"><a href="#FGAdaHAT.clip_grad_by_adjustment-363"><span class="linenos">363</span></a>            <span class="n">weight_mask</span><span class="p">,</span> <span class="n">bias_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_measure_parameter_wise</span><span class="p">(</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-364"><a href="#FGAdaHAT.clip_grad_by_adjustment-364"><span class="linenos">364</span></a>                <span class="n">unit_wise_measure</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">cumulative_mask_for_previous_tasks</span><span class="p">,</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-365"><a href="#FGAdaHAT.clip_grad_by_adjustment-365"><span class="linenos">365</span></a>                <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-366"><a href="#FGAdaHAT.clip_grad_by_adjustment-366"><span class="linenos">366</span></a>                <span class="n">aggregation_mode</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-367"><a href="#FGAdaHAT.clip_grad_by_adjustment-367"><span class="linenos">367</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-368"><a href="#FGAdaHAT.clip_grad_by_adjustment-368"><span class="linenos">368</span></a>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-369"><a href="#FGAdaHAT.clip_grad_by_adjustment-369"><span class="linenos">369</span></a>            <span class="c1"># filter the weight importance by the cumulative mask</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-370"><a href="#FGAdaHAT.clip_grad_by_adjustment-370"><span class="linenos">370</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">filter_by_cumulative_mask</span><span class="p">:</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-371"><a href="#FGAdaHAT.clip_grad_by_adjustment-371"><span class="linenos">371</span></a>                <span class="n">weight_importance</span> <span class="o">=</span> <span class="n">weight_importance</span> <span class="o">*</span> <span class="n">weight_mask</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-372"><a href="#FGAdaHAT.clip_grad_by_adjustment-372"><span class="linenos">372</span></a>                <span class="n">bias_importance</span> <span class="o">=</span> <span class="n">bias_importance</span> <span class="o">*</span> <span class="n">bias_mask</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-373"><a href="#FGAdaHAT.clip_grad_by_adjustment-373"><span class="linenos">373</span></a>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-374"><a href="#FGAdaHAT.clip_grad_by_adjustment-374"><span class="linenos">374</span></a>            <span class="n">network_sparsity_layer</span> <span class="o">=</span> <span class="n">network_sparsity</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-375"><a href="#FGAdaHAT.clip_grad_by_adjustment-375"><span class="linenos">375</span></a>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-376"><a href="#FGAdaHAT.clip_grad_by_adjustment-376"><span class="linenos">376</span></a>            <span class="c1"># calculate importance scheduler (the factor of importance). See Eq. (3) in the paper</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-377"><a href="#FGAdaHAT.clip_grad_by_adjustment-377"><span class="linenos">377</span></a>            <span class="n">factor</span> <span class="o">=</span> <span class="n">network_sparsity_layer</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_mask_sparsity_reg</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-378"><a href="#FGAdaHAT.clip_grad_by_adjustment-378"><span class="linenos">378</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_scheduler_type</span> <span class="o">==</span> <span class="s2">&quot;linear_sparsity_reg&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-379"><a href="#FGAdaHAT.clip_grad_by_adjustment-379"><span class="linenos">379</span></a>                <span class="n">factor</span> <span class="o">=</span> <span class="n">factor</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_linear</span><span class="p">)</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-380"><a href="#FGAdaHAT.clip_grad_by_adjustment-380"><span class="linenos">380</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_scheduler_type</span> <span class="o">==</span> <span class="s2">&quot;sparsity_reg&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-381"><a href="#FGAdaHAT.clip_grad_by_adjustment-381"><span class="linenos">381</span></a>                <span class="k">pass</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-382"><a href="#FGAdaHAT.clip_grad_by_adjustment-382"><span class="linenos">382</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_scheduler_type</span> <span class="o">==</span> <span class="s2">&quot;summative_mask_sparsity_reg&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-383"><a href="#FGAdaHAT.clip_grad_by_adjustment-383"><span class="linenos">383</span></a>                <span class="n">factor</span> <span class="o">=</span> <span class="n">factor</span> <span class="o">*</span> <span class="p">(</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-384"><a href="#FGAdaHAT.clip_grad_by_adjustment-384"><span class="linenos">384</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">summative_mask_for_previous_tasks</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_linear</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-385"><a href="#FGAdaHAT.clip_grad_by_adjustment-385"><span class="linenos">385</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-386"><a href="#FGAdaHAT.clip_grad_by_adjustment-386"><span class="linenos">386</span></a>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-387"><a href="#FGAdaHAT.clip_grad_by_adjustment-387"><span class="linenos">387</span></a>            <span class="c1"># calculate the adjustment rate</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-388"><a href="#FGAdaHAT.clip_grad_by_adjustment-388"><span class="linenos">388</span></a>            <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-389"><a href="#FGAdaHAT.clip_grad_by_adjustment-389"><span class="linenos">389</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span><span class="p">,</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-390"><a href="#FGAdaHAT.clip_grad_by_adjustment-390"><span class="linenos">390</span></a>                <span class="p">(</span><span class="n">factor</span> <span class="o">*</span> <span class="n">weight_importance</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span><span class="p">),</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-391"><a href="#FGAdaHAT.clip_grad_by_adjustment-391"><span class="linenos">391</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-392"><a href="#FGAdaHAT.clip_grad_by_adjustment-392"><span class="linenos">392</span></a>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-393"><a href="#FGAdaHAT.clip_grad_by_adjustment-393"><span class="linenos">393</span></a>            <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-394"><a href="#FGAdaHAT.clip_grad_by_adjustment-394"><span class="linenos">394</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span><span class="p">,</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-395"><a href="#FGAdaHAT.clip_grad_by_adjustment-395"><span class="linenos">395</span></a>                <span class="p">(</span><span class="n">factor</span> <span class="o">*</span> <span class="n">bias_importance</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span><span class="p">),</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-396"><a href="#FGAdaHAT.clip_grad_by_adjustment-396"><span class="linenos">396</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-397"><a href="#FGAdaHAT.clip_grad_by_adjustment-397"><span class="linenos">397</span></a>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-398"><a href="#FGAdaHAT.clip_grad_by_adjustment-398"><span class="linenos">398</span></a>            <span class="c1"># apply the adjustment rate to the gradients</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-399"><a href="#FGAdaHAT.clip_grad_by_adjustment-399"><span class="linenos">399</span></a>            <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">adjustment_rate_weight_layer</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-400"><a href="#FGAdaHAT.clip_grad_by_adjustment-400"><span class="linenos">400</span></a>            <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-401"><a href="#FGAdaHAT.clip_grad_by_adjustment-401"><span class="linenos">401</span></a>                <span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">adjustment_rate_bias_layer</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-402"><a href="#FGAdaHAT.clip_grad_by_adjustment-402"><span class="linenos">402</span></a>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-403"><a href="#FGAdaHAT.clip_grad_by_adjustment-403"><span class="linenos">403</span></a>            <span class="c1"># store the adjustment rate for logging</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-404"><a href="#FGAdaHAT.clip_grad_by_adjustment-404"><span class="linenos">404</span></a>            <span class="n">adjustment_rate_weight</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">adjustment_rate_weight_layer</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-405"><a href="#FGAdaHAT.clip_grad_by_adjustment-405"><span class="linenos">405</span></a>            <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-406"><a href="#FGAdaHAT.clip_grad_by_adjustment-406"><span class="linenos">406</span></a>                <span class="n">adjustment_rate_bias</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">adjustment_rate_bias_layer</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-407"><a href="#FGAdaHAT.clip_grad_by_adjustment-407"><span class="linenos">407</span></a>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-408"><a href="#FGAdaHAT.clip_grad_by_adjustment-408"><span class="linenos">408</span></a>            <span class="c1"># update network capacity metric</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-409"><a href="#FGAdaHAT.clip_grad_by_adjustment-409"><span class="linenos">409</span></a>            <span class="n">capacity</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">adjustment_rate_weight_layer</span><span class="p">,</span> <span class="n">adjustment_rate_bias_layer</span><span class="p">)</span>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-410"><a href="#FGAdaHAT.clip_grad_by_adjustment-410"><span class="linenos">410</span></a>
</span><span id="FGAdaHAT.clip_grad_by_adjustment-411"><a href="#FGAdaHAT.clip_grad_by_adjustment-411"><span class="linenos">411</span></a>        <span class="k">return</span> <span class="n">adjustment_rate_weight</span><span class="p">,</span> <span class="n">adjustment_rate_bias</span><span class="p">,</span> <span class="n">capacity</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
</span></pre></div>


            <div class="docstring"><p>Clip the gradients by the adjustment rate. See Eq. (1) in the paper.</p>

<p>Note that because the task embedding fully covers every layer in the backbone network, no parameters are left out of this system. This applies not only to parameters between layers with task embeddings, but also to those before the first layer. We design it separately in the code.</p>

<p>Network capacity is measured alongside this method. Network capacity is defined as the average adjustment rate over all parameters. See Sec. 4.1 in the <a href="https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9">AdaHAT paper</a>.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>network_sparsity</strong> (<code>dict[str, Tensor]</code>): the network sparsity (i.e., mask sparsity loss of each layer) for the current task. Keys are layer names and values are the network sparsity values. It is used to calculate the adjustment rate for gradients. In FG-AdaHAT, it is used to construct the importance scheduler.</li>
</ul>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>adjustment_rate_weight</strong> (<code>dict[str, Tensor]</code>): the adjustment rate for weights. Keys (<code>str</code>) are layer names and values (<code>Tensor</code>) are the adjustment rate tensors.</li>
<li><strong>adjustment_rate_bias</strong> (<code>dict[str, Tensor]</code>): the adjustment rate for biases. Keys (<code>str</code>) are layer names and values (<code>Tensor</code>) are the adjustment rate tensors.</li>
<li><strong>capacity</strong> (<code>Tensor</code>): the calculated network capacity.</li>
</ul>
</div>


                            </div>
                            <div id="FGAdaHAT.on_train_batch_end" class="classattr">
                                        <input id="FGAdaHAT.on_train_batch_end-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">on_train_batch_end</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">outputs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">typing</span><span class="o">.</span><span class="n">Any</span><span class="p">]</span>, </span><span class="param"><span class="n">batch</span><span class="p">:</span> <span class="n">Any</span>, </span><span class="param"><span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span></span><span class="return-annotation">) -> <span class="kc">None</span>:</span></span>

                <label class="view-source-button" for="FGAdaHAT.on_train_batch_end-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FGAdaHAT.on_train_batch_end"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FGAdaHAT.on_train_batch_end-413"><a href="#FGAdaHAT.on_train_batch_end-413"><span class="linenos">413</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_batch_end</span><span class="p">(</span>
</span><span id="FGAdaHAT.on_train_batch_end-414"><a href="#FGAdaHAT.on_train_batch_end-414"><span class="linenos">414</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span>
</span><span id="FGAdaHAT.on_train_batch_end-415"><a href="#FGAdaHAT.on_train_batch_end-415"><span class="linenos">415</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FGAdaHAT.on_train_batch_end-416"><a href="#FGAdaHAT.on_train_batch_end-416"><span class="linenos">416</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Calculate the step-wise importance, update the accumulated importance and number of steps counter after each training step.</span>
</span><span id="FGAdaHAT.on_train_batch_end-417"><a href="#FGAdaHAT.on_train_batch_end-417"><span class="linenos">417</span></a>
</span><span id="FGAdaHAT.on_train_batch_end-418"><a href="#FGAdaHAT.on_train_batch_end-418"><span class="linenos">418</span></a><span class="sd">        **Args:**</span>
</span><span id="FGAdaHAT.on_train_batch_end-419"><a href="#FGAdaHAT.on_train_batch_end-419"><span class="linenos">419</span></a><span class="sd">        - **outputs** (`dict[str, Any]`): outputs of the training step (returns of `training_step()` in `CLAlgorithm`).</span>
</span><span id="FGAdaHAT.on_train_batch_end-420"><a href="#FGAdaHAT.on_train_batch_end-420"><span class="linenos">420</span></a><span class="sd">        - **batch** (`Any`): training data batch.</span>
</span><span id="FGAdaHAT.on_train_batch_end-421"><a href="#FGAdaHAT.on_train_batch_end-421"><span class="linenos">421</span></a><span class="sd">        - **batch_idx** (`int`): index of the current batch (for mask figure file name).</span>
</span><span id="FGAdaHAT.on_train_batch_end-422"><a href="#FGAdaHAT.on_train_batch_end-422"><span class="linenos">422</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT.on_train_batch_end-423"><a href="#FGAdaHAT.on_train_batch_end-423"><span class="linenos">423</span></a>
</span><span id="FGAdaHAT.on_train_batch_end-424"><a href="#FGAdaHAT.on_train_batch_end-424"><span class="linenos">424</span></a>        <span class="c1"># get potential useful information from training batch</span>
</span><span id="FGAdaHAT.on_train_batch_end-425"><a href="#FGAdaHAT.on_train_batch_end-425"><span class="linenos">425</span></a>        <span class="n">activations</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;activations&quot;</span><span class="p">]</span>
</span><span id="FGAdaHAT.on_train_batch_end-426"><a href="#FGAdaHAT.on_train_batch_end-426"><span class="linenos">426</span></a>        <span class="nb">input</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">]</span>
</span><span id="FGAdaHAT.on_train_batch_end-427"><a href="#FGAdaHAT.on_train_batch_end-427"><span class="linenos">427</span></a>        <span class="n">target</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;target&quot;</span><span class="p">]</span>
</span><span id="FGAdaHAT.on_train_batch_end-428"><a href="#FGAdaHAT.on_train_batch_end-428"><span class="linenos">428</span></a>        <span class="n">mask</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;mask&quot;</span><span class="p">]</span>
</span><span id="FGAdaHAT.on_train_batch_end-429"><a href="#FGAdaHAT.on_train_batch_end-429"><span class="linenos">429</span></a>        <span class="n">num_batches</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">num_training_batches</span>
</span><span id="FGAdaHAT.on_train_batch_end-430"><a href="#FGAdaHAT.on_train_batch_end-430"><span class="linenos">430</span></a>
</span><span id="FGAdaHAT.on_train_batch_end-431"><a href="#FGAdaHAT.on_train_batch_end-431"><span class="linenos">431</span></a>        <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="FGAdaHAT.on_train_batch_end-432"><a href="#FGAdaHAT.on_train_batch_end-432"><span class="linenos">432</span></a>            <span class="c1"># layer-wise operation</span>
</span><span id="FGAdaHAT.on_train_batch_end-433"><a href="#FGAdaHAT.on_train_batch_end-433"><span class="linenos">433</span></a>
</span><span id="FGAdaHAT.on_train_batch_end-434"><a href="#FGAdaHAT.on_train_batch_end-434"><span class="linenos">434</span></a>            <span class="n">activation</span> <span class="o">=</span> <span class="n">activations</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="FGAdaHAT.on_train_batch_end-435"><a href="#FGAdaHAT.on_train_batch_end-435"><span class="linenos">435</span></a>
</span><span id="FGAdaHAT.on_train_batch_end-436"><a href="#FGAdaHAT.on_train_batch_end-436"><span class="linenos">436</span></a>            <span class="c1"># calculate neuron-wise importance of the training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper.</span>
</span><span id="FGAdaHAT.on_train_batch_end-437"><a href="#FGAdaHAT.on_train_batch_end-437"><span class="linenos">437</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;input_weight_abs_sum&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT.on_train_batch_end-438"><a href="#FGAdaHAT.on_train_batch_end-438"><span class="linenos">438</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_weight_abs_sum</span><span class="p">(</span>
</span><span id="FGAdaHAT.on_train_batch_end-439"><a href="#FGAdaHAT.on_train_batch_end-439"><span class="linenos">439</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-440"><a href="#FGAdaHAT.on_train_batch_end-440"><span class="linenos">440</span></a>                    <span class="n">if_output_weight</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-441"><a href="#FGAdaHAT.on_train_batch_end-441"><span class="linenos">441</span></a>                    <span class="n">reciprocal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-442"><a href="#FGAdaHAT.on_train_batch_end-442"><span class="linenos">442</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT.on_train_batch_end-443"><a href="#FGAdaHAT.on_train_batch_end-443"><span class="linenos">443</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;output_weight_abs_sum&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT.on_train_batch_end-444"><a href="#FGAdaHAT.on_train_batch_end-444"><span class="linenos">444</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_weight_abs_sum</span><span class="p">(</span>
</span><span id="FGAdaHAT.on_train_batch_end-445"><a href="#FGAdaHAT.on_train_batch_end-445"><span class="linenos">445</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-446"><a href="#FGAdaHAT.on_train_batch_end-446"><span class="linenos">446</span></a>                    <span class="n">if_output_weight</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-447"><a href="#FGAdaHAT.on_train_batch_end-447"><span class="linenos">447</span></a>                    <span class="n">reciprocal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-448"><a href="#FGAdaHAT.on_train_batch_end-448"><span class="linenos">448</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT.on_train_batch_end-449"><a href="#FGAdaHAT.on_train_batch_end-449"><span class="linenos">449</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;input_weight_gradient_abs_sum&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT.on_train_batch_end-450"><a href="#FGAdaHAT.on_train_batch_end-450"><span class="linenos">450</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FGAdaHAT.on_train_batch_end-451"><a href="#FGAdaHAT.on_train_batch_end-451"><span class="linenos">451</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_weight_gradient_abs_sum</span><span class="p">(</span>
</span><span id="FGAdaHAT.on_train_batch_end-452"><a href="#FGAdaHAT.on_train_batch_end-452"><span class="linenos">452</span></a>                        <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span> <span class="n">if_output_weight</span><span class="o">=</span><span class="kc">False</span>
</span><span id="FGAdaHAT.on_train_batch_end-453"><a href="#FGAdaHAT.on_train_batch_end-453"><span class="linenos">453</span></a>                    <span class="p">)</span>
</span><span id="FGAdaHAT.on_train_batch_end-454"><a href="#FGAdaHAT.on_train_batch_end-454"><span class="linenos">454</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT.on_train_batch_end-455"><a href="#FGAdaHAT.on_train_batch_end-455"><span class="linenos">455</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;output_weight_gradient_abs_sum&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT.on_train_batch_end-456"><a href="#FGAdaHAT.on_train_batch_end-456"><span class="linenos">456</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FGAdaHAT.on_train_batch_end-457"><a href="#FGAdaHAT.on_train_batch_end-457"><span class="linenos">457</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_weight_gradient_abs_sum</span><span class="p">(</span>
</span><span id="FGAdaHAT.on_train_batch_end-458"><a href="#FGAdaHAT.on_train_batch_end-458"><span class="linenos">458</span></a>                        <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span> <span class="n">if_output_weight</span><span class="o">=</span><span class="kc">True</span>
</span><span id="FGAdaHAT.on_train_batch_end-459"><a href="#FGAdaHAT.on_train_batch_end-459"><span class="linenos">459</span></a>                    <span class="p">)</span>
</span><span id="FGAdaHAT.on_train_batch_end-460"><a href="#FGAdaHAT.on_train_batch_end-460"><span class="linenos">460</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT.on_train_batch_end-461"><a href="#FGAdaHAT.on_train_batch_end-461"><span class="linenos">461</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;activation_abs&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT.on_train_batch_end-462"><a href="#FGAdaHAT.on_train_batch_end-462"><span class="linenos">462</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_activation_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT.on_train_batch_end-463"><a href="#FGAdaHAT.on_train_batch_end-463"><span class="linenos">463</span></a>                    <span class="n">activation</span><span class="o">=</span><span class="n">activation</span>
</span><span id="FGAdaHAT.on_train_batch_end-464"><a href="#FGAdaHAT.on_train_batch_end-464"><span class="linenos">464</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT.on_train_batch_end-465"><a href="#FGAdaHAT.on_train_batch_end-465"><span class="linenos">465</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;input_weight_abs_sum_x_activation_abs&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT.on_train_batch_end-466"><a href="#FGAdaHAT.on_train_batch_end-466"><span class="linenos">466</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FGAdaHAT.on_train_batch_end-467"><a href="#FGAdaHAT.on_train_batch_end-467"><span class="linenos">467</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_weight_abs_sum_x_activation_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT.on_train_batch_end-468"><a href="#FGAdaHAT.on_train_batch_end-468"><span class="linenos">468</span></a>                        <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-469"><a href="#FGAdaHAT.on_train_batch_end-469"><span class="linenos">469</span></a>                        <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-470"><a href="#FGAdaHAT.on_train_batch_end-470"><span class="linenos">470</span></a>                        <span class="n">if_output_weight</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-471"><a href="#FGAdaHAT.on_train_batch_end-471"><span class="linenos">471</span></a>                    <span class="p">)</span>
</span><span id="FGAdaHAT.on_train_batch_end-472"><a href="#FGAdaHAT.on_train_batch_end-472"><span class="linenos">472</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT.on_train_batch_end-473"><a href="#FGAdaHAT.on_train_batch_end-473"><span class="linenos">473</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;output_weight_abs_sum_x_activation_abs&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT.on_train_batch_end-474"><a href="#FGAdaHAT.on_train_batch_end-474"><span class="linenos">474</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FGAdaHAT.on_train_batch_end-475"><a href="#FGAdaHAT.on_train_batch_end-475"><span class="linenos">475</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_weight_abs_sum_x_activation_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT.on_train_batch_end-476"><a href="#FGAdaHAT.on_train_batch_end-476"><span class="linenos">476</span></a>                        <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-477"><a href="#FGAdaHAT.on_train_batch_end-477"><span class="linenos">477</span></a>                        <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-478"><a href="#FGAdaHAT.on_train_batch_end-478"><span class="linenos">478</span></a>                        <span class="n">if_output_weight</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-479"><a href="#FGAdaHAT.on_train_batch_end-479"><span class="linenos">479</span></a>                    <span class="p">)</span>
</span><span id="FGAdaHAT.on_train_batch_end-480"><a href="#FGAdaHAT.on_train_batch_end-480"><span class="linenos">480</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT.on_train_batch_end-481"><a href="#FGAdaHAT.on_train_batch_end-481"><span class="linenos">481</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;gradient_x_activation_abs&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT.on_train_batch_end-482"><a href="#FGAdaHAT.on_train_batch_end-482"><span class="linenos">482</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FGAdaHAT.on_train_batch_end-483"><a href="#FGAdaHAT.on_train_batch_end-483"><span class="linenos">483</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_gradient_x_activation_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT.on_train_batch_end-484"><a href="#FGAdaHAT.on_train_batch_end-484"><span class="linenos">484</span></a>                        <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-485"><a href="#FGAdaHAT.on_train_batch_end-485"><span class="linenos">485</span></a>                        <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-486"><a href="#FGAdaHAT.on_train_batch_end-486"><span class="linenos">486</span></a>                        <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-487"><a href="#FGAdaHAT.on_train_batch_end-487"><span class="linenos">487</span></a>                        <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-488"><a href="#FGAdaHAT.on_train_batch_end-488"><span class="linenos">488</span></a>                        <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-489"><a href="#FGAdaHAT.on_train_batch_end-489"><span class="linenos">489</span></a>                    <span class="p">)</span>
</span><span id="FGAdaHAT.on_train_batch_end-490"><a href="#FGAdaHAT.on_train_batch_end-490"><span class="linenos">490</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT.on_train_batch_end-491"><a href="#FGAdaHAT.on_train_batch_end-491"><span class="linenos">491</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;input_weight_gradient_square_sum&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT.on_train_batch_end-492"><a href="#FGAdaHAT.on_train_batch_end-492"><span class="linenos">492</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FGAdaHAT.on_train_batch_end-493"><a href="#FGAdaHAT.on_train_batch_end-493"><span class="linenos">493</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_weight_gradient_square_sum</span><span class="p">(</span>
</span><span id="FGAdaHAT.on_train_batch_end-494"><a href="#FGAdaHAT.on_train_batch_end-494"><span class="linenos">494</span></a>                        <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-495"><a href="#FGAdaHAT.on_train_batch_end-495"><span class="linenos">495</span></a>                        <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-496"><a href="#FGAdaHAT.on_train_batch_end-496"><span class="linenos">496</span></a>                        <span class="n">if_output_weight</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-497"><a href="#FGAdaHAT.on_train_batch_end-497"><span class="linenos">497</span></a>                    <span class="p">)</span>
</span><span id="FGAdaHAT.on_train_batch_end-498"><a href="#FGAdaHAT.on_train_batch_end-498"><span class="linenos">498</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT.on_train_batch_end-499"><a href="#FGAdaHAT.on_train_batch_end-499"><span class="linenos">499</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;output_weight_gradient_square_sum&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT.on_train_batch_end-500"><a href="#FGAdaHAT.on_train_batch_end-500"><span class="linenos">500</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FGAdaHAT.on_train_batch_end-501"><a href="#FGAdaHAT.on_train_batch_end-501"><span class="linenos">501</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_weight_gradient_square_sum</span><span class="p">(</span>
</span><span id="FGAdaHAT.on_train_batch_end-502"><a href="#FGAdaHAT.on_train_batch_end-502"><span class="linenos">502</span></a>                        <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-503"><a href="#FGAdaHAT.on_train_batch_end-503"><span class="linenos">503</span></a>                        <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-504"><a href="#FGAdaHAT.on_train_batch_end-504"><span class="linenos">504</span></a>                        <span class="n">if_output_weight</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-505"><a href="#FGAdaHAT.on_train_batch_end-505"><span class="linenos">505</span></a>                    <span class="p">)</span>
</span><span id="FGAdaHAT.on_train_batch_end-506"><a href="#FGAdaHAT.on_train_batch_end-506"><span class="linenos">506</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT.on_train_batch_end-507"><a href="#FGAdaHAT.on_train_batch_end-507"><span class="linenos">507</span></a>            <span class="k">elif</span> <span class="p">(</span>
</span><span id="FGAdaHAT.on_train_batch_end-508"><a href="#FGAdaHAT.on_train_batch_end-508"><span class="linenos">508</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span>
</span><span id="FGAdaHAT.on_train_batch_end-509"><a href="#FGAdaHAT.on_train_batch_end-509"><span class="linenos">509</span></a>                <span class="o">==</span> <span class="s2">&quot;input_weight_gradient_square_sum_x_activation_abs&quot;</span>
</span><span id="FGAdaHAT.on_train_batch_end-510"><a href="#FGAdaHAT.on_train_batch_end-510"><span class="linenos">510</span></a>            <span class="p">):</span>
</span><span id="FGAdaHAT.on_train_batch_end-511"><a href="#FGAdaHAT.on_train_batch_end-511"><span class="linenos">511</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_weight_gradient_square_sum_x_activation_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT.on_train_batch_end-512"><a href="#FGAdaHAT.on_train_batch_end-512"><span class="linenos">512</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-513"><a href="#FGAdaHAT.on_train_batch_end-513"><span class="linenos">513</span></a>                    <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-514"><a href="#FGAdaHAT.on_train_batch_end-514"><span class="linenos">514</span></a>                    <span class="n">if_output_weight</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-515"><a href="#FGAdaHAT.on_train_batch_end-515"><span class="linenos">515</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT.on_train_batch_end-516"><a href="#FGAdaHAT.on_train_batch_end-516"><span class="linenos">516</span></a>            <span class="k">elif</span> <span class="p">(</span>
</span><span id="FGAdaHAT.on_train_batch_end-517"><a href="#FGAdaHAT.on_train_batch_end-517"><span class="linenos">517</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span>
</span><span id="FGAdaHAT.on_train_batch_end-518"><a href="#FGAdaHAT.on_train_batch_end-518"><span class="linenos">518</span></a>                <span class="o">==</span> <span class="s2">&quot;output_weight_gradient_square_sum_x_activation_abs&quot;</span>
</span><span id="FGAdaHAT.on_train_batch_end-519"><a href="#FGAdaHAT.on_train_batch_end-519"><span class="linenos">519</span></a>            <span class="p">):</span>
</span><span id="FGAdaHAT.on_train_batch_end-520"><a href="#FGAdaHAT.on_train_batch_end-520"><span class="linenos">520</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_weight_gradient_square_sum_x_activation_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT.on_train_batch_end-521"><a href="#FGAdaHAT.on_train_batch_end-521"><span class="linenos">521</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-522"><a href="#FGAdaHAT.on_train_batch_end-522"><span class="linenos">522</span></a>                    <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-523"><a href="#FGAdaHAT.on_train_batch_end-523"><span class="linenos">523</span></a>                    <span class="n">if_output_weight</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-524"><a href="#FGAdaHAT.on_train_batch_end-524"><span class="linenos">524</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT.on_train_batch_end-525"><a href="#FGAdaHAT.on_train_batch_end-525"><span class="linenos">525</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;conductance_abs&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT.on_train_batch_end-526"><a href="#FGAdaHAT.on_train_batch_end-526"><span class="linenos">526</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_conductance_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT.on_train_batch_end-527"><a href="#FGAdaHAT.on_train_batch_end-527"><span class="linenos">527</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-528"><a href="#FGAdaHAT.on_train_batch_end-528"><span class="linenos">528</span></a>                    <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-529"><a href="#FGAdaHAT.on_train_batch_end-529"><span class="linenos">529</span></a>                    <span class="n">baselines</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-530"><a href="#FGAdaHAT.on_train_batch_end-530"><span class="linenos">530</span></a>                    <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-531"><a href="#FGAdaHAT.on_train_batch_end-531"><span class="linenos">531</span></a>                    <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-532"><a href="#FGAdaHAT.on_train_batch_end-532"><span class="linenos">532</span></a>                    <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-533"><a href="#FGAdaHAT.on_train_batch_end-533"><span class="linenos">533</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT.on_train_batch_end-534"><a href="#FGAdaHAT.on_train_batch_end-534"><span class="linenos">534</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;internal_influence_abs&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT.on_train_batch_end-535"><a href="#FGAdaHAT.on_train_batch_end-535"><span class="linenos">535</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_internal_influence_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT.on_train_batch_end-536"><a href="#FGAdaHAT.on_train_batch_end-536"><span class="linenos">536</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-537"><a href="#FGAdaHAT.on_train_batch_end-537"><span class="linenos">537</span></a>                    <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-538"><a href="#FGAdaHAT.on_train_batch_end-538"><span class="linenos">538</span></a>                    <span class="n">baselines</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-539"><a href="#FGAdaHAT.on_train_batch_end-539"><span class="linenos">539</span></a>                    <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-540"><a href="#FGAdaHAT.on_train_batch_end-540"><span class="linenos">540</span></a>                    <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-541"><a href="#FGAdaHAT.on_train_batch_end-541"><span class="linenos">541</span></a>                    <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-542"><a href="#FGAdaHAT.on_train_batch_end-542"><span class="linenos">542</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT.on_train_batch_end-543"><a href="#FGAdaHAT.on_train_batch_end-543"><span class="linenos">543</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;gradcam_abs&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT.on_train_batch_end-544"><a href="#FGAdaHAT.on_train_batch_end-544"><span class="linenos">544</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_gradcam_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT.on_train_batch_end-545"><a href="#FGAdaHAT.on_train_batch_end-545"><span class="linenos">545</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-546"><a href="#FGAdaHAT.on_train_batch_end-546"><span class="linenos">546</span></a>                    <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-547"><a href="#FGAdaHAT.on_train_batch_end-547"><span class="linenos">547</span></a>                    <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-548"><a href="#FGAdaHAT.on_train_batch_end-548"><span class="linenos">548</span></a>                    <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-549"><a href="#FGAdaHAT.on_train_batch_end-549"><span class="linenos">549</span></a>                    <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-550"><a href="#FGAdaHAT.on_train_batch_end-550"><span class="linenos">550</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT.on_train_batch_end-551"><a href="#FGAdaHAT.on_train_batch_end-551"><span class="linenos">551</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;deeplift_abs&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT.on_train_batch_end-552"><a href="#FGAdaHAT.on_train_batch_end-552"><span class="linenos">552</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_deeplift_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT.on_train_batch_end-553"><a href="#FGAdaHAT.on_train_batch_end-553"><span class="linenos">553</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-554"><a href="#FGAdaHAT.on_train_batch_end-554"><span class="linenos">554</span></a>                    <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-555"><a href="#FGAdaHAT.on_train_batch_end-555"><span class="linenos">555</span></a>                    <span class="n">baselines</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-556"><a href="#FGAdaHAT.on_train_batch_end-556"><span class="linenos">556</span></a>                    <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-557"><a href="#FGAdaHAT.on_train_batch_end-557"><span class="linenos">557</span></a>                    <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-558"><a href="#FGAdaHAT.on_train_batch_end-558"><span class="linenos">558</span></a>                    <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-559"><a href="#FGAdaHAT.on_train_batch_end-559"><span class="linenos">559</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT.on_train_batch_end-560"><a href="#FGAdaHAT.on_train_batch_end-560"><span class="linenos">560</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;deepliftshap_abs&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT.on_train_batch_end-561"><a href="#FGAdaHAT.on_train_batch_end-561"><span class="linenos">561</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_deepliftshap_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT.on_train_batch_end-562"><a href="#FGAdaHAT.on_train_batch_end-562"><span class="linenos">562</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-563"><a href="#FGAdaHAT.on_train_batch_end-563"><span class="linenos">563</span></a>                    <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-564"><a href="#FGAdaHAT.on_train_batch_end-564"><span class="linenos">564</span></a>                    <span class="n">baselines</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-565"><a href="#FGAdaHAT.on_train_batch_end-565"><span class="linenos">565</span></a>                    <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-566"><a href="#FGAdaHAT.on_train_batch_end-566"><span class="linenos">566</span></a>                    <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-567"><a href="#FGAdaHAT.on_train_batch_end-567"><span class="linenos">567</span></a>                    <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-568"><a href="#FGAdaHAT.on_train_batch_end-568"><span class="linenos">568</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT.on_train_batch_end-569"><a href="#FGAdaHAT.on_train_batch_end-569"><span class="linenos">569</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;gradientshap_abs&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT.on_train_batch_end-570"><a href="#FGAdaHAT.on_train_batch_end-570"><span class="linenos">570</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_gradientshap_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT.on_train_batch_end-571"><a href="#FGAdaHAT.on_train_batch_end-571"><span class="linenos">571</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-572"><a href="#FGAdaHAT.on_train_batch_end-572"><span class="linenos">572</span></a>                    <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-573"><a href="#FGAdaHAT.on_train_batch_end-573"><span class="linenos">573</span></a>                    <span class="n">baselines</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-574"><a href="#FGAdaHAT.on_train_batch_end-574"><span class="linenos">574</span></a>                    <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-575"><a href="#FGAdaHAT.on_train_batch_end-575"><span class="linenos">575</span></a>                    <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-576"><a href="#FGAdaHAT.on_train_batch_end-576"><span class="linenos">576</span></a>                    <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-577"><a href="#FGAdaHAT.on_train_batch_end-577"><span class="linenos">577</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT.on_train_batch_end-578"><a href="#FGAdaHAT.on_train_batch_end-578"><span class="linenos">578</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;integrated_gradients_abs&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT.on_train_batch_end-579"><a href="#FGAdaHAT.on_train_batch_end-579"><span class="linenos">579</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FGAdaHAT.on_train_batch_end-580"><a href="#FGAdaHAT.on_train_batch_end-580"><span class="linenos">580</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_integrated_gradients_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT.on_train_batch_end-581"><a href="#FGAdaHAT.on_train_batch_end-581"><span class="linenos">581</span></a>                        <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-582"><a href="#FGAdaHAT.on_train_batch_end-582"><span class="linenos">582</span></a>                        <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-583"><a href="#FGAdaHAT.on_train_batch_end-583"><span class="linenos">583</span></a>                        <span class="n">baselines</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-584"><a href="#FGAdaHAT.on_train_batch_end-584"><span class="linenos">584</span></a>                        <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-585"><a href="#FGAdaHAT.on_train_batch_end-585"><span class="linenos">585</span></a>                        <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-586"><a href="#FGAdaHAT.on_train_batch_end-586"><span class="linenos">586</span></a>                        <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-587"><a href="#FGAdaHAT.on_train_batch_end-587"><span class="linenos">587</span></a>                    <span class="p">)</span>
</span><span id="FGAdaHAT.on_train_batch_end-588"><a href="#FGAdaHAT.on_train_batch_end-588"><span class="linenos">588</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT.on_train_batch_end-589"><a href="#FGAdaHAT.on_train_batch_end-589"><span class="linenos">589</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;feature_ablation_abs&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT.on_train_batch_end-590"><a href="#FGAdaHAT.on_train_batch_end-590"><span class="linenos">590</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_feature_ablation_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT.on_train_batch_end-591"><a href="#FGAdaHAT.on_train_batch_end-591"><span class="linenos">591</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-592"><a href="#FGAdaHAT.on_train_batch_end-592"><span class="linenos">592</span></a>                    <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-593"><a href="#FGAdaHAT.on_train_batch_end-593"><span class="linenos">593</span></a>                    <span class="n">layer_baselines</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-594"><a href="#FGAdaHAT.on_train_batch_end-594"><span class="linenos">594</span></a>                    <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-595"><a href="#FGAdaHAT.on_train_batch_end-595"><span class="linenos">595</span></a>                    <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-596"><a href="#FGAdaHAT.on_train_batch_end-596"><span class="linenos">596</span></a>                    <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-597"><a href="#FGAdaHAT.on_train_batch_end-597"><span class="linenos">597</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT.on_train_batch_end-598"><a href="#FGAdaHAT.on_train_batch_end-598"><span class="linenos">598</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;lrp_abs&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT.on_train_batch_end-599"><a href="#FGAdaHAT.on_train_batch_end-599"><span class="linenos">599</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_lrp_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT.on_train_batch_end-600"><a href="#FGAdaHAT.on_train_batch_end-600"><span class="linenos">600</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-601"><a href="#FGAdaHAT.on_train_batch_end-601"><span class="linenos">601</span></a>                    <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-602"><a href="#FGAdaHAT.on_train_batch_end-602"><span class="linenos">602</span></a>                    <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-603"><a href="#FGAdaHAT.on_train_batch_end-603"><span class="linenos">603</span></a>                    <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-604"><a href="#FGAdaHAT.on_train_batch_end-604"><span class="linenos">604</span></a>                    <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-605"><a href="#FGAdaHAT.on_train_batch_end-605"><span class="linenos">605</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT.on_train_batch_end-606"><a href="#FGAdaHAT.on_train_batch_end-606"><span class="linenos">606</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;cbp_adaptation&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT.on_train_batch_end-607"><a href="#FGAdaHAT.on_train_batch_end-607"><span class="linenos">607</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_weight_abs_sum</span><span class="p">(</span>
</span><span id="FGAdaHAT.on_train_batch_end-608"><a href="#FGAdaHAT.on_train_batch_end-608"><span class="linenos">608</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-609"><a href="#FGAdaHAT.on_train_batch_end-609"><span class="linenos">609</span></a>                    <span class="n">if_output_weight</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-610"><a href="#FGAdaHAT.on_train_batch_end-610"><span class="linenos">610</span></a>                    <span class="n">reciprocal</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-611"><a href="#FGAdaHAT.on_train_batch_end-611"><span class="linenos">611</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT.on_train_batch_end-612"><a href="#FGAdaHAT.on_train_batch_end-612"><span class="linenos">612</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_type</span> <span class="o">==</span> <span class="s2">&quot;cbp_adaptive_contribution&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT.on_train_batch_end-613"><a href="#FGAdaHAT.on_train_batch_end-613"><span class="linenos">613</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FGAdaHAT.on_train_batch_end-614"><a href="#FGAdaHAT.on_train_batch_end-614"><span class="linenos">614</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">get_importance_step_layer_cbp_adaptive_contribution</span><span class="p">(</span>
</span><span id="FGAdaHAT.on_train_batch_end-615"><a href="#FGAdaHAT.on_train_batch_end-615"><span class="linenos">615</span></a>                        <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-616"><a href="#FGAdaHAT.on_train_batch_end-616"><span class="linenos">616</span></a>                        <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
</span><span id="FGAdaHAT.on_train_batch_end-617"><a href="#FGAdaHAT.on_train_batch_end-617"><span class="linenos">617</span></a>                    <span class="p">)</span>
</span><span id="FGAdaHAT.on_train_batch_end-618"><a href="#FGAdaHAT.on_train_batch_end-618"><span class="linenos">618</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT.on_train_batch_end-619"><a href="#FGAdaHAT.on_train_batch_end-619"><span class="linenos">619</span></a>
</span><span id="FGAdaHAT.on_train_batch_end-620"><a href="#FGAdaHAT.on_train_batch_end-620"><span class="linenos">620</span></a>            <span class="n">importance_step</span> <span class="o">=</span> <span class="n">min_max_normalize</span><span class="p">(</span>
</span><span id="FGAdaHAT.on_train_batch_end-621"><a href="#FGAdaHAT.on_train_batch_end-621"><span class="linenos">621</span></a>                <span class="n">importance_step</span>
</span><span id="FGAdaHAT.on_train_batch_end-622"><a href="#FGAdaHAT.on_train_batch_end-622"><span class="linenos">622</span></a>            <span class="p">)</span>  <span class="c1"># min-max scaling the utility to $[0, 1]$. See Eq. (5) in the paper</span>
</span><span id="FGAdaHAT.on_train_batch_end-623"><a href="#FGAdaHAT.on_train_batch_end-623"><span class="linenos">623</span></a>
</span><span id="FGAdaHAT.on_train_batch_end-624"><a href="#FGAdaHAT.on_train_batch_end-624"><span class="linenos">624</span></a>            <span class="c1"># multiply the importance by the training mask. See Eq. (6) in the paper</span>
</span><span id="FGAdaHAT.on_train_batch_end-625"><a href="#FGAdaHAT.on_train_batch_end-625"><span class="linenos">625</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_multiply_training_mask</span><span class="p">:</span>
</span><span id="FGAdaHAT.on_train_batch_end-626"><a href="#FGAdaHAT.on_train_batch_end-626"><span class="linenos">626</span></a>                <span class="n">importance_step</span> <span class="o">=</span> <span class="n">importance_step</span> <span class="o">*</span> <span class="n">mask</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="FGAdaHAT.on_train_batch_end-627"><a href="#FGAdaHAT.on_train_batch_end-627"><span class="linenos">627</span></a>
</span><span id="FGAdaHAT.on_train_batch_end-628"><a href="#FGAdaHAT.on_train_batch_end-628"><span class="linenos">628</span></a>            <span class="c1"># update accumulated importance</span>
</span><span id="FGAdaHAT.on_train_batch_end-629"><a href="#FGAdaHAT.on_train_batch_end-629"><span class="linenos">629</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">importances</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">][</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FGAdaHAT.on_train_batch_end-630"><a href="#FGAdaHAT.on_train_batch_end-630"><span class="linenos">630</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">importances</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">][</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">+</span> <span class="n">importance_step</span>
</span><span id="FGAdaHAT.on_train_batch_end-631"><a href="#FGAdaHAT.on_train_batch_end-631"><span class="linenos">631</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT.on_train_batch_end-632"><a href="#FGAdaHAT.on_train_batch_end-632"><span class="linenos">632</span></a>
</span><span id="FGAdaHAT.on_train_batch_end-633"><a href="#FGAdaHAT.on_train_batch_end-633"><span class="linenos">633</span></a>        <span class="c1"># update number of steps counter</span>
</span><span id="FGAdaHAT.on_train_batch_end-634"><a href="#FGAdaHAT.on_train_batch_end-634"><span class="linenos">634</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">num_steps_t</span> <span class="o">+=</span> <span class="mi">1</span>
</span></pre></div>


            <div class="docstring"><p>Calculate the step-wise importance, update the accumulated importance and number of steps counter after each training step.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>outputs</strong> (<code>dict[str, Any]</code>): outputs of the training step (returns of <code><a href="#FGAdaHAT.training_step">training_step()</a></code> in <code>CLAlgorithm</code>).</li>
<li><strong>batch</strong> (<code>Any</code>): training data batch.</li>
<li><strong>batch_idx</strong> (<code>int</code>): index of the current batch (for mask figure file name).</li>
</ul>
</div>


                            </div>
                            <div id="FGAdaHAT.on_train_end" class="classattr">
                                        <input id="FGAdaHAT.on_train_end-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">on_train_end</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span></span><span class="return-annotation">) -> <span class="kc">None</span>:</span></span>

                <label class="view-source-button" for="FGAdaHAT.on_train_end-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FGAdaHAT.on_train_end"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FGAdaHAT.on_train_end-636"><a href="#FGAdaHAT.on_train_end-636"><span class="linenos">636</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_end</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FGAdaHAT.on_train_end-637"><a href="#FGAdaHAT.on_train_end-637"><span class="linenos">637</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Additionally calculate neuron-wise importance for previous tasks at the end of training each task.&quot;&quot;&quot;</span>
</span><span id="FGAdaHAT.on_train_end-638"><a href="#FGAdaHAT.on_train_end-638"><span class="linenos">638</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">on_train_end</span><span class="p">()</span>  <span class="c1"># store the mask and update cumulative and summative masks</span>
</span><span id="FGAdaHAT.on_train_end-639"><a href="#FGAdaHAT.on_train_end-639"><span class="linenos">639</span></a>
</span><span id="FGAdaHAT.on_train_end-640"><a href="#FGAdaHAT.on_train_end-640"><span class="linenos">640</span></a>        <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="FGAdaHAT.on_train_end-641"><a href="#FGAdaHAT.on_train_end-641"><span class="linenos">641</span></a>
</span><span id="FGAdaHAT.on_train_end-642"><a href="#FGAdaHAT.on_train_end-642"><span class="linenos">642</span></a>            <span class="c1"># average the neuron-wise step importance. See Eq. (4) in the paper</span>
</span><span id="FGAdaHAT.on_train_end-643"><a href="#FGAdaHAT.on_train_end-643"><span class="linenos">643</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">importances</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">][</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FGAdaHAT.on_train_end-644"><a href="#FGAdaHAT.on_train_end-644"><span class="linenos">644</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">importances</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">][</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="FGAdaHAT.on_train_end-645"><a href="#FGAdaHAT.on_train_end-645"><span class="linenos">645</span></a>            <span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_steps_t</span>
</span><span id="FGAdaHAT.on_train_end-646"><a href="#FGAdaHAT.on_train_end-646"><span class="linenos">646</span></a>
</span><span id="FGAdaHAT.on_train_end-647"><a href="#FGAdaHAT.on_train_end-647"><span class="linenos">647</span></a>            <span class="c1"># add the base importance. See Eq. (6) in the paper</span>
</span><span id="FGAdaHAT.on_train_end-648"><a href="#FGAdaHAT.on_train_end-648"><span class="linenos">648</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">importances</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">][</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FGAdaHAT.on_train_end-649"><a href="#FGAdaHAT.on_train_end-649"><span class="linenos">649</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">importances</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">][</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_importance</span>
</span><span id="FGAdaHAT.on_train_end-650"><a href="#FGAdaHAT.on_train_end-650"><span class="linenos">650</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT.on_train_end-651"><a href="#FGAdaHAT.on_train_end-651"><span class="linenos">651</span></a>
</span><span id="FGAdaHAT.on_train_end-652"><a href="#FGAdaHAT.on_train_end-652"><span class="linenos">652</span></a>            <span class="c1"># filter unmasked importance</span>
</span><span id="FGAdaHAT.on_train_end-653"><a href="#FGAdaHAT.on_train_end-653"><span class="linenos">653</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">filter_unmasked_importance</span><span class="p">:</span>
</span><span id="FGAdaHAT.on_train_end-654"><a href="#FGAdaHAT.on_train_end-654"><span class="linenos">654</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">importances</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">][</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FGAdaHAT.on_train_end-655"><a href="#FGAdaHAT.on_train_end-655"><span class="linenos">655</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">importances</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">][</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="FGAdaHAT.on_train_end-656"><a href="#FGAdaHAT.on_train_end-656"><span class="linenos">656</span></a>                    <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">masks</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">][</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="FGAdaHAT.on_train_end-657"><a href="#FGAdaHAT.on_train_end-657"><span class="linenos">657</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT.on_train_end-658"><a href="#FGAdaHAT.on_train_end-658"><span class="linenos">658</span></a>
</span><span id="FGAdaHAT.on_train_end-659"><a href="#FGAdaHAT.on_train_end-659"><span class="linenos">659</span></a>            <span class="c1"># calculate the summative neuron-wise importance for previous tasks. See Eq. (4) in the paper</span>
</span><span id="FGAdaHAT.on_train_end-660"><a href="#FGAdaHAT.on_train_end-660"><span class="linenos">660</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy</span> <span class="o">==</span> <span class="s2">&quot;add_latest&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT.on_train_end-661"><a href="#FGAdaHAT.on_train_end-661"><span class="linenos">661</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">summative_importance_for_previous_tasks</span><span class="p">[</span>
</span><span id="FGAdaHAT.on_train_end-662"><a href="#FGAdaHAT.on_train_end-662"><span class="linenos">662</span></a>                    <span class="n">layer_name</span>
</span><span id="FGAdaHAT.on_train_end-663"><a href="#FGAdaHAT.on_train_end-663"><span class="linenos">663</span></a>                <span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">importances</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">][</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="FGAdaHAT.on_train_end-664"><a href="#FGAdaHAT.on_train_end-664"><span class="linenos">664</span></a>
</span><span id="FGAdaHAT.on_train_end-665"><a href="#FGAdaHAT.on_train_end-665"><span class="linenos">665</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy</span> <span class="o">==</span> <span class="s2">&quot;add_all&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT.on_train_end-666"><a href="#FGAdaHAT.on_train_end-666"><span class="linenos">666</span></a>                <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
</span><span id="FGAdaHAT.on_train_end-667"><a href="#FGAdaHAT.on_train_end-667"><span class="linenos">667</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">summative_importance_for_previous_tasks</span><span class="p">[</span>
</span><span id="FGAdaHAT.on_train_end-668"><a href="#FGAdaHAT.on_train_end-668"><span class="linenos">668</span></a>                        <span class="n">layer_name</span>
</span><span id="FGAdaHAT.on_train_end-669"><a href="#FGAdaHAT.on_train_end-669"><span class="linenos">669</span></a>                    <span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">importances</span><span class="p">[</span><span class="n">t</span><span class="p">][</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="FGAdaHAT.on_train_end-670"><a href="#FGAdaHAT.on_train_end-670"><span class="linenos">670</span></a>
</span><span id="FGAdaHAT.on_train_end-671"><a href="#FGAdaHAT.on_train_end-671"><span class="linenos">671</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy</span> <span class="o">==</span> <span class="s2">&quot;add_average&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT.on_train_end-672"><a href="#FGAdaHAT.on_train_end-672"><span class="linenos">672</span></a>                <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
</span><span id="FGAdaHAT.on_train_end-673"><a href="#FGAdaHAT.on_train_end-673"><span class="linenos">673</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">summative_importance_for_previous_tasks</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span>
</span><span id="FGAdaHAT.on_train_end-674"><a href="#FGAdaHAT.on_train_end-674"><span class="linenos">674</span></a>                        <span class="bp">self</span><span class="o">.</span><span class="n">importances</span><span class="p">[</span><span class="n">t</span><span class="p">][</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span>
</span><span id="FGAdaHAT.on_train_end-675"><a href="#FGAdaHAT.on_train_end-675"><span class="linenos">675</span></a>                    <span class="p">)</span>
</span><span id="FGAdaHAT.on_train_end-676"><a href="#FGAdaHAT.on_train_end-676"><span class="linenos">676</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="FGAdaHAT.on_train_end-677"><a href="#FGAdaHAT.on_train_end-677"><span class="linenos">677</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">summative_importance_for_previous_tasks</span><span class="p">[</span>
</span><span id="FGAdaHAT.on_train_end-678"><a href="#FGAdaHAT.on_train_end-678"><span class="linenos">678</span></a>                    <span class="n">layer_name</span>
</span><span id="FGAdaHAT.on_train_end-679"><a href="#FGAdaHAT.on_train_end-679"><span class="linenos">679</span></a>                <span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span>
</span><span id="FGAdaHAT.on_train_end-680"><a href="#FGAdaHAT.on_train_end-680"><span class="linenos">680</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">summative_importance_for_previous_tasks</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="FGAdaHAT.on_train_end-681"><a href="#FGAdaHAT.on_train_end-681"><span class="linenos">681</span></a>                <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
</span><span id="FGAdaHAT.on_train_end-682"><a href="#FGAdaHAT.on_train_end-682"><span class="linenos">682</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
</span><span id="FGAdaHAT.on_train_end-683"><a href="#FGAdaHAT.on_train_end-683"><span class="linenos">683</span></a>                <span class="p">)</span>  <span class="c1"># starting adding from 0</span>
</span><span id="FGAdaHAT.on_train_end-684"><a href="#FGAdaHAT.on_train_end-684"><span class="linenos">684</span></a>
</span><span id="FGAdaHAT.on_train_end-685"><a href="#FGAdaHAT.on_train_end-685"><span class="linenos">685</span></a>                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy</span> <span class="o">==</span> <span class="s2">&quot;linear_decrease&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT.on_train_end-686"><a href="#FGAdaHAT.on_train_end-686"><span class="linenos">686</span></a>                    <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy_linear_step</span>
</span><span id="FGAdaHAT.on_train_end-687"><a href="#FGAdaHAT.on_train_end-687"><span class="linenos">687</span></a>                    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
</span><span id="FGAdaHAT.on_train_end-688"><a href="#FGAdaHAT.on_train_end-688"><span class="linenos">688</span></a>                        <span class="n">w_t</span> <span class="o">=</span> <span class="n">s</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">-</span> <span class="n">t</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="FGAdaHAT.on_train_end-689"><a href="#FGAdaHAT.on_train_end-689"><span class="linenos">689</span></a>
</span><span id="FGAdaHAT.on_train_end-690"><a href="#FGAdaHAT.on_train_end-690"><span class="linenos">690</span></a>                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy</span> <span class="o">==</span> <span class="s2">&quot;quadratic_decrease&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT.on_train_end-691"><a href="#FGAdaHAT.on_train_end-691"><span class="linenos">691</span></a>                    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
</span><span id="FGAdaHAT.on_train_end-692"><a href="#FGAdaHAT.on_train_end-692"><span class="linenos">692</span></a>                        <span class="n">w_t</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">-</span> <span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
</span><span id="FGAdaHAT.on_train_end-693"><a href="#FGAdaHAT.on_train_end-693"><span class="linenos">693</span></a>                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy</span> <span class="o">==</span> <span class="s2">&quot;cubic_decrease&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT.on_train_end-694"><a href="#FGAdaHAT.on_train_end-694"><span class="linenos">694</span></a>                    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
</span><span id="FGAdaHAT.on_train_end-695"><a href="#FGAdaHAT.on_train_end-695"><span class="linenos">695</span></a>                        <span class="n">w_t</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">-</span> <span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">3</span>
</span><span id="FGAdaHAT.on_train_end-696"><a href="#FGAdaHAT.on_train_end-696"><span class="linenos">696</span></a>                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy</span> <span class="o">==</span> <span class="s2">&quot;exponential_decrease&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT.on_train_end-697"><a href="#FGAdaHAT.on_train_end-697"><span class="linenos">697</span></a>                    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
</span><span id="FGAdaHAT.on_train_end-698"><a href="#FGAdaHAT.on_train_end-698"><span class="linenos">698</span></a>                        <span class="n">r</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy_exponential_rate</span>
</span><span id="FGAdaHAT.on_train_end-699"><a href="#FGAdaHAT.on_train_end-699"><span class="linenos">699</span></a>
</span><span id="FGAdaHAT.on_train_end-700"><a href="#FGAdaHAT.on_train_end-700"><span class="linenos">700</span></a>                        <span class="n">w_t</span> <span class="o">=</span> <span class="n">r</span> <span class="o">**</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">-</span> <span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="FGAdaHAT.on_train_end-701"><a href="#FGAdaHAT.on_train_end-701"><span class="linenos">701</span></a>                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy</span> <span class="o">==</span> <span class="s2">&quot;log_decrease&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT.on_train_end-702"><a href="#FGAdaHAT.on_train_end-702"><span class="linenos">702</span></a>                    <span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy_log_base</span>
</span><span id="FGAdaHAT.on_train_end-703"><a href="#FGAdaHAT.on_train_end-703"><span class="linenos">703</span></a>                    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
</span><span id="FGAdaHAT.on_train_end-704"><a href="#FGAdaHAT.on_train_end-704"><span class="linenos">704</span></a>                        <span class="n">w_t</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">-</span> <span class="n">t</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="FGAdaHAT.on_train_end-705"><a href="#FGAdaHAT.on_train_end-705"><span class="linenos">705</span></a>                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">importance_summing_strategy</span> <span class="o">==</span> <span class="s2">&quot;factorial_decrease&quot;</span><span class="p">:</span>
</span><span id="FGAdaHAT.on_train_end-706"><a href="#FGAdaHAT.on_train_end-706"><span class="linenos">706</span></a>                    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
</span><span id="FGAdaHAT.on_train_end-707"><a href="#FGAdaHAT.on_train_end-707"><span class="linenos">707</span></a>                        <span class="n">w_t</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">factorial</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">-</span> <span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="FGAdaHAT.on_train_end-708"><a href="#FGAdaHAT.on_train_end-708"><span class="linenos">708</span></a>                <span class="k">else</span><span class="p">:</span>
</span><span id="FGAdaHAT.on_train_end-709"><a href="#FGAdaHAT.on_train_end-709"><span class="linenos">709</span></a>                    <span class="k">raise</span> <span class="ne">ValueError</span>
</span><span id="FGAdaHAT.on_train_end-710"><a href="#FGAdaHAT.on_train_end-710"><span class="linenos">710</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">summative_importance_for_previous_tasks</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span>
</span><span id="FGAdaHAT.on_train_end-711"><a href="#FGAdaHAT.on_train_end-711"><span class="linenos">711</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">importances</span><span class="p">[</span><span class="n">t</span><span class="p">][</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">*</span> <span class="n">w_t</span>
</span><span id="FGAdaHAT.on_train_end-712"><a href="#FGAdaHAT.on_train_end-712"><span class="linenos">712</span></a>                <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Additionally calculate neuron-wise importance for previous tasks at the end of training each task.</p>
</div>


                            </div>
                            <div id="FGAdaHAT.get_importance_step_layer_weight_abs_sum" class="classattr">
                                        <input id="FGAdaHAT.get_importance_step_layer_weight_abs_sum-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">get_importance_step_layer_weight_abs_sum</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">if_output_weight</span><span class="p">:</span> <span class="nb">bool</span>,</span><span class="param">	<span class="n">reciprocal</span><span class="p">:</span> <span class="nb">bool</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="FGAdaHAT.get_importance_step_layer_weight_abs_sum-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum-714"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum-714"><span class="linenos">714</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_weight_abs_sum</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum-715"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum-715"><span class="linenos">715</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum-716"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum-716"><span class="linenos">716</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum-717"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum-717"><span class="linenos">717</span></a>        <span class="n">if_output_weight</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum-718"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum-718"><span class="linenos">718</span></a>        <span class="n">reciprocal</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum-719"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum-719"><span class="linenos">719</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum-720"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum-720"><span class="linenos">720</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the sum of absolute values of layer input or output weights.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum-721"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum-721"><span class="linenos">721</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum-722"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum-722"><span class="linenos">722</span></a><span class="sd">        **Args:**</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum-723"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum-723"><span class="linenos">723</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum-724"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum-724"><span class="linenos">724</span></a><span class="sd">        - **if_output_weight** (`bool`): whether to use the output weights or input weights.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum-725"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum-725"><span class="linenos">725</span></a><span class="sd">        - **reciprocal** (`bool`): whether to take reciprocal.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum-726"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum-726"><span class="linenos">726</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum-727"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum-727"><span class="linenos">727</span></a><span class="sd">        **Returns:**</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum-728"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum-728"><span class="linenos">728</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum-729"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum-729"><span class="linenos">729</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum-730"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum-730"><span class="linenos">730</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum-731"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum-731"><span class="linenos">731</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum-732"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum-732"><span class="linenos">732</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="n">if_output_weight</span><span class="p">:</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum-733"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum-733"><span class="linenos">733</span></a>            <span class="n">weight_abs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum-734"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum-734"><span class="linenos">734</span></a>            <span class="n">weight_abs_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum-735"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum-735"><span class="linenos">735</span></a>                <span class="n">weight_abs</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum-736"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum-736"><span class="linenos">736</span></a>                <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum-737"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum-737"><span class="linenos">737</span></a>                    <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">weight_abs</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">0</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum-738"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum-738"><span class="linenos">738</span></a>                <span class="p">],</span>  <span class="c1"># sum over the input dimension</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum-739"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum-739"><span class="linenos">739</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum-740"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum-740"><span class="linenos">740</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum-741"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum-741"><span class="linenos">741</span></a>            <span class="n">weight_abs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">next_layer</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum-742"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum-742"><span class="linenos">742</span></a>            <span class="n">weight_abs_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum-743"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum-743"><span class="linenos">743</span></a>                <span class="n">weight_abs</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum-744"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum-744"><span class="linenos">744</span></a>                <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum-745"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum-745"><span class="linenos">745</span></a>                    <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">weight_abs</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum-746"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum-746"><span class="linenos">746</span></a>                <span class="p">],</span>  <span class="c1"># sum over the output dimension</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum-747"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum-747"><span class="linenos">747</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum-748"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum-748"><span class="linenos">748</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum-749"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum-749"><span class="linenos">749</span></a>        <span class="k">if</span> <span class="n">reciprocal</span><span class="p">:</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum-750"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum-750"><span class="linenos">750</span></a>            <span class="n">weight_abs_sum_reciprocal</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">reciprocal</span><span class="p">(</span><span class="n">weight_abs_sum</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum-751"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum-751"><span class="linenos">751</span></a>            <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">weight_abs_sum_reciprocal</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum-752"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum-752"><span class="linenos">752</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum-753"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum-753"><span class="linenos">753</span></a>            <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">weight_abs_sum</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum-754"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum-754"><span class="linenos">754</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum-755"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum-755"><span class="linenos">755</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum-756"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum-756"><span class="linenos">756</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span></pre></div>


            <div class="docstring"><p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the sum of absolute values of layer input or output weights.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>
<li><strong>if_output_weight</strong> (<code>bool</code>): whether to use the output weights or input weights.</li>
<li><strong>reciprocal</strong> (<code>bool</code>): whether to take reciprocal.</li>
</ul>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>
</ul>
</div>


                            </div>
                            <div id="FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum" class="classattr">
                                        <input id="FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">get_importance_step_layer_weight_gradient_abs_sum</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span><span class="p">:</span> <span class="nb">str</span>, </span><span class="param"><span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span>, </span><span class="param"><span class="n">if_output_weight</span><span class="p">:</span> <span class="nb">bool</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-758"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-758"><span class="linenos">758</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_weight_gradient_abs_sum</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-759"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-759"><span class="linenos">759</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-760"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-760"><span class="linenos">760</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-761"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-761"><span class="linenos">761</span></a>        <span class="n">if_output_weight</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-762"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-762"><span class="linenos">762</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-763"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-763"><span class="linenos">763</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the sum of absolute values of gradients of the layer input or output weights.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-764"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-764"><span class="linenos">764</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-765"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-765"><span class="linenos">765</span></a><span class="sd">        **Args:**</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-766"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-766"><span class="linenos">766</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-767"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-767"><span class="linenos">767</span></a><span class="sd">        - **if_output_weight** (`bool`): whether to use the output weights or input weights.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-768"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-768"><span class="linenos">768</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-769"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-769"><span class="linenos">769</span></a><span class="sd">        **Returns:**</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-770"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-770"><span class="linenos">770</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-771"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-771"><span class="linenos">771</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-772"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-772"><span class="linenos">772</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-773"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-773"><span class="linenos">773</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-774"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-774"><span class="linenos">774</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="n">if_output_weight</span><span class="p">:</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-775"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-775"><span class="linenos">775</span></a>            <span class="n">gradient_abs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-776"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-776"><span class="linenos">776</span></a>            <span class="n">gradient_abs_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-777"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-777"><span class="linenos">777</span></a>                <span class="n">gradient_abs</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-778"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-778"><span class="linenos">778</span></a>                <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-779"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-779"><span class="linenos">779</span></a>                    <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">gradient_abs</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">0</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-780"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-780"><span class="linenos">780</span></a>                <span class="p">],</span>  <span class="c1"># sum over the input dimension</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-781"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-781"><span class="linenos">781</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-782"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-782"><span class="linenos">782</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-783"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-783"><span class="linenos">783</span></a>            <span class="n">gradient_abs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">next_layer</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-784"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-784"><span class="linenos">784</span></a>            <span class="n">gradient_abs_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-785"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-785"><span class="linenos">785</span></a>                <span class="n">gradient_abs</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-786"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-786"><span class="linenos">786</span></a>                <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-787"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-787"><span class="linenos">787</span></a>                    <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">gradient_abs</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-788"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-788"><span class="linenos">788</span></a>                <span class="p">],</span>  <span class="c1"># sum over the output dimension</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-789"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-789"><span class="linenos">789</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-790"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-790"><span class="linenos">790</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-791"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-791"><span class="linenos">791</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">gradient_abs_sum</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-792"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-792"><span class="linenos">792</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-793"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-793"><span class="linenos">793</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-794"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum-794"><span class="linenos">794</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span></pre></div>


            <div class="docstring"><p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the sum of absolute values of gradients of the layer input or output weights.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>
<li><strong>if_output_weight</strong> (<code>bool</code>): whether to use the output weights or input weights.</li>
</ul>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>
</ul>
</div>


                            </div>
                            <div id="FGAdaHAT.get_importance_step_layer_activation_abs" class="classattr">
                                        <input id="FGAdaHAT.get_importance_step_layer_activation_abs-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">get_importance_step_layer_activation_abs</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span><span class="p">:</span> <span class="nb">str</span>, </span><span class="param"><span class="n">activation</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="FGAdaHAT.get_importance_step_layer_activation_abs-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FGAdaHAT.get_importance_step_layer_activation_abs"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FGAdaHAT.get_importance_step_layer_activation_abs-796"><a href="#FGAdaHAT.get_importance_step_layer_activation_abs-796"><span class="linenos">796</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_activation_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_activation_abs-797"><a href="#FGAdaHAT.get_importance_step_layer_activation_abs-797"><span class="linenos">797</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_activation_abs-798"><a href="#FGAdaHAT.get_importance_step_layer_activation_abs-798"><span class="linenos">798</span></a>        <span class="n">activation</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_activation_abs-799"><a href="#FGAdaHAT.get_importance_step_layer_activation_abs-799"><span class="linenos">799</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="FGAdaHAT.get_importance_step_layer_activation_abs-800"><a href="#FGAdaHAT.get_importance_step_layer_activation_abs-800"><span class="linenos">800</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute value of activation of the layer. This is our own implementation of [Layer Activation](https://captum.ai/api/layer.html#layer-activation) in Captum.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_activation_abs-801"><a href="#FGAdaHAT.get_importance_step_layer_activation_abs-801"><span class="linenos">801</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_activation_abs-802"><a href="#FGAdaHAT.get_importance_step_layer_activation_abs-802"><span class="linenos">802</span></a><span class="sd">        **Args:**</span>
</span><span id="FGAdaHAT.get_importance_step_layer_activation_abs-803"><a href="#FGAdaHAT.get_importance_step_layer_activation_abs-803"><span class="linenos">803</span></a><span class="sd">        - **activation** (`Tensor`): the activation tensor of the layer. It has the same size of (number of units, ).</span>
</span><span id="FGAdaHAT.get_importance_step_layer_activation_abs-804"><a href="#FGAdaHAT.get_importance_step_layer_activation_abs-804"><span class="linenos">804</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_activation_abs-805"><a href="#FGAdaHAT.get_importance_step_layer_activation_abs-805"><span class="linenos">805</span></a><span class="sd">        **Returns:**</span>
</span><span id="FGAdaHAT.get_importance_step_layer_activation_abs-806"><a href="#FGAdaHAT.get_importance_step_layer_activation_abs-806"><span class="linenos">806</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_activation_abs-807"><a href="#FGAdaHAT.get_importance_step_layer_activation_abs-807"><span class="linenos">807</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT.get_importance_step_layer_activation_abs-808"><a href="#FGAdaHAT.get_importance_step_layer_activation_abs-808"><span class="linenos">808</span></a>        <span class="n">activation_abs_batch_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_activation_abs-809"><a href="#FGAdaHAT.get_importance_step_layer_activation_abs-809"><span class="linenos">809</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">activation</span><span class="p">),</span>
</span><span id="FGAdaHAT.get_importance_step_layer_activation_abs-810"><a href="#FGAdaHAT.get_importance_step_layer_activation_abs-810"><span class="linenos">810</span></a>            <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT.get_importance_step_layer_activation_abs-811"><a href="#FGAdaHAT.get_importance_step_layer_activation_abs-811"><span class="linenos">811</span></a>                <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">activation</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="FGAdaHAT.get_importance_step_layer_activation_abs-812"><a href="#FGAdaHAT.get_importance_step_layer_activation_abs-812"><span class="linenos">812</span></a>            <span class="p">],</span>  <span class="c1"># average the features over batch samples</span>
</span><span id="FGAdaHAT.get_importance_step_layer_activation_abs-813"><a href="#FGAdaHAT.get_importance_step_layer_activation_abs-813"><span class="linenos">813</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_activation_abs-814"><a href="#FGAdaHAT.get_importance_step_layer_activation_abs-814"><span class="linenos">814</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">activation_abs_batch_mean</span>
</span><span id="FGAdaHAT.get_importance_step_layer_activation_abs-815"><a href="#FGAdaHAT.get_importance_step_layer_activation_abs-815"><span class="linenos">815</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="FGAdaHAT.get_importance_step_layer_activation_abs-816"><a href="#FGAdaHAT.get_importance_step_layer_activation_abs-816"><span class="linenos">816</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_activation_abs-817"><a href="#FGAdaHAT.get_importance_step_layer_activation_abs-817"><span class="linenos">817</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span></pre></div>


            <div class="docstring"><p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute value of activation of the layer. This is our own implementation of <a href="https://captum.ai/api/layer.html#layer-activation">Layer Activation</a> in Captum.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>activation</strong> (<code>Tensor</code>): the activation tensor of the layer. It has the same size of (number of units, ).</li>
</ul>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>
</ul>
</div>


                            </div>
                            <div id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs" class="classattr">
                                        <input id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">get_importance_step_layer_weight_abs_sum_x_activation_abs</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">activation</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">if_output_weight</span><span class="p">:</span> <span class="nb">bool</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-819"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-819"><span class="linenos">819</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_weight_abs_sum_x_activation_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-820"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-820"><span class="linenos">820</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-821"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-821"><span class="linenos">821</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-822"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-822"><span class="linenos">822</span></a>        <span class="n">activation</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-823"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-823"><span class="linenos">823</span></a>        <span class="n">if_output_weight</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-824"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-824"><span class="linenos">824</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-825"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-825"><span class="linenos">825</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the sum of absolute values of layer input / output weights multiplied by absolute values of activation. The input weights version is equal to the contribution utility in [CBP](https://www.nature.com/articles/s41586-024-07711-7).</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-826"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-826"><span class="linenos">826</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-827"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-827"><span class="linenos">827</span></a><span class="sd">        **Args:**</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-828"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-828"><span class="linenos">828</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-829"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-829"><span class="linenos">829</span></a><span class="sd">        - **activation** (`Tensor`): the activation tensor of the layer. It has the same size of (number of units, ).</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-830"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-830"><span class="linenos">830</span></a><span class="sd">        - **if_output_weight** (`bool`): whether to use the output weights or input weights.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-831"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-831"><span class="linenos">831</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-832"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-832"><span class="linenos">832</span></a><span class="sd">        **Returns:**</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-833"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-833"><span class="linenos">833</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-834"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-834"><span class="linenos">834</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-835"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-835"><span class="linenos">835</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-836"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-836"><span class="linenos">836</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-837"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-837"><span class="linenos">837</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="n">if_output_weight</span><span class="p">:</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-838"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-838"><span class="linenos">838</span></a>            <span class="n">weight_abs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-839"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-839"><span class="linenos">839</span></a>            <span class="n">weight_abs_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-840"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-840"><span class="linenos">840</span></a>                <span class="n">weight_abs</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-841"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-841"><span class="linenos">841</span></a>                <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-842"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-842"><span class="linenos">842</span></a>                    <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">weight_abs</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">0</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-843"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-843"><span class="linenos">843</span></a>                <span class="p">],</span>  <span class="c1"># sum over the input dimension</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-844"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-844"><span class="linenos">844</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-845"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-845"><span class="linenos">845</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-846"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-846"><span class="linenos">846</span></a>            <span class="n">weight_abs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">next_layer</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-847"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-847"><span class="linenos">847</span></a>            <span class="n">weight_abs_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-848"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-848"><span class="linenos">848</span></a>                <span class="n">weight_abs</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-849"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-849"><span class="linenos">849</span></a>                <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-850"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-850"><span class="linenos">850</span></a>                    <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">weight_abs</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-851"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-851"><span class="linenos">851</span></a>                <span class="p">],</span>  <span class="c1"># sum over the output dimension</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-852"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-852"><span class="linenos">852</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-853"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-853"><span class="linenos">853</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-854"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-854"><span class="linenos">854</span></a>        <span class="n">activation_abs_batch_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-855"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-855"><span class="linenos">855</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">activation</span><span class="p">),</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-856"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-856"><span class="linenos">856</span></a>            <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-857"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-857"><span class="linenos">857</span></a>                <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">activation</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-858"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-858"><span class="linenos">858</span></a>            <span class="p">],</span>  <span class="c1"># average the features over batch samples</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-859"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-859"><span class="linenos">859</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-860"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-860"><span class="linenos">860</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-861"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-861"><span class="linenos">861</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">weight_abs_sum</span> <span class="o">*</span> <span class="n">activation_abs_batch_mean</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-862"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-862"><span class="linenos">862</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-863"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-863"><span class="linenos">863</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-864"><a href="#FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs-864"><span class="linenos">864</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span></pre></div>


            <div class="docstring"><p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the sum of absolute values of layer input / output weights multiplied by absolute values of activation. The input weights version is equal to the contribution utility in <a href="https://www.nature.com/articles/s41586-024-07711-7">CBP</a>.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>
<li><strong>activation</strong> (<code>Tensor</code>): the activation tensor of the layer. It has the same size of (number of units, ).</li>
<li><strong>if_output_weight</strong> (<code>bool</code>): whether to use the output weights or input weights.</li>
</ul>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>
</ul>
</div>


                            </div>
                            <div id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs" class="classattr">
                                        <input id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">get_importance_step_layer_gradient_x_activation_abs</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>,</span><span class="param">	<span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-866"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-866"><span class="linenos">866</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_gradient_x_activation_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-867"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-867"><span class="linenos">867</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-868"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-868"><span class="linenos">868</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-869"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-869"><span class="linenos">869</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-870"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-870"><span class="linenos">870</span></a>        <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-871"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-871"><span class="linenos">871</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-872"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-872"><span class="linenos">872</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-873"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-873"><span class="linenos">873</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-874"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-874"><span class="linenos">874</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of the gradient of layer activation multiplied by the activation. We implement this using [Layer Gradient X Activation](https://captum.ai/api/layer.html#layer-gradient-x-activation) in Captum.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-875"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-875"><span class="linenos">875</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-876"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-876"><span class="linenos">876</span></a><span class="sd">        **Args:**</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-877"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-877"><span class="linenos">877</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-878"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-878"><span class="linenos">878</span></a><span class="sd">        - **input** (`Tensor` | `tuple[Tensor, ...]`): the input batch of the training step.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-879"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-879"><span class="linenos">879</span></a><span class="sd">        - **target** (`Tensor` | `None`): the target batch of the training step.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-880"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-880"><span class="linenos">880</span></a><span class="sd">        - **batch_idx** (`int`): the index of the current batch. This is an argument of the forward function during training.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-881"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-881"><span class="linenos">881</span></a><span class="sd">        - **num_batches** (`int`): the number of batches in the training step. This is an argument of the forward function during training.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-882"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-882"><span class="linenos">882</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-883"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-883"><span class="linenos">883</span></a><span class="sd">        **Returns:**</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-884"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-884"><span class="linenos">884</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-885"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-885"><span class="linenos">885</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-886"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-886"><span class="linenos">886</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-887"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-887"><span class="linenos">887</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-888"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-888"><span class="linenos">888</span></a>        <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-889"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-889"><span class="linenos">889</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-890"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-890"><span class="linenos">890</span></a>        <span class="c1"># initialize the Layer Gradient X Activation object</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-891"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-891"><span class="linenos">891</span></a>        <span class="n">layer_gradient_x_activation</span> <span class="o">=</span> <span class="n">LayerGradientXActivation</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-892"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-892"><span class="linenos">892</span></a>            <span class="n">forward_func</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="n">layer</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-893"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-893"><span class="linenos">893</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-894"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-894"><span class="linenos">894</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-895"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-895"><span class="linenos">895</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-896"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-896"><span class="linenos">896</span></a>        <span class="c1"># calculate layer attribution of the step</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-897"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-897"><span class="linenos">897</span></a>        <span class="n">attribution</span> <span class="o">=</span> <span class="n">layer_gradient_x_activation</span><span class="o">.</span><span class="n">attribute</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-898"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-898"><span class="linenos">898</span></a>            <span class="n">inputs</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-899"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-899"><span class="linenos">899</span></a>            <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-900"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-900"><span class="linenos">900</span></a>            <span class="n">additional_forward_args</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">num_batches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">),</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-901"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-901"><span class="linenos">901</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-902"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-902"><span class="linenos">902</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-903"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-903"><span class="linenos">903</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-904"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-904"><span class="linenos">904</span></a>        <span class="n">attribution_abs_batch_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-905"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-905"><span class="linenos">905</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">attribution</span><span class="p">),</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-906"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-906"><span class="linenos">906</span></a>            <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-907"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-907"><span class="linenos">907</span></a>                <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">attribution</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-908"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-908"><span class="linenos">908</span></a>            <span class="p">],</span>  <span class="c1"># average the features over batch samples</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-909"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-909"><span class="linenos">909</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-910"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-910"><span class="linenos">910</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-911"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-911"><span class="linenos">911</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">attribution_abs_batch_mean</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-912"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-912"><span class="linenos">912</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-913"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-913"><span class="linenos">913</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-914"><a href="#FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs-914"><span class="linenos">914</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span></pre></div>


            <div class="docstring"><p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of the gradient of layer activation multiplied by the activation. We implement this using <a href="https://captum.ai/api/layer.html#layer-gradient-x-activation">Layer Gradient X Activation</a> in Captum.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>
<li><strong>input</strong> (<code>Tensor</code> | <code>tuple[Tensor, ...]</code>): the input batch of the training step.</li>
<li><strong>target</strong> (<code>Tensor</code> | <code>None</code>): the target batch of the training step.</li>
<li><strong>batch_idx</strong> (<code>int</code>): the index of the current batch. This is an argument of the forward function during training.</li>
<li><strong>num_batches</strong> (<code>int</code>): the number of batches in the training step. This is an argument of the forward function during training.</li>
</ul>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>
</ul>
</div>


                            </div>
                            <div id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum" class="classattr">
                                        <input id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">get_importance_step_layer_weight_gradient_square_sum</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">activation</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">if_output_weight</span><span class="p">:</span> <span class="nb">bool</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-916"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-916"><span class="linenos">916</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_weight_gradient_square_sum</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-917"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-917"><span class="linenos">917</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-918"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-918"><span class="linenos">918</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-919"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-919"><span class="linenos">919</span></a>        <span class="n">activation</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-920"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-920"><span class="linenos">920</span></a>        <span class="n">if_output_weight</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-921"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-921"><span class="linenos">921</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-922"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-922"><span class="linenos">922</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the sum of layer weight gradient squares. The weight gradient square is equal to fisher information in [EWC](https://www.pnas.org/doi/10.1073/pnas.1611835114).</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-923"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-923"><span class="linenos">923</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-924"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-924"><span class="linenos">924</span></a><span class="sd">        **Args:**</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-925"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-925"><span class="linenos">925</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-926"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-926"><span class="linenos">926</span></a><span class="sd">        - **activation** (`Tensor`): the activation tensor of the layer. It has the same size of (number of units, ).</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-927"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-927"><span class="linenos">927</span></a><span class="sd">        - **if_output_weight** (`bool`): whether to use the output weights or input weights.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-928"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-928"><span class="linenos">928</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-929"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-929"><span class="linenos">929</span></a><span class="sd">        **Returns:**</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-930"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-930"><span class="linenos">930</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-931"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-931"><span class="linenos">931</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-932"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-932"><span class="linenos">932</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-933"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-933"><span class="linenos">933</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-934"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-934"><span class="linenos">934</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="n">if_output_weight</span><span class="p">:</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-935"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-935"><span class="linenos">935</span></a>            <span class="n">gradient_square</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">**</span><span class="mi">2</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-936"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-936"><span class="linenos">936</span></a>            <span class="n">gradient_square_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-937"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-937"><span class="linenos">937</span></a>                <span class="n">gradient_square</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-938"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-938"><span class="linenos">938</span></a>                <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-939"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-939"><span class="linenos">939</span></a>                    <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">gradient_square</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">0</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-940"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-940"><span class="linenos">940</span></a>                <span class="p">],</span>  <span class="c1"># sum over the input dimension</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-941"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-941"><span class="linenos">941</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-942"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-942"><span class="linenos">942</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-943"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-943"><span class="linenos">943</span></a>            <span class="n">gradient_square</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">next_layer</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">**</span><span class="mi">2</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-944"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-944"><span class="linenos">944</span></a>            <span class="n">gradient_square_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-945"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-945"><span class="linenos">945</span></a>                <span class="n">gradient_square</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-946"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-946"><span class="linenos">946</span></a>                <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-947"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-947"><span class="linenos">947</span></a>                    <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">gradient_square</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-948"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-948"><span class="linenos">948</span></a>                <span class="p">],</span>  <span class="c1"># sum over the output dimension</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-949"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-949"><span class="linenos">949</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-950"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-950"><span class="linenos">950</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-951"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-951"><span class="linenos">951</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">gradient_square_sum</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-952"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-952"><span class="linenos">952</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-953"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-953"><span class="linenos">953</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-954"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum-954"><span class="linenos">954</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span></pre></div>


            <div class="docstring"><p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the sum of layer weight gradient squares. The weight gradient square is equal to fisher information in <a href="https://www.pnas.org/doi/10.1073/pnas.1611835114">EWC</a>.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>
<li><strong>activation</strong> (<code>Tensor</code>): the activation tensor of the layer. It has the same size of (number of units, ).</li>
<li><strong>if_output_weight</strong> (<code>bool</code>): whether to use the output weights or input weights.</li>
</ul>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>
</ul>
</div>


                            </div>
                            <div id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs" class="classattr">
                                        <input id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">get_importance_step_layer_weight_gradient_square_sum_x_activation_abs</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">activation</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">if_output_weight</span><span class="p">:</span> <span class="nb">bool</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-956"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-956"><span class="linenos"> 956</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_weight_gradient_square_sum_x_activation_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-957"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-957"><span class="linenos"> 957</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-958"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-958"><span class="linenos"> 958</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-959"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-959"><span class="linenos"> 959</span></a>        <span class="n">activation</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-960"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-960"><span class="linenos"> 960</span></a>        <span class="n">if_output_weight</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-961"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-961"><span class="linenos"> 961</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-962"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-962"><span class="linenos"> 962</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the sum of layer weight gradient squares multiplied by absolute values of activation. The weight gradient square is equal to fisher information in [EWC](https://www.pnas.org/doi/10.1073/pnas.1611835114).</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-963"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-963"><span class="linenos"> 963</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-964"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-964"><span class="linenos"> 964</span></a><span class="sd">        **Args:**</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-965"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-965"><span class="linenos"> 965</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-966"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-966"><span class="linenos"> 966</span></a><span class="sd">        - **activation** (`Tensor`): the activation tensor of the layer. It has the same size of (number of units, ).</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-967"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-967"><span class="linenos"> 967</span></a><span class="sd">        - **if_output_weight** (`bool`): whether to use the output weights or input weights.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-968"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-968"><span class="linenos"> 968</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-969"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-969"><span class="linenos"> 969</span></a><span class="sd">        **Returns:**</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-970"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-970"><span class="linenos"> 970</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-971"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-971"><span class="linenos"> 971</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-972"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-972"><span class="linenos"> 972</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-973"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-973"><span class="linenos"> 973</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-974"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-974"><span class="linenos"> 974</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="n">if_output_weight</span><span class="p">:</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-975"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-975"><span class="linenos"> 975</span></a>            <span class="n">gradient_square</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">**</span><span class="mi">2</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-976"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-976"><span class="linenos"> 976</span></a>            <span class="n">gradient_square_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-977"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-977"><span class="linenos"> 977</span></a>                <span class="n">gradient_square</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-978"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-978"><span class="linenos"> 978</span></a>                <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-979"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-979"><span class="linenos"> 979</span></a>                    <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">gradient_square</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">0</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-980"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-980"><span class="linenos"> 980</span></a>                <span class="p">],</span>  <span class="c1"># sum over the input dimension</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-981"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-981"><span class="linenos"> 981</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-982"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-982"><span class="linenos"> 982</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-983"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-983"><span class="linenos"> 983</span></a>            <span class="n">gradient_square</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">next_layer</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">**</span><span class="mi">2</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-984"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-984"><span class="linenos"> 984</span></a>            <span class="n">gradient_square_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-985"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-985"><span class="linenos"> 985</span></a>                <span class="n">gradient_square</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-986"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-986"><span class="linenos"> 986</span></a>                <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-987"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-987"><span class="linenos"> 987</span></a>                    <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">gradient_square</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-988"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-988"><span class="linenos"> 988</span></a>                <span class="p">],</span>  <span class="c1"># sum over the output dimension</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-989"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-989"><span class="linenos"> 989</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-990"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-990"><span class="linenos"> 990</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-991"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-991"><span class="linenos"> 991</span></a>        <span class="n">activation_abs_batch_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-992"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-992"><span class="linenos"> 992</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">activation</span><span class="p">),</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-993"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-993"><span class="linenos"> 993</span></a>            <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-994"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-994"><span class="linenos"> 994</span></a>                <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">activation</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-995"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-995"><span class="linenos"> 995</span></a>            <span class="p">],</span>  <span class="c1"># average the features over batch samples</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-996"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-996"><span class="linenos"> 996</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-997"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-997"><span class="linenos"> 997</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-998"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-998"><span class="linenos"> 998</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">gradient_square_sum</span> <span class="o">*</span> <span class="n">activation_abs_batch_mean</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-999"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-999"><span class="linenos"> 999</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-1000"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-1000"><span class="linenos">1000</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-1001"><a href="#FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs-1001"><span class="linenos">1001</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span></pre></div>


            <div class="docstring"><p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the sum of layer weight gradient squares multiplied by absolute values of activation. The weight gradient square is equal to fisher information in <a href="https://www.pnas.org/doi/10.1073/pnas.1611835114">EWC</a>.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>
<li><strong>activation</strong> (<code>Tensor</code>): the activation tensor of the layer. It has the same size of (number of units, ).</li>
<li><strong>if_output_weight</strong> (<code>bool</code>): whether to use the output weights or input weights.</li>
</ul>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>
</ul>
</div>


                            </div>
                            <div id="FGAdaHAT.get_importance_step_layer_conductance_abs" class="classattr">
                                        <input id="FGAdaHAT.get_importance_step_layer_conductance_abs-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">get_importance_step_layer_conductance_abs</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>,</span><span class="param">	<span class="n">baselines</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>,</span><span class="param">	<span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="FGAdaHAT.get_importance_step_layer_conductance_abs-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FGAdaHAT.get_importance_step_layer_conductance_abs"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1003"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1003"><span class="linenos">1003</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_conductance_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1004"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1004"><span class="linenos">1004</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1005"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1005"><span class="linenos">1005</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1006"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1006"><span class="linenos">1006</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1007"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1007"><span class="linenos">1007</span></a>        <span class="n">baselines</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1008"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1008"><span class="linenos">1008</span></a>        <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1009"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1009"><span class="linenos">1009</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1010"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1010"><span class="linenos">1010</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1011"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1011"><span class="linenos">1011</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1012"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1012"><span class="linenos">1012</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of [conductance](https://openreview.net/forum?id=SylKoo0cKm). We implement this using [Layer Conductance](https://captum.ai/api/layer.html#layer-conductance) in Captum.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1013"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1013"><span class="linenos">1013</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1014"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1014"><span class="linenos">1014</span></a><span class="sd">        **Args:**</span>
</span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1015"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1015"><span class="linenos">1015</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1016"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1016"><span class="linenos">1016</span></a><span class="sd">        - **input** (`Tensor` | `tuple[Tensor, ...]`): the input batch of the training step.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1017"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1017"><span class="linenos">1017</span></a><span class="sd">        - **baselines** (`None` | `int` | `float` | `Tensor` | `tuple[int | float | Tensor, ...]`): starting point from which integral is computed in this method. Please refer to the [Captum documentation](https://captum.ai/api/layer.html#captum.attr.LayerConductance.attribute) for more details.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1018"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1018"><span class="linenos">1018</span></a><span class="sd">        - **target** (`Tensor` | `None`): the target batch of the training step.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1019"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1019"><span class="linenos">1019</span></a><span class="sd">        - **batch_idx** (`int`): the index of the current batch. This is an argument of the forward function during training.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1020"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1020"><span class="linenos">1020</span></a><span class="sd">        - **num_batches** (`int`): the number of batches in the training step. This is an argument of the forward function during training.- **mask** (`Tensor`): the mask tensor of the layer. It has the same size as the feature tensor with size (number of units, ).</span>
</span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1021"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1021"><span class="linenos">1021</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1022"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1022"><span class="linenos">1022</span></a><span class="sd">        **Returns:**</span>
</span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1023"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1023"><span class="linenos">1023</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1024"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1024"><span class="linenos">1024</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1025"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1025"><span class="linenos">1025</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1026"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1026"><span class="linenos">1026</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1027"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1027"><span class="linenos">1027</span></a>        <span class="c1"># initialize the Layer Conductance object</span>
</span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1028"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1028"><span class="linenos">1028</span></a>        <span class="n">layer_conductance</span> <span class="o">=</span> <span class="n">LayerConductance</span><span class="p">(</span><span class="n">forward_func</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="n">layer</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1029"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1029"><span class="linenos">1029</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1030"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1030"><span class="linenos">1030</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1031"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1031"><span class="linenos">1031</span></a>        <span class="c1"># calculate layer attribution of the step</span>
</span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1032"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1032"><span class="linenos">1032</span></a>        <span class="n">attribution</span> <span class="o">=</span> <span class="n">layer_conductance</span><span class="o">.</span><span class="n">attribute</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1033"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1033"><span class="linenos">1033</span></a>            <span class="n">inputs</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1034"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1034"><span class="linenos">1034</span></a>            <span class="n">baselines</span><span class="o">=</span><span class="n">baselines</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1035"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1035"><span class="linenos">1035</span></a>            <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1036"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1036"><span class="linenos">1036</span></a>            <span class="n">additional_forward_args</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">num_batches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">),</span>
</span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1037"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1037"><span class="linenos">1037</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1038"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1038"><span class="linenos">1038</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1039"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1039"><span class="linenos">1039</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1040"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1040"><span class="linenos">1040</span></a>        <span class="n">attribution_abs_batch_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1041"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1041"><span class="linenos">1041</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">attribution</span><span class="p">),</span>
</span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1042"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1042"><span class="linenos">1042</span></a>            <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1043"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1043"><span class="linenos">1043</span></a>                <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">attribution</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1044"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1044"><span class="linenos">1044</span></a>            <span class="p">],</span>  <span class="c1"># average the features over batch samples</span>
</span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1045"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1045"><span class="linenos">1045</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1046"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1046"><span class="linenos">1046</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1047"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1047"><span class="linenos">1047</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">attribution_abs_batch_mean</span>
</span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1048"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1048"><span class="linenos">1048</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1049"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1049"><span class="linenos">1049</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_conductance_abs-1050"><a href="#FGAdaHAT.get_importance_step_layer_conductance_abs-1050"><span class="linenos">1050</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span></pre></div>


            <div class="docstring"><p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of <a href="https://openreview.net/forum?id=SylKoo0cKm">conductance</a>. We implement this using <a href="https://captum.ai/api/layer.html#layer-conductance">Layer Conductance</a> in Captum.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>
<li><strong>input</strong> (<code>Tensor</code> | <code>tuple[Tensor, ...]</code>): the input batch of the training step.</li>
<li><strong>baselines</strong> (<code>None</code> | <code>int</code> | <code><a href="#FGAdaHAT.float">float</a></code> | <code>Tensor</code> | <code>tuple[int | float | Tensor, ...]</code>): starting point from which integral is computed in this method. Please refer to the <a href="https://captum.ai/api/layer.html#captum.attr.LayerConductance.attribute">Captum documentation</a> for more details.</li>
<li><strong>target</strong> (<code>Tensor</code> | <code>None</code>): the target batch of the training step.</li>
<li><strong>batch_idx</strong> (<code>int</code>): the index of the current batch. This is an argument of the forward function during training.</li>
<li><strong>num_batches</strong> (<code>int</code>): the number of batches in the training step. This is an argument of the forward function during training.- <strong>mask</strong> (<code>Tensor</code>): the mask tensor of the layer. It has the same size as the feature tensor with size (number of units, ).</li>
</ul>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>
</ul>
</div>


                            </div>
                            <div id="FGAdaHAT.get_importance_step_layer_internal_influence_abs" class="classattr">
                                        <input id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">get_importance_step_layer_internal_influence_abs</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>,</span><span class="param">	<span class="n">baselines</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>,</span><span class="param">	<span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="FGAdaHAT.get_importance_step_layer_internal_influence_abs-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1052"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1052"><span class="linenos">1052</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_internal_influence_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1053"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1053"><span class="linenos">1053</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1054"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1054"><span class="linenos">1054</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1055"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1055"><span class="linenos">1055</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1056"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1056"><span class="linenos">1056</span></a>        <span class="n">baselines</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1057"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1057"><span class="linenos">1057</span></a>        <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1058"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1058"><span class="linenos">1058</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1059"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1059"><span class="linenos">1059</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1060"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1060"><span class="linenos">1060</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1061"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1061"><span class="linenos">1061</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of [internal influence](https://openreview.net/forum?id=SJPpHzW0-). We implement this using [Internal Influence](https://captum.ai/api/layer.html#internal-influence) in Captum.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1062"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1062"><span class="linenos">1062</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1063"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1063"><span class="linenos">1063</span></a><span class="sd">        **Args:**</span>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1064"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1064"><span class="linenos">1064</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1065"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1065"><span class="linenos">1065</span></a><span class="sd">        - **input** (`Tensor` | `tuple[Tensor, ...]`): the input batch of the training step.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1066"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1066"><span class="linenos">1066</span></a><span class="sd">        - **baselines** (`None` | `int` | `float` | `Tensor` | `tuple[int | float | Tensor, ...]`): starting point from which integral is computed in this method. Please refer to the [Captum documentation](https://captum.ai/api/layer.html#captum.attr.InternalInfluence.attribute) for more details.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1067"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1067"><span class="linenos">1067</span></a><span class="sd">        - **target** (`Tensor` | `None`): the target batch of the training step.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1068"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1068"><span class="linenos">1068</span></a><span class="sd">        - **batch_idx** (`int`): the index of the current batch. This is an argument of the forward function during training.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1069"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1069"><span class="linenos">1069</span></a><span class="sd">        - **num_batches** (`int`): the number of batches in the training step. This is an argument of the forward function during training.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1070"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1070"><span class="linenos">1070</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1071"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1071"><span class="linenos">1071</span></a><span class="sd">        **Returns:**</span>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1072"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1072"><span class="linenos">1072</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1073"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1073"><span class="linenos">1073</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1074"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1074"><span class="linenos">1074</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1075"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1075"><span class="linenos">1075</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1076"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1076"><span class="linenos">1076</span></a>        <span class="c1"># initialize the Internal Influence object</span>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1077"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1077"><span class="linenos">1077</span></a>        <span class="n">internal_influence</span> <span class="o">=</span> <span class="n">InternalInfluence</span><span class="p">(</span><span class="n">forward_func</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="n">layer</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1078"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1078"><span class="linenos">1078</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1079"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1079"><span class="linenos">1079</span></a>        <span class="c1"># convert the target to long type to avoid error</span>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1080"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1080"><span class="linenos">1080</span></a>        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">long</span><span class="p">()</span> <span class="k">if</span> <span class="n">target</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1081"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1081"><span class="linenos">1081</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1082"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1082"><span class="linenos">1082</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1083"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1083"><span class="linenos">1083</span></a>        <span class="c1"># calculate layer attribution of the step</span>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1084"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1084"><span class="linenos">1084</span></a>        <span class="n">attribution</span> <span class="o">=</span> <span class="n">internal_influence</span><span class="o">.</span><span class="n">attribute</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1085"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1085"><span class="linenos">1085</span></a>            <span class="n">inputs</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1086"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1086"><span class="linenos">1086</span></a>            <span class="n">baselines</span><span class="o">=</span><span class="n">baselines</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1087"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1087"><span class="linenos">1087</span></a>            <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1088"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1088"><span class="linenos">1088</span></a>            <span class="n">additional_forward_args</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">num_batches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">),</span>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1089"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1089"><span class="linenos">1089</span></a>            <span class="n">n_steps</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>  <span class="c1"># set 10 instead of default 50 to accelerate the computation</span>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1090"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1090"><span class="linenos">1090</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1091"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1091"><span class="linenos">1091</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1092"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1092"><span class="linenos">1092</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1093"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1093"><span class="linenos">1093</span></a>        <span class="n">attribution_abs_batch_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1094"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1094"><span class="linenos">1094</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">attribution</span><span class="p">),</span>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1095"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1095"><span class="linenos">1095</span></a>            <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1096"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1096"><span class="linenos">1096</span></a>                <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">attribution</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1097"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1097"><span class="linenos">1097</span></a>            <span class="p">],</span>  <span class="c1"># average the features over batch samples</span>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1098"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1098"><span class="linenos">1098</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1099"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1099"><span class="linenos">1099</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1100"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1100"><span class="linenos">1100</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">attribution_abs_batch_mean</span>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1101"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1101"><span class="linenos">1101</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1102"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1102"><span class="linenos">1102</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_internal_influence_abs-1103"><a href="#FGAdaHAT.get_importance_step_layer_internal_influence_abs-1103"><span class="linenos">1103</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span></pre></div>


            <div class="docstring"><p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of <a href="https://openreview.net/forum?id=SJPpHzW0-">internal influence</a>. We implement this using <a href="https://captum.ai/api/layer.html#internal-influence">Internal Influence</a> in Captum.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>
<li><strong>input</strong> (<code>Tensor</code> | <code>tuple[Tensor, ...]</code>): the input batch of the training step.</li>
<li><strong>baselines</strong> (<code>None</code> | <code>int</code> | <code><a href="#FGAdaHAT.float">float</a></code> | <code>Tensor</code> | <code>tuple[int | float | Tensor, ...]</code>): starting point from which integral is computed in this method. Please refer to the <a href="https://captum.ai/api/layer.html#captum.attr.InternalInfluence.attribute">Captum documentation</a> for more details.</li>
<li><strong>target</strong> (<code>Tensor</code> | <code>None</code>): the target batch of the training step.</li>
<li><strong>batch_idx</strong> (<code>int</code>): the index of the current batch. This is an argument of the forward function during training.</li>
<li><strong>num_batches</strong> (<code>int</code>): the number of batches in the training step. This is an argument of the forward function during training.</li>
</ul>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>
</ul>
</div>


                            </div>
                            <div id="FGAdaHAT.get_importance_step_layer_gradcam_abs" class="classattr">
                                        <input id="FGAdaHAT.get_importance_step_layer_gradcam_abs-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">get_importance_step_layer_gradcam_abs</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>,</span><span class="param">	<span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="FGAdaHAT.get_importance_step_layer_gradcam_abs-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FGAdaHAT.get_importance_step_layer_gradcam_abs"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FGAdaHAT.get_importance_step_layer_gradcam_abs-1105"><a href="#FGAdaHAT.get_importance_step_layer_gradcam_abs-1105"><span class="linenos">1105</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_gradcam_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradcam_abs-1106"><a href="#FGAdaHAT.get_importance_step_layer_gradcam_abs-1106"><span class="linenos">1106</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradcam_abs-1107"><a href="#FGAdaHAT.get_importance_step_layer_gradcam_abs-1107"><span class="linenos">1107</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradcam_abs-1108"><a href="#FGAdaHAT.get_importance_step_layer_gradcam_abs-1108"><span class="linenos">1108</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradcam_abs-1109"><a href="#FGAdaHAT.get_importance_step_layer_gradcam_abs-1109"><span class="linenos">1109</span></a>        <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradcam_abs-1110"><a href="#FGAdaHAT.get_importance_step_layer_gradcam_abs-1110"><span class="linenos">1110</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradcam_abs-1111"><a href="#FGAdaHAT.get_importance_step_layer_gradcam_abs-1111"><span class="linenos">1111</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradcam_abs-1112"><a href="#FGAdaHAT.get_importance_step_layer_gradcam_abs-1112"><span class="linenos">1112</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradcam_abs-1113"><a href="#FGAdaHAT.get_importance_step_layer_gradcam_abs-1113"><span class="linenos">1113</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of [Grad-CAM](https://openreview.net/forum?id=SJPpHzW0-). We implement this using [Layer Grad-CAM](https://captum.ai/api/layer.html#gradcam) in Captum.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradcam_abs-1114"><a href="#FGAdaHAT.get_importance_step_layer_gradcam_abs-1114"><span class="linenos">1114</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_gradcam_abs-1115"><a href="#FGAdaHAT.get_importance_step_layer_gradcam_abs-1115"><span class="linenos">1115</span></a><span class="sd">        **Args:**</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradcam_abs-1116"><a href="#FGAdaHAT.get_importance_step_layer_gradcam_abs-1116"><span class="linenos">1116</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradcam_abs-1117"><a href="#FGAdaHAT.get_importance_step_layer_gradcam_abs-1117"><span class="linenos">1117</span></a><span class="sd">        - **input** (`Tensor` | `tuple[Tensor, ...]`): the input batch of the training step.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradcam_abs-1118"><a href="#FGAdaHAT.get_importance_step_layer_gradcam_abs-1118"><span class="linenos">1118</span></a><span class="sd">        - **target** (`Tensor` | `None`): the target batch of the training step.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradcam_abs-1119"><a href="#FGAdaHAT.get_importance_step_layer_gradcam_abs-1119"><span class="linenos">1119</span></a><span class="sd">        - **batch_idx** (`int`): the index of the current batch. This is an argument of the forward function during training.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradcam_abs-1120"><a href="#FGAdaHAT.get_importance_step_layer_gradcam_abs-1120"><span class="linenos">1120</span></a><span class="sd">        - **num_batches** (`int`): the number of batches in the training step. This is an argument of the forward function during training.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradcam_abs-1121"><a href="#FGAdaHAT.get_importance_step_layer_gradcam_abs-1121"><span class="linenos">1121</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_gradcam_abs-1122"><a href="#FGAdaHAT.get_importance_step_layer_gradcam_abs-1122"><span class="linenos">1122</span></a><span class="sd">        **Returns:**</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradcam_abs-1123"><a href="#FGAdaHAT.get_importance_step_layer_gradcam_abs-1123"><span class="linenos">1123</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradcam_abs-1124"><a href="#FGAdaHAT.get_importance_step_layer_gradcam_abs-1124"><span class="linenos">1124</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradcam_abs-1125"><a href="#FGAdaHAT.get_importance_step_layer_gradcam_abs-1125"><span class="linenos">1125</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradcam_abs-1126"><a href="#FGAdaHAT.get_importance_step_layer_gradcam_abs-1126"><span class="linenos">1126</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_gradcam_abs-1127"><a href="#FGAdaHAT.get_importance_step_layer_gradcam_abs-1127"><span class="linenos">1127</span></a>        <span class="c1"># initialize the GradCAM object</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradcam_abs-1128"><a href="#FGAdaHAT.get_importance_step_layer_gradcam_abs-1128"><span class="linenos">1128</span></a>        <span class="n">gradcam</span> <span class="o">=</span> <span class="n">LayerGradCam</span><span class="p">(</span><span class="n">forward_func</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="n">layer</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradcam_abs-1129"><a href="#FGAdaHAT.get_importance_step_layer_gradcam_abs-1129"><span class="linenos">1129</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_gradcam_abs-1130"><a href="#FGAdaHAT.get_importance_step_layer_gradcam_abs-1130"><span class="linenos">1130</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradcam_abs-1131"><a href="#FGAdaHAT.get_importance_step_layer_gradcam_abs-1131"><span class="linenos">1131</span></a>        <span class="c1"># calculate layer attribution of the step</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradcam_abs-1132"><a href="#FGAdaHAT.get_importance_step_layer_gradcam_abs-1132"><span class="linenos">1132</span></a>        <span class="n">attribution</span> <span class="o">=</span> <span class="n">gradcam</span><span class="o">.</span><span class="n">attribute</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradcam_abs-1133"><a href="#FGAdaHAT.get_importance_step_layer_gradcam_abs-1133"><span class="linenos">1133</span></a>            <span class="n">inputs</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradcam_abs-1134"><a href="#FGAdaHAT.get_importance_step_layer_gradcam_abs-1134"><span class="linenos">1134</span></a>            <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradcam_abs-1135"><a href="#FGAdaHAT.get_importance_step_layer_gradcam_abs-1135"><span class="linenos">1135</span></a>            <span class="n">additional_forward_args</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">num_batches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">),</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradcam_abs-1136"><a href="#FGAdaHAT.get_importance_step_layer_gradcam_abs-1136"><span class="linenos">1136</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradcam_abs-1137"><a href="#FGAdaHAT.get_importance_step_layer_gradcam_abs-1137"><span class="linenos">1137</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradcam_abs-1138"><a href="#FGAdaHAT.get_importance_step_layer_gradcam_abs-1138"><span class="linenos">1138</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_gradcam_abs-1139"><a href="#FGAdaHAT.get_importance_step_layer_gradcam_abs-1139"><span class="linenos">1139</span></a>        <span class="n">attribution_abs_batch_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradcam_abs-1140"><a href="#FGAdaHAT.get_importance_step_layer_gradcam_abs-1140"><span class="linenos">1140</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">attribution</span><span class="p">),</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradcam_abs-1141"><a href="#FGAdaHAT.get_importance_step_layer_gradcam_abs-1141"><span class="linenos">1141</span></a>            <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradcam_abs-1142"><a href="#FGAdaHAT.get_importance_step_layer_gradcam_abs-1142"><span class="linenos">1142</span></a>                <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">attribution</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradcam_abs-1143"><a href="#FGAdaHAT.get_importance_step_layer_gradcam_abs-1143"><span class="linenos">1143</span></a>            <span class="p">],</span>  <span class="c1"># average the features over batch samples</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradcam_abs-1144"><a href="#FGAdaHAT.get_importance_step_layer_gradcam_abs-1144"><span class="linenos">1144</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradcam_abs-1145"><a href="#FGAdaHAT.get_importance_step_layer_gradcam_abs-1145"><span class="linenos">1145</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_gradcam_abs-1146"><a href="#FGAdaHAT.get_importance_step_layer_gradcam_abs-1146"><span class="linenos">1146</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">attribution_abs_batch_mean</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradcam_abs-1147"><a href="#FGAdaHAT.get_importance_step_layer_gradcam_abs-1147"><span class="linenos">1147</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradcam_abs-1148"><a href="#FGAdaHAT.get_importance_step_layer_gradcam_abs-1148"><span class="linenos">1148</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_gradcam_abs-1149"><a href="#FGAdaHAT.get_importance_step_layer_gradcam_abs-1149"><span class="linenos">1149</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span></pre></div>


            <div class="docstring"><p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of <a href="https://openreview.net/forum?id=SJPpHzW0-">Grad-CAM</a>. We implement this using <a href="https://captum.ai/api/layer.html#gradcam">Layer Grad-CAM</a> in Captum.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>
<li><strong>input</strong> (<code>Tensor</code> | <code>tuple[Tensor, ...]</code>): the input batch of the training step.</li>
<li><strong>target</strong> (<code>Tensor</code> | <code>None</code>): the target batch of the training step.</li>
<li><strong>batch_idx</strong> (<code>int</code>): the index of the current batch. This is an argument of the forward function during training.</li>
<li><strong>num_batches</strong> (<code>int</code>): the number of batches in the training step. This is an argument of the forward function during training.</li>
</ul>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>
</ul>
</div>


                            </div>
                            <div id="FGAdaHAT.get_importance_step_layer_deeplift_abs" class="classattr">
                                        <input id="FGAdaHAT.get_importance_step_layer_deeplift_abs-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">get_importance_step_layer_deeplift_abs</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>,</span><span class="param">	<span class="n">baselines</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>,</span><span class="param">	<span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="FGAdaHAT.get_importance_step_layer_deeplift_abs-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FGAdaHAT.get_importance_step_layer_deeplift_abs"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1151"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1151"><span class="linenos">1151</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_deeplift_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1152"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1152"><span class="linenos">1152</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1153"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1153"><span class="linenos">1153</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1154"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1154"><span class="linenos">1154</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1155"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1155"><span class="linenos">1155</span></a>        <span class="n">baselines</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1156"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1156"><span class="linenos">1156</span></a>        <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1157"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1157"><span class="linenos">1157</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1158"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1158"><span class="linenos">1158</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1159"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1159"><span class="linenos">1159</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1160"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1160"><span class="linenos">1160</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of [DeepLift](https://proceedings.mlr.press/v70/shrikumar17a/shrikumar17a.pdf). We implement this using [Layer DeepLift](https://captum.ai/api/layer.html#layer-deeplift) in Captum.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1161"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1161"><span class="linenos">1161</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1162"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1162"><span class="linenos">1162</span></a><span class="sd">        **Args:**</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1163"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1163"><span class="linenos">1163</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1164"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1164"><span class="linenos">1164</span></a><span class="sd">        - **input** (`Tensor` | `tuple[Tensor, ...]`): the input batch of the training step.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1165"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1165"><span class="linenos">1165</span></a><span class="sd">        - **baselines** (`None` | `int` | `float` | `Tensor` | `tuple[int | float | Tensor, ...]`): baselines define reference samples that are compared with the inputs. Please refer to the [Captum documentation](https://captum.ai/api/layer.html#captum.attr.LayerDeepLift.attribute) for more details.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1166"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1166"><span class="linenos">1166</span></a><span class="sd">        - **target** (`Tensor` | `None`): the target batch of the training step.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1167"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1167"><span class="linenos">1167</span></a><span class="sd">        - **batch_idx** (`int`): the index of the current batch. This is an argument of the forward function during training.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1168"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1168"><span class="linenos">1168</span></a><span class="sd">        - **num_batches** (`int`): the number of batches in the training step. This is an argument of the forward function during training.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1169"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1169"><span class="linenos">1169</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1170"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1170"><span class="linenos">1170</span></a><span class="sd">        **Returns:**</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1171"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1171"><span class="linenos">1171</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1172"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1172"><span class="linenos">1172</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1173"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1173"><span class="linenos">1173</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1174"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1174"><span class="linenos">1174</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1175"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1175"><span class="linenos">1175</span></a>        <span class="c1"># initialize the Layer DeepLift object</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1176"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1176"><span class="linenos">1176</span></a>        <span class="n">layer_deeplift</span> <span class="o">=</span> <span class="n">LayerDeepLift</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="n">layer</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1177"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1177"><span class="linenos">1177</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1178"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1178"><span class="linenos">1178</span></a>        <span class="c1"># convert the target to long type to avoid error</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1179"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1179"><span class="linenos">1179</span></a>        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">long</span><span class="p">()</span> <span class="k">if</span> <span class="n">target</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1180"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1180"><span class="linenos">1180</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1181"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1181"><span class="linenos">1181</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1182"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1182"><span class="linenos">1182</span></a>        <span class="c1"># calculate layer attribution of the step</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1183"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1183"><span class="linenos">1183</span></a>        <span class="n">attribution</span> <span class="o">=</span> <span class="n">layer_deeplift</span><span class="o">.</span><span class="n">attribute</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1184"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1184"><span class="linenos">1184</span></a>            <span class="n">inputs</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1185"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1185"><span class="linenos">1185</span></a>            <span class="n">baselines</span><span class="o">=</span><span class="n">baselines</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1186"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1186"><span class="linenos">1186</span></a>            <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1187"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1187"><span class="linenos">1187</span></a>            <span class="n">additional_forward_args</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">num_batches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">),</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1188"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1188"><span class="linenos">1188</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1189"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1189"><span class="linenos">1189</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1190"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1190"><span class="linenos">1190</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1191"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1191"><span class="linenos">1191</span></a>        <span class="n">attribution_abs_batch_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1192"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1192"><span class="linenos">1192</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">attribution</span><span class="p">),</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1193"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1193"><span class="linenos">1193</span></a>            <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1194"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1194"><span class="linenos">1194</span></a>                <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">attribution</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1195"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1195"><span class="linenos">1195</span></a>            <span class="p">],</span>  <span class="c1"># average the features over batch samples</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1196"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1196"><span class="linenos">1196</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1197"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1197"><span class="linenos">1197</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1198"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1198"><span class="linenos">1198</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">attribution_abs_batch_mean</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1199"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1199"><span class="linenos">1199</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1200"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1200"><span class="linenos">1200</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_deeplift_abs-1201"><a href="#FGAdaHAT.get_importance_step_layer_deeplift_abs-1201"><span class="linenos">1201</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span></pre></div>


            <div class="docstring"><p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of <a href="https://proceedings.mlr.press/v70/shrikumar17a/shrikumar17a.pdf">DeepLift</a>. We implement this using <a href="https://captum.ai/api/layer.html#layer-deeplift">Layer DeepLift</a> in Captum.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>
<li><strong>input</strong> (<code>Tensor</code> | <code>tuple[Tensor, ...]</code>): the input batch of the training step.</li>
<li><strong>baselines</strong> (<code>None</code> | <code>int</code> | <code><a href="#FGAdaHAT.float">float</a></code> | <code>Tensor</code> | <code>tuple[int | float | Tensor, ...]</code>): baselines define reference samples that are compared with the inputs. Please refer to the <a href="https://captum.ai/api/layer.html#captum.attr.LayerDeepLift.attribute">Captum documentation</a> for more details.</li>
<li><strong>target</strong> (<code>Tensor</code> | <code>None</code>): the target batch of the training step.</li>
<li><strong>batch_idx</strong> (<code>int</code>): the index of the current batch. This is an argument of the forward function during training.</li>
<li><strong>num_batches</strong> (<code>int</code>): the number of batches in the training step. This is an argument of the forward function during training.</li>
</ul>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>
</ul>
</div>


                            </div>
                            <div id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs" class="classattr">
                                        <input id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">get_importance_step_layer_deepliftshap_abs</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>,</span><span class="param">	<span class="n">baselines</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>,</span><span class="param">	<span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1203"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1203"><span class="linenos">1203</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_deepliftshap_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1204"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1204"><span class="linenos">1204</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1205"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1205"><span class="linenos">1205</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1206"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1206"><span class="linenos">1206</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1207"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1207"><span class="linenos">1207</span></a>        <span class="n">baselines</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1208"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1208"><span class="linenos">1208</span></a>        <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1209"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1209"><span class="linenos">1209</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1210"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1210"><span class="linenos">1210</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1211"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1211"><span class="linenos">1211</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1212"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1212"><span class="linenos">1212</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of [DeepLift SHAP](https://proceedings.neurips.cc/paper_files/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf). We implement this using [Layer DeepLiftShap](https://captum.ai/api/layer.html#layer-deepliftshap) in Captum.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1213"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1213"><span class="linenos">1213</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1214"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1214"><span class="linenos">1214</span></a><span class="sd">        **Args:**</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1215"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1215"><span class="linenos">1215</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1216"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1216"><span class="linenos">1216</span></a><span class="sd">        - **input** (`Tensor` | `tuple[Tensor, ...]`): the input batch of the training step.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1217"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1217"><span class="linenos">1217</span></a><span class="sd">        - **baselines** (`None` | `int` | `float` | `Tensor` | `tuple[int | float | Tensor, ...]`): baselines define reference samples that are compared with the inputs. Please refer to the [Captum documentation](https://captum.ai/api/layer.html#captum.attr.LayerDeepLiftShap.attribute) for more details.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1218"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1218"><span class="linenos">1218</span></a><span class="sd">        - **target** (`Tensor` | `None`): the target batch of the training step.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1219"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1219"><span class="linenos">1219</span></a><span class="sd">        - **batch_idx** (`int`): the index of the current batch. This is an argument of the forward function during training.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1220"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1220"><span class="linenos">1220</span></a><span class="sd">        - **num_batches** (`int`): the number of batches in the training step. This is an argument of the forward function during training.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1221"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1221"><span class="linenos">1221</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1222"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1222"><span class="linenos">1222</span></a><span class="sd">        **Returns:**</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1223"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1223"><span class="linenos">1223</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1224"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1224"><span class="linenos">1224</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1225"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1225"><span class="linenos">1225</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1226"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1226"><span class="linenos">1226</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1227"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1227"><span class="linenos">1227</span></a>        <span class="c1"># initialize the Layer DeepLiftShap object</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1228"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1228"><span class="linenos">1228</span></a>        <span class="n">layer_deepliftshap</span> <span class="o">=</span> <span class="n">LayerDeepLiftShap</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="n">layer</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1229"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1229"><span class="linenos">1229</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1230"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1230"><span class="linenos">1230</span></a>        <span class="c1"># convert the target to long type to avoid error</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1231"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1231"><span class="linenos">1231</span></a>        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">long</span><span class="p">()</span> <span class="k">if</span> <span class="n">target</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1232"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1232"><span class="linenos">1232</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1233"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1233"><span class="linenos">1233</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1234"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1234"><span class="linenos">1234</span></a>        <span class="c1"># calculate layer attribution of the step</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1235"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1235"><span class="linenos">1235</span></a>        <span class="n">attribution</span> <span class="o">=</span> <span class="n">layer_deepliftshap</span><span class="o">.</span><span class="n">attribute</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1236"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1236"><span class="linenos">1236</span></a>            <span class="n">inputs</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1237"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1237"><span class="linenos">1237</span></a>            <span class="n">baselines</span><span class="o">=</span><span class="n">baselines</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1238"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1238"><span class="linenos">1238</span></a>            <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1239"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1239"><span class="linenos">1239</span></a>            <span class="n">additional_forward_args</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">num_batches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">),</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1240"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1240"><span class="linenos">1240</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1241"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1241"><span class="linenos">1241</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1242"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1242"><span class="linenos">1242</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1243"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1243"><span class="linenos">1243</span></a>        <span class="n">attribution_abs_batch_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1244"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1244"><span class="linenos">1244</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">attribution</span><span class="p">),</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1245"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1245"><span class="linenos">1245</span></a>            <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1246"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1246"><span class="linenos">1246</span></a>                <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">attribution</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1247"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1247"><span class="linenos">1247</span></a>            <span class="p">],</span>  <span class="c1"># average the features over batch samples</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1248"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1248"><span class="linenos">1248</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1249"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1249"><span class="linenos">1249</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1250"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1250"><span class="linenos">1250</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">attribution_abs_batch_mean</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1251"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1251"><span class="linenos">1251</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1252"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1252"><span class="linenos">1252</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1253"><a href="#FGAdaHAT.get_importance_step_layer_deepliftshap_abs-1253"><span class="linenos">1253</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span></pre></div>


            <div class="docstring"><p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of <a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf">DeepLift SHAP</a>. We implement this using <a href="https://captum.ai/api/layer.html#layer-deepliftshap">Layer DeepLiftShap</a> in Captum.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>
<li><strong>input</strong> (<code>Tensor</code> | <code>tuple[Tensor, ...]</code>): the input batch of the training step.</li>
<li><strong>baselines</strong> (<code>None</code> | <code>int</code> | <code><a href="#FGAdaHAT.float">float</a></code> | <code>Tensor</code> | <code>tuple[int | float | Tensor, ...]</code>): baselines define reference samples that are compared with the inputs. Please refer to the <a href="https://captum.ai/api/layer.html#captum.attr.LayerDeepLiftShap.attribute">Captum documentation</a> for more details.</li>
<li><strong>target</strong> (<code>Tensor</code> | <code>None</code>): the target batch of the training step.</li>
<li><strong>batch_idx</strong> (<code>int</code>): the index of the current batch. This is an argument of the forward function during training.</li>
<li><strong>num_batches</strong> (<code>int</code>): the number of batches in the training step. This is an argument of the forward function during training.</li>
</ul>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>
</ul>
</div>


                            </div>
                            <div id="FGAdaHAT.get_importance_step_layer_gradientshap_abs" class="classattr">
                                        <input id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">get_importance_step_layer_gradientshap_abs</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>,</span><span class="param">	<span class="n">baselines</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>,</span><span class="param">	<span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="FGAdaHAT.get_importance_step_layer_gradientshap_abs-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1255"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1255"><span class="linenos">1255</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_gradientshap_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1256"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1256"><span class="linenos">1256</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1257"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1257"><span class="linenos">1257</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1258"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1258"><span class="linenos">1258</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1259"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1259"><span class="linenos">1259</span></a>        <span class="n">baselines</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1260"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1260"><span class="linenos">1260</span></a>        <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1261"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1261"><span class="linenos">1261</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1262"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1262"><span class="linenos">1262</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1263"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1263"><span class="linenos">1263</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1264"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1264"><span class="linenos">1264</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of gradient SHAP. We implement this using [Layer GradientShap](https://captum.ai/api/layer.html#layer-gradientshap) in Captum.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1265"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1265"><span class="linenos">1265</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1266"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1266"><span class="linenos">1266</span></a><span class="sd">        **Args:**</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1267"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1267"><span class="linenos">1267</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1268"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1268"><span class="linenos">1268</span></a><span class="sd">        - **input** (`Tensor` | `tuple[Tensor, ...]`): the input batch of the training step.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1269"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1269"><span class="linenos">1269</span></a><span class="sd">        - **baselines** (`None` | `int` | `float` | `Tensor` | `tuple[int | float | Tensor, ...]`): starting point from which expectation is computed. Please refer to the [Captum documentation](https://captum.ai/api/layer.html#captum.attr.LayerGradientShap.attribute) for more details. If `None`, the baselines are set to zero.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1270"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1270"><span class="linenos">1270</span></a><span class="sd">        - **target** (`Tensor` | `None`): the target batch of the training step.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1271"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1271"><span class="linenos">1271</span></a><span class="sd">        - **batch_idx** (`int`): the index of the current batch. This is an argument of the forward function during training.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1272"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1272"><span class="linenos">1272</span></a><span class="sd">        - **num_batches** (`int`): the number of batches in the training step. This is an argument of the forward function during training.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1273"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1273"><span class="linenos">1273</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1274"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1274"><span class="linenos">1274</span></a><span class="sd">        **Returns:**</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1275"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1275"><span class="linenos">1275</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1276"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1276"><span class="linenos">1276</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1277"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1277"><span class="linenos">1277</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1278"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1278"><span class="linenos">1278</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1279"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1279"><span class="linenos">1279</span></a>        <span class="k">if</span> <span class="n">baselines</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1280"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1280"><span class="linenos">1280</span></a>            <span class="n">baselines</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1281"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1281"><span class="linenos">1281</span></a>                <span class="nb">input</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1282"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1282"><span class="linenos">1282</span></a>            <span class="p">)</span>  <span class="c1"># baselines are mandatory for GradientShap API. We explicitly set them to zero</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1283"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1283"><span class="linenos">1283</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1284"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1284"><span class="linenos">1284</span></a>        <span class="c1"># initialize the Layer GradientShap object</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1285"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1285"><span class="linenos">1285</span></a>        <span class="n">layer_gradientshap</span> <span class="o">=</span> <span class="n">LayerGradientShap</span><span class="p">(</span><span class="n">forward_func</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="n">layer</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1286"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1286"><span class="linenos">1286</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1287"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1287"><span class="linenos">1287</span></a>        <span class="c1"># convert the target to long type to avoid error</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1288"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1288"><span class="linenos">1288</span></a>        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">long</span><span class="p">()</span> <span class="k">if</span> <span class="n">target</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1289"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1289"><span class="linenos">1289</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1290"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1290"><span class="linenos">1290</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1291"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1291"><span class="linenos">1291</span></a>        <span class="c1"># calculate layer attribution of the step</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1292"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1292"><span class="linenos">1292</span></a>        <span class="n">attribution</span> <span class="o">=</span> <span class="n">layer_gradientshap</span><span class="o">.</span><span class="n">attribute</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1293"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1293"><span class="linenos">1293</span></a>            <span class="n">inputs</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1294"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1294"><span class="linenos">1294</span></a>            <span class="n">baselines</span><span class="o">=</span><span class="n">baselines</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1295"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1295"><span class="linenos">1295</span></a>            <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1296"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1296"><span class="linenos">1296</span></a>            <span class="n">additional_forward_args</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">num_batches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">),</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1297"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1297"><span class="linenos">1297</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1298"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1298"><span class="linenos">1298</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1299"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1299"><span class="linenos">1299</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1300"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1300"><span class="linenos">1300</span></a>        <span class="n">attribution_abs_batch_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1301"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1301"><span class="linenos">1301</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">attribution</span><span class="p">),</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1302"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1302"><span class="linenos">1302</span></a>            <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1303"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1303"><span class="linenos">1303</span></a>                <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">attribution</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1304"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1304"><span class="linenos">1304</span></a>            <span class="p">],</span>  <span class="c1"># average the features over batch samples</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1305"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1305"><span class="linenos">1305</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1306"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1306"><span class="linenos">1306</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1307"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1307"><span class="linenos">1307</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">attribution_abs_batch_mean</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1308"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1308"><span class="linenos">1308</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1309"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1309"><span class="linenos">1309</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_gradientshap_abs-1310"><a href="#FGAdaHAT.get_importance_step_layer_gradientshap_abs-1310"><span class="linenos">1310</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span></pre></div>


            <div class="docstring"><p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of gradient SHAP. We implement this using <a href="https://captum.ai/api/layer.html#layer-gradientshap">Layer GradientShap</a> in Captum.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>
<li><strong>input</strong> (<code>Tensor</code> | <code>tuple[Tensor, ...]</code>): the input batch of the training step.</li>
<li><strong>baselines</strong> (<code>None</code> | <code>int</code> | <code><a href="#FGAdaHAT.float">float</a></code> | <code>Tensor</code> | <code>tuple[int | float | Tensor, ...]</code>): starting point from which expectation is computed. Please refer to the <a href="https://captum.ai/api/layer.html#captum.attr.LayerGradientShap.attribute">Captum documentation</a> for more details. If <code>None</code>, the baselines are set to zero.</li>
<li><strong>target</strong> (<code>Tensor</code> | <code>None</code>): the target batch of the training step.</li>
<li><strong>batch_idx</strong> (<code>int</code>): the index of the current batch. This is an argument of the forward function during training.</li>
<li><strong>num_batches</strong> (<code>int</code>): the number of batches in the training step. This is an argument of the forward function during training.</li>
</ul>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>
</ul>
</div>


                            </div>
                            <div id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs" class="classattr">
                                        <input id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">get_importance_step_layer_integrated_gradients_abs</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>,</span><span class="param">	<span class="n">baselines</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>,</span><span class="param">	<span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1312"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1312"><span class="linenos">1312</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_integrated_gradients_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1313"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1313"><span class="linenos">1313</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1314"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1314"><span class="linenos">1314</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1315"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1315"><span class="linenos">1315</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1316"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1316"><span class="linenos">1316</span></a>        <span class="n">baselines</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1317"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1317"><span class="linenos">1317</span></a>        <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1318"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1318"><span class="linenos">1318</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1319"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1319"><span class="linenos">1319</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1320"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1320"><span class="linenos">1320</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1321"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1321"><span class="linenos">1321</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of [integrated gradients](https://proceedings.mlr.press/v70/sundararajan17a/sundararajan17a.pdf). We implement this using [Layer Integrated Gradients](https://captum.ai/api/layer.html#layer-integrated-gradients) in Captum.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1322"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1322"><span class="linenos">1322</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1323"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1323"><span class="linenos">1323</span></a><span class="sd">        **Args:**</span>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1324"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1324"><span class="linenos">1324</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1325"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1325"><span class="linenos">1325</span></a><span class="sd">        - **input** (`Tensor` | `tuple[Tensor, ...]`): the input batch of the training step.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1326"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1326"><span class="linenos">1326</span></a><span class="sd">        - **baselines** (`None` | `int` | `float` | `Tensor` | `tuple[int | float | Tensor, ...]`): starting point from which integral is computed. Please refer to the [Captum documentation](https://captum.ai/api/layer.html#captum.attr.LayerIntegratedGradients.attribute) for more details.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1327"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1327"><span class="linenos">1327</span></a><span class="sd">        - **target** (`Tensor` | `None`): the target batch of the training step.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1328"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1328"><span class="linenos">1328</span></a><span class="sd">        - **batch_idx** (`int`): the index of the current batch. This is an argument of the forward function during training.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1329"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1329"><span class="linenos">1329</span></a><span class="sd">        - **num_batches** (`int`): the number of batches in the training step. This is an argument of the forward function during training.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1330"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1330"><span class="linenos">1330</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1331"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1331"><span class="linenos">1331</span></a><span class="sd">        **Returns:**</span>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1332"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1332"><span class="linenos">1332</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1333"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1333"><span class="linenos">1333</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1334"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1334"><span class="linenos">1334</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1335"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1335"><span class="linenos">1335</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1336"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1336"><span class="linenos">1336</span></a>        <span class="c1"># initialize the Layer Integrated Gradients object</span>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1337"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1337"><span class="linenos">1337</span></a>        <span class="n">layer_integrated_gradients</span> <span class="o">=</span> <span class="n">LayerIntegratedGradients</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1338"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1338"><span class="linenos">1338</span></a>            <span class="n">forward_func</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="n">layer</span>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1339"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1339"><span class="linenos">1339</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1340"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1340"><span class="linenos">1340</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1341"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1341"><span class="linenos">1341</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1342"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1342"><span class="linenos">1342</span></a>        <span class="c1"># calculate layer attribution of the step</span>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1343"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1343"><span class="linenos">1343</span></a>        <span class="n">attribution</span> <span class="o">=</span> <span class="n">layer_integrated_gradients</span><span class="o">.</span><span class="n">attribute</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1344"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1344"><span class="linenos">1344</span></a>            <span class="n">inputs</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1345"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1345"><span class="linenos">1345</span></a>            <span class="n">baselines</span><span class="o">=</span><span class="n">baselines</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1346"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1346"><span class="linenos">1346</span></a>            <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1347"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1347"><span class="linenos">1347</span></a>            <span class="n">additional_forward_args</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">num_batches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">),</span>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1348"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1348"><span class="linenos">1348</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1349"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1349"><span class="linenos">1349</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1350"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1350"><span class="linenos">1350</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1351"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1351"><span class="linenos">1351</span></a>        <span class="n">attribution_abs_batch_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1352"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1352"><span class="linenos">1352</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">attribution</span><span class="p">),</span>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1353"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1353"><span class="linenos">1353</span></a>            <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1354"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1354"><span class="linenos">1354</span></a>                <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">attribution</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1355"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1355"><span class="linenos">1355</span></a>            <span class="p">],</span>  <span class="c1"># average the features over batch samples</span>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1356"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1356"><span class="linenos">1356</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1357"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1357"><span class="linenos">1357</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1358"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1358"><span class="linenos">1358</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">attribution_abs_batch_mean</span>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1359"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1359"><span class="linenos">1359</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1360"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1360"><span class="linenos">1360</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1361"><a href="#FGAdaHAT.get_importance_step_layer_integrated_gradients_abs-1361"><span class="linenos">1361</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span></pre></div>


            <div class="docstring"><p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of <a href="https://proceedings.mlr.press/v70/sundararajan17a/sundararajan17a.pdf">integrated gradients</a>. We implement this using <a href="https://captum.ai/api/layer.html#layer-integrated-gradients">Layer Integrated Gradients</a> in Captum.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>
<li><strong>input</strong> (<code>Tensor</code> | <code>tuple[Tensor, ...]</code>): the input batch of the training step.</li>
<li><strong>baselines</strong> (<code>None</code> | <code>int</code> | <code><a href="#FGAdaHAT.float">float</a></code> | <code>Tensor</code> | <code>tuple[int | float | Tensor, ...]</code>): starting point from which integral is computed. Please refer to the <a href="https://captum.ai/api/layer.html#captum.attr.LayerIntegratedGradients.attribute">Captum documentation</a> for more details.</li>
<li><strong>target</strong> (<code>Tensor</code> | <code>None</code>): the target batch of the training step.</li>
<li><strong>batch_idx</strong> (<code>int</code>): the index of the current batch. This is an argument of the forward function during training.</li>
<li><strong>num_batches</strong> (<code>int</code>): the number of batches in the training step. This is an argument of the forward function during training.</li>
</ul>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>
</ul>
</div>


                            </div>
                            <div id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs" class="classattr">
                                        <input id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">get_importance_step_layer_feature_ablation_abs</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>,</span><span class="param">	<span class="n">layer_baselines</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>,</span><span class="param">	<span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">if_captum</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1363"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1363"><span class="linenos">1363</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_feature_ablation_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1364"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1364"><span class="linenos">1364</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1365"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1365"><span class="linenos">1365</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1366"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1366"><span class="linenos">1366</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1367"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1367"><span class="linenos">1367</span></a>        <span class="n">layer_baselines</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1368"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1368"><span class="linenos">1368</span></a>        <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1369"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1369"><span class="linenos">1369</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1370"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1370"><span class="linenos">1370</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1371"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1371"><span class="linenos">1371</span></a>        <span class="n">if_captum</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1372"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1372"><span class="linenos">1372</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1373"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1373"><span class="linenos">1373</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of [feature ablation](https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53) attribution. We implement this using [Layer Feature Ablation](https://captum.ai/api/layer.html#layer-feature-ablation) in Captum.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1374"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1374"><span class="linenos">1374</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1375"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1375"><span class="linenos">1375</span></a><span class="sd">        **Args:**</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1376"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1376"><span class="linenos">1376</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1377"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1377"><span class="linenos">1377</span></a><span class="sd">        - **input** (`Tensor` | `tuple[Tensor, ...]`): the input batch of the training step.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1378"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1378"><span class="linenos">1378</span></a><span class="sd">        - **layer_baselines** (`None` | `int` | `float` | `Tensor` | `tuple[int | float | Tensor, ...]`): reference values which replace each layer input / output value when ablated. Please refer to the [Captum documentation](https://captum.ai/api/layer.html#captum.attr.LayerFeatureAblation.attribute) for more details.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1379"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1379"><span class="linenos">1379</span></a><span class="sd">        - **target** (`Tensor` | `None`): the target batch of the training step.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1380"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1380"><span class="linenos">1380</span></a><span class="sd">        - **batch_idx** (`int`): the index of the current batch. This is an argument of the forward function during training.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1381"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1381"><span class="linenos">1381</span></a><span class="sd">        - **num_batches** (`int`): the number of batches in the training step. This is an argument of the forward function during training.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1382"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1382"><span class="linenos">1382</span></a><span class="sd">        - **if_captum** (`bool`): whether to use Captum or not. If `True`, we use Captum to calculate the feature ablation. If `False`, we use our implementation. Default is `False`, because our implementation is much faster.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1383"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1383"><span class="linenos">1383</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1384"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1384"><span class="linenos">1384</span></a><span class="sd">        **Returns:**</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1385"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1385"><span class="linenos">1385</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1386"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1386"><span class="linenos">1386</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1387"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1387"><span class="linenos">1387</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1388"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1388"><span class="linenos">1388</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1389"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1389"><span class="linenos">1389</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="n">if_captum</span><span class="p">:</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1390"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1390"><span class="linenos">1390</span></a>            <span class="c1"># 1. Baseline logits (take first element of forward output)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1391"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1391"><span class="linenos">1391</span></a>            <span class="n">baseline_out</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1392"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1392"><span class="linenos">1392</span></a>                <span class="nb">input</span><span class="p">,</span> <span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">num_batches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1393"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1393"><span class="linenos">1393</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1394"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1394"><span class="linenos">1394</span></a>            <span class="k">if</span> <span class="n">target</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1395"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1395"><span class="linenos">1395</span></a>                <span class="n">baseline_scores</span> <span class="o">=</span> <span class="n">baseline_out</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1396"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1396"><span class="linenos">1396</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1397"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1397"><span class="linenos">1397</span></a>                <span class="n">baseline_scores</span> <span class="o">=</span> <span class="n">baseline_out</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1398"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1398"><span class="linenos">1398</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1399"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1399"><span class="linenos">1399</span></a>            <span class="c1"># 2. Capture layer’s output shape</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1400"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1400"><span class="linenos">1400</span></a>            <span class="n">activs</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1401"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1401"><span class="linenos">1401</span></a>            <span class="n">handle</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">register_forward_hook</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1402"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1402"><span class="linenos">1402</span></a>                <span class="k">lambda</span> <span class="n">module</span><span class="p">,</span> <span class="n">inp</span><span class="p">,</span> <span class="n">out</span><span class="p">:</span> <span class="n">activs</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;output&quot;</span><span class="p">,</span> <span class="n">out</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1403"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1403"><span class="linenos">1403</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1404"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1404"><span class="linenos">1404</span></a>            <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">num_batches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1405"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1405"><span class="linenos">1405</span></a>            <span class="n">handle</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1406"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1406"><span class="linenos">1406</span></a>            <span class="n">layer_output</span> <span class="o">=</span> <span class="n">activs</span><span class="p">[</span><span class="s2">&quot;output&quot;</span><span class="p">]</span>  <span class="c1"># shape (B, F, ...)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1407"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1407"><span class="linenos">1407</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1408"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1408"><span class="linenos">1408</span></a>            <span class="c1"># 3. Build baseline tensor matching that shape</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1409"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1409"><span class="linenos">1409</span></a>            <span class="k">if</span> <span class="n">layer_baselines</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1410"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1410"><span class="linenos">1410</span></a>                <span class="n">baseline_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">layer_output</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1411"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1411"><span class="linenos">1411</span></a>            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer_baselines</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">)):</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1412"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1412"><span class="linenos">1412</span></a>                <span class="n">baseline_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">layer_output</span><span class="p">,</span> <span class="n">layer_baselines</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1413"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1413"><span class="linenos">1413</span></a>            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer_baselines</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1414"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1414"><span class="linenos">1414</span></a>                <span class="k">if</span> <span class="n">layer_baselines</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">layer_output</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1415"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1415"><span class="linenos">1415</span></a>                    <span class="n">baseline_tensor</span> <span class="o">=</span> <span class="n">layer_baselines</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1416"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1416"><span class="linenos">1416</span></a>                <span class="k">elif</span> <span class="n">layer_baselines</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">layer_output</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1417"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1417"><span class="linenos">1417</span></a>                    <span class="n">baseline_tensor</span> <span class="o">=</span> <span class="n">layer_baselines</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1418"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1418"><span class="linenos">1418</span></a>                        <span class="n">layer_output</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">*</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">layer_baselines</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1419"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1419"><span class="linenos">1419</span></a>                    <span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1420"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1420"><span class="linenos">1420</span></a>                <span class="k">else</span><span class="p">:</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1421"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1421"><span class="linenos">1421</span></a>                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1422"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1422"><span class="linenos">1422</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1423"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1423"><span class="linenos">1423</span></a>                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1424"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1424"><span class="linenos">1424</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1425"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1425"><span class="linenos">1425</span></a>            <span class="n">B</span><span class="p">,</span> <span class="n">F</span> <span class="o">=</span> <span class="n">layer_output</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">layer_output</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1426"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1426"><span class="linenos">1426</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1427"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1427"><span class="linenos">1427</span></a>            <span class="c1"># 4. Create a “mega-batch” replicating the input F times</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1428"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1428"><span class="linenos">1428</span></a>            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1429"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1429"><span class="linenos">1429</span></a>                <span class="n">mega_inputs</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1430"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1430"><span class="linenos">1430</span></a>                    <span class="n">t</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">F</span><span class="p">,</span> <span class="o">*</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">t</span><span class="o">.</span><span class="n">ndim</span><span class="p">))</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="n">t</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1431"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1431"><span class="linenos">1431</span></a>                    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">input</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1432"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1432"><span class="linenos">1432</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1433"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1433"><span class="linenos">1433</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1434"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1434"><span class="linenos">1434</span></a>                <span class="n">mega_inputs</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1435"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1435"><span class="linenos">1435</span></a>                    <span class="nb">input</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1436"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1436"><span class="linenos">1436</span></a>                    <span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">F</span><span class="p">,</span> <span class="o">*</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">input</span><span class="o">.</span><span class="n">ndim</span><span class="p">))</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1437"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1437"><span class="linenos">1437</span></a>                    <span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1438"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1438"><span class="linenos">1438</span></a>                <span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1439"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1439"><span class="linenos">1439</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1440"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1440"><span class="linenos">1440</span></a>            <span class="c1"># 5. Equally replicate the baseline tensor</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1441"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1441"><span class="linenos">1441</span></a>            <span class="n">mega_baseline</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1442"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1442"><span class="linenos">1442</span></a>                <span class="n">baseline_tensor</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1443"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1443"><span class="linenos">1443</span></a>                <span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">F</span><span class="p">,</span> <span class="o">*</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">baseline_tensor</span><span class="o">.</span><span class="n">ndim</span><span class="p">))</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1444"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1444"><span class="linenos">1444</span></a>                <span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="n">baseline_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1445"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1445"><span class="linenos">1445</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1446"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1446"><span class="linenos">1446</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1447"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1447"><span class="linenos">1447</span></a>            <span class="c1"># 6. Precompute vectorized indices</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1448"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1448"><span class="linenos">1448</span></a>            <span class="n">device</span> <span class="o">=</span> <span class="n">layer_output</span><span class="o">.</span><span class="n">device</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1449"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1449"><span class="linenos">1449</span></a>            <span class="n">positions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">F</span> <span class="o">*</span> <span class="n">B</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># [0,1,...,F*B-1]</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1450"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1450"><span class="linenos">1450</span></a>            <span class="n">feat_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">F</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1451"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1451"><span class="linenos">1451</span></a>                <span class="n">B</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1452"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1452"><span class="linenos">1452</span></a>            <span class="p">)</span>  <span class="c1"># [0,0,...,1,1,...,F-1]</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1453"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1453"><span class="linenos">1453</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1454"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1454"><span class="linenos">1454</span></a>            <span class="c1"># 7. One hook to zero out each channel slice across the mega-batch</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1455"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1455"><span class="linenos">1455</span></a>            <span class="k">def</span><span class="w"> </span><span class="nf">mega_ablate_hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">inp</span><span class="p">,</span> <span class="n">out</span><span class="p">):</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1456"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1456"><span class="linenos">1456</span></a>                <span class="n">out_mod</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1457"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1457"><span class="linenos">1457</span></a>                <span class="c1"># for each sample in mega-batch, zero its corresponding channel</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1458"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1458"><span class="linenos">1458</span></a>                <span class="n">out_mod</span><span class="p">[</span><span class="n">positions</span><span class="p">,</span> <span class="n">feat_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">mega_baseline</span><span class="p">[</span><span class="n">positions</span><span class="p">,</span> <span class="n">feat_idx</span><span class="p">]</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1459"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1459"><span class="linenos">1459</span></a>                <span class="k">return</span> <span class="n">out_mod</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1460"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1460"><span class="linenos">1460</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1461"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1461"><span class="linenos">1461</span></a>            <span class="n">h</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">register_forward_hook</span><span class="p">(</span><span class="n">mega_ablate_hook</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1462"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1462"><span class="linenos">1462</span></a>            <span class="n">out_all</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1463"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1463"><span class="linenos">1463</span></a>                <span class="n">mega_inputs</span><span class="p">,</span> <span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">num_batches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1464"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1464"><span class="linenos">1464</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1465"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1465"><span class="linenos">1465</span></a>            <span class="n">h</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1466"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1466"><span class="linenos">1466</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1467"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1467"><span class="linenos">1467</span></a>            <span class="c1"># 8. Recover scores, reshape [F*B] → [F, B], diff &amp; mean</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1468"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1468"><span class="linenos">1468</span></a>            <span class="k">if</span> <span class="n">target</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1469"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1469"><span class="linenos">1469</span></a>                <span class="n">tgt_flat</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">F</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1470"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1470"><span class="linenos">1470</span></a>                <span class="n">scores_all</span> <span class="o">=</span> <span class="n">out_all</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">tgt_flat</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1471"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1471"><span class="linenos">1471</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1472"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1472"><span class="linenos">1472</span></a>                <span class="n">scores_all</span> <span class="o">=</span> <span class="n">out_all</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1473"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1473"><span class="linenos">1473</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1474"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1474"><span class="linenos">1474</span></a>            <span class="n">scores_all</span> <span class="o">=</span> <span class="n">scores_all</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">F</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1475"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1475"><span class="linenos">1475</span></a>            <span class="n">diffs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">baseline_scores</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="n">scores_all</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1476"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1476"><span class="linenos">1476</span></a>            <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">diffs</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>  <span class="c1"># [F]</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1477"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1477"><span class="linenos">1477</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1478"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1478"><span class="linenos">1478</span></a>            <span class="k">return</span> <span class="n">importance_step_layer</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1479"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1479"><span class="linenos">1479</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1480"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1480"><span class="linenos">1480</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1481"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1481"><span class="linenos">1481</span></a>            <span class="c1"># initialize the Layer Feature Ablation object</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1482"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1482"><span class="linenos">1482</span></a>            <span class="n">layer_feature_ablation</span> <span class="o">=</span> <span class="n">LayerFeatureAblation</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1483"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1483"><span class="linenos">1483</span></a>                <span class="n">forward_func</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="n">layer</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1484"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1484"><span class="linenos">1484</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1485"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1485"><span class="linenos">1485</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1486"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1486"><span class="linenos">1486</span></a>            <span class="c1"># calculate layer attribution of the step</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1487"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1487"><span class="linenos">1487</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1488"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1488"><span class="linenos">1488</span></a>            <span class="n">attribution</span> <span class="o">=</span> <span class="n">layer_feature_ablation</span><span class="o">.</span><span class="n">attribute</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1489"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1489"><span class="linenos">1489</span></a>                <span class="n">inputs</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1490"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1490"><span class="linenos">1490</span></a>                <span class="n">layer_baselines</span><span class="o">=</span><span class="n">layer_baselines</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1491"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1491"><span class="linenos">1491</span></a>                <span class="c1"># target=target, # disable target to enable perturbations_per_eval</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1492"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1492"><span class="linenos">1492</span></a>                <span class="n">additional_forward_args</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">num_batches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">),</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1493"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1493"><span class="linenos">1493</span></a>                <span class="n">perturbations_per_eval</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>  <span class="c1"># to accelerate the computation</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1494"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1494"><span class="linenos">1494</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1495"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1495"><span class="linenos">1495</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1496"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1496"><span class="linenos">1496</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1497"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1497"><span class="linenos">1497</span></a>            <span class="n">attribution_abs_batch_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1498"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1498"><span class="linenos">1498</span></a>                <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">attribution</span><span class="p">),</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1499"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1499"><span class="linenos">1499</span></a>                <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1500"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1500"><span class="linenos">1500</span></a>                    <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">attribution</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1501"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1501"><span class="linenos">1501</span></a>                <span class="p">],</span>  <span class="c1"># average the features over batch samples</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1502"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1502"><span class="linenos">1502</span></a>            <span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1503"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1503"><span class="linenos">1503</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1504"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1504"><span class="linenos">1504</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">attribution_abs_batch_mean</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1505"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1505"><span class="linenos">1505</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1506"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1506"><span class="linenos">1506</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1507"><a href="#FGAdaHAT.get_importance_step_layer_feature_ablation_abs-1507"><span class="linenos">1507</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span></pre></div>


            <div class="docstring"><p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of <a href="https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53">feature ablation</a> attribution. We implement this using <a href="https://captum.ai/api/layer.html#layer-feature-ablation">Layer Feature Ablation</a> in Captum.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>
<li><strong>input</strong> (<code>Tensor</code> | <code>tuple[Tensor, ...]</code>): the input batch of the training step.</li>
<li><strong>layer_baselines</strong> (<code>None</code> | <code>int</code> | <code><a href="#FGAdaHAT.float">float</a></code> | <code>Tensor</code> | <code>tuple[int | float | Tensor, ...]</code>): reference values which replace each layer input / output value when ablated. Please refer to the <a href="https://captum.ai/api/layer.html#captum.attr.LayerFeatureAblation.attribute">Captum documentation</a> for more details.</li>
<li><strong>target</strong> (<code>Tensor</code> | <code>None</code>): the target batch of the training step.</li>
<li><strong>batch_idx</strong> (<code>int</code>): the index of the current batch. This is an argument of the forward function during training.</li>
<li><strong>num_batches</strong> (<code>int</code>): the number of batches in the training step. This is an argument of the forward function during training.</li>
<li><strong>if_captum</strong> (<code>bool</code>): whether to use Captum or not. If <code>True</code>, we use Captum to calculate the feature ablation. If <code>False</code>, we use our implementation. Default is <code>False</code>, because our implementation is much faster.</li>
</ul>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>
</ul>
</div>


                            </div>
                            <div id="FGAdaHAT.get_importance_step_layer_lrp_abs" class="classattr">
                                        <input id="FGAdaHAT.get_importance_step_layer_lrp_abs-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">get_importance_step_layer_lrp_abs</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>,</span><span class="param">	<span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="FGAdaHAT.get_importance_step_layer_lrp_abs-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FGAdaHAT.get_importance_step_layer_lrp_abs"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1509"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1509"><span class="linenos">1509</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_lrp_abs</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1510"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1510"><span class="linenos">1510</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1511"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1511"><span class="linenos">1511</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1512"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1512"><span class="linenos">1512</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
</span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1513"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1513"><span class="linenos">1513</span></a>        <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1514"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1514"><span class="linenos">1514</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1515"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1515"><span class="linenos">1515</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1516"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1516"><span class="linenos">1516</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1517"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1517"><span class="linenos">1517</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of [LRP](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140). We implement this using [Layer LRP](https://captum.ai/api/layer.html#layer-lrp) in Captum.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1518"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1518"><span class="linenos">1518</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1519"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1519"><span class="linenos">1519</span></a><span class="sd">        **Args:**</span>
</span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1520"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1520"><span class="linenos">1520</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1521"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1521"><span class="linenos">1521</span></a><span class="sd">        - **input** (`Tensor` | `tuple[Tensor, ...]`): the input batch of the training step.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1522"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1522"><span class="linenos">1522</span></a><span class="sd">        - **target** (`Tensor` | `None`): the target batch of the training step.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1523"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1523"><span class="linenos">1523</span></a><span class="sd">        - **batch_idx** (`int`): the index of the current batch. This is an argument of the forward function during training.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1524"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1524"><span class="linenos">1524</span></a><span class="sd">        - **num_batches** (`int`): the number of batches in the training step. This is an argument of the forward function during training.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1525"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1525"><span class="linenos">1525</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1526"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1526"><span class="linenos">1526</span></a><span class="sd">        **Returns:**</span>
</span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1527"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1527"><span class="linenos">1527</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1528"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1528"><span class="linenos">1528</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1529"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1529"><span class="linenos">1529</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1530"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1530"><span class="linenos">1530</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1531"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1531"><span class="linenos">1531</span></a>        <span class="c1"># initialize the Layer LRP object</span>
</span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1532"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1532"><span class="linenos">1532</span></a>        <span class="n">layer_lrp</span> <span class="o">=</span> <span class="n">LayerLRP</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="n">layer</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1533"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1533"><span class="linenos">1533</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1534"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1534"><span class="linenos">1534</span></a>        <span class="c1"># set model to evaluation mode to prevent updating the model parameters</span>
</span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1535"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1535"><span class="linenos">1535</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1536"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1536"><span class="linenos">1536</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1537"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1537"><span class="linenos">1537</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1538"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1538"><span class="linenos">1538</span></a>        <span class="c1"># calculate layer attribution of the step</span>
</span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1539"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1539"><span class="linenos">1539</span></a>        <span class="n">attribution</span> <span class="o">=</span> <span class="n">layer_lrp</span><span class="o">.</span><span class="n">attribute</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1540"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1540"><span class="linenos">1540</span></a>            <span class="n">inputs</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1541"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1541"><span class="linenos">1541</span></a>            <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1542"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1542"><span class="linenos">1542</span></a>            <span class="n">additional_forward_args</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">num_batches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">),</span>
</span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1543"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1543"><span class="linenos">1543</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1544"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1544"><span class="linenos">1544</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">set_forward_func_return_logits_only</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1545"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1545"><span class="linenos">1545</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1546"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1546"><span class="linenos">1546</span></a>        <span class="n">attribution_abs_batch_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1547"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1547"><span class="linenos">1547</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">attribution</span><span class="p">),</span>
</span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1548"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1548"><span class="linenos">1548</span></a>            <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1549"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1549"><span class="linenos">1549</span></a>                <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">attribution</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1550"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1550"><span class="linenos">1550</span></a>            <span class="p">],</span>  <span class="c1"># average the features over batch samples</span>
</span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1551"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1551"><span class="linenos">1551</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1552"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1552"><span class="linenos">1552</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1553"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1553"><span class="linenos">1553</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">attribution_abs_batch_mean</span>
</span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1554"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1554"><span class="linenos">1554</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1555"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1555"><span class="linenos">1555</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_lrp_abs-1556"><a href="#FGAdaHAT.get_importance_step_layer_lrp_abs-1556"><span class="linenos">1556</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span></pre></div>


            <div class="docstring"><p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140">LRP</a>. We implement this using <a href="https://captum.ai/api/layer.html#layer-lrp">Layer LRP</a> in Captum.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>
<li><strong>input</strong> (<code>Tensor</code> | <code>tuple[Tensor, ...]</code>): the input batch of the training step.</li>
<li><strong>target</strong> (<code>Tensor</code> | <code>None</code>): the target batch of the training step.</li>
<li><strong>batch_idx</strong> (<code>int</code>): the index of the current batch. This is an argument of the forward function during training.</li>
<li><strong>num_batches</strong> (<code>int</code>): the number of batches in the training step. This is an argument of the forward function during training.</li>
</ul>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>
</ul>
</div>


                            </div>
                            <div id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution" class="classattr">
                                        <input id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">get_importance_step_layer_cbp_adaptive_contribution</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span><span class="p">:</span> <span class="nb">str</span>, </span><span class="param"><span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span>, </span><span class="param"><span class="n">activation</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1558"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1558"><span class="linenos">1558</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_importance_step_layer_cbp_adaptive_contribution</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1559"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1559"><span class="linenos">1559</span></a>        <span class="bp">self</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1560"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1560"><span class="linenos">1560</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1561"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1561"><span class="linenos">1561</span></a>        <span class="n">activation</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1562"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1562"><span class="linenos">1562</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1563"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1563"><span class="linenos">1563</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the sum of absolute values of layer output weights multiplied by absolute values of activation, then divided by the reciprocal of sum of absolute values of layer input weights. It is equal to the adaptive contribution utility in [CBP](https://www.nature.com/articles/s41586-024-07711-7).</span>
</span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1564"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1564"><span class="linenos">1564</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1565"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1565"><span class="linenos">1565</span></a><span class="sd">        **Args:**</span>
</span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1566"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1566"><span class="linenos">1566</span></a><span class="sd">        - **layer_name** (`str`): the name of layer to get neuron-wise importance.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1567"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1567"><span class="linenos">1567</span></a><span class="sd">        - **activation** (`Tensor`): the activation tensor of the layer. It has the same size of (number of units, ).</span>
</span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1568"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1568"><span class="linenos">1568</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1569"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1569"><span class="linenos">1569</span></a><span class="sd">        **Returns:**</span>
</span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1570"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1570"><span class="linenos">1570</span></a><span class="sd">        - **importance_step_layer** (`Tensor`): the neuron-wise importance of the layer of the training step.</span>
</span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1571"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1571"><span class="linenos">1571</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1572"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1572"><span class="linenos">1572</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1573"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1573"><span class="linenos">1573</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1574"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1574"><span class="linenos">1574</span></a>        <span class="n">input_weight_abs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1575"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1575"><span class="linenos">1575</span></a>        <span class="n">input_weight_abs_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1576"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1576"><span class="linenos">1576</span></a>            <span class="n">input_weight_abs</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1577"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1577"><span class="linenos">1577</span></a>            <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1578"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1578"><span class="linenos">1578</span></a>                <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">input_weight_abs</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">0</span>
</span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1579"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1579"><span class="linenos">1579</span></a>            <span class="p">],</span>  <span class="c1"># sum over the input dimension</span>
</span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1580"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1580"><span class="linenos">1580</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1581"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1581"><span class="linenos">1581</span></a>        <span class="n">input_weight_abs_sum_reciprocal</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">reciprocal</span><span class="p">(</span><span class="n">input_weight_abs_sum</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1582"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1582"><span class="linenos">1582</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1583"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1583"><span class="linenos">1583</span></a>        <span class="n">output_weight_abs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">next_layer</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1584"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1584"><span class="linenos">1584</span></a>        <span class="n">output_weight_abs_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1585"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1585"><span class="linenos">1585</span></a>            <span class="n">output_weight_abs</span><span class="p">,</span>
</span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1586"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1586"><span class="linenos">1586</span></a>            <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1587"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1587"><span class="linenos">1587</span></a>                <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">output_weight_abs</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1588"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1588"><span class="linenos">1588</span></a>            <span class="p">],</span>  <span class="c1"># sum over the output dimension</span>
</span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1589"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1589"><span class="linenos">1589</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1590"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1590"><span class="linenos">1590</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1591"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1591"><span class="linenos">1591</span></a>        <span class="n">activation_abs_batch_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1592"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1592"><span class="linenos">1592</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">activation</span><span class="p">),</span>
</span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1593"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1593"><span class="linenos">1593</span></a>            <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1594"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1594"><span class="linenos">1594</span></a>                <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">activation</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1595"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1595"><span class="linenos">1595</span></a>            <span class="p">],</span>  <span class="c1"># average the features over batch samples</span>
</span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1596"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1596"><span class="linenos">1596</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1597"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1597"><span class="linenos">1597</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1598"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1598"><span class="linenos">1598</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1599"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1599"><span class="linenos">1599</span></a>            <span class="n">output_weight_abs_sum</span>
</span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1600"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1600"><span class="linenos">1600</span></a>            <span class="o">*</span> <span class="n">activation_abs_batch_mean</span>
</span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1601"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1601"><span class="linenos">1601</span></a>            <span class="o">*</span> <span class="n">input_weight_abs_sum_reciprocal</span>
</span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1602"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1602"><span class="linenos">1602</span></a>        <span class="p">)</span>
</span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1603"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1603"><span class="linenos">1603</span></a>        <span class="n">importance_step_layer</span> <span class="o">=</span> <span class="n">importance_step_layer</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1604"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1604"><span class="linenos">1604</span></a>
</span><span id="FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1605"><a href="#FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution-1605"><span class="linenos">1605</span></a>        <span class="k">return</span> <span class="n">importance_step_layer</span>
</span></pre></div>


            <div class="docstring"><p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\tau}_l(\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the sum of absolute values of layer output weights multiplied by absolute values of activation, then divided by the reciprocal of sum of absolute values of layer input weights. It is equal to the adaptive contribution utility in <a href="https://www.nature.com/articles/s41586-024-07711-7">CBP</a>.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>
<li><strong>activation</strong> (<code>Tensor</code>): the activation tensor of the layer. It has the same size of (number of units, ).</li>
</ul>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>
</ul>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt><a href="adahat.html#AdaHAT">clarena.cl_algorithms.adahat.AdaHAT</a></dt>
                                <dd id="FGAdaHAT.adjustment_intensity" class="variable"><a href="adahat.html#AdaHAT.adjustment_intensity">adjustment_intensity</a></dd>
                <dd id="FGAdaHAT.epsilon" class="variable"><a href="adahat.html#AdaHAT.epsilon">epsilon</a></dd>
                <dd id="FGAdaHAT.summative_mask_for_previous_tasks" class="variable"><a href="adahat.html#AdaHAT.summative_mask_for_previous_tasks">summative_mask_for_previous_tasks</a></dd>

            </div>
            <div><dt><a href="hat.html#HAT">clarena.cl_algorithms.hat.HAT</a></dt>
                                <dd id="FGAdaHAT.adjustment_mode" class="variable"><a href="hat.html#HAT.adjustment_mode">adjustment_mode</a></dd>
                <dd id="FGAdaHAT.s_max" class="variable"><a href="hat.html#HAT.s_max">s_max</a></dd>
                <dd id="FGAdaHAT.clamp_threshold" class="variable"><a href="hat.html#HAT.clamp_threshold">clamp_threshold</a></dd>
                <dd id="FGAdaHAT.mask_sparsity_reg_factor" class="variable"><a href="hat.html#HAT.mask_sparsity_reg_factor">mask_sparsity_reg_factor</a></dd>
                <dd id="FGAdaHAT.mask_sparsity_reg_mode" class="variable"><a href="hat.html#HAT.mask_sparsity_reg_mode">mask_sparsity_reg_mode</a></dd>
                <dd id="FGAdaHAT.mark_sparsity_reg" class="variable"><a href="hat.html#HAT.mark_sparsity_reg">mark_sparsity_reg</a></dd>
                <dd id="FGAdaHAT.task_embedding_init_mode" class="variable"><a href="hat.html#HAT.task_embedding_init_mode">task_embedding_init_mode</a></dd>
                <dd id="FGAdaHAT.alpha" class="variable"><a href="hat.html#HAT.alpha">alpha</a></dd>
                <dd id="FGAdaHAT.cumulative_mask_for_previous_tasks" class="variable"><a href="hat.html#HAT.cumulative_mask_for_previous_tasks">cumulative_mask_for_previous_tasks</a></dd>
                <dd id="FGAdaHAT.compensate_task_embedding_gradients" class="function"><a href="hat.html#HAT.compensate_task_embedding_gradients">compensate_task_embedding_gradients</a></dd>
                <dd id="FGAdaHAT.forward" class="function"><a href="hat.html#HAT.forward">forward</a></dd>
                <dd id="FGAdaHAT.training_step" class="function"><a href="hat.html#HAT.training_step">training_step</a></dd>
                <dd id="FGAdaHAT.validation_step" class="function"><a href="hat.html#HAT.validation_step">validation_step</a></dd>
                <dd id="FGAdaHAT.test_step" class="function"><a href="hat.html#HAT.test_step">test_step</a></dd>

            </div>
                                </dl>
                            </div>
                </section>
    </main>
<script>
    function escapeHTML(html) {
        return document.createElement('div').appendChild(document.createTextNode(html)).parentNode.innerHTML;
    }

    const originalContent = document.querySelector("main.pdoc");
    let currentContent = originalContent;

    function setContent(innerHTML) {
        let elem;
        if (innerHTML) {
            elem = document.createElement("main");
            elem.classList.add("pdoc");
            elem.innerHTML = innerHTML;
        } else {
            elem = originalContent;
        }
        if (currentContent !== elem) {
            currentContent.replaceWith(elem);
            currentContent = elem;
        }
    }

    function getSearchTerm() {
        return (new URL(window.location)).searchParams.get("search");
    }

    const searchBox = document.querySelector(".pdoc input[type=search]");
    searchBox.addEventListener("input", function () {
        let url = new URL(window.location);
        if (searchBox.value.trim()) {
            url.hash = "";
            url.searchParams.set("search", searchBox.value);
        } else {
            url.searchParams.delete("search");
        }
        history.replaceState("", "", url.toString());
        onInput();
    });
    window.addEventListener("popstate", onInput);


    let search, searchErr;

    async function initialize() {
        try {
            search = await new Promise((resolve, reject) => {
                const script = document.createElement("script");
                script.type = "text/javascript";
                script.async = true;
                script.onload = () => resolve(window.pdocSearch);
                script.onerror = (e) => reject(e);
                script.src = "../../search.js";
                document.getElementsByTagName("head")[0].appendChild(script);
            });
        } catch (e) {
            console.error("Cannot fetch pdoc search index");
            searchErr = "Cannot fetch search index.";
        }
        onInput();

        document.querySelector("nav.pdoc").addEventListener("click", e => {
            if (e.target.hash) {
                searchBox.value = "";
                searchBox.dispatchEvent(new Event("input"));
            }
        });
    }

    function onInput() {
        setContent((() => {
            const term = getSearchTerm();
            if (!term) {
                return null
            }
            if (searchErr) {
                return `<h3>Error: ${searchErr}</h3>`
            }
            if (!search) {
                return "<h3>Searching...</h3>"
            }

            window.scrollTo({top: 0, left: 0, behavior: 'auto'});

            const results = search(term);

            let html;
            if (results.length === 0) {
                html = `No search results for '${escapeHTML(term)}'.`
            } else {
                html = `<h4>${results.length} search result${results.length > 1 ? "s" : ""} for '${escapeHTML(term)}'.</h4>`;
            }
            for (let result of results.slice(0, 10)) {
                let doc = result.doc;
                let url = `../../${doc.modulename.replaceAll(".", "/")}.html`;
                if (doc.qualname) {
                    url += `#${doc.qualname}`;
                }

                let heading;
                switch (result.doc.kind) {
                    case "function":
                        if (doc.fullname.endsWith(".__init__")) {
                            heading = `<span class="name">${doc.fullname.replace(/\.__init__$/, "")}</span>${doc.signature}`;
                        } else {
                            heading = `<span class="def">${doc.funcdef}</span> <span class="name">${doc.fullname}</span>${doc.signature}`;
                        }
                        break;
                    case "class":
                        heading = `<span class="def">class</span> <span class="name">${doc.fullname}</span>`;
                        if (doc.bases)
                            heading += `<wbr>(<span class="base">${doc.bases}</span>)`;
                        heading += `:`;
                        break;
                    case "variable":
                        heading = `<span class="name">${doc.fullname}</span>`;
                        if (doc.annotation)
                            heading += `<span class="annotation">${doc.annotation}</span>`;
                        if (doc.default_value)
                            heading += `<span class="default_value"> = ${doc.default_value}</span>`;
                        break;
                    default:
                        heading = `<span class="name">${doc.fullname}</span>`;
                        break;
                }
                html += `
                        <section class="search-result">
                        <a href="${url}" class="attr ${doc.kind}">${heading}</a>
                        <div class="docstring">${doc.doc}</div>
                        </section>
                    `;

            }
            return html;
        })());
    }

    if (getSearchTerm()) {
        initialize();
        searchBox.value = getSearchTerm();
        onInput();
    } else {
        searchBox.addEventListener("focus", initialize, {once: true});
    }

    searchBox.addEventListener("keydown", e => {
        if (["ArrowDown", "ArrowUp", "Enter"].includes(e.key)) {
            let focused = currentContent.querySelector(".search-result.focused");
            if (!focused) {
                currentContent.querySelector(".search-result").classList.add("focused");
            } else if (
                e.key === "ArrowDown"
                && focused.nextElementSibling
                && focused.nextElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.nextElementSibling.classList.add("focused");
                focused.nextElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "ArrowUp"
                && focused.previousElementSibling
                && focused.previousElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.previousElementSibling.classList.add("focused");
                focused.previousElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "Enter"
            ) {
                focused.querySelector("a").click();
            }
        }
    });
</script></body>
</html>