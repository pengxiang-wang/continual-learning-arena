<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="pdoc 15.0.1"/>
    <title>clarena.cl_algorithms.cbphat API documentation</title>

    <style>/*! * Bootstrap Reboot v5.0.0 (https://getbootstrap.com/) * Copyright 2011-2021 The Bootstrap Authors * Copyright 2011-2021 Twitter, Inc. * Licensed under MIT (https://github.com/twbs/bootstrap/blob/main/LICENSE) * Forked from Normalize.css, licensed MIT (https://github.com/necolas/normalize.css/blob/master/LICENSE.md) */*,::after,::before{box-sizing:border-box}@media (prefers-reduced-motion:no-preference){:root{scroll-behavior:smooth}}body{margin:0;font-family:system-ui,-apple-system,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans","Liberation Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";font-size:1rem;font-weight:400;line-height:1.5;color:#212529;background-color:#fff;-webkit-text-size-adjust:100%;-webkit-tap-highlight-color:transparent}hr{margin:1rem 0;color:inherit;background-color:currentColor;border:0;opacity:.25}hr:not([size]){height:1px}h1,h2,h3,h4,h5,h6{margin-top:0;margin-bottom:.5rem;font-weight:500;line-height:1.2}h1{font-size:calc(1.375rem + 1.5vw)}@media (min-width:1200px){h1{font-size:2.5rem}}h2{font-size:calc(1.325rem + .9vw)}@media (min-width:1200px){h2{font-size:2rem}}h3{font-size:calc(1.3rem + .6vw)}@media (min-width:1200px){h3{font-size:1.75rem}}h4{font-size:calc(1.275rem + .3vw)}@media (min-width:1200px){h4{font-size:1.5rem}}h5{font-size:1.25rem}h6{font-size:1rem}p{margin-top:0;margin-bottom:1rem}abbr[data-bs-original-title],abbr[title]{-webkit-text-decoration:underline dotted;text-decoration:underline dotted;cursor:help;-webkit-text-decoration-skip-ink:none;text-decoration-skip-ink:none}address{margin-bottom:1rem;font-style:normal;line-height:inherit}ol,ul{padding-left:2rem}dl,ol,ul{margin-top:0;margin-bottom:1rem}ol ol,ol ul,ul ol,ul ul{margin-bottom:0}dt{font-weight:700}dd{margin-bottom:.5rem;margin-left:0}blockquote{margin:0 0 1rem}b,strong{font-weight:bolder}small{font-size:.875em}mark{padding:.2em;background-color:#fcf8e3}sub,sup{position:relative;font-size:.75em;line-height:0;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}a{color:#0d6efd;text-decoration:underline}a:hover{color:#0a58ca}a:not([href]):not([class]),a:not([href]):not([class]):hover{color:inherit;text-decoration:none}code,kbd,pre,samp{font-family:SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;font-size:1em;direction:ltr;unicode-bidi:bidi-override}pre{display:block;margin-top:0;margin-bottom:1rem;overflow:auto;font-size:.875em}pre code{font-size:inherit;color:inherit;word-break:normal}code{font-size:.875em;color:#d63384;word-wrap:break-word}a>code{color:inherit}kbd{padding:.2rem .4rem;font-size:.875em;color:#fff;background-color:#212529;border-radius:.2rem}kbd kbd{padding:0;font-size:1em;font-weight:700}figure{margin:0 0 1rem}img,svg{vertical-align:middle}table{caption-side:bottom;border-collapse:collapse}caption{padding-top:.5rem;padding-bottom:.5rem;color:#6c757d;text-align:left}th{text-align:inherit;text-align:-webkit-match-parent}tbody,td,tfoot,th,thead,tr{border-color:inherit;border-style:solid;border-width:0}label{display:inline-block}button{border-radius:0}button:focus:not(:focus-visible){outline:0}button,input,optgroup,select,textarea{margin:0;font-family:inherit;font-size:inherit;line-height:inherit}button,select{text-transform:none}[role=button]{cursor:pointer}select{word-wrap:normal}select:disabled{opacity:1}[list]::-webkit-calendar-picker-indicator{display:none}[type=button],[type=reset],[type=submit],button{-webkit-appearance:button}[type=button]:not(:disabled),[type=reset]:not(:disabled),[type=submit]:not(:disabled),button:not(:disabled){cursor:pointer}::-moz-focus-inner{padding:0;border-style:none}textarea{resize:vertical}fieldset{min-width:0;padding:0;margin:0;border:0}legend{float:left;width:100%;padding:0;margin-bottom:.5rem;font-size:calc(1.275rem + .3vw);line-height:inherit}@media (min-width:1200px){legend{font-size:1.5rem}}legend+*{clear:left}::-webkit-datetime-edit-day-field,::-webkit-datetime-edit-fields-wrapper,::-webkit-datetime-edit-hour-field,::-webkit-datetime-edit-minute,::-webkit-datetime-edit-month-field,::-webkit-datetime-edit-text,::-webkit-datetime-edit-year-field{padding:0}::-webkit-inner-spin-button{height:auto}[type=search]{outline-offset:-2px;-webkit-appearance:textfield}::-webkit-search-decoration{-webkit-appearance:none}::-webkit-color-swatch-wrapper{padding:0}::file-selector-button{font:inherit}::-webkit-file-upload-button{font:inherit;-webkit-appearance:button}output{display:inline-block}iframe{border:0}summary{display:list-item;cursor:pointer}progress{vertical-align:baseline}[hidden]{display:none!important}</style>
    <style>/*! syntax-highlighting.css */pre{line-height:125%;}span.linenos{color:inherit; background-color:transparent; padding-left:5px; padding-right:20px;}.pdoc-code .hll{background-color:#ffffcc}.pdoc-code{background:#f8f8f8;}.pdoc-code .c{color:#3D7B7B; font-style:italic}.pdoc-code .err{border:1px solid #FF0000}.pdoc-code .k{color:#008000; font-weight:bold}.pdoc-code .o{color:#666666}.pdoc-code .ch{color:#3D7B7B; font-style:italic}.pdoc-code .cm{color:#3D7B7B; font-style:italic}.pdoc-code .cp{color:#9C6500}.pdoc-code .cpf{color:#3D7B7B; font-style:italic}.pdoc-code .c1{color:#3D7B7B; font-style:italic}.pdoc-code .cs{color:#3D7B7B; font-style:italic}.pdoc-code .gd{color:#A00000}.pdoc-code .ge{font-style:italic}.pdoc-code .gr{color:#E40000}.pdoc-code .gh{color:#000080; font-weight:bold}.pdoc-code .gi{color:#008400}.pdoc-code .go{color:#717171}.pdoc-code .gp{color:#000080; font-weight:bold}.pdoc-code .gs{font-weight:bold}.pdoc-code .gu{color:#800080; font-weight:bold}.pdoc-code .gt{color:#0044DD}.pdoc-code .kc{color:#008000; font-weight:bold}.pdoc-code .kd{color:#008000; font-weight:bold}.pdoc-code .kn{color:#008000; font-weight:bold}.pdoc-code .kp{color:#008000}.pdoc-code .kr{color:#008000; font-weight:bold}.pdoc-code .kt{color:#B00040}.pdoc-code .m{color:#666666}.pdoc-code .s{color:#BA2121}.pdoc-code .na{color:#687822}.pdoc-code .nb{color:#008000}.pdoc-code .nc{color:#0000FF; font-weight:bold}.pdoc-code .no{color:#880000}.pdoc-code .nd{color:#AA22FF}.pdoc-code .ni{color:#717171; font-weight:bold}.pdoc-code .ne{color:#CB3F38; font-weight:bold}.pdoc-code .nf{color:#0000FF}.pdoc-code .nl{color:#767600}.pdoc-code .nn{color:#0000FF; font-weight:bold}.pdoc-code .nt{color:#008000; font-weight:bold}.pdoc-code .nv{color:#19177C}.pdoc-code .ow{color:#AA22FF; font-weight:bold}.pdoc-code .w{color:#bbbbbb}.pdoc-code .mb{color:#666666}.pdoc-code .mf{color:#666666}.pdoc-code .mh{color:#666666}.pdoc-code .mi{color:#666666}.pdoc-code .mo{color:#666666}.pdoc-code .sa{color:#BA2121}.pdoc-code .sb{color:#BA2121}.pdoc-code .sc{color:#BA2121}.pdoc-code .dl{color:#BA2121}.pdoc-code .sd{color:#BA2121; font-style:italic}.pdoc-code .s2{color:#BA2121}.pdoc-code .se{color:#AA5D1F; font-weight:bold}.pdoc-code .sh{color:#BA2121}.pdoc-code .si{color:#A45A77; font-weight:bold}.pdoc-code .sx{color:#008000}.pdoc-code .sr{color:#A45A77}.pdoc-code .s1{color:#BA2121}.pdoc-code .ss{color:#19177C}.pdoc-code .bp{color:#008000}.pdoc-code .fm{color:#0000FF}.pdoc-code .vc{color:#19177C}.pdoc-code .vg{color:#19177C}.pdoc-code .vi{color:#19177C}.pdoc-code .vm{color:#19177C}.pdoc-code .il{color:#666666}</style>
    <style>/*! theme.css */:root{--pdoc-background:#fff;}.pdoc{--text:#212529;--muted:#6c757d;--link:#3660a5;--link-hover:#1659c5;--code:#f8f8f8;--active:#fff598;--accent:#eee;--accent2:#c1c1c1;--nav-hover:rgba(255, 255, 255, 0.5);--name:#0066BB;--def:#008800;--annotation:#007020;}</style>
    <style>/*! layout.css */html, body{width:100%;height:100%;}html, main{scroll-behavior:smooth;}body{background-color:var(--pdoc-background);}@media (max-width:769px){#navtoggle{cursor:pointer;position:absolute;width:50px;height:40px;top:1rem;right:1rem;border-color:var(--text);color:var(--text);display:flex;opacity:0.8;z-index:999;}#navtoggle:hover{opacity:1;}#togglestate + div{display:none;}#togglestate:checked + div{display:inherit;}main, header{padding:2rem 3vw;}header + main{margin-top:-3rem;}.git-button{display:none !important;}nav input[type="search"]{max-width:77%;}nav input[type="search"]:first-child{margin-top:-6px;}nav input[type="search"]:valid ~ *{display:none !important;}}@media (min-width:770px){:root{--sidebar-width:clamp(12.5rem, 28vw, 22rem);}nav{position:fixed;overflow:auto;height:100vh;width:var(--sidebar-width);}main, header{padding:3rem 2rem 3rem calc(var(--sidebar-width) + 3rem);width:calc(54rem + var(--sidebar-width));max-width:100%;}header + main{margin-top:-4rem;}#navtoggle{display:none;}}#togglestate{position:absolute;height:0;opacity:0;}nav.pdoc{--pad:clamp(0.5rem, 2vw, 1.75rem);--indent:1.5rem;background-color:var(--accent);border-right:1px solid var(--accent2);box-shadow:0 0 20px rgba(50, 50, 50, .2) inset;padding:0 0 0 var(--pad);overflow-wrap:anywhere;scrollbar-width:thin; scrollbar-color:var(--accent2) transparent; z-index:1}nav.pdoc::-webkit-scrollbar{width:.4rem; }nav.pdoc::-webkit-scrollbar-thumb{background-color:var(--accent2); }nav.pdoc > div{padding:var(--pad) 0;}nav.pdoc .module-list-button{display:inline-flex;align-items:center;color:var(--text);border-color:var(--muted);margin-bottom:1rem;}nav.pdoc .module-list-button:hover{border-color:var(--text);}nav.pdoc input[type=search]{display:block;outline-offset:0;width:calc(100% - var(--pad));}nav.pdoc .logo{max-width:calc(100% - var(--pad));max-height:35vh;display:block;margin:0 auto 1rem;transform:translate(calc(-.5 * var(--pad)), 0);}nav.pdoc ul{list-style:none;padding-left:0;}nav.pdoc > div > ul{margin-left:calc(0px - var(--pad));}nav.pdoc li a{padding:.2rem 0 .2rem calc(var(--pad) + var(--indent));}nav.pdoc > div > ul > li > a{padding-left:var(--pad);}nav.pdoc li{transition:all 100ms;}nav.pdoc li:hover{background-color:var(--nav-hover);}nav.pdoc a, nav.pdoc a:hover{color:var(--text);}nav.pdoc a{display:block;}nav.pdoc > h2:first-of-type{margin-top:1.5rem;}nav.pdoc .class:before{content:"class ";color:var(--muted);}nav.pdoc .function:after{content:"()";color:var(--muted);}nav.pdoc footer:before{content:"";display:block;width:calc(100% - var(--pad));border-top:solid var(--accent2) 1px;margin-top:1.5rem;padding-top:.5rem;}nav.pdoc footer{font-size:small;}</style>
    <style>/*! content.css */.pdoc{color:var(--text);box-sizing:border-box;line-height:1.5;background:none;}.pdoc .pdoc-button{cursor:pointer;display:inline-block;border:solid black 1px;border-radius:2px;font-size:.75rem;padding:calc(0.5em - 1px) 1em;transition:100ms all;}.pdoc .alert{padding:1rem 1rem 1rem calc(1.5rem + 24px);border:1px solid transparent;border-radius:.25rem;background-repeat:no-repeat;background-position:.75rem center;margin-bottom:1rem;}.pdoc .alert > em{display:none;}.pdoc .alert > *:last-child{margin-bottom:0;}.pdoc .alert.note {color:#084298;background-color:#cfe2ff;border-color:#b6d4fe;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23084298%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8%2016A8%208%200%201%200%208%200a8%208%200%200%200%200%2016zm.93-9.412-1%204.705c-.07.34.029.533.304.533.194%200%20.487-.07.686-.246l-.088.416c-.287.346-.92.598-1.465.598-.703%200-1.002-.422-.808-1.319l.738-3.468c.064-.293.006-.399-.287-.47l-.451-.081.082-.381%202.29-.287zM8%205.5a1%201%200%201%201%200-2%201%201%200%200%201%200%202z%22/%3E%3C/svg%3E");}.pdoc .alert.warning{color:#664d03;background-color:#fff3cd;border-color:#ffecb5;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23664d03%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8.982%201.566a1.13%201.13%200%200%200-1.96%200L.165%2013.233c-.457.778.091%201.767.98%201.767h13.713c.889%200%201.438-.99.98-1.767L8.982%201.566zM8%205c.535%200%20.954.462.9.995l-.35%203.507a.552.552%200%200%201-1.1%200L7.1%205.995A.905.905%200%200%201%208%205zm.002%206a1%201%200%201%201%200%202%201%201%200%200%201%200-2z%22/%3E%3C/svg%3E");}.pdoc .alert.danger{color:#842029;background-color:#f8d7da;border-color:#f5c2c7;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23842029%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M5.52.359A.5.5%200%200%201%206%200h4a.5.5%200%200%201%20.474.658L8.694%206H12.5a.5.5%200%200%201%20.395.807l-7%209a.5.5%200%200%201-.873-.454L6.823%209.5H3.5a.5.5%200%200%201-.48-.641l2.5-8.5z%22/%3E%3C/svg%3E");}.pdoc .visually-hidden{position:absolute !important;width:1px !important;height:1px !important;padding:0 !important;margin:-1px !important;overflow:hidden !important;clip:rect(0, 0, 0, 0) !important;white-space:nowrap !important;border:0 !important;}.pdoc h1, .pdoc h2, .pdoc h3{font-weight:300;margin:.3em 0;padding:.2em 0;}.pdoc > section:not(.module-info) h1{font-size:1.5rem;font-weight:500;}.pdoc > section:not(.module-info) h2{font-size:1.4rem;font-weight:500;}.pdoc > section:not(.module-info) h3{font-size:1.3rem;font-weight:500;}.pdoc > section:not(.module-info) h4{font-size:1.2rem;}.pdoc > section:not(.module-info) h5{font-size:1.1rem;}.pdoc a{text-decoration:none;color:var(--link);}.pdoc a:hover{color:var(--link-hover);}.pdoc blockquote{margin-left:2rem;}.pdoc pre{border-top:1px solid var(--accent2);border-bottom:1px solid var(--accent2);margin-top:0;margin-bottom:1em;padding:.5rem 0 .5rem .5rem;overflow-x:auto;background-color:var(--code);}.pdoc code{color:var(--text);padding:.2em .4em;margin:0;font-size:85%;background-color:var(--accent);border-radius:6px;}.pdoc a > code{color:inherit;}.pdoc pre > code{display:inline-block;font-size:inherit;background:none;border:none;padding:0;}.pdoc > section:not(.module-info){margin-bottom:1.5rem;}.pdoc .modulename{margin-top:0;font-weight:bold;}.pdoc .modulename a{color:var(--link);transition:100ms all;}.pdoc .git-button{float:right;border:solid var(--link) 1px;}.pdoc .git-button:hover{background-color:var(--link);color:var(--pdoc-background);}.view-source-toggle-state,.view-source-toggle-state ~ .pdoc-code{display:none;}.view-source-toggle-state:checked ~ .pdoc-code{display:block;}.view-source-button{display:inline-block;float:right;font-size:.75rem;line-height:1.5rem;color:var(--muted);padding:0 .4rem 0 1.3rem;cursor:pointer;text-indent:-2px;}.view-source-button > span{visibility:hidden;}.module-info .view-source-button{float:none;display:flex;justify-content:flex-end;margin:-1.2rem .4rem -.2rem 0;}.view-source-button::before{position:absolute;content:"View Source";display:list-item;list-style-type:disclosure-closed;}.view-source-toggle-state:checked ~ .attr .view-source-button::before,.view-source-toggle-state:checked ~ .view-source-button::before{list-style-type:disclosure-open;}.pdoc .docstring{margin-bottom:1.5rem;}.pdoc section:not(.module-info) .docstring{margin-left:clamp(0rem, 5vw - 2rem, 1rem);}.pdoc .docstring .pdoc-code{margin-left:1em;margin-right:1em;}.pdoc h1:target,.pdoc h2:target,.pdoc h3:target,.pdoc h4:target,.pdoc h5:target,.pdoc h6:target,.pdoc .pdoc-code > pre > span:target{background-color:var(--active);box-shadow:-1rem 0 0 0 var(--active);}.pdoc .pdoc-code > pre > span:target{display:block;}.pdoc div:target > .attr,.pdoc section:target > .attr,.pdoc dd:target > a{background-color:var(--active);}.pdoc *{scroll-margin:2rem;}.pdoc .pdoc-code .linenos{user-select:none;}.pdoc .attr:hover{filter:contrast(0.95);}.pdoc section, .pdoc .classattr{position:relative;}.pdoc .headerlink{--width:clamp(1rem, 3vw, 2rem);position:absolute;top:0;left:calc(0rem - var(--width));transition:all 100ms ease-in-out;opacity:0;}.pdoc .headerlink::before{content:"#";display:block;text-align:center;width:var(--width);height:2.3rem;line-height:2.3rem;font-size:1.5rem;}.pdoc .attr:hover ~ .headerlink,.pdoc *:target > .headerlink,.pdoc .headerlink:hover{opacity:1;}.pdoc .attr{display:block;margin:.5rem 0 .5rem;padding:.4rem .4rem .4rem 1rem;background-color:var(--accent);overflow-x:auto;}.pdoc .classattr{margin-left:2rem;}.pdoc .decorator-deprecated{color:#842029;}.pdoc .decorator-deprecated ~ span{filter:grayscale(1) opacity(0.8);}.pdoc .name{color:var(--name);font-weight:bold;}.pdoc .def{color:var(--def);font-weight:bold;}.pdoc .signature{background-color:transparent;}.pdoc .param, .pdoc .return-annotation{white-space:pre;}.pdoc .signature.multiline .param{display:block;}.pdoc .signature.condensed .param{display:inline-block;}.pdoc .annotation{color:var(--annotation);}.pdoc .view-value-toggle-state,.pdoc .view-value-toggle-state ~ .default_value{display:none;}.pdoc .view-value-toggle-state:checked ~ .default_value{display:inherit;}.pdoc .view-value-button{font-size:.5rem;vertical-align:middle;border-style:dashed;margin-top:-0.1rem;}.pdoc .view-value-button:hover{background:white;}.pdoc .view-value-button::before{content:"show";text-align:center;width:2.2em;display:inline-block;}.pdoc .view-value-toggle-state:checked ~ .view-value-button::before{content:"hide";}.pdoc .inherited{margin-left:2rem;}.pdoc .inherited dt{font-weight:700;}.pdoc .inherited dt, .pdoc .inherited dd{display:inline;margin-left:0;margin-bottom:.5rem;}.pdoc .inherited dd:not(:last-child):after{content:", ";}.pdoc .inherited .class:before{content:"class ";}.pdoc .inherited .function a:after{content:"()";}.pdoc .search-result .docstring{overflow:auto;max-height:25vh;}.pdoc .search-result.focused > .attr{background-color:var(--active);}.pdoc .attribution{margin-top:2rem;display:block;opacity:0.5;transition:all 200ms;filter:grayscale(100%);}.pdoc .attribution:hover{opacity:1;filter:grayscale(0%);}.pdoc .attribution img{margin-left:5px;height:35px;vertical-align:middle;width:70px;transition:all 200ms;}.pdoc table{display:block;width:max-content;max-width:100%;overflow:auto;margin-bottom:1rem;}.pdoc table th{font-weight:600;}.pdoc table th, .pdoc table td{padding:6px 13px;border:1px solid var(--accent2);}</style>
    <style>/*! custom.css */</style></head>
<body>
    <nav class="pdoc">
        <label id="navtoggle" for="togglestate" class="pdoc-button"><svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 30 30'><path stroke-linecap='round' stroke="currentColor" stroke-miterlimit='10' stroke-width='2' d='M4 7h22M4 15h22M4 23h22'/></svg></label>
        <input id="togglestate" type="checkbox" aria-hidden="true" tabindex="-1">
        <div>            <a class="pdoc-button module-list-button" href="../cl_algorithms.html">
<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-box-arrow-in-left" viewBox="0 0 16 16">
  <path fill-rule="evenodd" d="M10 3.5a.5.5 0 0 0-.5-.5h-8a.5.5 0 0 0-.5.5v9a.5.5 0 0 0 .5.5h8a.5.5 0 0 0 .5-.5v-2a.5.5 0 0 1 1 0v2A1.5 1.5 0 0 1 9.5 14h-8A1.5 1.5 0 0 1 0 12.5v-9A1.5 1.5 0 0 1 1.5 2h8A1.5 1.5 0 0 1 11 3.5v2a.5.5 0 0 1-1 0v-2z"/>
  <path fill-rule="evenodd" d="M4.146 8.354a.5.5 0 0 1 0-.708l3-3a.5.5 0 1 1 .708.708L5.707 7.5H14.5a.5.5 0 0 1 0 1H5.707l2.147 2.146a.5.5 0 0 1-.708.708l-3-3z"/>
</svg>                &nbsp;clarena.cl_algorithms</a>


            <input type="search" placeholder="Search..." role="searchbox" aria-label="search"
                   pattern=".+" required>



            <h2>API Documentation</h2>
                <ul class="memberlist">
            <li>
                    <a class="class" href="#CBPHAT">CBPHAT</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#CBPHAT.__init__">CBPHAT</a>
                        </li>
                        <li>
                                <a class="variable" href="#CBPHAT.utility_decay_rate">utility_decay_rate</a>
                        </li>
                        <li>
                                <a class="variable" href="#CBPHAT.contribution_utility_t">contribution_utility_t</a>
                        </li>
                        <li>
                                <a class="variable" href="#CBPHAT.unit_importance_for_previous_tasks">unit_importance_for_previous_tasks</a>
                        </li>
                        <li>
                                <a class="variable" href="#CBPHAT.age_t">age_t</a>
                        </li>
                        <li>
                                <a class="variable" href="#CBPHAT.automatic_optimization">automatic_optimization</a>
                        </li>
                        <li>
                                <a class="function" href="#CBPHAT.sanity_check">sanity_check</a>
                        </li>
                        <li>
                                <a class="function" href="#CBPHAT.on_train_start">on_train_start</a>
                        </li>
                        <li>
                                <a class="function" href="#CBPHAT.clip_grad_by_adjustment">clip_grad_by_adjustment</a>
                        </li>
                        <li>
                                <a class="function" href="#CBPHAT.on_train_batch_end">on_train_batch_end</a>
                        </li>
                        <li>
                                <a class="function" href="#CBPHAT.on_train_end">on_train_end</a>
                        </li>
                </ul>

            </li>
    </ul>



        <a class="attribution" title="pdoc: Python API documentation generator" href="https://pdoc.dev" target="_blank">
            built with <span class="visually-hidden">pdoc</span><img
                alt="pdoc logo"
                src="data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20role%3D%22img%22%20aria-label%3D%22pdoc%20logo%22%20width%3D%22300%22%20height%3D%22150%22%20viewBox%3D%22-1%200%2060%2030%22%3E%3Ctitle%3Epdoc%3C/title%3E%3Cpath%20d%3D%22M29.621%2021.293c-.011-.273-.214-.475-.511-.481a.5.5%200%200%200-.489.503l-.044%201.393c-.097.551-.695%201.215-1.566%201.704-.577.428-1.306.486-2.193.182-1.426-.617-2.467-1.654-3.304-2.487l-.173-.172a3.43%203.43%200%200%200-.365-.306.49.49%200%200%200-.286-.196c-1.718-1.06-4.931-1.47-7.353.191l-.219.15c-1.707%201.187-3.413%202.131-4.328%201.03-.02-.027-.49-.685-.141-1.763.233-.721.546-2.408.772-4.076.042-.09.067-.187.046-.288.166-1.347.277-2.625.241-3.351%201.378-1.008%202.271-2.586%202.271-4.362%200-.976-.272-1.935-.788-2.774-.057-.094-.122-.18-.184-.268.033-.167.052-.339.052-.516%200-1.477-1.202-2.679-2.679-2.679-.791%200-1.496.352-1.987.9a6.3%206.3%200%200%200-1.001.029c-.492-.564-1.207-.929-2.012-.929-1.477%200-2.679%201.202-2.679%202.679A2.65%202.65%200%200%200%20.97%206.554c-.383.747-.595%201.572-.595%202.41%200%202.311%201.507%204.29%203.635%205.107-.037.699-.147%202.27-.423%203.294l-.137.461c-.622%202.042-2.515%208.257%201.727%2010.643%201.614.908%203.06%201.248%204.317%201.248%202.665%200%204.492-1.524%205.322-2.401%201.476-1.559%202.886-1.854%206.491.82%201.877%201.393%203.514%201.753%204.861%201.068%202.223-1.713%202.811-3.867%203.399-6.374.077-.846.056-1.469.054-1.537zm-4.835%204.313c-.054.305-.156.586-.242.629-.034-.007-.131-.022-.307-.157-.145-.111-.314-.478-.456-.908.221.121.432.25.675.355.115.039.219.051.33.081zm-2.251-1.238c-.05.33-.158.648-.252.694-.022.001-.125-.018-.307-.157-.217-.166-.488-.906-.639-1.573.358.344.754.693%201.198%201.036zm-3.887-2.337c-.006-.116-.018-.231-.041-.342.635.145%201.189.368%201.599.625.097.231.166.481.174.642-.03.049-.055.101-.067.158-.046.013-.128.026-.298.004-.278-.037-.901-.57-1.367-1.087zm-1.127-.497c.116.306.176.625.12.71-.019.014-.117.045-.345.016-.206-.027-.604-.332-.986-.695.41-.051.816-.056%201.211-.031zm-4.535%201.535c.209.22.379.47.358.598-.006.041-.088.138-.351.234-.144.055-.539-.063-.979-.259a11.66%2011.66%200%200%200%20.972-.573zm.983-.664c.359-.237.738-.418%201.126-.554.25.237.479.548.457.694-.006.042-.087.138-.351.235-.174.064-.694-.105-1.232-.375zm-3.381%201.794c-.022.145-.061.29-.149.401-.133.166-.358.248-.69.251h-.002c-.133%200-.306-.26-.45-.621.417.091.854.07%201.291-.031zm-2.066-8.077a4.78%204.78%200%200%201-.775-.584c.172-.115.505-.254.88-.378l-.105.962zm-.331%202.302a10.32%2010.32%200%200%201-.828-.502c.202-.143.576-.328.984-.49l-.156.992zm-.45%202.157l-.701-.403c.214-.115.536-.249.891-.376a11.57%2011.57%200%200%201-.19.779zm-.181%201.716c.064.398.194.702.298.893-.194-.051-.435-.162-.736-.398.061-.119.224-.3.438-.495zM8.87%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zm-.735-.389a1.15%201.15%200%200%200-.314.783%201.16%201.16%200%200%200%201.162%201.162c.457%200%20.842-.27%201.032-.653.026.117.042.238.042.362a1.68%201.68%200%200%201-1.679%201.679%201.68%201.68%200%200%201-1.679-1.679c0-.843.626-1.535%201.436-1.654zM5.059%205.406A1.68%201.68%200%200%201%203.38%207.085a1.68%201.68%200%200%201-1.679-1.679c0-.037.009-.072.011-.109.21.3.541.508.935.508a1.16%201.16%200%200%200%201.162-1.162%201.14%201.14%200%200%200-.474-.912c.015%200%20.03-.005.045-.005.926.001%201.679.754%201.679%201.68zM3.198%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zM1.375%208.964c0-.52.103-1.035.288-1.52.466.394%201.06.64%201.717.64%201.144%200%202.116-.725%202.499-1.738.383%201.012%201.355%201.738%202.499%201.738.867%200%201.631-.421%202.121-1.062.307.605.478%201.267.478%201.942%200%202.486-2.153%204.51-4.801%204.51s-4.801-2.023-4.801-4.51zm24.342%2019.349c-.985.498-2.267.168-3.813-.979-3.073-2.281-5.453-3.199-7.813-.705-1.315%201.391-4.163%203.365-8.423.97-3.174-1.786-2.239-6.266-1.261-9.479l.146-.492c.276-1.02.395-2.457.444-3.268a6.11%206.11%200%200%200%201.18.115%206.01%206.01%200%200%200%202.536-.562l-.006.175c-.802.215-1.848.612-2.021%201.25-.079.295.021.601.274.837.219.203.415.364.598.501-.667.304-1.243.698-1.311%201.179-.02.144-.022.507.393.787.213.144.395.26.564.365-1.285.521-1.361.96-1.381%201.126-.018.142-.011.496.427.746l.854.489c-.473.389-.971.914-.999%201.429-.018.278.095.532.316.713.675.556%201.231.721%201.653.721.059%200%20.104-.014.158-.02.207.707.641%201.64%201.513%201.64h.013c.8-.008%201.236-.345%201.462-.626.173-.216.268-.457.325-.692.424.195.93.374%201.372.374.151%200%20.294-.021.423-.068.732-.27.944-.704.993-1.021.009-.061.003-.119.002-.179.266.086.538.147.789.147.15%200%20.294-.021.423-.069.542-.2.797-.489.914-.754.237.147.478.258.704.288.106.014.205.021.296.021.356%200%20.595-.101.767-.229.438.435%201.094.992%201.656%201.067.106.014.205.021.296.021a1.56%201.56%200%200%200%20.323-.035c.17.575.453%201.289.866%201.605.358.273.665.362.914.362a.99.99%200%200%200%20.421-.093%201.03%201.03%200%200%200%20.245-.164c.168.428.39.846.68%201.068.358.273.665.362.913.362a.99.99%200%200%200%20.421-.093c.317-.148.512-.448.639-.762.251.157.495.257.726.257.127%200%20.25-.024.37-.071.427-.17.706-.617.841-1.314.022-.015.047-.022.068-.038.067-.051.133-.104.196-.159-.443%201.486-1.107%202.761-2.086%203.257zM8.66%209.925a.5.5%200%201%200-1%200c0%20.653-.818%201.205-1.787%201.205s-1.787-.552-1.787-1.205a.5.5%200%201%200-1%200c0%201.216%201.25%202.205%202.787%202.205s2.787-.989%202.787-2.205zm4.4%2015.965l-.208.097c-2.661%201.258-4.708%201.436-6.086.527-1.542-1.017-1.88-3.19-1.844-4.198a.4.4%200%200%200-.385-.414c-.242-.029-.406.164-.414.385-.046%201.249.367%203.686%202.202%204.896.708.467%201.547.7%202.51.7%201.248%200%202.706-.392%204.362-1.174l.185-.086a.4.4%200%200%200%20.205-.527c-.089-.204-.326-.291-.527-.206zM9.547%202.292c.093.077.205.114.317.114a.5.5%200%200%200%20.318-.886L8.817.397a.5.5%200%200%200-.703.068.5.5%200%200%200%20.069.703l1.364%201.124zm-7.661-.065c.086%200%20.173-.022.253-.068l1.523-.893a.5.5%200%200%200-.506-.863l-1.523.892a.5.5%200%200%200-.179.685c.094.158.261.247.432.247z%22%20transform%3D%22matrix%28-1%200%200%201%2058%200%29%22%20fill%3D%22%233bb300%22/%3E%3Cpath%20d%3D%22M.3%2021.86V10.18q0-.46.02-.68.04-.22.18-.5.28-.54%201.34-.54%201.06%200%201.42.28.38.26.44.78.76-1.04%202.38-1.04%201.64%200%203.1%201.54%201.46%201.54%201.46%203.58%200%202.04-1.46%203.58-1.44%201.54-3.08%201.54-1.64%200-2.38-.92v4.04q0%20.46-.04.68-.02.22-.18.5-.14.3-.5.42-.36.12-.98.12-.62%200-1-.12-.36-.12-.52-.4-.14-.28-.18-.5-.02-.22-.02-.68zm3.96-9.42q-.46.54-.46%201.18%200%20.64.46%201.18.48.52%201.2.52.74%200%201.24-.52.52-.52.52-1.18%200-.66-.48-1.18-.48-.54-1.26-.54-.76%200-1.22.54zm14.741-8.36q.16-.3.54-.42.38-.12%201-.12.64%200%201.02.12.38.12.52.42.16.3.18.54.04.22.04.68v11.94q0%20.46-.04.7-.02.22-.18.5-.3.54-1.7.54-1.38%200-1.54-.98-.84.96-2.34.96-1.8%200-3.28-1.56-1.48-1.58-1.48-3.66%200-2.1%201.48-3.68%201.5-1.58%203.28-1.58%201.48%200%202.3%201v-4.2q0-.46.02-.68.04-.24.18-.52zm-3.24%2010.86q.52.54%201.26.54.74%200%201.22-.54.5-.54.5-1.18%200-.66-.48-1.22-.46-.56-1.26-.56-.8%200-1.28.56-.48.54-.48%201.2%200%20.66.52%201.2zm7.833-1.2q0-2.4%201.68-3.96%201.68-1.56%203.84-1.56%202.16%200%203.82%201.56%201.66%201.54%201.66%203.94%200%201.66-.86%202.96-.86%201.28-2.1%201.9-1.22.6-2.54.6-1.32%200-2.56-.64-1.24-.66-2.1-1.92-.84-1.28-.84-2.88zm4.18%201.44q.64.48%201.3.48.66%200%201.32-.5.66-.5.66-1.48%200-.98-.62-1.46-.62-.48-1.34-.48-.72%200-1.34.5-.62.5-.62%201.48%200%20.96.64%201.46zm11.412-1.44q0%20.84.56%201.32.56.46%201.18.46.64%200%201.18-.36.56-.38.9-.38.6%200%201.46%201.06.46.58.46%201.04%200%20.76-1.1%201.42-1.14.8-2.8.8-1.86%200-3.58-1.34-.82-.64-1.34-1.7-.52-1.08-.52-2.36%200-1.3.52-2.34.52-1.06%201.34-1.7%201.66-1.32%203.54-1.32.76%200%201.48.22.72.2%201.06.4l.32.2q.36.24.56.38.52.4.52.92%200%20.5-.42%201.14-.72%201.1-1.38%201.1-.38%200-1.08-.44-.36-.34-1.04-.34-.66%200-1.24.48-.58.48-.58%201.34z%22%20fill%3D%22green%22/%3E%3C/svg%3E"/>
        </a>
</div>
    </nav>
    <main class="pdoc">
            <section class="module-info">
                    <h1 class="modulename">
<a href="./../../clarena.html">clarena</a><wbr>.<a href="./../cl_algorithms.html">cl_algorithms</a><wbr>.cbphat    </h1>

                        <div class="docstring"><p>The submodule in <code>cl_algorithms</code> for CBPHAT algorithm.</p>
</div>

                        <input id="mod-cbphat-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">

                        <label class="view-source-button" for="mod-cbphat-view-source"><span>View Source</span></label>

                        <div class="pdoc-code codehilite"><pre><span></span><span id="L-1"><a href="#L-1"><span class="linenos">  1</span></a><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-2"><a href="#L-2"><span class="linenos">  2</span></a><span class="sd">The submodule in `cl_algorithms` for CBPHAT algorithm.</span>
</span><span id="L-3"><a href="#L-3"><span class="linenos">  3</span></a><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-4"><a href="#L-4"><span class="linenos">  4</span></a>
</span><span id="L-5"><a href="#L-5"><span class="linenos">  5</span></a><span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;CBPHAT&quot;</span><span class="p">]</span>
</span><span id="L-6"><a href="#L-6"><span class="linenos">  6</span></a>
</span><span id="L-7"><a href="#L-7"><span class="linenos">  7</span></a><span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
</span><span id="L-8"><a href="#L-8"><span class="linenos">  8</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Any</span>
</span><span id="L-9"><a href="#L-9"><span class="linenos">  9</span></a>
</span><span id="L-10"><a href="#L-10"><span class="linenos"> 10</span></a><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span id="L-11"><a href="#L-11"><span class="linenos"> 11</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tensor</span>
</span><span id="L-12"><a href="#L-12"><span class="linenos"> 12</span></a>
</span><span id="L-13"><a href="#L-13"><span class="linenos"> 13</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">clarena.backbones</span><span class="w"> </span><span class="kn">import</span> <span class="n">HATMaskBackbone</span>
</span><span id="L-14"><a href="#L-14"><span class="linenos"> 14</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">clarena.cl_algorithms</span><span class="w"> </span><span class="kn">import</span> <span class="n">AdaHAT</span>
</span><span id="L-15"><a href="#L-15"><span class="linenos"> 15</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">clarena.cl_heads</span><span class="w"> </span><span class="kn">import</span> <span class="n">HeadsCIL</span><span class="p">,</span> <span class="n">HeadsTIL</span>
</span><span id="L-16"><a href="#L-16"><span class="linenos"> 16</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">clarena.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">HATNetworkCapacity</span>
</span><span id="L-17"><a href="#L-17"><span class="linenos"> 17</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">clarena.utils.transforms</span><span class="w"> </span><span class="kn">import</span> <span class="n">min_max_normalise</span>
</span><span id="L-18"><a href="#L-18"><span class="linenos"> 18</span></a>
</span><span id="L-19"><a href="#L-19"><span class="linenos"> 19</span></a><span class="c1"># always get logger for built-in logging in each module</span>
</span><span id="L-20"><a href="#L-20"><span class="linenos"> 20</span></a><span class="n">pylogger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>
</span><span id="L-21"><a href="#L-21"><span class="linenos"> 21</span></a>
</span><span id="L-22"><a href="#L-22"><span class="linenos"> 22</span></a>
</span><span id="L-23"><a href="#L-23"><span class="linenos"> 23</span></a><span class="k">class</span><span class="w"> </span><span class="nc">CBPHAT</span><span class="p">(</span><span class="n">AdaHAT</span><span class="p">):</span>
</span><span id="L-24"><a href="#L-24"><span class="linenos"> 24</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;CBPHAT algorithm.</span>
</span><span id="L-25"><a href="#L-25"><span class="linenos"> 25</span></a>
</span><span id="L-26"><a href="#L-26"><span class="linenos"> 26</span></a><span class="sd">    CBPHAT is what I am working on, trying combining HAT (Hard Attention to the Task) algorithm with Continual Backpropagation (CBP) by leveraging the contribution utility as the parameter importance like in AdaHAT (Adaptive Hard Attention to the Task) algorithm.</span>
</span><span id="L-27"><a href="#L-27"><span class="linenos"> 27</span></a>
</span><span id="L-28"><a href="#L-28"><span class="linenos"> 28</span></a><span class="sd">    We implement CBPHAT as a subclass of AdaHAT algorithm because CBPHAT adopt the similar idea as AdaHAT.</span>
</span><span id="L-29"><a href="#L-29"><span class="linenos"> 29</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-30"><a href="#L-30"><span class="linenos"> 30</span></a>
</span><span id="L-31"><a href="#L-31"><span class="linenos"> 31</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-32"><a href="#L-32"><span class="linenos"> 32</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-33"><a href="#L-33"><span class="linenos"> 33</span></a>        <span class="n">backbone</span><span class="p">:</span> <span class="n">HATMaskBackbone</span><span class="p">,</span>
</span><span id="L-34"><a href="#L-34"><span class="linenos"> 34</span></a>        <span class="n">heads</span><span class="p">:</span> <span class="n">HeadsTIL</span> <span class="o">|</span> <span class="n">HeadsCIL</span><span class="p">,</span>
</span><span id="L-35"><a href="#L-35"><span class="linenos"> 35</span></a>        <span class="n">adjustment_mode</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-36"><a href="#L-36"><span class="linenos"> 36</span></a>        <span class="n">adjustment_intensity</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="L-37"><a href="#L-37"><span class="linenos"> 37</span></a>        <span class="n">utility_decay_rate</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="L-38"><a href="#L-38"><span class="linenos"> 38</span></a>        <span class="n">s_max</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="L-39"><a href="#L-39"><span class="linenos"> 39</span></a>        <span class="n">clamp_threshold</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="L-40"><a href="#L-40"><span class="linenos"> 40</span></a>        <span class="n">mask_sparsity_reg_factor</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="L-41"><a href="#L-41"><span class="linenos"> 41</span></a>        <span class="n">mask_sparsity_reg_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;original&quot;</span><span class="p">,</span>
</span><span id="L-42"><a href="#L-42"><span class="linenos"> 42</span></a>        <span class="n">task_embedding_init_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;N01&quot;</span><span class="p">,</span>
</span><span id="L-43"><a href="#L-43"><span class="linenos"> 43</span></a>        <span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
</span><span id="L-44"><a href="#L-44"><span class="linenos"> 44</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-45"><a href="#L-45"><span class="linenos"> 45</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialise the CBPHAT algorithm with the network.</span>
</span><span id="L-46"><a href="#L-46"><span class="linenos"> 46</span></a>
</span><span id="L-47"><a href="#L-47"><span class="linenos"> 47</span></a><span class="sd">        **Args:**</span>
</span><span id="L-48"><a href="#L-48"><span class="linenos"> 48</span></a><span class="sd">        - **backbone** (`HATMaskBackbone`): must be a backbone network with HAT mask mechanism.</span>
</span><span id="L-49"><a href="#L-49"><span class="linenos"> 49</span></a><span class="sd">        - **heads** (`HeadsTIL` | `HeadsCIL`): output heads.</span>
</span><span id="L-50"><a href="#L-50"><span class="linenos"> 50</span></a><span class="sd">        - **adjustment_mode** (`str`): the strategy of adjustment i.e. the mode of gradient clipping, should be one of the following:</span>
</span><span id="L-51"><a href="#L-51"><span class="linenos"> 51</span></a><span class="sd">            1. &#39;cbphat&#39;: our original CBP mode.</span>
</span><span id="L-52"><a href="#L-52"><span class="linenos"> 52</span></a><span class="sd">        - **adjustment_intensity** (`float`): hyperparameter, control the overall intensity of gradient adjustment. It&#39;s the $\alpha$ in equation (9) in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="L-53"><a href="#L-53"><span class="linenos"> 53</span></a><span class="sd">        - **utility_decay_rate** (`float`): the utility decay rate of units. It is the rate at which the utility of a unit decays over time.</span>
</span><span id="L-54"><a href="#L-54"><span class="linenos"> 54</span></a><span class="sd">        - **s_max** (`float`): hyperparameter, the maximum scaling factor in the gate function. See chapter 2.4 &quot;Hard Attention Training&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="L-55"><a href="#L-55"><span class="linenos"> 55</span></a><span class="sd">        - **clamp_threshold** (`float`): the threshold for task embedding gradient compensation. See chapter 2.5 &quot;Embedding Gradient Compensation&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="L-56"><a href="#L-56"><span class="linenos"> 56</span></a><span class="sd">        - **mask_sparsity_reg_factor** (`float`): hyperparameter, the regularisation factor for mask sparsity.</span>
</span><span id="L-57"><a href="#L-57"><span class="linenos"> 57</span></a><span class="sd">        - **mask_sparsity_reg_mode** (`str`): the mode of mask sparsity regularisation, should be one of the following:</span>
</span><span id="L-58"><a href="#L-58"><span class="linenos"> 58</span></a><span class="sd">            1. &#39;original&#39; (default): the original mask sparsity regularisation in HAT paper.</span>
</span><span id="L-59"><a href="#L-59"><span class="linenos"> 59</span></a><span class="sd">            2. &#39;cross&#39;: the cross version mask sparsity regularisation.</span>
</span><span id="L-60"><a href="#L-60"><span class="linenos"> 60</span></a><span class="sd">        - **task_embedding_init_mode** (`str`): the initialisation method for task embeddings, should be one of the following:</span>
</span><span id="L-61"><a href="#L-61"><span class="linenos"> 61</span></a><span class="sd">            1. &#39;N01&#39; (default): standard normal distribution $N(0, 1)$.</span>
</span><span id="L-62"><a href="#L-62"><span class="linenos"> 62</span></a><span class="sd">            2. &#39;U-11&#39;: uniform distribution $U(-1, 1)$.</span>
</span><span id="L-63"><a href="#L-63"><span class="linenos"> 63</span></a><span class="sd">            3. &#39;U01&#39;: uniform distribution $U(0, 1)$.</span>
</span><span id="L-64"><a href="#L-64"><span class="linenos"> 64</span></a><span class="sd">            4. &#39;U-10&#39;: uniform distribution $U(-1, 0)$.</span>
</span><span id="L-65"><a href="#L-65"><span class="linenos"> 65</span></a><span class="sd">            5. &#39;last&#39;: inherit task embedding from last task.</span>
</span><span id="L-66"><a href="#L-66"><span class="linenos"> 66</span></a><span class="sd">        - **epsilon** (`float`): the value added to network sparsity to avoid zero appeared in equation (9) in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="L-67"><a href="#L-67"><span class="linenos"> 67</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-68"><a href="#L-68"><span class="linenos"> 68</span></a>        <span class="n">AdaHAT</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-69"><a href="#L-69"><span class="linenos"> 69</span></a>            <span class="bp">self</span><span class="p">,</span>
</span><span id="L-70"><a href="#L-70"><span class="linenos"> 70</span></a>            <span class="n">backbone</span><span class="o">=</span><span class="n">backbone</span><span class="p">,</span>
</span><span id="L-71"><a href="#L-71"><span class="linenos"> 71</span></a>            <span class="n">heads</span><span class="o">=</span><span class="n">heads</span><span class="p">,</span>
</span><span id="L-72"><a href="#L-72"><span class="linenos"> 72</span></a>            <span class="n">adjustment_mode</span><span class="o">=</span><span class="n">adjustment_mode</span><span class="p">,</span>
</span><span id="L-73"><a href="#L-73"><span class="linenos"> 73</span></a>            <span class="n">adjustment_intensity</span><span class="o">=</span><span class="n">adjustment_intensity</span><span class="p">,</span>
</span><span id="L-74"><a href="#L-74"><span class="linenos"> 74</span></a>            <span class="n">s_max</span><span class="o">=</span><span class="n">s_max</span><span class="p">,</span>
</span><span id="L-75"><a href="#L-75"><span class="linenos"> 75</span></a>            <span class="n">clamp_threshold</span><span class="o">=</span><span class="n">clamp_threshold</span><span class="p">,</span>
</span><span id="L-76"><a href="#L-76"><span class="linenos"> 76</span></a>            <span class="n">mask_sparsity_reg_factor</span><span class="o">=</span><span class="n">mask_sparsity_reg_factor</span><span class="p">,</span>
</span><span id="L-77"><a href="#L-77"><span class="linenos"> 77</span></a>            <span class="n">mask_sparsity_reg_mode</span><span class="o">=</span><span class="n">mask_sparsity_reg_mode</span><span class="p">,</span>
</span><span id="L-78"><a href="#L-78"><span class="linenos"> 78</span></a>            <span class="n">task_embedding_init_mode</span><span class="o">=</span><span class="n">task_embedding_init_mode</span><span class="p">,</span>
</span><span id="L-79"><a href="#L-79"><span class="linenos"> 79</span></a>            <span class="n">epsilon</span><span class="o">=</span><span class="n">epsilon</span><span class="p">,</span>
</span><span id="L-80"><a href="#L-80"><span class="linenos"> 80</span></a>        <span class="p">)</span>
</span><span id="L-81"><a href="#L-81"><span class="linenos"> 81</span></a>
</span><span id="L-82"><a href="#L-82"><span class="linenos"> 82</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">utility_decay_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">utility_decay_rate</span>
</span><span id="L-83"><a href="#L-83"><span class="linenos"> 83</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the utility decay rate of units. &quot;&quot;&quot;</span>
</span><span id="L-84"><a href="#L-84"><span class="linenos"> 84</span></a>
</span><span id="L-85"><a href="#L-85"><span class="linenos"> 85</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">contribution_utility_t</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="L-86"><a href="#L-86"><span class="linenos"> 86</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the summative min-max scaled contribution utility of units. See $U$ in the paper draft. Keys are layer names and values are the utility tensor for the layer. The utility tensor is the same size as the feature tensor with size (number of units). &quot;&quot;&quot;</span>
</span><span id="L-87"><a href="#L-87"><span class="linenos"> 87</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">unit_importance_for_previous_tasks</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="L-88"><a href="#L-88"><span class="linenos"> 88</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the unit importance values of units for previous tasks (1, \cdots, self.task_id - 1). See the &quot;screenshot&quot; $I^{(t-1)}$ in the paper draft. Keys are layer names and values are the importance tensor for the layer. The importance tensor is the same size as the feature tensor with size (number of units). &quot;&quot;&quot;</span>
</span><span id="L-89"><a href="#L-89"><span class="linenos"> 89</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">age_t</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="L-90"><a href="#L-90"><span class="linenos"> 90</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the age of units. Keys are layer names and values are the age tensor for the layer for current task. The age tensor is the same size as the feature tensor with size (number of units). &quot;&quot;&quot;</span>
</span><span id="L-91"><a href="#L-91"><span class="linenos"> 91</span></a>
</span><span id="L-92"><a href="#L-92"><span class="linenos"> 92</span></a>        <span class="c1"># set manual optimisation</span>
</span><span id="L-93"><a href="#L-93"><span class="linenos"> 93</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">automatic_optimization</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="L-94"><a href="#L-94"><span class="linenos"> 94</span></a>
</span><span id="L-95"><a href="#L-95"><span class="linenos"> 95</span></a>        <span class="n">CBPHAT</span><span class="o">.</span><span class="n">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</span><span id="L-96"><a href="#L-96"><span class="linenos"> 96</span></a>
</span><span id="L-97"><a href="#L-97"><span class="linenos"> 97</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-98"><a href="#L-98"><span class="linenos"> 98</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Check the sanity of the arguments.</span>
</span><span id="L-99"><a href="#L-99"><span class="linenos"> 99</span></a>
</span><span id="L-100"><a href="#L-100"><span class="linenos">100</span></a><span class="sd">        **Raises:**</span>
</span><span id="L-101"><a href="#L-101"><span class="linenos">101</span></a><span class="sd">        - **ValueError**: If the utility decay rate is not in the range (0, 1].</span>
</span><span id="L-102"><a href="#L-102"><span class="linenos">102</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-103"><a href="#L-103"><span class="linenos">103</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">utility_decay_rate</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">utility_decay_rate</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="L-104"><a href="#L-104"><span class="linenos">104</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="L-105"><a href="#L-105"><span class="linenos">105</span></a>                <span class="sa">f</span><span class="s2">&quot;The utility decay rate should be in the range (0, 1], but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">utility_decay_rate</span><span class="si">}</span><span class="s2">.&quot;</span>
</span><span id="L-106"><a href="#L-106"><span class="linenos">106</span></a>            <span class="p">)</span>
</span><span id="L-107"><a href="#L-107"><span class="linenos">107</span></a>
</span><span id="L-108"><a href="#L-108"><span class="linenos">108</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_start</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-109"><a href="#L-109"><span class="linenos">109</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Additionally initialise the utility, age and the CBPHAT unit importance for each layer as zeros.&quot;&quot;&quot;</span>
</span><span id="L-110"><a href="#L-110"><span class="linenos">110</span></a>        <span class="n">AdaHAT</span><span class="o">.</span><span class="n">on_train_start</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</span><span id="L-111"><a href="#L-111"><span class="linenos">111</span></a>
</span><span id="L-112"><a href="#L-112"><span class="linenos">112</span></a>        <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="L-113"><a href="#L-113"><span class="linenos">113</span></a>            <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span>
</span><span id="L-114"><a href="#L-114"><span class="linenos">114</span></a>                <span class="n">layer_name</span>
</span><span id="L-115"><a href="#L-115"><span class="linenos">115</span></a>            <span class="p">)</span>  <span class="c1"># get the layer by its name</span>
</span><span id="L-116"><a href="#L-116"><span class="linenos">116</span></a>            <span class="n">num_units</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="L-117"><a href="#L-117"><span class="linenos">117</span></a>
</span><span id="L-118"><a href="#L-118"><span class="linenos">118</span></a>            <span class="c1"># initialise the utility and age at the beginning of first task</span>
</span><span id="L-119"><a href="#L-119"><span class="linenos">119</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">contribution_utility_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_units</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
</span><span id="L-120"><a href="#L-120"><span class="linenos">120</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
</span><span id="L-121"><a href="#L-121"><span class="linenos">121</span></a>            <span class="p">)</span>
</span><span id="L-122"><a href="#L-122"><span class="linenos">122</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">age_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_units</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="L-123"><a href="#L-123"><span class="linenos">123</span></a>
</span><span id="L-124"><a href="#L-124"><span class="linenos">124</span></a>            <span class="c1"># initialise the unit importance at the beginning of first task. This should not be called in `__init__()` method as the `self.device` is not available at that time.</span>
</span><span id="L-125"><a href="#L-125"><span class="linenos">125</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="L-126"><a href="#L-126"><span class="linenos">126</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">unit_importance_for_previous_tasks</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
</span><span id="L-127"><a href="#L-127"><span class="linenos">127</span></a>                    <span class="n">num_units</span>
</span><span id="L-128"><a href="#L-128"><span class="linenos">128</span></a>                <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
</span><span id="L-129"><a href="#L-129"><span class="linenos">129</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
</span><span id="L-130"><a href="#L-130"><span class="linenos">130</span></a>                <span class="p">)</span>  <span class="c1"># the unit importance $I^{(t-1)}$ is initialised as zeros mask ($t = 1$). See the paper draft.</span>
</span><span id="L-131"><a href="#L-131"><span class="linenos">131</span></a>
</span><span id="L-132"><a href="#L-132"><span class="linenos">132</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">clip_grad_by_adjustment</span><span class="p">(</span>
</span><span id="L-133"><a href="#L-133"><span class="linenos">133</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-134"><a href="#L-134"><span class="linenos">134</span></a>        <span class="n">network_sparsity</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-135"><a href="#L-135"><span class="linenos">135</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-136"><a href="#L-136"><span class="linenos">136</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Clip the gradients by the adjustment rate.</span>
</span><span id="L-137"><a href="#L-137"><span class="linenos">137</span></a>
</span><span id="L-138"><a href="#L-138"><span class="linenos">138</span></a><span class="sd">        Note that as the task embedding fully covers every layer in the backbone network, no parameters are left out of this system. This applies not only the parameters in between layers with task embedding, but also those before the first layer. We designed it seperately in the codes.</span>
</span><span id="L-139"><a href="#L-139"><span class="linenos">139</span></a>
</span><span id="L-140"><a href="#L-140"><span class="linenos">140</span></a><span class="sd">        Network capacity is measured along with this method. Network capacity is defined as the average adjustment rate over all parameters. See chapter 4.1 in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="L-141"><a href="#L-141"><span class="linenos">141</span></a>
</span><span id="L-142"><a href="#L-142"><span class="linenos">142</span></a>
</span><span id="L-143"><a href="#L-143"><span class="linenos">143</span></a><span class="sd">        **Args:**</span>
</span><span id="L-144"><a href="#L-144"><span class="linenos">144</span></a><span class="sd">        - **network_sparsity** (`dict[str, Tensor]` | `None`): The network sparsity i.e. the mask sparsity loss of each layer for the current task. It applies only to AdaHAT modes, as it is used to calculate the adjustment rate for the gradients.</span>
</span><span id="L-145"><a href="#L-145"><span class="linenos">145</span></a>
</span><span id="L-146"><a href="#L-146"><span class="linenos">146</span></a><span class="sd">        **Returns:**</span>
</span><span id="L-147"><a href="#L-147"><span class="linenos">147</span></a><span class="sd">        - **capacity** (`Tensor`): the calculated network capacity.</span>
</span><span id="L-148"><a href="#L-148"><span class="linenos">148</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-149"><a href="#L-149"><span class="linenos">149</span></a>
</span><span id="L-150"><a href="#L-150"><span class="linenos">150</span></a>        <span class="c1"># initialise network capacity metric</span>
</span><span id="L-151"><a href="#L-151"><span class="linenos">151</span></a>        <span class="n">capacity</span> <span class="o">=</span> <span class="n">HATNetworkCapacity</span><span class="p">()</span>
</span><span id="L-152"><a href="#L-152"><span class="linenos">152</span></a>
</span><span id="L-153"><a href="#L-153"><span class="linenos">153</span></a>        <span class="c1"># Calculate the adjustment rate for gradients of the parameters, both weights and biases (if exists)</span>
</span><span id="L-154"><a href="#L-154"><span class="linenos">154</span></a>        <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="L-155"><a href="#L-155"><span class="linenos">155</span></a>
</span><span id="L-156"><a href="#L-156"><span class="linenos">156</span></a>            <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span>
</span><span id="L-157"><a href="#L-157"><span class="linenos">157</span></a>                <span class="n">layer_name</span>
</span><span id="L-158"><a href="#L-158"><span class="linenos">158</span></a>            <span class="p">)</span>  <span class="c1"># get the layer by its name</span>
</span><span id="L-159"><a href="#L-159"><span class="linenos">159</span></a>
</span><span id="L-160"><a href="#L-160"><span class="linenos">160</span></a>            <span class="c1"># placeholder for the adjustment rate to avoid the error of using it before assignment</span>
</span><span id="L-161"><a href="#L-161"><span class="linenos">161</span></a>            <span class="n">adjustment_rate_weight</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="L-162"><a href="#L-162"><span class="linenos">162</span></a>            <span class="n">adjustment_rate_bias</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="L-163"><a href="#L-163"><span class="linenos">163</span></a>
</span><span id="L-164"><a href="#L-164"><span class="linenos">164</span></a>            <span class="n">weight_importance</span><span class="p">,</span> <span class="n">bias_importance</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-165"><a href="#L-165"><span class="linenos">165</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_measure_parameter_wise</span><span class="p">(</span>
</span><span id="L-166"><a href="#L-166"><span class="linenos">166</span></a>                    <span class="n">unit_wise_measure</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">unit_importance_for_previous_tasks</span><span class="p">,</span>
</span><span id="L-167"><a href="#L-167"><span class="linenos">167</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="L-168"><a href="#L-168"><span class="linenos">168</span></a>                    <span class="n">aggregation</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span>
</span><span id="L-169"><a href="#L-169"><span class="linenos">169</span></a>                <span class="p">)</span>
</span><span id="L-170"><a href="#L-170"><span class="linenos">170</span></a>            <span class="p">)</span>
</span><span id="L-171"><a href="#L-171"><span class="linenos">171</span></a>
</span><span id="L-172"><a href="#L-172"><span class="linenos">172</span></a>            <span class="n">weight_summative_mask</span><span class="p">,</span> <span class="n">bias_summative_mask</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-173"><a href="#L-173"><span class="linenos">173</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_measure_parameter_wise</span><span class="p">(</span>
</span><span id="L-174"><a href="#L-174"><span class="linenos">174</span></a>                    <span class="n">unit_wise_measure</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">summative_mask_for_previous_tasks</span><span class="p">,</span>
</span><span id="L-175"><a href="#L-175"><span class="linenos">175</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="L-176"><a href="#L-176"><span class="linenos">176</span></a>                    <span class="n">aggregation</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span>
</span><span id="L-177"><a href="#L-177"><span class="linenos">177</span></a>                <span class="p">)</span>
</span><span id="L-178"><a href="#L-178"><span class="linenos">178</span></a>            <span class="p">)</span>
</span><span id="L-179"><a href="#L-179"><span class="linenos">179</span></a>
</span><span id="L-180"><a href="#L-180"><span class="linenos">180</span></a>            <span class="n">network_sparsity_layer</span> <span class="o">=</span> <span class="n">network_sparsity</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="L-181"><a href="#L-181"><span class="linenos">181</span></a>
</span><span id="L-182"><a href="#L-182"><span class="linenos">182</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;cbphat&quot;</span><span class="p">:</span>
</span><span id="L-183"><a href="#L-183"><span class="linenos">183</span></a>
</span><span id="L-184"><a href="#L-184"><span class="linenos">184</span></a>                <span class="n">r_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span> <span class="o">/</span> <span class="p">(</span>
</span><span id="L-185"><a href="#L-185"><span class="linenos">185</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">+</span> <span class="n">network_sparsity_layer</span>
</span><span id="L-186"><a href="#L-186"><span class="linenos">186</span></a>                <span class="p">)</span>
</span><span id="L-187"><a href="#L-187"><span class="linenos">187</span></a>                <span class="n">adjustment_rate_weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span>
</span><span id="L-188"><a href="#L-188"><span class="linenos">188</span></a>                    <span class="n">r_layer</span><span class="p">,</span> <span class="p">(</span><span class="n">weight_importance</span> <span class="o">+</span> <span class="n">weight_summative_mask</span> <span class="o">+</span> <span class="n">r_layer</span><span class="p">)</span>
</span><span id="L-189"><a href="#L-189"><span class="linenos">189</span></a>                <span class="p">)</span>
</span><span id="L-190"><a href="#L-190"><span class="linenos">190</span></a>
</span><span id="L-191"><a href="#L-191"><span class="linenos">191</span></a>                <span class="n">adjustment_rate_bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span>
</span><span id="L-192"><a href="#L-192"><span class="linenos">192</span></a>                    <span class="n">r_layer</span><span class="p">,</span> <span class="p">(</span><span class="n">bias_importance</span> <span class="o">+</span> <span class="n">bias_summative_mask</span> <span class="o">+</span> <span class="n">r_layer</span><span class="p">)</span>
</span><span id="L-193"><a href="#L-193"><span class="linenos">193</span></a>                <span class="p">)</span>
</span><span id="L-194"><a href="#L-194"><span class="linenos">194</span></a>
</span><span id="L-195"><a href="#L-195"><span class="linenos">195</span></a>            <span class="c1"># apply the adjustment rate to the gradients</span>
</span><span id="L-196"><a href="#L-196"><span class="linenos">196</span></a>            <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">adjustment_rate_weight</span>
</span><span id="L-197"><a href="#L-197"><span class="linenos">197</span></a>            <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-198"><a href="#L-198"><span class="linenos">198</span></a>                <span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">adjustment_rate_bias</span>
</span><span id="L-199"><a href="#L-199"><span class="linenos">199</span></a>
</span><span id="L-200"><a href="#L-200"><span class="linenos">200</span></a>            <span class="c1"># update network capacity metric</span>
</span><span id="L-201"><a href="#L-201"><span class="linenos">201</span></a>            <span class="n">capacity</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">adjustment_rate_weight</span><span class="p">,</span> <span class="n">adjustment_rate_bias</span><span class="p">)</span>
</span><span id="L-202"><a href="#L-202"><span class="linenos">202</span></a>
</span><span id="L-203"><a href="#L-203"><span class="linenos">203</span></a>        <span class="k">return</span> <span class="n">capacity</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
</span><span id="L-204"><a href="#L-204"><span class="linenos">204</span></a>
</span><span id="L-205"><a href="#L-205"><span class="linenos">205</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_batch_end</span><span class="p">(</span>
</span><span id="L-206"><a href="#L-206"><span class="linenos">206</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span>
</span><span id="L-207"><a href="#L-207"><span class="linenos">207</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-208"><a href="#L-208"><span class="linenos">208</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Update the contribution utility and age of units after each training step.</span>
</span><span id="L-209"><a href="#L-209"><span class="linenos">209</span></a>
</span><span id="L-210"><a href="#L-210"><span class="linenos">210</span></a><span class="sd">        **Args:**</span>
</span><span id="L-211"><a href="#L-211"><span class="linenos">211</span></a><span class="sd">        - **outputs** (`dict[str, Any]`): the outputs of the training step, which is the returns of the `training_step()` method in the `CLAlgorithm`.</span>
</span><span id="L-212"><a href="#L-212"><span class="linenos">212</span></a><span class="sd">        - **batch** (`Any`): the training data batch.</span>
</span><span id="L-213"><a href="#L-213"><span class="linenos">213</span></a><span class="sd">        - **batch_idx** (`int`): the index of the current batch. This is for the file name of mask figures.</span>
</span><span id="L-214"><a href="#L-214"><span class="linenos">214</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-215"><a href="#L-215"><span class="linenos">215</span></a>
</span><span id="L-216"><a href="#L-216"><span class="linenos">216</span></a>        <span class="n">hidden_features</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;hidden_features&quot;</span><span class="p">]</span>
</span><span id="L-217"><a href="#L-217"><span class="linenos">217</span></a>        <span class="n">mask</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;mask&quot;</span><span class="p">]</span>
</span><span id="L-218"><a href="#L-218"><span class="linenos">218</span></a>
</span><span id="L-219"><a href="#L-219"><span class="linenos">219</span></a>        <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="L-220"><a href="#L-220"><span class="linenos">220</span></a>            <span class="c1"># layer-wise operation</span>
</span><span id="L-221"><a href="#L-221"><span class="linenos">221</span></a>            <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span>
</span><span id="L-222"><a href="#L-222"><span class="linenos">222</span></a>                <span class="n">layer_name</span>
</span><span id="L-223"><a href="#L-223"><span class="linenos">223</span></a>            <span class="p">)</span>  <span class="c1"># get the layer by its name</span>
</span><span id="L-224"><a href="#L-224"><span class="linenos">224</span></a>
</span><span id="L-225"><a href="#L-225"><span class="linenos">225</span></a>            <span class="c1"># update age</span>
</span><span id="L-226"><a href="#L-226"><span class="linenos">226</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">age_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span id="L-227"><a href="#L-227"><span class="linenos">227</span></a>
</span><span id="L-228"><a href="#L-228"><span class="linenos">228</span></a>            <span class="c1"># calculate current contribution utility</span>
</span><span id="L-229"><a href="#L-229"><span class="linenos">229</span></a>            <span class="n">current_contribution_utility</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-230"><a href="#L-230"><span class="linenos">230</span></a>                <span class="p">(</span>
</span><span id="L-231"><a href="#L-231"><span class="linenos">231</span></a>                    <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
</span><span id="L-232"><a href="#L-232"><span class="linenos">232</span></a>                        <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">hidden_features</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]),</span>
</span><span id="L-233"><a href="#L-233"><span class="linenos">233</span></a>                        <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="L-234"><a href="#L-234"><span class="linenos">234</span></a>                            <span class="n">i</span>
</span><span id="L-235"><a href="#L-235"><span class="linenos">235</span></a>                            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">hidden_features</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span>
</span><span id="L-236"><a href="#L-236"><span class="linenos">236</span></a>                            <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="L-237"><a href="#L-237"><span class="linenos">237</span></a>                        <span class="p">],</span>  <span class="c1"># average the features over batch samples</span>
</span><span id="L-238"><a href="#L-238"><span class="linenos">238</span></a>                    <span class="p">)</span>
</span><span id="L-239"><a href="#L-239"><span class="linenos">239</span></a>                    <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
</span><span id="L-240"><a href="#L-240"><span class="linenos">240</span></a>                        <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="p">),</span>
</span><span id="L-241"><a href="#L-241"><span class="linenos">241</span></a>                        <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="L-242"><a href="#L-242"><span class="linenos">242</span></a>                            <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">0</span>
</span><span id="L-243"><a href="#L-243"><span class="linenos">243</span></a>                        <span class="p">],</span>  <span class="c1"># sum over the output dimension</span>
</span><span id="L-244"><a href="#L-244"><span class="linenos">244</span></a>                    <span class="p">)</span>
</span><span id="L-245"><a href="#L-245"><span class="linenos">245</span></a>                <span class="p">)</span>
</span><span id="L-246"><a href="#L-246"><span class="linenos">246</span></a>                <span class="o">*</span> <span class="n">mask</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="L-247"><a href="#L-247"><span class="linenos">247</span></a>            <span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="L-248"><a href="#L-248"><span class="linenos">248</span></a>            <span class="n">current_contribution_utility</span> <span class="o">=</span> <span class="n">min_max_normalise</span><span class="p">(</span>
</span><span id="L-249"><a href="#L-249"><span class="linenos">249</span></a>                <span class="n">current_contribution_utility</span>
</span><span id="L-250"><a href="#L-250"><span class="linenos">250</span></a>            <span class="p">)</span>  <span class="c1"># normalise the utility to [0,1] to avoid linearly increasing utility</span>
</span><span id="L-251"><a href="#L-251"><span class="linenos">251</span></a>
</span><span id="L-252"><a href="#L-252"><span class="linenos">252</span></a>            <span class="c1"># update utility</span>
</span><span id="L-253"><a href="#L-253"><span class="linenos">253</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">contribution_utility_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-254"><a href="#L-254"><span class="linenos">254</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">utility_decay_rate</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">contribution_utility_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="L-255"><a href="#L-255"><span class="linenos">255</span></a>                <span class="o">+</span> <span class="n">current_contribution_utility</span>
</span><span id="L-256"><a href="#L-256"><span class="linenos">256</span></a>            <span class="p">)</span>
</span><span id="L-257"><a href="#L-257"><span class="linenos">257</span></a>
</span><span id="L-258"><a href="#L-258"><span class="linenos">258</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_end</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-259"><a href="#L-259"><span class="linenos">259</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Additionally convert the contribution utility into importance and store (take screenshot of) it as unit importance for previous tasks at the end of a task training.&quot;&quot;&quot;</span>
</span><span id="L-260"><a href="#L-260"><span class="linenos">260</span></a>        <span class="n">AdaHAT</span><span class="o">.</span><span class="n">on_train_end</span><span class="p">(</span>
</span><span id="L-261"><a href="#L-261"><span class="linenos">261</span></a>            <span class="bp">self</span>
</span><span id="L-262"><a href="#L-262"><span class="linenos">262</span></a>        <span class="p">)</span>  <span class="c1"># store the mask and update cumulative and summative masks</span>
</span><span id="L-263"><a href="#L-263"><span class="linenos">263</span></a>
</span><span id="L-264"><a href="#L-264"><span class="linenos">264</span></a>        <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="L-265"><a href="#L-265"><span class="linenos">265</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">unit_importance_for_previous_tasks</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span>
</span><span id="L-266"><a href="#L-266"><span class="linenos">266</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">contribution_utility_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="L-267"><a href="#L-267"><span class="linenos">267</span></a>            <span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">age_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span></pre></div>


            </section>
                <section id="CBPHAT">
                            <input id="CBPHAT-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">CBPHAT</span><wbr>(<span class="base"><a href="adahat.html#AdaHAT">clarena.cl_algorithms.adahat.AdaHAT</a></span>):

                <label class="view-source-button" for="CBPHAT-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#CBPHAT"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="CBPHAT-24"><a href="#CBPHAT-24"><span class="linenos"> 24</span></a><span class="k">class</span><span class="w"> </span><span class="nc">CBPHAT</span><span class="p">(</span><span class="n">AdaHAT</span><span class="p">):</span>
</span><span id="CBPHAT-25"><a href="#CBPHAT-25"><span class="linenos"> 25</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;CBPHAT algorithm.</span>
</span><span id="CBPHAT-26"><a href="#CBPHAT-26"><span class="linenos"> 26</span></a>
</span><span id="CBPHAT-27"><a href="#CBPHAT-27"><span class="linenos"> 27</span></a><span class="sd">    CBPHAT is what I am working on, trying combining HAT (Hard Attention to the Task) algorithm with Continual Backpropagation (CBP) by leveraging the contribution utility as the parameter importance like in AdaHAT (Adaptive Hard Attention to the Task) algorithm.</span>
</span><span id="CBPHAT-28"><a href="#CBPHAT-28"><span class="linenos"> 28</span></a>
</span><span id="CBPHAT-29"><a href="#CBPHAT-29"><span class="linenos"> 29</span></a><span class="sd">    We implement CBPHAT as a subclass of AdaHAT algorithm because CBPHAT adopt the similar idea as AdaHAT.</span>
</span><span id="CBPHAT-30"><a href="#CBPHAT-30"><span class="linenos"> 30</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="CBPHAT-31"><a href="#CBPHAT-31"><span class="linenos"> 31</span></a>
</span><span id="CBPHAT-32"><a href="#CBPHAT-32"><span class="linenos"> 32</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="CBPHAT-33"><a href="#CBPHAT-33"><span class="linenos"> 33</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="CBPHAT-34"><a href="#CBPHAT-34"><span class="linenos"> 34</span></a>        <span class="n">backbone</span><span class="p">:</span> <span class="n">HATMaskBackbone</span><span class="p">,</span>
</span><span id="CBPHAT-35"><a href="#CBPHAT-35"><span class="linenos"> 35</span></a>        <span class="n">heads</span><span class="p">:</span> <span class="n">HeadsTIL</span> <span class="o">|</span> <span class="n">HeadsCIL</span><span class="p">,</span>
</span><span id="CBPHAT-36"><a href="#CBPHAT-36"><span class="linenos"> 36</span></a>        <span class="n">adjustment_mode</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="CBPHAT-37"><a href="#CBPHAT-37"><span class="linenos"> 37</span></a>        <span class="n">adjustment_intensity</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="CBPHAT-38"><a href="#CBPHAT-38"><span class="linenos"> 38</span></a>        <span class="n">utility_decay_rate</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="CBPHAT-39"><a href="#CBPHAT-39"><span class="linenos"> 39</span></a>        <span class="n">s_max</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="CBPHAT-40"><a href="#CBPHAT-40"><span class="linenos"> 40</span></a>        <span class="n">clamp_threshold</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="CBPHAT-41"><a href="#CBPHAT-41"><span class="linenos"> 41</span></a>        <span class="n">mask_sparsity_reg_factor</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="CBPHAT-42"><a href="#CBPHAT-42"><span class="linenos"> 42</span></a>        <span class="n">mask_sparsity_reg_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;original&quot;</span><span class="p">,</span>
</span><span id="CBPHAT-43"><a href="#CBPHAT-43"><span class="linenos"> 43</span></a>        <span class="n">task_embedding_init_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;N01&quot;</span><span class="p">,</span>
</span><span id="CBPHAT-44"><a href="#CBPHAT-44"><span class="linenos"> 44</span></a>        <span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
</span><span id="CBPHAT-45"><a href="#CBPHAT-45"><span class="linenos"> 45</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="CBPHAT-46"><a href="#CBPHAT-46"><span class="linenos"> 46</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialise the CBPHAT algorithm with the network.</span>
</span><span id="CBPHAT-47"><a href="#CBPHAT-47"><span class="linenos"> 47</span></a>
</span><span id="CBPHAT-48"><a href="#CBPHAT-48"><span class="linenos"> 48</span></a><span class="sd">        **Args:**</span>
</span><span id="CBPHAT-49"><a href="#CBPHAT-49"><span class="linenos"> 49</span></a><span class="sd">        - **backbone** (`HATMaskBackbone`): must be a backbone network with HAT mask mechanism.</span>
</span><span id="CBPHAT-50"><a href="#CBPHAT-50"><span class="linenos"> 50</span></a><span class="sd">        - **heads** (`HeadsTIL` | `HeadsCIL`): output heads.</span>
</span><span id="CBPHAT-51"><a href="#CBPHAT-51"><span class="linenos"> 51</span></a><span class="sd">        - **adjustment_mode** (`str`): the strategy of adjustment i.e. the mode of gradient clipping, should be one of the following:</span>
</span><span id="CBPHAT-52"><a href="#CBPHAT-52"><span class="linenos"> 52</span></a><span class="sd">            1. &#39;cbphat&#39;: our original CBP mode.</span>
</span><span id="CBPHAT-53"><a href="#CBPHAT-53"><span class="linenos"> 53</span></a><span class="sd">        - **adjustment_intensity** (`float`): hyperparameter, control the overall intensity of gradient adjustment. It&#39;s the $\alpha$ in equation (9) in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="CBPHAT-54"><a href="#CBPHAT-54"><span class="linenos"> 54</span></a><span class="sd">        - **utility_decay_rate** (`float`): the utility decay rate of units. It is the rate at which the utility of a unit decays over time.</span>
</span><span id="CBPHAT-55"><a href="#CBPHAT-55"><span class="linenos"> 55</span></a><span class="sd">        - **s_max** (`float`): hyperparameter, the maximum scaling factor in the gate function. See chapter 2.4 &quot;Hard Attention Training&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="CBPHAT-56"><a href="#CBPHAT-56"><span class="linenos"> 56</span></a><span class="sd">        - **clamp_threshold** (`float`): the threshold for task embedding gradient compensation. See chapter 2.5 &quot;Embedding Gradient Compensation&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="CBPHAT-57"><a href="#CBPHAT-57"><span class="linenos"> 57</span></a><span class="sd">        - **mask_sparsity_reg_factor** (`float`): hyperparameter, the regularisation factor for mask sparsity.</span>
</span><span id="CBPHAT-58"><a href="#CBPHAT-58"><span class="linenos"> 58</span></a><span class="sd">        - **mask_sparsity_reg_mode** (`str`): the mode of mask sparsity regularisation, should be one of the following:</span>
</span><span id="CBPHAT-59"><a href="#CBPHAT-59"><span class="linenos"> 59</span></a><span class="sd">            1. &#39;original&#39; (default): the original mask sparsity regularisation in HAT paper.</span>
</span><span id="CBPHAT-60"><a href="#CBPHAT-60"><span class="linenos"> 60</span></a><span class="sd">            2. &#39;cross&#39;: the cross version mask sparsity regularisation.</span>
</span><span id="CBPHAT-61"><a href="#CBPHAT-61"><span class="linenos"> 61</span></a><span class="sd">        - **task_embedding_init_mode** (`str`): the initialisation method for task embeddings, should be one of the following:</span>
</span><span id="CBPHAT-62"><a href="#CBPHAT-62"><span class="linenos"> 62</span></a><span class="sd">            1. &#39;N01&#39; (default): standard normal distribution $N(0, 1)$.</span>
</span><span id="CBPHAT-63"><a href="#CBPHAT-63"><span class="linenos"> 63</span></a><span class="sd">            2. &#39;U-11&#39;: uniform distribution $U(-1, 1)$.</span>
</span><span id="CBPHAT-64"><a href="#CBPHAT-64"><span class="linenos"> 64</span></a><span class="sd">            3. &#39;U01&#39;: uniform distribution $U(0, 1)$.</span>
</span><span id="CBPHAT-65"><a href="#CBPHAT-65"><span class="linenos"> 65</span></a><span class="sd">            4. &#39;U-10&#39;: uniform distribution $U(-1, 0)$.</span>
</span><span id="CBPHAT-66"><a href="#CBPHAT-66"><span class="linenos"> 66</span></a><span class="sd">            5. &#39;last&#39;: inherit task embedding from last task.</span>
</span><span id="CBPHAT-67"><a href="#CBPHAT-67"><span class="linenos"> 67</span></a><span class="sd">        - **epsilon** (`float`): the value added to network sparsity to avoid zero appeared in equation (9) in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="CBPHAT-68"><a href="#CBPHAT-68"><span class="linenos"> 68</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="CBPHAT-69"><a href="#CBPHAT-69"><span class="linenos"> 69</span></a>        <span class="n">AdaHAT</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="CBPHAT-70"><a href="#CBPHAT-70"><span class="linenos"> 70</span></a>            <span class="bp">self</span><span class="p">,</span>
</span><span id="CBPHAT-71"><a href="#CBPHAT-71"><span class="linenos"> 71</span></a>            <span class="n">backbone</span><span class="o">=</span><span class="n">backbone</span><span class="p">,</span>
</span><span id="CBPHAT-72"><a href="#CBPHAT-72"><span class="linenos"> 72</span></a>            <span class="n">heads</span><span class="o">=</span><span class="n">heads</span><span class="p">,</span>
</span><span id="CBPHAT-73"><a href="#CBPHAT-73"><span class="linenos"> 73</span></a>            <span class="n">adjustment_mode</span><span class="o">=</span><span class="n">adjustment_mode</span><span class="p">,</span>
</span><span id="CBPHAT-74"><a href="#CBPHAT-74"><span class="linenos"> 74</span></a>            <span class="n">adjustment_intensity</span><span class="o">=</span><span class="n">adjustment_intensity</span><span class="p">,</span>
</span><span id="CBPHAT-75"><a href="#CBPHAT-75"><span class="linenos"> 75</span></a>            <span class="n">s_max</span><span class="o">=</span><span class="n">s_max</span><span class="p">,</span>
</span><span id="CBPHAT-76"><a href="#CBPHAT-76"><span class="linenos"> 76</span></a>            <span class="n">clamp_threshold</span><span class="o">=</span><span class="n">clamp_threshold</span><span class="p">,</span>
</span><span id="CBPHAT-77"><a href="#CBPHAT-77"><span class="linenos"> 77</span></a>            <span class="n">mask_sparsity_reg_factor</span><span class="o">=</span><span class="n">mask_sparsity_reg_factor</span><span class="p">,</span>
</span><span id="CBPHAT-78"><a href="#CBPHAT-78"><span class="linenos"> 78</span></a>            <span class="n">mask_sparsity_reg_mode</span><span class="o">=</span><span class="n">mask_sparsity_reg_mode</span><span class="p">,</span>
</span><span id="CBPHAT-79"><a href="#CBPHAT-79"><span class="linenos"> 79</span></a>            <span class="n">task_embedding_init_mode</span><span class="o">=</span><span class="n">task_embedding_init_mode</span><span class="p">,</span>
</span><span id="CBPHAT-80"><a href="#CBPHAT-80"><span class="linenos"> 80</span></a>            <span class="n">epsilon</span><span class="o">=</span><span class="n">epsilon</span><span class="p">,</span>
</span><span id="CBPHAT-81"><a href="#CBPHAT-81"><span class="linenos"> 81</span></a>        <span class="p">)</span>
</span><span id="CBPHAT-82"><a href="#CBPHAT-82"><span class="linenos"> 82</span></a>
</span><span id="CBPHAT-83"><a href="#CBPHAT-83"><span class="linenos"> 83</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">utility_decay_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">utility_decay_rate</span>
</span><span id="CBPHAT-84"><a href="#CBPHAT-84"><span class="linenos"> 84</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the utility decay rate of units. &quot;&quot;&quot;</span>
</span><span id="CBPHAT-85"><a href="#CBPHAT-85"><span class="linenos"> 85</span></a>
</span><span id="CBPHAT-86"><a href="#CBPHAT-86"><span class="linenos"> 86</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">contribution_utility_t</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="CBPHAT-87"><a href="#CBPHAT-87"><span class="linenos"> 87</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the summative min-max scaled contribution utility of units. See $U$ in the paper draft. Keys are layer names and values are the utility tensor for the layer. The utility tensor is the same size as the feature tensor with size (number of units). &quot;&quot;&quot;</span>
</span><span id="CBPHAT-88"><a href="#CBPHAT-88"><span class="linenos"> 88</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">unit_importance_for_previous_tasks</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="CBPHAT-89"><a href="#CBPHAT-89"><span class="linenos"> 89</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the unit importance values of units for previous tasks (1, \cdots, self.task_id - 1). See the &quot;screenshot&quot; $I^{(t-1)}$ in the paper draft. Keys are layer names and values are the importance tensor for the layer. The importance tensor is the same size as the feature tensor with size (number of units). &quot;&quot;&quot;</span>
</span><span id="CBPHAT-90"><a href="#CBPHAT-90"><span class="linenos"> 90</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">age_t</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="CBPHAT-91"><a href="#CBPHAT-91"><span class="linenos"> 91</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the age of units. Keys are layer names and values are the age tensor for the layer for current task. The age tensor is the same size as the feature tensor with size (number of units). &quot;&quot;&quot;</span>
</span><span id="CBPHAT-92"><a href="#CBPHAT-92"><span class="linenos"> 92</span></a>
</span><span id="CBPHAT-93"><a href="#CBPHAT-93"><span class="linenos"> 93</span></a>        <span class="c1"># set manual optimisation</span>
</span><span id="CBPHAT-94"><a href="#CBPHAT-94"><span class="linenos"> 94</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">automatic_optimization</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="CBPHAT-95"><a href="#CBPHAT-95"><span class="linenos"> 95</span></a>
</span><span id="CBPHAT-96"><a href="#CBPHAT-96"><span class="linenos"> 96</span></a>        <span class="n">CBPHAT</span><span class="o">.</span><span class="n">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</span><span id="CBPHAT-97"><a href="#CBPHAT-97"><span class="linenos"> 97</span></a>
</span><span id="CBPHAT-98"><a href="#CBPHAT-98"><span class="linenos"> 98</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="CBPHAT-99"><a href="#CBPHAT-99"><span class="linenos"> 99</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Check the sanity of the arguments.</span>
</span><span id="CBPHAT-100"><a href="#CBPHAT-100"><span class="linenos">100</span></a>
</span><span id="CBPHAT-101"><a href="#CBPHAT-101"><span class="linenos">101</span></a><span class="sd">        **Raises:**</span>
</span><span id="CBPHAT-102"><a href="#CBPHAT-102"><span class="linenos">102</span></a><span class="sd">        - **ValueError**: If the utility decay rate is not in the range (0, 1].</span>
</span><span id="CBPHAT-103"><a href="#CBPHAT-103"><span class="linenos">103</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="CBPHAT-104"><a href="#CBPHAT-104"><span class="linenos">104</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">utility_decay_rate</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">utility_decay_rate</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="CBPHAT-105"><a href="#CBPHAT-105"><span class="linenos">105</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="CBPHAT-106"><a href="#CBPHAT-106"><span class="linenos">106</span></a>                <span class="sa">f</span><span class="s2">&quot;The utility decay rate should be in the range (0, 1], but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">utility_decay_rate</span><span class="si">}</span><span class="s2">.&quot;</span>
</span><span id="CBPHAT-107"><a href="#CBPHAT-107"><span class="linenos">107</span></a>            <span class="p">)</span>
</span><span id="CBPHAT-108"><a href="#CBPHAT-108"><span class="linenos">108</span></a>
</span><span id="CBPHAT-109"><a href="#CBPHAT-109"><span class="linenos">109</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_start</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="CBPHAT-110"><a href="#CBPHAT-110"><span class="linenos">110</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Additionally initialise the utility, age and the CBPHAT unit importance for each layer as zeros.&quot;&quot;&quot;</span>
</span><span id="CBPHAT-111"><a href="#CBPHAT-111"><span class="linenos">111</span></a>        <span class="n">AdaHAT</span><span class="o">.</span><span class="n">on_train_start</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</span><span id="CBPHAT-112"><a href="#CBPHAT-112"><span class="linenos">112</span></a>
</span><span id="CBPHAT-113"><a href="#CBPHAT-113"><span class="linenos">113</span></a>        <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="CBPHAT-114"><a href="#CBPHAT-114"><span class="linenos">114</span></a>            <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span>
</span><span id="CBPHAT-115"><a href="#CBPHAT-115"><span class="linenos">115</span></a>                <span class="n">layer_name</span>
</span><span id="CBPHAT-116"><a href="#CBPHAT-116"><span class="linenos">116</span></a>            <span class="p">)</span>  <span class="c1"># get the layer by its name</span>
</span><span id="CBPHAT-117"><a href="#CBPHAT-117"><span class="linenos">117</span></a>            <span class="n">num_units</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="CBPHAT-118"><a href="#CBPHAT-118"><span class="linenos">118</span></a>
</span><span id="CBPHAT-119"><a href="#CBPHAT-119"><span class="linenos">119</span></a>            <span class="c1"># initialise the utility and age at the beginning of first task</span>
</span><span id="CBPHAT-120"><a href="#CBPHAT-120"><span class="linenos">120</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">contribution_utility_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_units</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
</span><span id="CBPHAT-121"><a href="#CBPHAT-121"><span class="linenos">121</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
</span><span id="CBPHAT-122"><a href="#CBPHAT-122"><span class="linenos">122</span></a>            <span class="p">)</span>
</span><span id="CBPHAT-123"><a href="#CBPHAT-123"><span class="linenos">123</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">age_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_units</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="CBPHAT-124"><a href="#CBPHAT-124"><span class="linenos">124</span></a>
</span><span id="CBPHAT-125"><a href="#CBPHAT-125"><span class="linenos">125</span></a>            <span class="c1"># initialise the unit importance at the beginning of first task. This should not be called in `__init__()` method as the `self.device` is not available at that time.</span>
</span><span id="CBPHAT-126"><a href="#CBPHAT-126"><span class="linenos">126</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="CBPHAT-127"><a href="#CBPHAT-127"><span class="linenos">127</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">unit_importance_for_previous_tasks</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
</span><span id="CBPHAT-128"><a href="#CBPHAT-128"><span class="linenos">128</span></a>                    <span class="n">num_units</span>
</span><span id="CBPHAT-129"><a href="#CBPHAT-129"><span class="linenos">129</span></a>                <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
</span><span id="CBPHAT-130"><a href="#CBPHAT-130"><span class="linenos">130</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
</span><span id="CBPHAT-131"><a href="#CBPHAT-131"><span class="linenos">131</span></a>                <span class="p">)</span>  <span class="c1"># the unit importance $I^{(t-1)}$ is initialised as zeros mask ($t = 1$). See the paper draft.</span>
</span><span id="CBPHAT-132"><a href="#CBPHAT-132"><span class="linenos">132</span></a>
</span><span id="CBPHAT-133"><a href="#CBPHAT-133"><span class="linenos">133</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">clip_grad_by_adjustment</span><span class="p">(</span>
</span><span id="CBPHAT-134"><a href="#CBPHAT-134"><span class="linenos">134</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="CBPHAT-135"><a href="#CBPHAT-135"><span class="linenos">135</span></a>        <span class="n">network_sparsity</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="CBPHAT-136"><a href="#CBPHAT-136"><span class="linenos">136</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="CBPHAT-137"><a href="#CBPHAT-137"><span class="linenos">137</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Clip the gradients by the adjustment rate.</span>
</span><span id="CBPHAT-138"><a href="#CBPHAT-138"><span class="linenos">138</span></a>
</span><span id="CBPHAT-139"><a href="#CBPHAT-139"><span class="linenos">139</span></a><span class="sd">        Note that as the task embedding fully covers every layer in the backbone network, no parameters are left out of this system. This applies not only the parameters in between layers with task embedding, but also those before the first layer. We designed it seperately in the codes.</span>
</span><span id="CBPHAT-140"><a href="#CBPHAT-140"><span class="linenos">140</span></a>
</span><span id="CBPHAT-141"><a href="#CBPHAT-141"><span class="linenos">141</span></a><span class="sd">        Network capacity is measured along with this method. Network capacity is defined as the average adjustment rate over all parameters. See chapter 4.1 in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="CBPHAT-142"><a href="#CBPHAT-142"><span class="linenos">142</span></a>
</span><span id="CBPHAT-143"><a href="#CBPHAT-143"><span class="linenos">143</span></a>
</span><span id="CBPHAT-144"><a href="#CBPHAT-144"><span class="linenos">144</span></a><span class="sd">        **Args:**</span>
</span><span id="CBPHAT-145"><a href="#CBPHAT-145"><span class="linenos">145</span></a><span class="sd">        - **network_sparsity** (`dict[str, Tensor]` | `None`): The network sparsity i.e. the mask sparsity loss of each layer for the current task. It applies only to AdaHAT modes, as it is used to calculate the adjustment rate for the gradients.</span>
</span><span id="CBPHAT-146"><a href="#CBPHAT-146"><span class="linenos">146</span></a>
</span><span id="CBPHAT-147"><a href="#CBPHAT-147"><span class="linenos">147</span></a><span class="sd">        **Returns:**</span>
</span><span id="CBPHAT-148"><a href="#CBPHAT-148"><span class="linenos">148</span></a><span class="sd">        - **capacity** (`Tensor`): the calculated network capacity.</span>
</span><span id="CBPHAT-149"><a href="#CBPHAT-149"><span class="linenos">149</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="CBPHAT-150"><a href="#CBPHAT-150"><span class="linenos">150</span></a>
</span><span id="CBPHAT-151"><a href="#CBPHAT-151"><span class="linenos">151</span></a>        <span class="c1"># initialise network capacity metric</span>
</span><span id="CBPHAT-152"><a href="#CBPHAT-152"><span class="linenos">152</span></a>        <span class="n">capacity</span> <span class="o">=</span> <span class="n">HATNetworkCapacity</span><span class="p">()</span>
</span><span id="CBPHAT-153"><a href="#CBPHAT-153"><span class="linenos">153</span></a>
</span><span id="CBPHAT-154"><a href="#CBPHAT-154"><span class="linenos">154</span></a>        <span class="c1"># Calculate the adjustment rate for gradients of the parameters, both weights and biases (if exists)</span>
</span><span id="CBPHAT-155"><a href="#CBPHAT-155"><span class="linenos">155</span></a>        <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="CBPHAT-156"><a href="#CBPHAT-156"><span class="linenos">156</span></a>
</span><span id="CBPHAT-157"><a href="#CBPHAT-157"><span class="linenos">157</span></a>            <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span>
</span><span id="CBPHAT-158"><a href="#CBPHAT-158"><span class="linenos">158</span></a>                <span class="n">layer_name</span>
</span><span id="CBPHAT-159"><a href="#CBPHAT-159"><span class="linenos">159</span></a>            <span class="p">)</span>  <span class="c1"># get the layer by its name</span>
</span><span id="CBPHAT-160"><a href="#CBPHAT-160"><span class="linenos">160</span></a>
</span><span id="CBPHAT-161"><a href="#CBPHAT-161"><span class="linenos">161</span></a>            <span class="c1"># placeholder for the adjustment rate to avoid the error of using it before assignment</span>
</span><span id="CBPHAT-162"><a href="#CBPHAT-162"><span class="linenos">162</span></a>            <span class="n">adjustment_rate_weight</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="CBPHAT-163"><a href="#CBPHAT-163"><span class="linenos">163</span></a>            <span class="n">adjustment_rate_bias</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="CBPHAT-164"><a href="#CBPHAT-164"><span class="linenos">164</span></a>
</span><span id="CBPHAT-165"><a href="#CBPHAT-165"><span class="linenos">165</span></a>            <span class="n">weight_importance</span><span class="p">,</span> <span class="n">bias_importance</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="CBPHAT-166"><a href="#CBPHAT-166"><span class="linenos">166</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_measure_parameter_wise</span><span class="p">(</span>
</span><span id="CBPHAT-167"><a href="#CBPHAT-167"><span class="linenos">167</span></a>                    <span class="n">unit_wise_measure</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">unit_importance_for_previous_tasks</span><span class="p">,</span>
</span><span id="CBPHAT-168"><a href="#CBPHAT-168"><span class="linenos">168</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="CBPHAT-169"><a href="#CBPHAT-169"><span class="linenos">169</span></a>                    <span class="n">aggregation</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span>
</span><span id="CBPHAT-170"><a href="#CBPHAT-170"><span class="linenos">170</span></a>                <span class="p">)</span>
</span><span id="CBPHAT-171"><a href="#CBPHAT-171"><span class="linenos">171</span></a>            <span class="p">)</span>
</span><span id="CBPHAT-172"><a href="#CBPHAT-172"><span class="linenos">172</span></a>
</span><span id="CBPHAT-173"><a href="#CBPHAT-173"><span class="linenos">173</span></a>            <span class="n">weight_summative_mask</span><span class="p">,</span> <span class="n">bias_summative_mask</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="CBPHAT-174"><a href="#CBPHAT-174"><span class="linenos">174</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_measure_parameter_wise</span><span class="p">(</span>
</span><span id="CBPHAT-175"><a href="#CBPHAT-175"><span class="linenos">175</span></a>                    <span class="n">unit_wise_measure</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">summative_mask_for_previous_tasks</span><span class="p">,</span>
</span><span id="CBPHAT-176"><a href="#CBPHAT-176"><span class="linenos">176</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="CBPHAT-177"><a href="#CBPHAT-177"><span class="linenos">177</span></a>                    <span class="n">aggregation</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span>
</span><span id="CBPHAT-178"><a href="#CBPHAT-178"><span class="linenos">178</span></a>                <span class="p">)</span>
</span><span id="CBPHAT-179"><a href="#CBPHAT-179"><span class="linenos">179</span></a>            <span class="p">)</span>
</span><span id="CBPHAT-180"><a href="#CBPHAT-180"><span class="linenos">180</span></a>
</span><span id="CBPHAT-181"><a href="#CBPHAT-181"><span class="linenos">181</span></a>            <span class="n">network_sparsity_layer</span> <span class="o">=</span> <span class="n">network_sparsity</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="CBPHAT-182"><a href="#CBPHAT-182"><span class="linenos">182</span></a>
</span><span id="CBPHAT-183"><a href="#CBPHAT-183"><span class="linenos">183</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;cbphat&quot;</span><span class="p">:</span>
</span><span id="CBPHAT-184"><a href="#CBPHAT-184"><span class="linenos">184</span></a>
</span><span id="CBPHAT-185"><a href="#CBPHAT-185"><span class="linenos">185</span></a>                <span class="n">r_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span> <span class="o">/</span> <span class="p">(</span>
</span><span id="CBPHAT-186"><a href="#CBPHAT-186"><span class="linenos">186</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">+</span> <span class="n">network_sparsity_layer</span>
</span><span id="CBPHAT-187"><a href="#CBPHAT-187"><span class="linenos">187</span></a>                <span class="p">)</span>
</span><span id="CBPHAT-188"><a href="#CBPHAT-188"><span class="linenos">188</span></a>                <span class="n">adjustment_rate_weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span>
</span><span id="CBPHAT-189"><a href="#CBPHAT-189"><span class="linenos">189</span></a>                    <span class="n">r_layer</span><span class="p">,</span> <span class="p">(</span><span class="n">weight_importance</span> <span class="o">+</span> <span class="n">weight_summative_mask</span> <span class="o">+</span> <span class="n">r_layer</span><span class="p">)</span>
</span><span id="CBPHAT-190"><a href="#CBPHAT-190"><span class="linenos">190</span></a>                <span class="p">)</span>
</span><span id="CBPHAT-191"><a href="#CBPHAT-191"><span class="linenos">191</span></a>
</span><span id="CBPHAT-192"><a href="#CBPHAT-192"><span class="linenos">192</span></a>                <span class="n">adjustment_rate_bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span>
</span><span id="CBPHAT-193"><a href="#CBPHAT-193"><span class="linenos">193</span></a>                    <span class="n">r_layer</span><span class="p">,</span> <span class="p">(</span><span class="n">bias_importance</span> <span class="o">+</span> <span class="n">bias_summative_mask</span> <span class="o">+</span> <span class="n">r_layer</span><span class="p">)</span>
</span><span id="CBPHAT-194"><a href="#CBPHAT-194"><span class="linenos">194</span></a>                <span class="p">)</span>
</span><span id="CBPHAT-195"><a href="#CBPHAT-195"><span class="linenos">195</span></a>
</span><span id="CBPHAT-196"><a href="#CBPHAT-196"><span class="linenos">196</span></a>            <span class="c1"># apply the adjustment rate to the gradients</span>
</span><span id="CBPHAT-197"><a href="#CBPHAT-197"><span class="linenos">197</span></a>            <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">adjustment_rate_weight</span>
</span><span id="CBPHAT-198"><a href="#CBPHAT-198"><span class="linenos">198</span></a>            <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="CBPHAT-199"><a href="#CBPHAT-199"><span class="linenos">199</span></a>                <span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">adjustment_rate_bias</span>
</span><span id="CBPHAT-200"><a href="#CBPHAT-200"><span class="linenos">200</span></a>
</span><span id="CBPHAT-201"><a href="#CBPHAT-201"><span class="linenos">201</span></a>            <span class="c1"># update network capacity metric</span>
</span><span id="CBPHAT-202"><a href="#CBPHAT-202"><span class="linenos">202</span></a>            <span class="n">capacity</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">adjustment_rate_weight</span><span class="p">,</span> <span class="n">adjustment_rate_bias</span><span class="p">)</span>
</span><span id="CBPHAT-203"><a href="#CBPHAT-203"><span class="linenos">203</span></a>
</span><span id="CBPHAT-204"><a href="#CBPHAT-204"><span class="linenos">204</span></a>        <span class="k">return</span> <span class="n">capacity</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
</span><span id="CBPHAT-205"><a href="#CBPHAT-205"><span class="linenos">205</span></a>
</span><span id="CBPHAT-206"><a href="#CBPHAT-206"><span class="linenos">206</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_batch_end</span><span class="p">(</span>
</span><span id="CBPHAT-207"><a href="#CBPHAT-207"><span class="linenos">207</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span>
</span><span id="CBPHAT-208"><a href="#CBPHAT-208"><span class="linenos">208</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="CBPHAT-209"><a href="#CBPHAT-209"><span class="linenos">209</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Update the contribution utility and age of units after each training step.</span>
</span><span id="CBPHAT-210"><a href="#CBPHAT-210"><span class="linenos">210</span></a>
</span><span id="CBPHAT-211"><a href="#CBPHAT-211"><span class="linenos">211</span></a><span class="sd">        **Args:**</span>
</span><span id="CBPHAT-212"><a href="#CBPHAT-212"><span class="linenos">212</span></a><span class="sd">        - **outputs** (`dict[str, Any]`): the outputs of the training step, which is the returns of the `training_step()` method in the `CLAlgorithm`.</span>
</span><span id="CBPHAT-213"><a href="#CBPHAT-213"><span class="linenos">213</span></a><span class="sd">        - **batch** (`Any`): the training data batch.</span>
</span><span id="CBPHAT-214"><a href="#CBPHAT-214"><span class="linenos">214</span></a><span class="sd">        - **batch_idx** (`int`): the index of the current batch. This is for the file name of mask figures.</span>
</span><span id="CBPHAT-215"><a href="#CBPHAT-215"><span class="linenos">215</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="CBPHAT-216"><a href="#CBPHAT-216"><span class="linenos">216</span></a>
</span><span id="CBPHAT-217"><a href="#CBPHAT-217"><span class="linenos">217</span></a>        <span class="n">hidden_features</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;hidden_features&quot;</span><span class="p">]</span>
</span><span id="CBPHAT-218"><a href="#CBPHAT-218"><span class="linenos">218</span></a>        <span class="n">mask</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;mask&quot;</span><span class="p">]</span>
</span><span id="CBPHAT-219"><a href="#CBPHAT-219"><span class="linenos">219</span></a>
</span><span id="CBPHAT-220"><a href="#CBPHAT-220"><span class="linenos">220</span></a>        <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="CBPHAT-221"><a href="#CBPHAT-221"><span class="linenos">221</span></a>            <span class="c1"># layer-wise operation</span>
</span><span id="CBPHAT-222"><a href="#CBPHAT-222"><span class="linenos">222</span></a>            <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span>
</span><span id="CBPHAT-223"><a href="#CBPHAT-223"><span class="linenos">223</span></a>                <span class="n">layer_name</span>
</span><span id="CBPHAT-224"><a href="#CBPHAT-224"><span class="linenos">224</span></a>            <span class="p">)</span>  <span class="c1"># get the layer by its name</span>
</span><span id="CBPHAT-225"><a href="#CBPHAT-225"><span class="linenos">225</span></a>
</span><span id="CBPHAT-226"><a href="#CBPHAT-226"><span class="linenos">226</span></a>            <span class="c1"># update age</span>
</span><span id="CBPHAT-227"><a href="#CBPHAT-227"><span class="linenos">227</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">age_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span id="CBPHAT-228"><a href="#CBPHAT-228"><span class="linenos">228</span></a>
</span><span id="CBPHAT-229"><a href="#CBPHAT-229"><span class="linenos">229</span></a>            <span class="c1"># calculate current contribution utility</span>
</span><span id="CBPHAT-230"><a href="#CBPHAT-230"><span class="linenos">230</span></a>            <span class="n">current_contribution_utility</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="CBPHAT-231"><a href="#CBPHAT-231"><span class="linenos">231</span></a>                <span class="p">(</span>
</span><span id="CBPHAT-232"><a href="#CBPHAT-232"><span class="linenos">232</span></a>                    <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
</span><span id="CBPHAT-233"><a href="#CBPHAT-233"><span class="linenos">233</span></a>                        <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">hidden_features</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]),</span>
</span><span id="CBPHAT-234"><a href="#CBPHAT-234"><span class="linenos">234</span></a>                        <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="CBPHAT-235"><a href="#CBPHAT-235"><span class="linenos">235</span></a>                            <span class="n">i</span>
</span><span id="CBPHAT-236"><a href="#CBPHAT-236"><span class="linenos">236</span></a>                            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">hidden_features</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span>
</span><span id="CBPHAT-237"><a href="#CBPHAT-237"><span class="linenos">237</span></a>                            <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="CBPHAT-238"><a href="#CBPHAT-238"><span class="linenos">238</span></a>                        <span class="p">],</span>  <span class="c1"># average the features over batch samples</span>
</span><span id="CBPHAT-239"><a href="#CBPHAT-239"><span class="linenos">239</span></a>                    <span class="p">)</span>
</span><span id="CBPHAT-240"><a href="#CBPHAT-240"><span class="linenos">240</span></a>                    <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
</span><span id="CBPHAT-241"><a href="#CBPHAT-241"><span class="linenos">241</span></a>                        <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="p">),</span>
</span><span id="CBPHAT-242"><a href="#CBPHAT-242"><span class="linenos">242</span></a>                        <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="CBPHAT-243"><a href="#CBPHAT-243"><span class="linenos">243</span></a>                            <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">0</span>
</span><span id="CBPHAT-244"><a href="#CBPHAT-244"><span class="linenos">244</span></a>                        <span class="p">],</span>  <span class="c1"># sum over the output dimension</span>
</span><span id="CBPHAT-245"><a href="#CBPHAT-245"><span class="linenos">245</span></a>                    <span class="p">)</span>
</span><span id="CBPHAT-246"><a href="#CBPHAT-246"><span class="linenos">246</span></a>                <span class="p">)</span>
</span><span id="CBPHAT-247"><a href="#CBPHAT-247"><span class="linenos">247</span></a>                <span class="o">*</span> <span class="n">mask</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="CBPHAT-248"><a href="#CBPHAT-248"><span class="linenos">248</span></a>            <span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="CBPHAT-249"><a href="#CBPHAT-249"><span class="linenos">249</span></a>            <span class="n">current_contribution_utility</span> <span class="o">=</span> <span class="n">min_max_normalise</span><span class="p">(</span>
</span><span id="CBPHAT-250"><a href="#CBPHAT-250"><span class="linenos">250</span></a>                <span class="n">current_contribution_utility</span>
</span><span id="CBPHAT-251"><a href="#CBPHAT-251"><span class="linenos">251</span></a>            <span class="p">)</span>  <span class="c1"># normalise the utility to [0,1] to avoid linearly increasing utility</span>
</span><span id="CBPHAT-252"><a href="#CBPHAT-252"><span class="linenos">252</span></a>
</span><span id="CBPHAT-253"><a href="#CBPHAT-253"><span class="linenos">253</span></a>            <span class="c1"># update utility</span>
</span><span id="CBPHAT-254"><a href="#CBPHAT-254"><span class="linenos">254</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">contribution_utility_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="CBPHAT-255"><a href="#CBPHAT-255"><span class="linenos">255</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">utility_decay_rate</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">contribution_utility_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="CBPHAT-256"><a href="#CBPHAT-256"><span class="linenos">256</span></a>                <span class="o">+</span> <span class="n">current_contribution_utility</span>
</span><span id="CBPHAT-257"><a href="#CBPHAT-257"><span class="linenos">257</span></a>            <span class="p">)</span>
</span><span id="CBPHAT-258"><a href="#CBPHAT-258"><span class="linenos">258</span></a>
</span><span id="CBPHAT-259"><a href="#CBPHAT-259"><span class="linenos">259</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_end</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="CBPHAT-260"><a href="#CBPHAT-260"><span class="linenos">260</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Additionally convert the contribution utility into importance and store (take screenshot of) it as unit importance for previous tasks at the end of a task training.&quot;&quot;&quot;</span>
</span><span id="CBPHAT-261"><a href="#CBPHAT-261"><span class="linenos">261</span></a>        <span class="n">AdaHAT</span><span class="o">.</span><span class="n">on_train_end</span><span class="p">(</span>
</span><span id="CBPHAT-262"><a href="#CBPHAT-262"><span class="linenos">262</span></a>            <span class="bp">self</span>
</span><span id="CBPHAT-263"><a href="#CBPHAT-263"><span class="linenos">263</span></a>        <span class="p">)</span>  <span class="c1"># store the mask and update cumulative and summative masks</span>
</span><span id="CBPHAT-264"><a href="#CBPHAT-264"><span class="linenos">264</span></a>
</span><span id="CBPHAT-265"><a href="#CBPHAT-265"><span class="linenos">265</span></a>        <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="CBPHAT-266"><a href="#CBPHAT-266"><span class="linenos">266</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">unit_importance_for_previous_tasks</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span>
</span><span id="CBPHAT-267"><a href="#CBPHAT-267"><span class="linenos">267</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">contribution_utility_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="CBPHAT-268"><a href="#CBPHAT-268"><span class="linenos">268</span></a>            <span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">age_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span></pre></div>


            <div class="docstring"><p>CBPHAT algorithm.</p>

<p>CBPHAT is what I am working on, trying combining HAT (Hard Attention to the Task) algorithm with Continual Backpropagation (CBP) by leveraging the contribution utility as the parameter importance like in AdaHAT (Adaptive Hard Attention to the Task) algorithm.</p>

<p>We implement CBPHAT as a subclass of AdaHAT algorithm because CBPHAT adopt the similar idea as AdaHAT.</p>
</div>


                            <div id="CBPHAT.__init__" class="classattr">
                                        <input id="CBPHAT.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">CBPHAT</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">backbone</span><span class="p">:</span> <span class="n"><a href="../backbones.html#HATMaskBackbone">clarena.backbones.HATMaskBackbone</a></span>,</span><span class="param">	<span class="n">heads</span><span class="p">:</span> <span class="n"><a href="../cl_heads.html#HeadsTIL">clarena.cl_heads.HeadsTIL</a></span> <span class="o">|</span> <span class="n"><a href="../cl_heads.html#HeadsCIL">clarena.cl_heads.HeadsCIL</a></span>,</span><span class="param">	<span class="n">adjustment_mode</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">adjustment_intensity</span><span class="p">:</span> <span class="nb">float</span>,</span><span class="param">	<span class="n">utility_decay_rate</span><span class="p">:</span> <span class="nb">float</span>,</span><span class="param">	<span class="n">s_max</span><span class="p">:</span> <span class="nb">float</span>,</span><span class="param">	<span class="n">clamp_threshold</span><span class="p">:</span> <span class="nb">float</span>,</span><span class="param">	<span class="n">mask_sparsity_reg_factor</span><span class="p">:</span> <span class="nb">float</span>,</span><span class="param">	<span class="n">mask_sparsity_reg_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;original&#39;</span>,</span><span class="param">	<span class="n">task_embedding_init_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;N01&#39;</span>,</span><span class="param">	<span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span></span>)</span>

                <label class="view-source-button" for="CBPHAT.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#CBPHAT.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="CBPHAT.__init__-32"><a href="#CBPHAT.__init__-32"><span class="linenos">32</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="CBPHAT.__init__-33"><a href="#CBPHAT.__init__-33"><span class="linenos">33</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="CBPHAT.__init__-34"><a href="#CBPHAT.__init__-34"><span class="linenos">34</span></a>        <span class="n">backbone</span><span class="p">:</span> <span class="n">HATMaskBackbone</span><span class="p">,</span>
</span><span id="CBPHAT.__init__-35"><a href="#CBPHAT.__init__-35"><span class="linenos">35</span></a>        <span class="n">heads</span><span class="p">:</span> <span class="n">HeadsTIL</span> <span class="o">|</span> <span class="n">HeadsCIL</span><span class="p">,</span>
</span><span id="CBPHAT.__init__-36"><a href="#CBPHAT.__init__-36"><span class="linenos">36</span></a>        <span class="n">adjustment_mode</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="CBPHAT.__init__-37"><a href="#CBPHAT.__init__-37"><span class="linenos">37</span></a>        <span class="n">adjustment_intensity</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="CBPHAT.__init__-38"><a href="#CBPHAT.__init__-38"><span class="linenos">38</span></a>        <span class="n">utility_decay_rate</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="CBPHAT.__init__-39"><a href="#CBPHAT.__init__-39"><span class="linenos">39</span></a>        <span class="n">s_max</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="CBPHAT.__init__-40"><a href="#CBPHAT.__init__-40"><span class="linenos">40</span></a>        <span class="n">clamp_threshold</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="CBPHAT.__init__-41"><a href="#CBPHAT.__init__-41"><span class="linenos">41</span></a>        <span class="n">mask_sparsity_reg_factor</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="CBPHAT.__init__-42"><a href="#CBPHAT.__init__-42"><span class="linenos">42</span></a>        <span class="n">mask_sparsity_reg_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;original&quot;</span><span class="p">,</span>
</span><span id="CBPHAT.__init__-43"><a href="#CBPHAT.__init__-43"><span class="linenos">43</span></a>        <span class="n">task_embedding_init_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;N01&quot;</span><span class="p">,</span>
</span><span id="CBPHAT.__init__-44"><a href="#CBPHAT.__init__-44"><span class="linenos">44</span></a>        <span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
</span><span id="CBPHAT.__init__-45"><a href="#CBPHAT.__init__-45"><span class="linenos">45</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="CBPHAT.__init__-46"><a href="#CBPHAT.__init__-46"><span class="linenos">46</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialise the CBPHAT algorithm with the network.</span>
</span><span id="CBPHAT.__init__-47"><a href="#CBPHAT.__init__-47"><span class="linenos">47</span></a>
</span><span id="CBPHAT.__init__-48"><a href="#CBPHAT.__init__-48"><span class="linenos">48</span></a><span class="sd">        **Args:**</span>
</span><span id="CBPHAT.__init__-49"><a href="#CBPHAT.__init__-49"><span class="linenos">49</span></a><span class="sd">        - **backbone** (`HATMaskBackbone`): must be a backbone network with HAT mask mechanism.</span>
</span><span id="CBPHAT.__init__-50"><a href="#CBPHAT.__init__-50"><span class="linenos">50</span></a><span class="sd">        - **heads** (`HeadsTIL` | `HeadsCIL`): output heads.</span>
</span><span id="CBPHAT.__init__-51"><a href="#CBPHAT.__init__-51"><span class="linenos">51</span></a><span class="sd">        - **adjustment_mode** (`str`): the strategy of adjustment i.e. the mode of gradient clipping, should be one of the following:</span>
</span><span id="CBPHAT.__init__-52"><a href="#CBPHAT.__init__-52"><span class="linenos">52</span></a><span class="sd">            1. &#39;cbphat&#39;: our original CBP mode.</span>
</span><span id="CBPHAT.__init__-53"><a href="#CBPHAT.__init__-53"><span class="linenos">53</span></a><span class="sd">        - **adjustment_intensity** (`float`): hyperparameter, control the overall intensity of gradient adjustment. It&#39;s the $\alpha$ in equation (9) in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="CBPHAT.__init__-54"><a href="#CBPHAT.__init__-54"><span class="linenos">54</span></a><span class="sd">        - **utility_decay_rate** (`float`): the utility decay rate of units. It is the rate at which the utility of a unit decays over time.</span>
</span><span id="CBPHAT.__init__-55"><a href="#CBPHAT.__init__-55"><span class="linenos">55</span></a><span class="sd">        - **s_max** (`float`): hyperparameter, the maximum scaling factor in the gate function. See chapter 2.4 &quot;Hard Attention Training&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="CBPHAT.__init__-56"><a href="#CBPHAT.__init__-56"><span class="linenos">56</span></a><span class="sd">        - **clamp_threshold** (`float`): the threshold for task embedding gradient compensation. See chapter 2.5 &quot;Embedding Gradient Compensation&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="CBPHAT.__init__-57"><a href="#CBPHAT.__init__-57"><span class="linenos">57</span></a><span class="sd">        - **mask_sparsity_reg_factor** (`float`): hyperparameter, the regularisation factor for mask sparsity.</span>
</span><span id="CBPHAT.__init__-58"><a href="#CBPHAT.__init__-58"><span class="linenos">58</span></a><span class="sd">        - **mask_sparsity_reg_mode** (`str`): the mode of mask sparsity regularisation, should be one of the following:</span>
</span><span id="CBPHAT.__init__-59"><a href="#CBPHAT.__init__-59"><span class="linenos">59</span></a><span class="sd">            1. &#39;original&#39; (default): the original mask sparsity regularisation in HAT paper.</span>
</span><span id="CBPHAT.__init__-60"><a href="#CBPHAT.__init__-60"><span class="linenos">60</span></a><span class="sd">            2. &#39;cross&#39;: the cross version mask sparsity regularisation.</span>
</span><span id="CBPHAT.__init__-61"><a href="#CBPHAT.__init__-61"><span class="linenos">61</span></a><span class="sd">        - **task_embedding_init_mode** (`str`): the initialisation method for task embeddings, should be one of the following:</span>
</span><span id="CBPHAT.__init__-62"><a href="#CBPHAT.__init__-62"><span class="linenos">62</span></a><span class="sd">            1. &#39;N01&#39; (default): standard normal distribution $N(0, 1)$.</span>
</span><span id="CBPHAT.__init__-63"><a href="#CBPHAT.__init__-63"><span class="linenos">63</span></a><span class="sd">            2. &#39;U-11&#39;: uniform distribution $U(-1, 1)$.</span>
</span><span id="CBPHAT.__init__-64"><a href="#CBPHAT.__init__-64"><span class="linenos">64</span></a><span class="sd">            3. &#39;U01&#39;: uniform distribution $U(0, 1)$.</span>
</span><span id="CBPHAT.__init__-65"><a href="#CBPHAT.__init__-65"><span class="linenos">65</span></a><span class="sd">            4. &#39;U-10&#39;: uniform distribution $U(-1, 0)$.</span>
</span><span id="CBPHAT.__init__-66"><a href="#CBPHAT.__init__-66"><span class="linenos">66</span></a><span class="sd">            5. &#39;last&#39;: inherit task embedding from last task.</span>
</span><span id="CBPHAT.__init__-67"><a href="#CBPHAT.__init__-67"><span class="linenos">67</span></a><span class="sd">        - **epsilon** (`float`): the value added to network sparsity to avoid zero appeared in equation (9) in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="CBPHAT.__init__-68"><a href="#CBPHAT.__init__-68"><span class="linenos">68</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="CBPHAT.__init__-69"><a href="#CBPHAT.__init__-69"><span class="linenos">69</span></a>        <span class="n">AdaHAT</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="CBPHAT.__init__-70"><a href="#CBPHAT.__init__-70"><span class="linenos">70</span></a>            <span class="bp">self</span><span class="p">,</span>
</span><span id="CBPHAT.__init__-71"><a href="#CBPHAT.__init__-71"><span class="linenos">71</span></a>            <span class="n">backbone</span><span class="o">=</span><span class="n">backbone</span><span class="p">,</span>
</span><span id="CBPHAT.__init__-72"><a href="#CBPHAT.__init__-72"><span class="linenos">72</span></a>            <span class="n">heads</span><span class="o">=</span><span class="n">heads</span><span class="p">,</span>
</span><span id="CBPHAT.__init__-73"><a href="#CBPHAT.__init__-73"><span class="linenos">73</span></a>            <span class="n">adjustment_mode</span><span class="o">=</span><span class="n">adjustment_mode</span><span class="p">,</span>
</span><span id="CBPHAT.__init__-74"><a href="#CBPHAT.__init__-74"><span class="linenos">74</span></a>            <span class="n">adjustment_intensity</span><span class="o">=</span><span class="n">adjustment_intensity</span><span class="p">,</span>
</span><span id="CBPHAT.__init__-75"><a href="#CBPHAT.__init__-75"><span class="linenos">75</span></a>            <span class="n">s_max</span><span class="o">=</span><span class="n">s_max</span><span class="p">,</span>
</span><span id="CBPHAT.__init__-76"><a href="#CBPHAT.__init__-76"><span class="linenos">76</span></a>            <span class="n">clamp_threshold</span><span class="o">=</span><span class="n">clamp_threshold</span><span class="p">,</span>
</span><span id="CBPHAT.__init__-77"><a href="#CBPHAT.__init__-77"><span class="linenos">77</span></a>            <span class="n">mask_sparsity_reg_factor</span><span class="o">=</span><span class="n">mask_sparsity_reg_factor</span><span class="p">,</span>
</span><span id="CBPHAT.__init__-78"><a href="#CBPHAT.__init__-78"><span class="linenos">78</span></a>            <span class="n">mask_sparsity_reg_mode</span><span class="o">=</span><span class="n">mask_sparsity_reg_mode</span><span class="p">,</span>
</span><span id="CBPHAT.__init__-79"><a href="#CBPHAT.__init__-79"><span class="linenos">79</span></a>            <span class="n">task_embedding_init_mode</span><span class="o">=</span><span class="n">task_embedding_init_mode</span><span class="p">,</span>
</span><span id="CBPHAT.__init__-80"><a href="#CBPHAT.__init__-80"><span class="linenos">80</span></a>            <span class="n">epsilon</span><span class="o">=</span><span class="n">epsilon</span><span class="p">,</span>
</span><span id="CBPHAT.__init__-81"><a href="#CBPHAT.__init__-81"><span class="linenos">81</span></a>        <span class="p">)</span>
</span><span id="CBPHAT.__init__-82"><a href="#CBPHAT.__init__-82"><span class="linenos">82</span></a>
</span><span id="CBPHAT.__init__-83"><a href="#CBPHAT.__init__-83"><span class="linenos">83</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">utility_decay_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">utility_decay_rate</span>
</span><span id="CBPHAT.__init__-84"><a href="#CBPHAT.__init__-84"><span class="linenos">84</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the utility decay rate of units. &quot;&quot;&quot;</span>
</span><span id="CBPHAT.__init__-85"><a href="#CBPHAT.__init__-85"><span class="linenos">85</span></a>
</span><span id="CBPHAT.__init__-86"><a href="#CBPHAT.__init__-86"><span class="linenos">86</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">contribution_utility_t</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="CBPHAT.__init__-87"><a href="#CBPHAT.__init__-87"><span class="linenos">87</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the summative min-max scaled contribution utility of units. See $U$ in the paper draft. Keys are layer names and values are the utility tensor for the layer. The utility tensor is the same size as the feature tensor with size (number of units). &quot;&quot;&quot;</span>
</span><span id="CBPHAT.__init__-88"><a href="#CBPHAT.__init__-88"><span class="linenos">88</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">unit_importance_for_previous_tasks</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="CBPHAT.__init__-89"><a href="#CBPHAT.__init__-89"><span class="linenos">89</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the unit importance values of units for previous tasks (1, \cdots, self.task_id - 1). See the &quot;screenshot&quot; $I^{(t-1)}$ in the paper draft. Keys are layer names and values are the importance tensor for the layer. The importance tensor is the same size as the feature tensor with size (number of units). &quot;&quot;&quot;</span>
</span><span id="CBPHAT.__init__-90"><a href="#CBPHAT.__init__-90"><span class="linenos">90</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">age_t</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="CBPHAT.__init__-91"><a href="#CBPHAT.__init__-91"><span class="linenos">91</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the age of units. Keys are layer names and values are the age tensor for the layer for current task. The age tensor is the same size as the feature tensor with size (number of units). &quot;&quot;&quot;</span>
</span><span id="CBPHAT.__init__-92"><a href="#CBPHAT.__init__-92"><span class="linenos">92</span></a>
</span><span id="CBPHAT.__init__-93"><a href="#CBPHAT.__init__-93"><span class="linenos">93</span></a>        <span class="c1"># set manual optimisation</span>
</span><span id="CBPHAT.__init__-94"><a href="#CBPHAT.__init__-94"><span class="linenos">94</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">automatic_optimization</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="CBPHAT.__init__-95"><a href="#CBPHAT.__init__-95"><span class="linenos">95</span></a>
</span><span id="CBPHAT.__init__-96"><a href="#CBPHAT.__init__-96"><span class="linenos">96</span></a>        <span class="n">CBPHAT</span><span class="o">.</span><span class="n">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Initialise the CBPHAT algorithm with the network.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>backbone</strong> (<code>HATMaskBackbone</code>): must be a backbone network with HAT mask mechanism.</li>
<li><strong>heads</strong> (<code>HeadsTIL</code> | <code>HeadsCIL</code>): output heads.</li>
<li><strong>adjustment_mode</strong> (<code>str</code>): the strategy of adjustment i.e. the mode of gradient clipping, should be one of the following:
<ol>
<li>'cbphat': our original CBP mode.</li>
</ol></li>
<li><strong>adjustment_intensity</strong> (<code><a href="#CBPHAT.float">float</a></code>): hyperparameter, control the overall intensity of gradient adjustment. It's the $\alpha$ in equation (9) in <a href="https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9">AdaHAT paper</a>.</li>
<li><strong>utility_decay_rate</strong> (<code><a href="#CBPHAT.float">float</a></code>): the utility decay rate of units. It is the rate at which the utility of a unit decays over time.</li>
<li><strong>s_max</strong> (<code><a href="#CBPHAT.float">float</a></code>): hyperparameter, the maximum scaling factor in the gate function. See chapter 2.4 "Hard Attention Training" in <a href="http://proceedings.mlr.press/v80/serra18a">HAT paper</a>.</li>
<li><strong>clamp_threshold</strong> (<code><a href="#CBPHAT.float">float</a></code>): the threshold for task embedding gradient compensation. See chapter 2.5 "Embedding Gradient Compensation" in <a href="http://proceedings.mlr.press/v80/serra18a">HAT paper</a>.</li>
<li><strong>mask_sparsity_reg_factor</strong> (<code><a href="#CBPHAT.float">float</a></code>): hyperparameter, the regularisation factor for mask sparsity.</li>
<li><strong>mask_sparsity_reg_mode</strong> (<code>str</code>): the mode of mask sparsity regularisation, should be one of the following:
<ol>
<li>'original' (default): the original mask sparsity regularisation in HAT paper.</li>
<li>'cross': the cross version mask sparsity regularisation.</li>
</ol></li>
<li><strong>task_embedding_init_mode</strong> (<code>str</code>): the initialisation method for task embeddings, should be one of the following:
<ol>
<li>'N01' (default): standard normal distribution $N(0, 1)$.</li>
<li>'U-11': uniform distribution $U(-1, 1)$.</li>
<li>'U01': uniform distribution $U(0, 1)$.</li>
<li>'U-10': uniform distribution $U(-1, 0)$.</li>
<li>'last': inherit task embedding from last task.</li>
</ol></li>
<li><strong>epsilon</strong> (<code><a href="#CBPHAT.float">float</a></code>): the value added to network sparsity to avoid zero appeared in equation (9) in <a href="https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9">AdaHAT paper</a>.</li>
</ul>
</div>


                            </div>
                            <div id="CBPHAT.utility_decay_rate" class="classattr">
                                <div class="attr variable">
            <span class="name">utility_decay_rate</span><span class="annotation">: float</span>

        
    </div>
    <a class="headerlink" href="#CBPHAT.utility_decay_rate"></a>
    
            <div class="docstring"><p>Store the utility decay rate of units.</p>
</div>


                            </div>
                            <div id="CBPHAT.contribution_utility_t" class="classattr">
                                <div class="attr variable">
            <span class="name">contribution_utility_t</span><span class="annotation">: dict[str, torch.Tensor]</span>

        
    </div>
    <a class="headerlink" href="#CBPHAT.contribution_utility_t"></a>
    
            <div class="docstring"><p>Store the summative min-max scaled contribution utility of units. See $U$ in the paper draft. Keys are layer names and values are the utility tensor for the layer. The utility tensor is the same size as the feature tensor with size (number of units).</p>
</div>


                            </div>
                            <div id="CBPHAT.unit_importance_for_previous_tasks" class="classattr">
                                <div class="attr variable">
            <span class="name">unit_importance_for_previous_tasks</span><span class="annotation">: dict[str, torch.Tensor]</span>

        
    </div>
    <a class="headerlink" href="#CBPHAT.unit_importance_for_previous_tasks"></a>
    
            <div class="docstring"><p>Store the unit importance values of units for previous tasks (1, \cdots, self.task_id - 1). See the "screenshot" $I^{(t-1)}$ in the paper draft. Keys are layer names and values are the importance tensor for the layer. The importance tensor is the same size as the feature tensor with size (number of units).</p>
</div>


                            </div>
                            <div id="CBPHAT.age_t" class="classattr">
                                <div class="attr variable">
            <span class="name">age_t</span><span class="annotation">: dict[str, torch.Tensor]</span>

        
    </div>
    <a class="headerlink" href="#CBPHAT.age_t"></a>
    
            <div class="docstring"><p>Store the age of units. Keys are layer names and values are the age tensor for the layer for current task. The age tensor is the same size as the feature tensor with size (number of units).</p>
</div>


                            </div>
                            <div id="CBPHAT.automatic_optimization" class="classattr">
                                        <input id="CBPHAT.automatic_optimization-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr variable">
            <span class="name">automatic_optimization</span><span class="annotation">: bool</span>

                <label class="view-source-button" for="CBPHAT.automatic_optimization-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#CBPHAT.automatic_optimization"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="CBPHAT.automatic_optimization-290"><a href="#CBPHAT.automatic_optimization-290"><span class="linenos">290</span></a>    <span class="nd">@property</span>
</span><span id="CBPHAT.automatic_optimization-291"><a href="#CBPHAT.automatic_optimization-291"><span class="linenos">291</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">automatic_optimization</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
</span><span id="CBPHAT.automatic_optimization-292"><a href="#CBPHAT.automatic_optimization-292"><span class="linenos">292</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``.&quot;&quot;&quot;</span>
</span><span id="CBPHAT.automatic_optimization-293"><a href="#CBPHAT.automatic_optimization-293"><span class="linenos">293</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_automatic_optimization</span>
</span></pre></div>


            <div class="docstring"><p>If set to <code>False</code> you are responsible for calling <code>.backward()</code>, <code>.step()</code>, <code>.zero_grad()</code>.</p>
</div>


                            </div>
                            <div id="CBPHAT.sanity_check" class="classattr">
                                        <input id="CBPHAT.sanity_check-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">sanity_check</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span></span><span class="return-annotation">) -> <span class="kc">None</span>:</span></span>

                <label class="view-source-button" for="CBPHAT.sanity_check-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#CBPHAT.sanity_check"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="CBPHAT.sanity_check-98"><a href="#CBPHAT.sanity_check-98"><span class="linenos"> 98</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="CBPHAT.sanity_check-99"><a href="#CBPHAT.sanity_check-99"><span class="linenos"> 99</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Check the sanity of the arguments.</span>
</span><span id="CBPHAT.sanity_check-100"><a href="#CBPHAT.sanity_check-100"><span class="linenos">100</span></a>
</span><span id="CBPHAT.sanity_check-101"><a href="#CBPHAT.sanity_check-101"><span class="linenos">101</span></a><span class="sd">        **Raises:**</span>
</span><span id="CBPHAT.sanity_check-102"><a href="#CBPHAT.sanity_check-102"><span class="linenos">102</span></a><span class="sd">        - **ValueError**: If the utility decay rate is not in the range (0, 1].</span>
</span><span id="CBPHAT.sanity_check-103"><a href="#CBPHAT.sanity_check-103"><span class="linenos">103</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="CBPHAT.sanity_check-104"><a href="#CBPHAT.sanity_check-104"><span class="linenos">104</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">utility_decay_rate</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">utility_decay_rate</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="CBPHAT.sanity_check-105"><a href="#CBPHAT.sanity_check-105"><span class="linenos">105</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="CBPHAT.sanity_check-106"><a href="#CBPHAT.sanity_check-106"><span class="linenos">106</span></a>                <span class="sa">f</span><span class="s2">&quot;The utility decay rate should be in the range (0, 1], but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">utility_decay_rate</span><span class="si">}</span><span class="s2">.&quot;</span>
</span><span id="CBPHAT.sanity_check-107"><a href="#CBPHAT.sanity_check-107"><span class="linenos">107</span></a>            <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Check the sanity of the arguments.</p>

<p><strong>Raises:</strong></p>

<ul>
<li><strong>ValueError</strong>: If the utility decay rate is not in the range (0, 1].</li>
</ul>
</div>


                            </div>
                            <div id="CBPHAT.on_train_start" class="classattr">
                                        <input id="CBPHAT.on_train_start-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">on_train_start</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span></span><span class="return-annotation">) -> <span class="kc">None</span>:</span></span>

                <label class="view-source-button" for="CBPHAT.on_train_start-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#CBPHAT.on_train_start"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="CBPHAT.on_train_start-109"><a href="#CBPHAT.on_train_start-109"><span class="linenos">109</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_start</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="CBPHAT.on_train_start-110"><a href="#CBPHAT.on_train_start-110"><span class="linenos">110</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Additionally initialise the utility, age and the CBPHAT unit importance for each layer as zeros.&quot;&quot;&quot;</span>
</span><span id="CBPHAT.on_train_start-111"><a href="#CBPHAT.on_train_start-111"><span class="linenos">111</span></a>        <span class="n">AdaHAT</span><span class="o">.</span><span class="n">on_train_start</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</span><span id="CBPHAT.on_train_start-112"><a href="#CBPHAT.on_train_start-112"><span class="linenos">112</span></a>
</span><span id="CBPHAT.on_train_start-113"><a href="#CBPHAT.on_train_start-113"><span class="linenos">113</span></a>        <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="CBPHAT.on_train_start-114"><a href="#CBPHAT.on_train_start-114"><span class="linenos">114</span></a>            <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span>
</span><span id="CBPHAT.on_train_start-115"><a href="#CBPHAT.on_train_start-115"><span class="linenos">115</span></a>                <span class="n">layer_name</span>
</span><span id="CBPHAT.on_train_start-116"><a href="#CBPHAT.on_train_start-116"><span class="linenos">116</span></a>            <span class="p">)</span>  <span class="c1"># get the layer by its name</span>
</span><span id="CBPHAT.on_train_start-117"><a href="#CBPHAT.on_train_start-117"><span class="linenos">117</span></a>            <span class="n">num_units</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="CBPHAT.on_train_start-118"><a href="#CBPHAT.on_train_start-118"><span class="linenos">118</span></a>
</span><span id="CBPHAT.on_train_start-119"><a href="#CBPHAT.on_train_start-119"><span class="linenos">119</span></a>            <span class="c1"># initialise the utility and age at the beginning of first task</span>
</span><span id="CBPHAT.on_train_start-120"><a href="#CBPHAT.on_train_start-120"><span class="linenos">120</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">contribution_utility_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_units</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
</span><span id="CBPHAT.on_train_start-121"><a href="#CBPHAT.on_train_start-121"><span class="linenos">121</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
</span><span id="CBPHAT.on_train_start-122"><a href="#CBPHAT.on_train_start-122"><span class="linenos">122</span></a>            <span class="p">)</span>
</span><span id="CBPHAT.on_train_start-123"><a href="#CBPHAT.on_train_start-123"><span class="linenos">123</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">age_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_units</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="CBPHAT.on_train_start-124"><a href="#CBPHAT.on_train_start-124"><span class="linenos">124</span></a>
</span><span id="CBPHAT.on_train_start-125"><a href="#CBPHAT.on_train_start-125"><span class="linenos">125</span></a>            <span class="c1"># initialise the unit importance at the beginning of first task. This should not be called in `__init__()` method as the `self.device` is not available at that time.</span>
</span><span id="CBPHAT.on_train_start-126"><a href="#CBPHAT.on_train_start-126"><span class="linenos">126</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="CBPHAT.on_train_start-127"><a href="#CBPHAT.on_train_start-127"><span class="linenos">127</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">unit_importance_for_previous_tasks</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
</span><span id="CBPHAT.on_train_start-128"><a href="#CBPHAT.on_train_start-128"><span class="linenos">128</span></a>                    <span class="n">num_units</span>
</span><span id="CBPHAT.on_train_start-129"><a href="#CBPHAT.on_train_start-129"><span class="linenos">129</span></a>                <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
</span><span id="CBPHAT.on_train_start-130"><a href="#CBPHAT.on_train_start-130"><span class="linenos">130</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
</span><span id="CBPHAT.on_train_start-131"><a href="#CBPHAT.on_train_start-131"><span class="linenos">131</span></a>                <span class="p">)</span>  <span class="c1"># the unit importance $I^{(t-1)}$ is initialised as zeros mask ($t = 1$). See the paper draft.</span>
</span></pre></div>


            <div class="docstring"><p>Additionally initialise the utility, age and the CBPHAT unit importance for each layer as zeros.</p>
</div>


                            </div>
                            <div id="CBPHAT.clip_grad_by_adjustment" class="classattr">
                                        <input id="CBPHAT.clip_grad_by_adjustment-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">clip_grad_by_adjustment</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">network_sparsity</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="CBPHAT.clip_grad_by_adjustment-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#CBPHAT.clip_grad_by_adjustment"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="CBPHAT.clip_grad_by_adjustment-133"><a href="#CBPHAT.clip_grad_by_adjustment-133"><span class="linenos">133</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">clip_grad_by_adjustment</span><span class="p">(</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-134"><a href="#CBPHAT.clip_grad_by_adjustment-134"><span class="linenos">134</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-135"><a href="#CBPHAT.clip_grad_by_adjustment-135"><span class="linenos">135</span></a>        <span class="n">network_sparsity</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-136"><a href="#CBPHAT.clip_grad_by_adjustment-136"><span class="linenos">136</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-137"><a href="#CBPHAT.clip_grad_by_adjustment-137"><span class="linenos">137</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Clip the gradients by the adjustment rate.</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-138"><a href="#CBPHAT.clip_grad_by_adjustment-138"><span class="linenos">138</span></a>
</span><span id="CBPHAT.clip_grad_by_adjustment-139"><a href="#CBPHAT.clip_grad_by_adjustment-139"><span class="linenos">139</span></a><span class="sd">        Note that as the task embedding fully covers every layer in the backbone network, no parameters are left out of this system. This applies not only the parameters in between layers with task embedding, but also those before the first layer. We designed it seperately in the codes.</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-140"><a href="#CBPHAT.clip_grad_by_adjustment-140"><span class="linenos">140</span></a>
</span><span id="CBPHAT.clip_grad_by_adjustment-141"><a href="#CBPHAT.clip_grad_by_adjustment-141"><span class="linenos">141</span></a><span class="sd">        Network capacity is measured along with this method. Network capacity is defined as the average adjustment rate over all parameters. See chapter 4.1 in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-142"><a href="#CBPHAT.clip_grad_by_adjustment-142"><span class="linenos">142</span></a>
</span><span id="CBPHAT.clip_grad_by_adjustment-143"><a href="#CBPHAT.clip_grad_by_adjustment-143"><span class="linenos">143</span></a>
</span><span id="CBPHAT.clip_grad_by_adjustment-144"><a href="#CBPHAT.clip_grad_by_adjustment-144"><span class="linenos">144</span></a><span class="sd">        **Args:**</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-145"><a href="#CBPHAT.clip_grad_by_adjustment-145"><span class="linenos">145</span></a><span class="sd">        - **network_sparsity** (`dict[str, Tensor]` | `None`): The network sparsity i.e. the mask sparsity loss of each layer for the current task. It applies only to AdaHAT modes, as it is used to calculate the adjustment rate for the gradients.</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-146"><a href="#CBPHAT.clip_grad_by_adjustment-146"><span class="linenos">146</span></a>
</span><span id="CBPHAT.clip_grad_by_adjustment-147"><a href="#CBPHAT.clip_grad_by_adjustment-147"><span class="linenos">147</span></a><span class="sd">        **Returns:**</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-148"><a href="#CBPHAT.clip_grad_by_adjustment-148"><span class="linenos">148</span></a><span class="sd">        - **capacity** (`Tensor`): the calculated network capacity.</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-149"><a href="#CBPHAT.clip_grad_by_adjustment-149"><span class="linenos">149</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-150"><a href="#CBPHAT.clip_grad_by_adjustment-150"><span class="linenos">150</span></a>
</span><span id="CBPHAT.clip_grad_by_adjustment-151"><a href="#CBPHAT.clip_grad_by_adjustment-151"><span class="linenos">151</span></a>        <span class="c1"># initialise network capacity metric</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-152"><a href="#CBPHAT.clip_grad_by_adjustment-152"><span class="linenos">152</span></a>        <span class="n">capacity</span> <span class="o">=</span> <span class="n">HATNetworkCapacity</span><span class="p">()</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-153"><a href="#CBPHAT.clip_grad_by_adjustment-153"><span class="linenos">153</span></a>
</span><span id="CBPHAT.clip_grad_by_adjustment-154"><a href="#CBPHAT.clip_grad_by_adjustment-154"><span class="linenos">154</span></a>        <span class="c1"># Calculate the adjustment rate for gradients of the parameters, both weights and biases (if exists)</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-155"><a href="#CBPHAT.clip_grad_by_adjustment-155"><span class="linenos">155</span></a>        <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-156"><a href="#CBPHAT.clip_grad_by_adjustment-156"><span class="linenos">156</span></a>
</span><span id="CBPHAT.clip_grad_by_adjustment-157"><a href="#CBPHAT.clip_grad_by_adjustment-157"><span class="linenos">157</span></a>            <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-158"><a href="#CBPHAT.clip_grad_by_adjustment-158"><span class="linenos">158</span></a>                <span class="n">layer_name</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-159"><a href="#CBPHAT.clip_grad_by_adjustment-159"><span class="linenos">159</span></a>            <span class="p">)</span>  <span class="c1"># get the layer by its name</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-160"><a href="#CBPHAT.clip_grad_by_adjustment-160"><span class="linenos">160</span></a>
</span><span id="CBPHAT.clip_grad_by_adjustment-161"><a href="#CBPHAT.clip_grad_by_adjustment-161"><span class="linenos">161</span></a>            <span class="c1"># placeholder for the adjustment rate to avoid the error of using it before assignment</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-162"><a href="#CBPHAT.clip_grad_by_adjustment-162"><span class="linenos">162</span></a>            <span class="n">adjustment_rate_weight</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-163"><a href="#CBPHAT.clip_grad_by_adjustment-163"><span class="linenos">163</span></a>            <span class="n">adjustment_rate_bias</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-164"><a href="#CBPHAT.clip_grad_by_adjustment-164"><span class="linenos">164</span></a>
</span><span id="CBPHAT.clip_grad_by_adjustment-165"><a href="#CBPHAT.clip_grad_by_adjustment-165"><span class="linenos">165</span></a>            <span class="n">weight_importance</span><span class="p">,</span> <span class="n">bias_importance</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-166"><a href="#CBPHAT.clip_grad_by_adjustment-166"><span class="linenos">166</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_measure_parameter_wise</span><span class="p">(</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-167"><a href="#CBPHAT.clip_grad_by_adjustment-167"><span class="linenos">167</span></a>                    <span class="n">unit_wise_measure</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">unit_importance_for_previous_tasks</span><span class="p">,</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-168"><a href="#CBPHAT.clip_grad_by_adjustment-168"><span class="linenos">168</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-169"><a href="#CBPHAT.clip_grad_by_adjustment-169"><span class="linenos">169</span></a>                    <span class="n">aggregation</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-170"><a href="#CBPHAT.clip_grad_by_adjustment-170"><span class="linenos">170</span></a>                <span class="p">)</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-171"><a href="#CBPHAT.clip_grad_by_adjustment-171"><span class="linenos">171</span></a>            <span class="p">)</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-172"><a href="#CBPHAT.clip_grad_by_adjustment-172"><span class="linenos">172</span></a>
</span><span id="CBPHAT.clip_grad_by_adjustment-173"><a href="#CBPHAT.clip_grad_by_adjustment-173"><span class="linenos">173</span></a>            <span class="n">weight_summative_mask</span><span class="p">,</span> <span class="n">bias_summative_mask</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-174"><a href="#CBPHAT.clip_grad_by_adjustment-174"><span class="linenos">174</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_measure_parameter_wise</span><span class="p">(</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-175"><a href="#CBPHAT.clip_grad_by_adjustment-175"><span class="linenos">175</span></a>                    <span class="n">unit_wise_measure</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">summative_mask_for_previous_tasks</span><span class="p">,</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-176"><a href="#CBPHAT.clip_grad_by_adjustment-176"><span class="linenos">176</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-177"><a href="#CBPHAT.clip_grad_by_adjustment-177"><span class="linenos">177</span></a>                    <span class="n">aggregation</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-178"><a href="#CBPHAT.clip_grad_by_adjustment-178"><span class="linenos">178</span></a>                <span class="p">)</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-179"><a href="#CBPHAT.clip_grad_by_adjustment-179"><span class="linenos">179</span></a>            <span class="p">)</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-180"><a href="#CBPHAT.clip_grad_by_adjustment-180"><span class="linenos">180</span></a>
</span><span id="CBPHAT.clip_grad_by_adjustment-181"><a href="#CBPHAT.clip_grad_by_adjustment-181"><span class="linenos">181</span></a>            <span class="n">network_sparsity_layer</span> <span class="o">=</span> <span class="n">network_sparsity</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-182"><a href="#CBPHAT.clip_grad_by_adjustment-182"><span class="linenos">182</span></a>
</span><span id="CBPHAT.clip_grad_by_adjustment-183"><a href="#CBPHAT.clip_grad_by_adjustment-183"><span class="linenos">183</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;cbphat&quot;</span><span class="p">:</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-184"><a href="#CBPHAT.clip_grad_by_adjustment-184"><span class="linenos">184</span></a>
</span><span id="CBPHAT.clip_grad_by_adjustment-185"><a href="#CBPHAT.clip_grad_by_adjustment-185"><span class="linenos">185</span></a>                <span class="n">r_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span> <span class="o">/</span> <span class="p">(</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-186"><a href="#CBPHAT.clip_grad_by_adjustment-186"><span class="linenos">186</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">+</span> <span class="n">network_sparsity_layer</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-187"><a href="#CBPHAT.clip_grad_by_adjustment-187"><span class="linenos">187</span></a>                <span class="p">)</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-188"><a href="#CBPHAT.clip_grad_by_adjustment-188"><span class="linenos">188</span></a>                <span class="n">adjustment_rate_weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-189"><a href="#CBPHAT.clip_grad_by_adjustment-189"><span class="linenos">189</span></a>                    <span class="n">r_layer</span><span class="p">,</span> <span class="p">(</span><span class="n">weight_importance</span> <span class="o">+</span> <span class="n">weight_summative_mask</span> <span class="o">+</span> <span class="n">r_layer</span><span class="p">)</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-190"><a href="#CBPHAT.clip_grad_by_adjustment-190"><span class="linenos">190</span></a>                <span class="p">)</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-191"><a href="#CBPHAT.clip_grad_by_adjustment-191"><span class="linenos">191</span></a>
</span><span id="CBPHAT.clip_grad_by_adjustment-192"><a href="#CBPHAT.clip_grad_by_adjustment-192"><span class="linenos">192</span></a>                <span class="n">adjustment_rate_bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-193"><a href="#CBPHAT.clip_grad_by_adjustment-193"><span class="linenos">193</span></a>                    <span class="n">r_layer</span><span class="p">,</span> <span class="p">(</span><span class="n">bias_importance</span> <span class="o">+</span> <span class="n">bias_summative_mask</span> <span class="o">+</span> <span class="n">r_layer</span><span class="p">)</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-194"><a href="#CBPHAT.clip_grad_by_adjustment-194"><span class="linenos">194</span></a>                <span class="p">)</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-195"><a href="#CBPHAT.clip_grad_by_adjustment-195"><span class="linenos">195</span></a>
</span><span id="CBPHAT.clip_grad_by_adjustment-196"><a href="#CBPHAT.clip_grad_by_adjustment-196"><span class="linenos">196</span></a>            <span class="c1"># apply the adjustment rate to the gradients</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-197"><a href="#CBPHAT.clip_grad_by_adjustment-197"><span class="linenos">197</span></a>            <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">adjustment_rate_weight</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-198"><a href="#CBPHAT.clip_grad_by_adjustment-198"><span class="linenos">198</span></a>            <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-199"><a href="#CBPHAT.clip_grad_by_adjustment-199"><span class="linenos">199</span></a>                <span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">adjustment_rate_bias</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-200"><a href="#CBPHAT.clip_grad_by_adjustment-200"><span class="linenos">200</span></a>
</span><span id="CBPHAT.clip_grad_by_adjustment-201"><a href="#CBPHAT.clip_grad_by_adjustment-201"><span class="linenos">201</span></a>            <span class="c1"># update network capacity metric</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-202"><a href="#CBPHAT.clip_grad_by_adjustment-202"><span class="linenos">202</span></a>            <span class="n">capacity</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">adjustment_rate_weight</span><span class="p">,</span> <span class="n">adjustment_rate_bias</span><span class="p">)</span>
</span><span id="CBPHAT.clip_grad_by_adjustment-203"><a href="#CBPHAT.clip_grad_by_adjustment-203"><span class="linenos">203</span></a>
</span><span id="CBPHAT.clip_grad_by_adjustment-204"><a href="#CBPHAT.clip_grad_by_adjustment-204"><span class="linenos">204</span></a>        <span class="k">return</span> <span class="n">capacity</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
</span></pre></div>


            <div class="docstring"><p>Clip the gradients by the adjustment rate.</p>

<p>Note that as the task embedding fully covers every layer in the backbone network, no parameters are left out of this system. This applies not only the parameters in between layers with task embedding, but also those before the first layer. We designed it seperately in the codes.</p>

<p>Network capacity is measured along with this method. Network capacity is defined as the average adjustment rate over all parameters. See chapter 4.1 in <a href="https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9">AdaHAT paper</a>.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>network_sparsity</strong> (<code>dict[str, Tensor]</code> | <code>None</code>): The network sparsity i.e. the mask sparsity loss of each layer for the current task. It applies only to AdaHAT modes, as it is used to calculate the adjustment rate for the gradients.</li>
</ul>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>capacity</strong> (<code>Tensor</code>): the calculated network capacity.</li>
</ul>
</div>


                            </div>
                            <div id="CBPHAT.on_train_batch_end" class="classattr">
                                        <input id="CBPHAT.on_train_batch_end-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">on_train_batch_end</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">outputs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">typing</span><span class="o">.</span><span class="n">Any</span><span class="p">]</span>, </span><span class="param"><span class="n">batch</span><span class="p">:</span> <span class="n">Any</span>, </span><span class="param"><span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span></span><span class="return-annotation">) -> <span class="kc">None</span>:</span></span>

                <label class="view-source-button" for="CBPHAT.on_train_batch_end-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#CBPHAT.on_train_batch_end"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="CBPHAT.on_train_batch_end-206"><a href="#CBPHAT.on_train_batch_end-206"><span class="linenos">206</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_batch_end</span><span class="p">(</span>
</span><span id="CBPHAT.on_train_batch_end-207"><a href="#CBPHAT.on_train_batch_end-207"><span class="linenos">207</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span>
</span><span id="CBPHAT.on_train_batch_end-208"><a href="#CBPHAT.on_train_batch_end-208"><span class="linenos">208</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="CBPHAT.on_train_batch_end-209"><a href="#CBPHAT.on_train_batch_end-209"><span class="linenos">209</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Update the contribution utility and age of units after each training step.</span>
</span><span id="CBPHAT.on_train_batch_end-210"><a href="#CBPHAT.on_train_batch_end-210"><span class="linenos">210</span></a>
</span><span id="CBPHAT.on_train_batch_end-211"><a href="#CBPHAT.on_train_batch_end-211"><span class="linenos">211</span></a><span class="sd">        **Args:**</span>
</span><span id="CBPHAT.on_train_batch_end-212"><a href="#CBPHAT.on_train_batch_end-212"><span class="linenos">212</span></a><span class="sd">        - **outputs** (`dict[str, Any]`): the outputs of the training step, which is the returns of the `training_step()` method in the `CLAlgorithm`.</span>
</span><span id="CBPHAT.on_train_batch_end-213"><a href="#CBPHAT.on_train_batch_end-213"><span class="linenos">213</span></a><span class="sd">        - **batch** (`Any`): the training data batch.</span>
</span><span id="CBPHAT.on_train_batch_end-214"><a href="#CBPHAT.on_train_batch_end-214"><span class="linenos">214</span></a><span class="sd">        - **batch_idx** (`int`): the index of the current batch. This is for the file name of mask figures.</span>
</span><span id="CBPHAT.on_train_batch_end-215"><a href="#CBPHAT.on_train_batch_end-215"><span class="linenos">215</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="CBPHAT.on_train_batch_end-216"><a href="#CBPHAT.on_train_batch_end-216"><span class="linenos">216</span></a>
</span><span id="CBPHAT.on_train_batch_end-217"><a href="#CBPHAT.on_train_batch_end-217"><span class="linenos">217</span></a>        <span class="n">hidden_features</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;hidden_features&quot;</span><span class="p">]</span>
</span><span id="CBPHAT.on_train_batch_end-218"><a href="#CBPHAT.on_train_batch_end-218"><span class="linenos">218</span></a>        <span class="n">mask</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;mask&quot;</span><span class="p">]</span>
</span><span id="CBPHAT.on_train_batch_end-219"><a href="#CBPHAT.on_train_batch_end-219"><span class="linenos">219</span></a>
</span><span id="CBPHAT.on_train_batch_end-220"><a href="#CBPHAT.on_train_batch_end-220"><span class="linenos">220</span></a>        <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="CBPHAT.on_train_batch_end-221"><a href="#CBPHAT.on_train_batch_end-221"><span class="linenos">221</span></a>            <span class="c1"># layer-wise operation</span>
</span><span id="CBPHAT.on_train_batch_end-222"><a href="#CBPHAT.on_train_batch_end-222"><span class="linenos">222</span></a>            <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span>
</span><span id="CBPHAT.on_train_batch_end-223"><a href="#CBPHAT.on_train_batch_end-223"><span class="linenos">223</span></a>                <span class="n">layer_name</span>
</span><span id="CBPHAT.on_train_batch_end-224"><a href="#CBPHAT.on_train_batch_end-224"><span class="linenos">224</span></a>            <span class="p">)</span>  <span class="c1"># get the layer by its name</span>
</span><span id="CBPHAT.on_train_batch_end-225"><a href="#CBPHAT.on_train_batch_end-225"><span class="linenos">225</span></a>
</span><span id="CBPHAT.on_train_batch_end-226"><a href="#CBPHAT.on_train_batch_end-226"><span class="linenos">226</span></a>            <span class="c1"># update age</span>
</span><span id="CBPHAT.on_train_batch_end-227"><a href="#CBPHAT.on_train_batch_end-227"><span class="linenos">227</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">age_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span id="CBPHAT.on_train_batch_end-228"><a href="#CBPHAT.on_train_batch_end-228"><span class="linenos">228</span></a>
</span><span id="CBPHAT.on_train_batch_end-229"><a href="#CBPHAT.on_train_batch_end-229"><span class="linenos">229</span></a>            <span class="c1"># calculate current contribution utility</span>
</span><span id="CBPHAT.on_train_batch_end-230"><a href="#CBPHAT.on_train_batch_end-230"><span class="linenos">230</span></a>            <span class="n">current_contribution_utility</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="CBPHAT.on_train_batch_end-231"><a href="#CBPHAT.on_train_batch_end-231"><span class="linenos">231</span></a>                <span class="p">(</span>
</span><span id="CBPHAT.on_train_batch_end-232"><a href="#CBPHAT.on_train_batch_end-232"><span class="linenos">232</span></a>                    <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
</span><span id="CBPHAT.on_train_batch_end-233"><a href="#CBPHAT.on_train_batch_end-233"><span class="linenos">233</span></a>                        <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">hidden_features</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]),</span>
</span><span id="CBPHAT.on_train_batch_end-234"><a href="#CBPHAT.on_train_batch_end-234"><span class="linenos">234</span></a>                        <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="CBPHAT.on_train_batch_end-235"><a href="#CBPHAT.on_train_batch_end-235"><span class="linenos">235</span></a>                            <span class="n">i</span>
</span><span id="CBPHAT.on_train_batch_end-236"><a href="#CBPHAT.on_train_batch_end-236"><span class="linenos">236</span></a>                            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">hidden_features</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span>
</span><span id="CBPHAT.on_train_batch_end-237"><a href="#CBPHAT.on_train_batch_end-237"><span class="linenos">237</span></a>                            <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="CBPHAT.on_train_batch_end-238"><a href="#CBPHAT.on_train_batch_end-238"><span class="linenos">238</span></a>                        <span class="p">],</span>  <span class="c1"># average the features over batch samples</span>
</span><span id="CBPHAT.on_train_batch_end-239"><a href="#CBPHAT.on_train_batch_end-239"><span class="linenos">239</span></a>                    <span class="p">)</span>
</span><span id="CBPHAT.on_train_batch_end-240"><a href="#CBPHAT.on_train_batch_end-240"><span class="linenos">240</span></a>                    <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
</span><span id="CBPHAT.on_train_batch_end-241"><a href="#CBPHAT.on_train_batch_end-241"><span class="linenos">241</span></a>                        <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="p">),</span>
</span><span id="CBPHAT.on_train_batch_end-242"><a href="#CBPHAT.on_train_batch_end-242"><span class="linenos">242</span></a>                        <span class="n">dim</span><span class="o">=</span><span class="p">[</span>
</span><span id="CBPHAT.on_train_batch_end-243"><a href="#CBPHAT.on_train_batch_end-243"><span class="linenos">243</span></a>                            <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">0</span>
</span><span id="CBPHAT.on_train_batch_end-244"><a href="#CBPHAT.on_train_batch_end-244"><span class="linenos">244</span></a>                        <span class="p">],</span>  <span class="c1"># sum over the output dimension</span>
</span><span id="CBPHAT.on_train_batch_end-245"><a href="#CBPHAT.on_train_batch_end-245"><span class="linenos">245</span></a>                    <span class="p">)</span>
</span><span id="CBPHAT.on_train_batch_end-246"><a href="#CBPHAT.on_train_batch_end-246"><span class="linenos">246</span></a>                <span class="p">)</span>
</span><span id="CBPHAT.on_train_batch_end-247"><a href="#CBPHAT.on_train_batch_end-247"><span class="linenos">247</span></a>                <span class="o">*</span> <span class="n">mask</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="CBPHAT.on_train_batch_end-248"><a href="#CBPHAT.on_train_batch_end-248"><span class="linenos">248</span></a>            <span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="CBPHAT.on_train_batch_end-249"><a href="#CBPHAT.on_train_batch_end-249"><span class="linenos">249</span></a>            <span class="n">current_contribution_utility</span> <span class="o">=</span> <span class="n">min_max_normalise</span><span class="p">(</span>
</span><span id="CBPHAT.on_train_batch_end-250"><a href="#CBPHAT.on_train_batch_end-250"><span class="linenos">250</span></a>                <span class="n">current_contribution_utility</span>
</span><span id="CBPHAT.on_train_batch_end-251"><a href="#CBPHAT.on_train_batch_end-251"><span class="linenos">251</span></a>            <span class="p">)</span>  <span class="c1"># normalise the utility to [0,1] to avoid linearly increasing utility</span>
</span><span id="CBPHAT.on_train_batch_end-252"><a href="#CBPHAT.on_train_batch_end-252"><span class="linenos">252</span></a>
</span><span id="CBPHAT.on_train_batch_end-253"><a href="#CBPHAT.on_train_batch_end-253"><span class="linenos">253</span></a>            <span class="c1"># update utility</span>
</span><span id="CBPHAT.on_train_batch_end-254"><a href="#CBPHAT.on_train_batch_end-254"><span class="linenos">254</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">contribution_utility_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="CBPHAT.on_train_batch_end-255"><a href="#CBPHAT.on_train_batch_end-255"><span class="linenos">255</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">utility_decay_rate</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">contribution_utility_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="CBPHAT.on_train_batch_end-256"><a href="#CBPHAT.on_train_batch_end-256"><span class="linenos">256</span></a>                <span class="o">+</span> <span class="n">current_contribution_utility</span>
</span><span id="CBPHAT.on_train_batch_end-257"><a href="#CBPHAT.on_train_batch_end-257"><span class="linenos">257</span></a>            <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Update the contribution utility and age of units after each training step.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the training step, which is the returns of the <code><a href="#CBPHAT.training_step">training_step()</a></code> method in the <code>CLAlgorithm</code>.</li>
<li><strong>batch</strong> (<code>Any</code>): the training data batch.</li>
<li><strong>batch_idx</strong> (<code>int</code>): the index of the current batch. This is for the file name of mask figures.</li>
</ul>
</div>


                            </div>
                            <div id="CBPHAT.on_train_end" class="classattr">
                                        <input id="CBPHAT.on_train_end-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">on_train_end</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span></span><span class="return-annotation">) -> <span class="kc">None</span>:</span></span>

                <label class="view-source-button" for="CBPHAT.on_train_end-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#CBPHAT.on_train_end"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="CBPHAT.on_train_end-259"><a href="#CBPHAT.on_train_end-259"><span class="linenos">259</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_end</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="CBPHAT.on_train_end-260"><a href="#CBPHAT.on_train_end-260"><span class="linenos">260</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Additionally convert the contribution utility into importance and store (take screenshot of) it as unit importance for previous tasks at the end of a task training.&quot;&quot;&quot;</span>
</span><span id="CBPHAT.on_train_end-261"><a href="#CBPHAT.on_train_end-261"><span class="linenos">261</span></a>        <span class="n">AdaHAT</span><span class="o">.</span><span class="n">on_train_end</span><span class="p">(</span>
</span><span id="CBPHAT.on_train_end-262"><a href="#CBPHAT.on_train_end-262"><span class="linenos">262</span></a>            <span class="bp">self</span>
</span><span id="CBPHAT.on_train_end-263"><a href="#CBPHAT.on_train_end-263"><span class="linenos">263</span></a>        <span class="p">)</span>  <span class="c1"># store the mask and update cumulative and summative masks</span>
</span><span id="CBPHAT.on_train_end-264"><a href="#CBPHAT.on_train_end-264"><span class="linenos">264</span></a>
</span><span id="CBPHAT.on_train_end-265"><a href="#CBPHAT.on_train_end-265"><span class="linenos">265</span></a>        <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="CBPHAT.on_train_end-266"><a href="#CBPHAT.on_train_end-266"><span class="linenos">266</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">unit_importance_for_previous_tasks</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span>
</span><span id="CBPHAT.on_train_end-267"><a href="#CBPHAT.on_train_end-267"><span class="linenos">267</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">contribution_utility_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="CBPHAT.on_train_end-268"><a href="#CBPHAT.on_train_end-268"><span class="linenos">268</span></a>            <span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">age_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span></pre></div>


            <div class="docstring"><p>Additionally convert the contribution utility into importance and store (take screenshot of) it as unit importance for previous tasks at the end of a task training.</p>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt><a href="adahat.html#AdaHAT">clarena.cl_algorithms.adahat.AdaHAT</a></dt>
                                <dd id="CBPHAT.adjustment_intensity" class="variable"><a href="adahat.html#AdaHAT.adjustment_intensity">adjustment_intensity</a></dd>
                <dd id="CBPHAT.epsilon" class="variable"><a href="adahat.html#AdaHAT.epsilon">epsilon</a></dd>
                <dd id="CBPHAT.summative_mask_for_previous_tasks" class="variable"><a href="adahat.html#AdaHAT.summative_mask_for_previous_tasks">summative_mask_for_previous_tasks</a></dd>

            </div>
            <div><dt><a href="hat.html#HAT">clarena.cl_algorithms.hat.HAT</a></dt>
                                <dd id="CBPHAT.adjustment_mode" class="variable"><a href="hat.html#HAT.adjustment_mode">adjustment_mode</a></dd>
                <dd id="CBPHAT.s_max" class="variable"><a href="hat.html#HAT.s_max">s_max</a></dd>
                <dd id="CBPHAT.clamp_threshold" class="variable"><a href="hat.html#HAT.clamp_threshold">clamp_threshold</a></dd>
                <dd id="CBPHAT.mask_sparsity_reg_factor" class="variable"><a href="hat.html#HAT.mask_sparsity_reg_factor">mask_sparsity_reg_factor</a></dd>
                <dd id="CBPHAT.mask_sparsity_reg_mode" class="variable"><a href="hat.html#HAT.mask_sparsity_reg_mode">mask_sparsity_reg_mode</a></dd>
                <dd id="CBPHAT.mark_sparsity_reg" class="variable"><a href="hat.html#HAT.mark_sparsity_reg">mark_sparsity_reg</a></dd>
                <dd id="CBPHAT.task_embedding_init_mode" class="variable"><a href="hat.html#HAT.task_embedding_init_mode">task_embedding_init_mode</a></dd>
                <dd id="CBPHAT.alpha" class="variable"><a href="hat.html#HAT.alpha">alpha</a></dd>
                <dd id="CBPHAT.masks" class="variable"><a href="hat.html#HAT.masks">masks</a></dd>
                <dd id="CBPHAT.cumulative_mask_for_previous_tasks" class="variable"><a href="hat.html#HAT.cumulative_mask_for_previous_tasks">cumulative_mask_for_previous_tasks</a></dd>
                <dd id="CBPHAT.compensate_task_embedding_gradients" class="function"><a href="hat.html#HAT.compensate_task_embedding_gradients">compensate_task_embedding_gradients</a></dd>
                <dd id="CBPHAT.forward" class="function"><a href="hat.html#HAT.forward">forward</a></dd>
                <dd id="CBPHAT.training_step" class="function"><a href="hat.html#HAT.training_step">training_step</a></dd>
                <dd id="CBPHAT.validation_step" class="function"><a href="hat.html#HAT.validation_step">validation_step</a></dd>
                <dd id="CBPHAT.test_step" class="function"><a href="hat.html#HAT.test_step">test_step</a></dd>

            </div>
                                </dl>
                            </div>
                </section>
    </main>
<script>
    function escapeHTML(html) {
        return document.createElement('div').appendChild(document.createTextNode(html)).parentNode.innerHTML;
    }

    const originalContent = document.querySelector("main.pdoc");
    let currentContent = originalContent;

    function setContent(innerHTML) {
        let elem;
        if (innerHTML) {
            elem = document.createElement("main");
            elem.classList.add("pdoc");
            elem.innerHTML = innerHTML;
        } else {
            elem = originalContent;
        }
        if (currentContent !== elem) {
            currentContent.replaceWith(elem);
            currentContent = elem;
        }
    }

    function getSearchTerm() {
        return (new URL(window.location)).searchParams.get("search");
    }

    const searchBox = document.querySelector(".pdoc input[type=search]");
    searchBox.addEventListener("input", function () {
        let url = new URL(window.location);
        if (searchBox.value.trim()) {
            url.hash = "";
            url.searchParams.set("search", searchBox.value);
        } else {
            url.searchParams.delete("search");
        }
        history.replaceState("", "", url.toString());
        onInput();
    });
    window.addEventListener("popstate", onInput);


    let search, searchErr;

    async function initialize() {
        try {
            search = await new Promise((resolve, reject) => {
                const script = document.createElement("script");
                script.type = "text/javascript";
                script.async = true;
                script.onload = () => resolve(window.pdocSearch);
                script.onerror = (e) => reject(e);
                script.src = "../../search.js";
                document.getElementsByTagName("head")[0].appendChild(script);
            });
        } catch (e) {
            console.error("Cannot fetch pdoc search index");
            searchErr = "Cannot fetch search index.";
        }
        onInput();

        document.querySelector("nav.pdoc").addEventListener("click", e => {
            if (e.target.hash) {
                searchBox.value = "";
                searchBox.dispatchEvent(new Event("input"));
            }
        });
    }

    function onInput() {
        setContent((() => {
            const term = getSearchTerm();
            if (!term) {
                return null
            }
            if (searchErr) {
                return `<h3>Error: ${searchErr}</h3>`
            }
            if (!search) {
                return "<h3>Searching...</h3>"
            }

            window.scrollTo({top: 0, left: 0, behavior: 'auto'});

            const results = search(term);

            let html;
            if (results.length === 0) {
                html = `No search results for '${escapeHTML(term)}'.`
            } else {
                html = `<h4>${results.length} search result${results.length > 1 ? "s" : ""} for '${escapeHTML(term)}'.</h4>`;
            }
            for (let result of results.slice(0, 10)) {
                let doc = result.doc;
                let url = `../../${doc.modulename.replaceAll(".", "/")}.html`;
                if (doc.qualname) {
                    url += `#${doc.qualname}`;
                }

                let heading;
                switch (result.doc.kind) {
                    case "function":
                        if (doc.fullname.endsWith(".__init__")) {
                            heading = `<span class="name">${doc.fullname.replace(/\.__init__$/, "")}</span>${doc.signature}`;
                        } else {
                            heading = `<span class="def">${doc.funcdef}</span> <span class="name">${doc.fullname}</span>${doc.signature}`;
                        }
                        break;
                    case "class":
                        heading = `<span class="def">class</span> <span class="name">${doc.fullname}</span>`;
                        if (doc.bases)
                            heading += `<wbr>(<span class="base">${doc.bases}</span>)`;
                        heading += `:`;
                        break;
                    case "variable":
                        heading = `<span class="name">${doc.fullname}</span>`;
                        if (doc.annotation)
                            heading += `<span class="annotation">${doc.annotation}</span>`;
                        if (doc.default_value)
                            heading += `<span class="default_value"> = ${doc.default_value}</span>`;
                        break;
                    default:
                        heading = `<span class="name">${doc.fullname}</span>`;
                        break;
                }
                html += `
                        <section class="search-result">
                        <a href="${url}" class="attr ${doc.kind}">${heading}</a>
                        <div class="docstring">${doc.doc}</div>
                        </section>
                    `;

            }
            return html;
        })());
    }

    if (getSearchTerm()) {
        initialize();
        searchBox.value = getSearchTerm();
        onInput();
    } else {
        searchBox.addEventListener("focus", initialize, {once: true});
    }

    searchBox.addEventListener("keydown", e => {
        if (["ArrowDown", "ArrowUp", "Enter"].includes(e.key)) {
            let focused = currentContent.querySelector(".search-result.focused");
            if (!focused) {
                currentContent.querySelector(".search-result").classList.add("focused");
            } else if (
                e.key === "ArrowDown"
                && focused.nextElementSibling
                && focused.nextElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.nextElementSibling.classList.add("focused");
                focused.nextElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "ArrowUp"
                && focused.previousElementSibling
                && focused.previousElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.previousElementSibling.classList.add("focused");
                focused.previousElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "Enter"
            ) {
                focused.querySelector("a").click();
            }
        }
    });
</script></body>
</html>