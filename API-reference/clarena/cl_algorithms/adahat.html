<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="pdoc 15.0.1"/>
    <title>clarena.cl_algorithms.adahat API documentation</title>

    <style>/*! * Bootstrap Reboot v5.0.0 (https://getbootstrap.com/) * Copyright 2011-2021 The Bootstrap Authors * Copyright 2011-2021 Twitter, Inc. * Licensed under MIT (https://github.com/twbs/bootstrap/blob/main/LICENSE) * Forked from Normalize.css, licensed MIT (https://github.com/necolas/normalize.css/blob/master/LICENSE.md) */*,::after,::before{box-sizing:border-box}@media (prefers-reduced-motion:no-preference){:root{scroll-behavior:smooth}}body{margin:0;font-family:system-ui,-apple-system,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans","Liberation Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";font-size:1rem;font-weight:400;line-height:1.5;color:#212529;background-color:#fff;-webkit-text-size-adjust:100%;-webkit-tap-highlight-color:transparent}hr{margin:1rem 0;color:inherit;background-color:currentColor;border:0;opacity:.25}hr:not([size]){height:1px}h1,h2,h3,h4,h5,h6{margin-top:0;margin-bottom:.5rem;font-weight:500;line-height:1.2}h1{font-size:calc(1.375rem + 1.5vw)}@media (min-width:1200px){h1{font-size:2.5rem}}h2{font-size:calc(1.325rem + .9vw)}@media (min-width:1200px){h2{font-size:2rem}}h3{font-size:calc(1.3rem + .6vw)}@media (min-width:1200px){h3{font-size:1.75rem}}h4{font-size:calc(1.275rem + .3vw)}@media (min-width:1200px){h4{font-size:1.5rem}}h5{font-size:1.25rem}h6{font-size:1rem}p{margin-top:0;margin-bottom:1rem}abbr[data-bs-original-title],abbr[title]{-webkit-text-decoration:underline dotted;text-decoration:underline dotted;cursor:help;-webkit-text-decoration-skip-ink:none;text-decoration-skip-ink:none}address{margin-bottom:1rem;font-style:normal;line-height:inherit}ol,ul{padding-left:2rem}dl,ol,ul{margin-top:0;margin-bottom:1rem}ol ol,ol ul,ul ol,ul ul{margin-bottom:0}dt{font-weight:700}dd{margin-bottom:.5rem;margin-left:0}blockquote{margin:0 0 1rem}b,strong{font-weight:bolder}small{font-size:.875em}mark{padding:.2em;background-color:#fcf8e3}sub,sup{position:relative;font-size:.75em;line-height:0;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}a{color:#0d6efd;text-decoration:underline}a:hover{color:#0a58ca}a:not([href]):not([class]),a:not([href]):not([class]):hover{color:inherit;text-decoration:none}code,kbd,pre,samp{font-family:SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;font-size:1em;direction:ltr;unicode-bidi:bidi-override}pre{display:block;margin-top:0;margin-bottom:1rem;overflow:auto;font-size:.875em}pre code{font-size:inherit;color:inherit;word-break:normal}code{font-size:.875em;color:#d63384;word-wrap:break-word}a>code{color:inherit}kbd{padding:.2rem .4rem;font-size:.875em;color:#fff;background-color:#212529;border-radius:.2rem}kbd kbd{padding:0;font-size:1em;font-weight:700}figure{margin:0 0 1rem}img,svg{vertical-align:middle}table{caption-side:bottom;border-collapse:collapse}caption{padding-top:.5rem;padding-bottom:.5rem;color:#6c757d;text-align:left}th{text-align:inherit;text-align:-webkit-match-parent}tbody,td,tfoot,th,thead,tr{border-color:inherit;border-style:solid;border-width:0}label{display:inline-block}button{border-radius:0}button:focus:not(:focus-visible){outline:0}button,input,optgroup,select,textarea{margin:0;font-family:inherit;font-size:inherit;line-height:inherit}button,select{text-transform:none}[role=button]{cursor:pointer}select{word-wrap:normal}select:disabled{opacity:1}[list]::-webkit-calendar-picker-indicator{display:none}[type=button],[type=reset],[type=submit],button{-webkit-appearance:button}[type=button]:not(:disabled),[type=reset]:not(:disabled),[type=submit]:not(:disabled),button:not(:disabled){cursor:pointer}::-moz-focus-inner{padding:0;border-style:none}textarea{resize:vertical}fieldset{min-width:0;padding:0;margin:0;border:0}legend{float:left;width:100%;padding:0;margin-bottom:.5rem;font-size:calc(1.275rem + .3vw);line-height:inherit}@media (min-width:1200px){legend{font-size:1.5rem}}legend+*{clear:left}::-webkit-datetime-edit-day-field,::-webkit-datetime-edit-fields-wrapper,::-webkit-datetime-edit-hour-field,::-webkit-datetime-edit-minute,::-webkit-datetime-edit-month-field,::-webkit-datetime-edit-text,::-webkit-datetime-edit-year-field{padding:0}::-webkit-inner-spin-button{height:auto}[type=search]{outline-offset:-2px;-webkit-appearance:textfield}::-webkit-search-decoration{-webkit-appearance:none}::-webkit-color-swatch-wrapper{padding:0}::file-selector-button{font:inherit}::-webkit-file-upload-button{font:inherit;-webkit-appearance:button}output{display:inline-block}iframe{border:0}summary{display:list-item;cursor:pointer}progress{vertical-align:baseline}[hidden]{display:none!important}</style>
    <style>/*! syntax-highlighting.css */pre{line-height:125%;}span.linenos{color:inherit; background-color:transparent; padding-left:5px; padding-right:20px;}.pdoc-code .hll{background-color:#ffffcc}.pdoc-code{background:#f8f8f8;}.pdoc-code .c{color:#3D7B7B; font-style:italic}.pdoc-code .err{border:1px solid #FF0000}.pdoc-code .k{color:#008000; font-weight:bold}.pdoc-code .o{color:#666666}.pdoc-code .ch{color:#3D7B7B; font-style:italic}.pdoc-code .cm{color:#3D7B7B; font-style:italic}.pdoc-code .cp{color:#9C6500}.pdoc-code .cpf{color:#3D7B7B; font-style:italic}.pdoc-code .c1{color:#3D7B7B; font-style:italic}.pdoc-code .cs{color:#3D7B7B; font-style:italic}.pdoc-code .gd{color:#A00000}.pdoc-code .ge{font-style:italic}.pdoc-code .gr{color:#E40000}.pdoc-code .gh{color:#000080; font-weight:bold}.pdoc-code .gi{color:#008400}.pdoc-code .go{color:#717171}.pdoc-code .gp{color:#000080; font-weight:bold}.pdoc-code .gs{font-weight:bold}.pdoc-code .gu{color:#800080; font-weight:bold}.pdoc-code .gt{color:#0044DD}.pdoc-code .kc{color:#008000; font-weight:bold}.pdoc-code .kd{color:#008000; font-weight:bold}.pdoc-code .kn{color:#008000; font-weight:bold}.pdoc-code .kp{color:#008000}.pdoc-code .kr{color:#008000; font-weight:bold}.pdoc-code .kt{color:#B00040}.pdoc-code .m{color:#666666}.pdoc-code .s{color:#BA2121}.pdoc-code .na{color:#687822}.pdoc-code .nb{color:#008000}.pdoc-code .nc{color:#0000FF; font-weight:bold}.pdoc-code .no{color:#880000}.pdoc-code .nd{color:#AA22FF}.pdoc-code .ni{color:#717171; font-weight:bold}.pdoc-code .ne{color:#CB3F38; font-weight:bold}.pdoc-code .nf{color:#0000FF}.pdoc-code .nl{color:#767600}.pdoc-code .nn{color:#0000FF; font-weight:bold}.pdoc-code .nt{color:#008000; font-weight:bold}.pdoc-code .nv{color:#19177C}.pdoc-code .ow{color:#AA22FF; font-weight:bold}.pdoc-code .w{color:#bbbbbb}.pdoc-code .mb{color:#666666}.pdoc-code .mf{color:#666666}.pdoc-code .mh{color:#666666}.pdoc-code .mi{color:#666666}.pdoc-code .mo{color:#666666}.pdoc-code .sa{color:#BA2121}.pdoc-code .sb{color:#BA2121}.pdoc-code .sc{color:#BA2121}.pdoc-code .dl{color:#BA2121}.pdoc-code .sd{color:#BA2121; font-style:italic}.pdoc-code .s2{color:#BA2121}.pdoc-code .se{color:#AA5D1F; font-weight:bold}.pdoc-code .sh{color:#BA2121}.pdoc-code .si{color:#A45A77; font-weight:bold}.pdoc-code .sx{color:#008000}.pdoc-code .sr{color:#A45A77}.pdoc-code .s1{color:#BA2121}.pdoc-code .ss{color:#19177C}.pdoc-code .bp{color:#008000}.pdoc-code .fm{color:#0000FF}.pdoc-code .vc{color:#19177C}.pdoc-code .vg{color:#19177C}.pdoc-code .vi{color:#19177C}.pdoc-code .vm{color:#19177C}.pdoc-code .il{color:#666666}</style>
    <style>/*! theme.css */:root{--pdoc-background:#fff;}.pdoc{--text:#212529;--muted:#6c757d;--link:#3660a5;--link-hover:#1659c5;--code:#f8f8f8;--active:#fff598;--accent:#eee;--accent2:#c1c1c1;--nav-hover:rgba(255, 255, 255, 0.5);--name:#0066BB;--def:#008800;--annotation:#007020;}</style>
    <style>/*! layout.css */html, body{width:100%;height:100%;}html, main{scroll-behavior:smooth;}body{background-color:var(--pdoc-background);}@media (max-width:769px){#navtoggle{cursor:pointer;position:absolute;width:50px;height:40px;top:1rem;right:1rem;border-color:var(--text);color:var(--text);display:flex;opacity:0.8;z-index:999;}#navtoggle:hover{opacity:1;}#togglestate + div{display:none;}#togglestate:checked + div{display:inherit;}main, header{padding:2rem 3vw;}header + main{margin-top:-3rem;}.git-button{display:none !important;}nav input[type="search"]{max-width:77%;}nav input[type="search"]:first-child{margin-top:-6px;}nav input[type="search"]:valid ~ *{display:none !important;}}@media (min-width:770px){:root{--sidebar-width:clamp(12.5rem, 28vw, 22rem);}nav{position:fixed;overflow:auto;height:100vh;width:var(--sidebar-width);}main, header{padding:3rem 2rem 3rem calc(var(--sidebar-width) + 3rem);width:calc(54rem + var(--sidebar-width));max-width:100%;}header + main{margin-top:-4rem;}#navtoggle{display:none;}}#togglestate{position:absolute;height:0;opacity:0;}nav.pdoc{--pad:clamp(0.5rem, 2vw, 1.75rem);--indent:1.5rem;background-color:var(--accent);border-right:1px solid var(--accent2);box-shadow:0 0 20px rgba(50, 50, 50, .2) inset;padding:0 0 0 var(--pad);overflow-wrap:anywhere;scrollbar-width:thin; scrollbar-color:var(--accent2) transparent; z-index:1}nav.pdoc::-webkit-scrollbar{width:.4rem; }nav.pdoc::-webkit-scrollbar-thumb{background-color:var(--accent2); }nav.pdoc > div{padding:var(--pad) 0;}nav.pdoc .module-list-button{display:inline-flex;align-items:center;color:var(--text);border-color:var(--muted);margin-bottom:1rem;}nav.pdoc .module-list-button:hover{border-color:var(--text);}nav.pdoc input[type=search]{display:block;outline-offset:0;width:calc(100% - var(--pad));}nav.pdoc .logo{max-width:calc(100% - var(--pad));max-height:35vh;display:block;margin:0 auto 1rem;transform:translate(calc(-.5 * var(--pad)), 0);}nav.pdoc ul{list-style:none;padding-left:0;}nav.pdoc > div > ul{margin-left:calc(0px - var(--pad));}nav.pdoc li a{padding:.2rem 0 .2rem calc(var(--pad) + var(--indent));}nav.pdoc > div > ul > li > a{padding-left:var(--pad);}nav.pdoc li{transition:all 100ms;}nav.pdoc li:hover{background-color:var(--nav-hover);}nav.pdoc a, nav.pdoc a:hover{color:var(--text);}nav.pdoc a{display:block;}nav.pdoc > h2:first-of-type{margin-top:1.5rem;}nav.pdoc .class:before{content:"class ";color:var(--muted);}nav.pdoc .function:after{content:"()";color:var(--muted);}nav.pdoc footer:before{content:"";display:block;width:calc(100% - var(--pad));border-top:solid var(--accent2) 1px;margin-top:1.5rem;padding-top:.5rem;}nav.pdoc footer{font-size:small;}</style>
    <style>/*! content.css */.pdoc{color:var(--text);box-sizing:border-box;line-height:1.5;background:none;}.pdoc .pdoc-button{cursor:pointer;display:inline-block;border:solid black 1px;border-radius:2px;font-size:.75rem;padding:calc(0.5em - 1px) 1em;transition:100ms all;}.pdoc .alert{padding:1rem 1rem 1rem calc(1.5rem + 24px);border:1px solid transparent;border-radius:.25rem;background-repeat:no-repeat;background-position:.75rem center;margin-bottom:1rem;}.pdoc .alert > em{display:none;}.pdoc .alert > *:last-child{margin-bottom:0;}.pdoc .alert.note {color:#084298;background-color:#cfe2ff;border-color:#b6d4fe;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23084298%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8%2016A8%208%200%201%200%208%200a8%208%200%200%200%200%2016zm.93-9.412-1%204.705c-.07.34.029.533.304.533.194%200%20.487-.07.686-.246l-.088.416c-.287.346-.92.598-1.465.598-.703%200-1.002-.422-.808-1.319l.738-3.468c.064-.293.006-.399-.287-.47l-.451-.081.082-.381%202.29-.287zM8%205.5a1%201%200%201%201%200-2%201%201%200%200%201%200%202z%22/%3E%3C/svg%3E");}.pdoc .alert.warning{color:#664d03;background-color:#fff3cd;border-color:#ffecb5;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23664d03%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8.982%201.566a1.13%201.13%200%200%200-1.96%200L.165%2013.233c-.457.778.091%201.767.98%201.767h13.713c.889%200%201.438-.99.98-1.767L8.982%201.566zM8%205c.535%200%20.954.462.9.995l-.35%203.507a.552.552%200%200%201-1.1%200L7.1%205.995A.905.905%200%200%201%208%205zm.002%206a1%201%200%201%201%200%202%201%201%200%200%201%200-2z%22/%3E%3C/svg%3E");}.pdoc .alert.danger{color:#842029;background-color:#f8d7da;border-color:#f5c2c7;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23842029%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M5.52.359A.5.5%200%200%201%206%200h4a.5.5%200%200%201%20.474.658L8.694%206H12.5a.5.5%200%200%201%20.395.807l-7%209a.5.5%200%200%201-.873-.454L6.823%209.5H3.5a.5.5%200%200%201-.48-.641l2.5-8.5z%22/%3E%3C/svg%3E");}.pdoc .visually-hidden{position:absolute !important;width:1px !important;height:1px !important;padding:0 !important;margin:-1px !important;overflow:hidden !important;clip:rect(0, 0, 0, 0) !important;white-space:nowrap !important;border:0 !important;}.pdoc h1, .pdoc h2, .pdoc h3{font-weight:300;margin:.3em 0;padding:.2em 0;}.pdoc > section:not(.module-info) h1{font-size:1.5rem;font-weight:500;}.pdoc > section:not(.module-info) h2{font-size:1.4rem;font-weight:500;}.pdoc > section:not(.module-info) h3{font-size:1.3rem;font-weight:500;}.pdoc > section:not(.module-info) h4{font-size:1.2rem;}.pdoc > section:not(.module-info) h5{font-size:1.1rem;}.pdoc a{text-decoration:none;color:var(--link);}.pdoc a:hover{color:var(--link-hover);}.pdoc blockquote{margin-left:2rem;}.pdoc pre{border-top:1px solid var(--accent2);border-bottom:1px solid var(--accent2);margin-top:0;margin-bottom:1em;padding:.5rem 0 .5rem .5rem;overflow-x:auto;background-color:var(--code);}.pdoc code{color:var(--text);padding:.2em .4em;margin:0;font-size:85%;background-color:var(--accent);border-radius:6px;}.pdoc a > code{color:inherit;}.pdoc pre > code{display:inline-block;font-size:inherit;background:none;border:none;padding:0;}.pdoc > section:not(.module-info){margin-bottom:1.5rem;}.pdoc .modulename{margin-top:0;font-weight:bold;}.pdoc .modulename a{color:var(--link);transition:100ms all;}.pdoc .git-button{float:right;border:solid var(--link) 1px;}.pdoc .git-button:hover{background-color:var(--link);color:var(--pdoc-background);}.view-source-toggle-state,.view-source-toggle-state ~ .pdoc-code{display:none;}.view-source-toggle-state:checked ~ .pdoc-code{display:block;}.view-source-button{display:inline-block;float:right;font-size:.75rem;line-height:1.5rem;color:var(--muted);padding:0 .4rem 0 1.3rem;cursor:pointer;text-indent:-2px;}.view-source-button > span{visibility:hidden;}.module-info .view-source-button{float:none;display:flex;justify-content:flex-end;margin:-1.2rem .4rem -.2rem 0;}.view-source-button::before{position:absolute;content:"View Source";display:list-item;list-style-type:disclosure-closed;}.view-source-toggle-state:checked ~ .attr .view-source-button::before,.view-source-toggle-state:checked ~ .view-source-button::before{list-style-type:disclosure-open;}.pdoc .docstring{margin-bottom:1.5rem;}.pdoc section:not(.module-info) .docstring{margin-left:clamp(0rem, 5vw - 2rem, 1rem);}.pdoc .docstring .pdoc-code{margin-left:1em;margin-right:1em;}.pdoc h1:target,.pdoc h2:target,.pdoc h3:target,.pdoc h4:target,.pdoc h5:target,.pdoc h6:target,.pdoc .pdoc-code > pre > span:target{background-color:var(--active);box-shadow:-1rem 0 0 0 var(--active);}.pdoc .pdoc-code > pre > span:target{display:block;}.pdoc div:target > .attr,.pdoc section:target > .attr,.pdoc dd:target > a{background-color:var(--active);}.pdoc *{scroll-margin:2rem;}.pdoc .pdoc-code .linenos{user-select:none;}.pdoc .attr:hover{filter:contrast(0.95);}.pdoc section, .pdoc .classattr{position:relative;}.pdoc .headerlink{--width:clamp(1rem, 3vw, 2rem);position:absolute;top:0;left:calc(0rem - var(--width));transition:all 100ms ease-in-out;opacity:0;}.pdoc .headerlink::before{content:"#";display:block;text-align:center;width:var(--width);height:2.3rem;line-height:2.3rem;font-size:1.5rem;}.pdoc .attr:hover ~ .headerlink,.pdoc *:target > .headerlink,.pdoc .headerlink:hover{opacity:1;}.pdoc .attr{display:block;margin:.5rem 0 .5rem;padding:.4rem .4rem .4rem 1rem;background-color:var(--accent);overflow-x:auto;}.pdoc .classattr{margin-left:2rem;}.pdoc .decorator-deprecated{color:#842029;}.pdoc .decorator-deprecated ~ span{filter:grayscale(1) opacity(0.8);}.pdoc .name{color:var(--name);font-weight:bold;}.pdoc .def{color:var(--def);font-weight:bold;}.pdoc .signature{background-color:transparent;}.pdoc .param, .pdoc .return-annotation{white-space:pre;}.pdoc .signature.multiline .param{display:block;}.pdoc .signature.condensed .param{display:inline-block;}.pdoc .annotation{color:var(--annotation);}.pdoc .view-value-toggle-state,.pdoc .view-value-toggle-state ~ .default_value{display:none;}.pdoc .view-value-toggle-state:checked ~ .default_value{display:inherit;}.pdoc .view-value-button{font-size:.5rem;vertical-align:middle;border-style:dashed;margin-top:-0.1rem;}.pdoc .view-value-button:hover{background:white;}.pdoc .view-value-button::before{content:"show";text-align:center;width:2.2em;display:inline-block;}.pdoc .view-value-toggle-state:checked ~ .view-value-button::before{content:"hide";}.pdoc .inherited{margin-left:2rem;}.pdoc .inherited dt{font-weight:700;}.pdoc .inherited dt, .pdoc .inherited dd{display:inline;margin-left:0;margin-bottom:.5rem;}.pdoc .inherited dd:not(:last-child):after{content:", ";}.pdoc .inherited .class:before{content:"class ";}.pdoc .inherited .function a:after{content:"()";}.pdoc .search-result .docstring{overflow:auto;max-height:25vh;}.pdoc .search-result.focused > .attr{background-color:var(--active);}.pdoc .attribution{margin-top:2rem;display:block;opacity:0.5;transition:all 200ms;filter:grayscale(100%);}.pdoc .attribution:hover{opacity:1;filter:grayscale(0%);}.pdoc .attribution img{margin-left:5px;height:35px;vertical-align:middle;width:70px;transition:all 200ms;}.pdoc table{display:block;width:max-content;max-width:100%;overflow:auto;margin-bottom:1rem;}.pdoc table th{font-weight:600;}.pdoc table th, .pdoc table td{padding:6px 13px;border:1px solid var(--accent2);}</style>
    <style>/*! custom.css */</style></head>
<body>
    <nav class="pdoc">
        <label id="navtoggle" for="togglestate" class="pdoc-button"><svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 30 30'><path stroke-linecap='round' stroke="currentColor" stroke-miterlimit='10' stroke-width='2' d='M4 7h22M4 15h22M4 23h22'/></svg></label>
        <input id="togglestate" type="checkbox" aria-hidden="true" tabindex="-1">
        <div>            <a class="pdoc-button module-list-button" href="../cl_algorithms.html">
<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-box-arrow-in-left" viewBox="0 0 16 16">
  <path fill-rule="evenodd" d="M10 3.5a.5.5 0 0 0-.5-.5h-8a.5.5 0 0 0-.5.5v9a.5.5 0 0 0 .5.5h8a.5.5 0 0 0 .5-.5v-2a.5.5 0 0 1 1 0v2A1.5 1.5 0 0 1 9.5 14h-8A1.5 1.5 0 0 1 0 12.5v-9A1.5 1.5 0 0 1 1.5 2h8A1.5 1.5 0 0 1 11 3.5v2a.5.5 0 0 1-1 0v-2z"/>
  <path fill-rule="evenodd" d="M4.146 8.354a.5.5 0 0 1 0-.708l3-3a.5.5 0 1 1 .708.708L5.707 7.5H14.5a.5.5 0 0 1 0 1H5.707l2.147 2.146a.5.5 0 0 1-.708.708l-3-3z"/>
</svg>                &nbsp;clarena.cl_algorithms</a>


            <input type="search" placeholder="Search..." role="searchbox" aria-label="search"
                   pattern=".+" required>



            <h2>API Documentation</h2>
                <ul class="memberlist">
            <li>
                    <a class="class" href="#AdaHAT">AdaHAT</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#AdaHAT.__init__">AdaHAT</a>
                        </li>
                        <li>
                                <a class="variable" href="#AdaHAT.adjustment_intensity">adjustment_intensity</a>
                        </li>
                        <li>
                                <a class="variable" href="#AdaHAT.epsilon">epsilon</a>
                        </li>
                        <li>
                                <a class="variable" href="#AdaHAT.summative_mask_for_previous_tasks">summative_mask_for_previous_tasks</a>
                        </li>
                        <li>
                                <a class="variable" href="#AdaHAT.automatic_optimization">automatic_optimization</a>
                        </li>
                        <li>
                                <a class="function" href="#AdaHAT.sanity_check">sanity_check</a>
                        </li>
                        <li>
                                <a class="function" href="#AdaHAT.on_train_start">on_train_start</a>
                        </li>
                        <li>
                                <a class="function" href="#AdaHAT.clip_grad_by_adjustment">clip_grad_by_adjustment</a>
                        </li>
                        <li>
                                <a class="function" href="#AdaHAT.on_train_end">on_train_end</a>
                        </li>
                </ul>

            </li>
    </ul>



        <a class="attribution" title="pdoc: Python API documentation generator" href="https://pdoc.dev" target="_blank">
            built with <span class="visually-hidden">pdoc</span><img
                alt="pdoc logo"
                src="data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20role%3D%22img%22%20aria-label%3D%22pdoc%20logo%22%20width%3D%22300%22%20height%3D%22150%22%20viewBox%3D%22-1%200%2060%2030%22%3E%3Ctitle%3Epdoc%3C/title%3E%3Cpath%20d%3D%22M29.621%2021.293c-.011-.273-.214-.475-.511-.481a.5.5%200%200%200-.489.503l-.044%201.393c-.097.551-.695%201.215-1.566%201.704-.577.428-1.306.486-2.193.182-1.426-.617-2.467-1.654-3.304-2.487l-.173-.172a3.43%203.43%200%200%200-.365-.306.49.49%200%200%200-.286-.196c-1.718-1.06-4.931-1.47-7.353.191l-.219.15c-1.707%201.187-3.413%202.131-4.328%201.03-.02-.027-.49-.685-.141-1.763.233-.721.546-2.408.772-4.076.042-.09.067-.187.046-.288.166-1.347.277-2.625.241-3.351%201.378-1.008%202.271-2.586%202.271-4.362%200-.976-.272-1.935-.788-2.774-.057-.094-.122-.18-.184-.268.033-.167.052-.339.052-.516%200-1.477-1.202-2.679-2.679-2.679-.791%200-1.496.352-1.987.9a6.3%206.3%200%200%200-1.001.029c-.492-.564-1.207-.929-2.012-.929-1.477%200-2.679%201.202-2.679%202.679A2.65%202.65%200%200%200%20.97%206.554c-.383.747-.595%201.572-.595%202.41%200%202.311%201.507%204.29%203.635%205.107-.037.699-.147%202.27-.423%203.294l-.137.461c-.622%202.042-2.515%208.257%201.727%2010.643%201.614.908%203.06%201.248%204.317%201.248%202.665%200%204.492-1.524%205.322-2.401%201.476-1.559%202.886-1.854%206.491.82%201.877%201.393%203.514%201.753%204.861%201.068%202.223-1.713%202.811-3.867%203.399-6.374.077-.846.056-1.469.054-1.537zm-4.835%204.313c-.054.305-.156.586-.242.629-.034-.007-.131-.022-.307-.157-.145-.111-.314-.478-.456-.908.221.121.432.25.675.355.115.039.219.051.33.081zm-2.251-1.238c-.05.33-.158.648-.252.694-.022.001-.125-.018-.307-.157-.217-.166-.488-.906-.639-1.573.358.344.754.693%201.198%201.036zm-3.887-2.337c-.006-.116-.018-.231-.041-.342.635.145%201.189.368%201.599.625.097.231.166.481.174.642-.03.049-.055.101-.067.158-.046.013-.128.026-.298.004-.278-.037-.901-.57-1.367-1.087zm-1.127-.497c.116.306.176.625.12.71-.019.014-.117.045-.345.016-.206-.027-.604-.332-.986-.695.41-.051.816-.056%201.211-.031zm-4.535%201.535c.209.22.379.47.358.598-.006.041-.088.138-.351.234-.144.055-.539-.063-.979-.259a11.66%2011.66%200%200%200%20.972-.573zm.983-.664c.359-.237.738-.418%201.126-.554.25.237.479.548.457.694-.006.042-.087.138-.351.235-.174.064-.694-.105-1.232-.375zm-3.381%201.794c-.022.145-.061.29-.149.401-.133.166-.358.248-.69.251h-.002c-.133%200-.306-.26-.45-.621.417.091.854.07%201.291-.031zm-2.066-8.077a4.78%204.78%200%200%201-.775-.584c.172-.115.505-.254.88-.378l-.105.962zm-.331%202.302a10.32%2010.32%200%200%201-.828-.502c.202-.143.576-.328.984-.49l-.156.992zm-.45%202.157l-.701-.403c.214-.115.536-.249.891-.376a11.57%2011.57%200%200%201-.19.779zm-.181%201.716c.064.398.194.702.298.893-.194-.051-.435-.162-.736-.398.061-.119.224-.3.438-.495zM8.87%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zm-.735-.389a1.15%201.15%200%200%200-.314.783%201.16%201.16%200%200%200%201.162%201.162c.457%200%20.842-.27%201.032-.653.026.117.042.238.042.362a1.68%201.68%200%200%201-1.679%201.679%201.68%201.68%200%200%201-1.679-1.679c0-.843.626-1.535%201.436-1.654zM5.059%205.406A1.68%201.68%200%200%201%203.38%207.085a1.68%201.68%200%200%201-1.679-1.679c0-.037.009-.072.011-.109.21.3.541.508.935.508a1.16%201.16%200%200%200%201.162-1.162%201.14%201.14%200%200%200-.474-.912c.015%200%20.03-.005.045-.005.926.001%201.679.754%201.679%201.68zM3.198%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zM1.375%208.964c0-.52.103-1.035.288-1.52.466.394%201.06.64%201.717.64%201.144%200%202.116-.725%202.499-1.738.383%201.012%201.355%201.738%202.499%201.738.867%200%201.631-.421%202.121-1.062.307.605.478%201.267.478%201.942%200%202.486-2.153%204.51-4.801%204.51s-4.801-2.023-4.801-4.51zm24.342%2019.349c-.985.498-2.267.168-3.813-.979-3.073-2.281-5.453-3.199-7.813-.705-1.315%201.391-4.163%203.365-8.423.97-3.174-1.786-2.239-6.266-1.261-9.479l.146-.492c.276-1.02.395-2.457.444-3.268a6.11%206.11%200%200%200%201.18.115%206.01%206.01%200%200%200%202.536-.562l-.006.175c-.802.215-1.848.612-2.021%201.25-.079.295.021.601.274.837.219.203.415.364.598.501-.667.304-1.243.698-1.311%201.179-.02.144-.022.507.393.787.213.144.395.26.564.365-1.285.521-1.361.96-1.381%201.126-.018.142-.011.496.427.746l.854.489c-.473.389-.971.914-.999%201.429-.018.278.095.532.316.713.675.556%201.231.721%201.653.721.059%200%20.104-.014.158-.02.207.707.641%201.64%201.513%201.64h.013c.8-.008%201.236-.345%201.462-.626.173-.216.268-.457.325-.692.424.195.93.374%201.372.374.151%200%20.294-.021.423-.068.732-.27.944-.704.993-1.021.009-.061.003-.119.002-.179.266.086.538.147.789.147.15%200%20.294-.021.423-.069.542-.2.797-.489.914-.754.237.147.478.258.704.288.106.014.205.021.296.021.356%200%20.595-.101.767-.229.438.435%201.094.992%201.656%201.067.106.014.205.021.296.021a1.56%201.56%200%200%200%20.323-.035c.17.575.453%201.289.866%201.605.358.273.665.362.914.362a.99.99%200%200%200%20.421-.093%201.03%201.03%200%200%200%20.245-.164c.168.428.39.846.68%201.068.358.273.665.362.913.362a.99.99%200%200%200%20.421-.093c.317-.148.512-.448.639-.762.251.157.495.257.726.257.127%200%20.25-.024.37-.071.427-.17.706-.617.841-1.314.022-.015.047-.022.068-.038.067-.051.133-.104.196-.159-.443%201.486-1.107%202.761-2.086%203.257zM8.66%209.925a.5.5%200%201%200-1%200c0%20.653-.818%201.205-1.787%201.205s-1.787-.552-1.787-1.205a.5.5%200%201%200-1%200c0%201.216%201.25%202.205%202.787%202.205s2.787-.989%202.787-2.205zm4.4%2015.965l-.208.097c-2.661%201.258-4.708%201.436-6.086.527-1.542-1.017-1.88-3.19-1.844-4.198a.4.4%200%200%200-.385-.414c-.242-.029-.406.164-.414.385-.046%201.249.367%203.686%202.202%204.896.708.467%201.547.7%202.51.7%201.248%200%202.706-.392%204.362-1.174l.185-.086a.4.4%200%200%200%20.205-.527c-.089-.204-.326-.291-.527-.206zM9.547%202.292c.093.077.205.114.317.114a.5.5%200%200%200%20.318-.886L8.817.397a.5.5%200%200%200-.703.068.5.5%200%200%200%20.069.703l1.364%201.124zm-7.661-.065c.086%200%20.173-.022.253-.068l1.523-.893a.5.5%200%200%200-.506-.863l-1.523.892a.5.5%200%200%200-.179.685c.094.158.261.247.432.247z%22%20transform%3D%22matrix%28-1%200%200%201%2058%200%29%22%20fill%3D%22%233bb300%22/%3E%3Cpath%20d%3D%22M.3%2021.86V10.18q0-.46.02-.68.04-.22.18-.5.28-.54%201.34-.54%201.06%200%201.42.28.38.26.44.78.76-1.04%202.38-1.04%201.64%200%203.1%201.54%201.46%201.54%201.46%203.58%200%202.04-1.46%203.58-1.44%201.54-3.08%201.54-1.64%200-2.38-.92v4.04q0%20.46-.04.68-.02.22-.18.5-.14.3-.5.42-.36.12-.98.12-.62%200-1-.12-.36-.12-.52-.4-.14-.28-.18-.5-.02-.22-.02-.68zm3.96-9.42q-.46.54-.46%201.18%200%20.64.46%201.18.48.52%201.2.52.74%200%201.24-.52.52-.52.52-1.18%200-.66-.48-1.18-.48-.54-1.26-.54-.76%200-1.22.54zm14.741-8.36q.16-.3.54-.42.38-.12%201-.12.64%200%201.02.12.38.12.52.42.16.3.18.54.04.22.04.68v11.94q0%20.46-.04.7-.02.22-.18.5-.3.54-1.7.54-1.38%200-1.54-.98-.84.96-2.34.96-1.8%200-3.28-1.56-1.48-1.58-1.48-3.66%200-2.1%201.48-3.68%201.5-1.58%203.28-1.58%201.48%200%202.3%201v-4.2q0-.46.02-.68.04-.24.18-.52zm-3.24%2010.86q.52.54%201.26.54.74%200%201.22-.54.5-.54.5-1.18%200-.66-.48-1.22-.46-.56-1.26-.56-.8%200-1.28.56-.48.54-.48%201.2%200%20.66.52%201.2zm7.833-1.2q0-2.4%201.68-3.96%201.68-1.56%203.84-1.56%202.16%200%203.82%201.56%201.66%201.54%201.66%203.94%200%201.66-.86%202.96-.86%201.28-2.1%201.9-1.22.6-2.54.6-1.32%200-2.56-.64-1.24-.66-2.1-1.92-.84-1.28-.84-2.88zm4.18%201.44q.64.48%201.3.48.66%200%201.32-.5.66-.5.66-1.48%200-.98-.62-1.46-.62-.48-1.34-.48-.72%200-1.34.5-.62.5-.62%201.48%200%20.96.64%201.46zm11.412-1.44q0%20.84.56%201.32.56.46%201.18.46.64%200%201.18-.36.56-.38.9-.38.6%200%201.46%201.06.46.58.46%201.04%200%20.76-1.1%201.42-1.14.8-2.8.8-1.86%200-3.58-1.34-.82-.64-1.34-1.7-.52-1.08-.52-2.36%200-1.3.52-2.34.52-1.06%201.34-1.7%201.66-1.32%203.54-1.32.76%200%201.48.22.72.2%201.06.4l.32.2q.36.24.56.38.52.4.52.92%200%20.5-.42%201.14-.72%201.1-1.38%201.1-.38%200-1.08-.44-.36-.34-1.04-.34-.66%200-1.24.48-.58.48-.58%201.34z%22%20fill%3D%22green%22/%3E%3C/svg%3E"/>
        </a>
</div>
    </nav>
    <main class="pdoc">
            <section class="module-info">
                    <h1 class="modulename">
<a href="./../../clarena.html">clarena</a><wbr>.<a href="./../cl_algorithms.html">cl_algorithms</a><wbr>.adahat    </h1>

                        <div class="docstring"><p>The submodule in <code>cl_algorithms</code> for <a href="https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9">AdaHAT (Adaptive Hard Attention to the Task) algorithm</a>.</p>
</div>

                        <input id="mod-adahat-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">

                        <label class="view-source-button" for="mod-adahat-view-source"><span>View Source</span></label>

                        <div class="pdoc-code codehilite"><pre><span></span><span id="L-1"><a href="#L-1"><span class="linenos">  1</span></a><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-2"><a href="#L-2"><span class="linenos">  2</span></a><span class="sd">The submodule in `cl_algorithms` for [AdaHAT (Adaptive Hard Attention to the Task) algorithm](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="L-3"><a href="#L-3"><span class="linenos">  3</span></a><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-4"><a href="#L-4"><span class="linenos">  4</span></a>
</span><span id="L-5"><a href="#L-5"><span class="linenos">  5</span></a><span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;AdaHAT&quot;</span><span class="p">]</span>
</span><span id="L-6"><a href="#L-6"><span class="linenos">  6</span></a>
</span><span id="L-7"><a href="#L-7"><span class="linenos">  7</span></a><span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
</span><span id="L-8"><a href="#L-8"><span class="linenos">  8</span></a>
</span><span id="L-9"><a href="#L-9"><span class="linenos">  9</span></a><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span id="L-10"><a href="#L-10"><span class="linenos"> 10</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tensor</span>
</span><span id="L-11"><a href="#L-11"><span class="linenos"> 11</span></a>
</span><span id="L-12"><a href="#L-12"><span class="linenos"> 12</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">clarena.backbones</span><span class="w"> </span><span class="kn">import</span> <span class="n">HATMaskBackbone</span>
</span><span id="L-13"><a href="#L-13"><span class="linenos"> 13</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">clarena.cl_algorithms</span><span class="w"> </span><span class="kn">import</span> <span class="n">HAT</span>
</span><span id="L-14"><a href="#L-14"><span class="linenos"> 14</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">clarena.cl_heads</span><span class="w"> </span><span class="kn">import</span> <span class="n">HeadsCIL</span><span class="p">,</span> <span class="n">HeadsTIL</span>
</span><span id="L-15"><a href="#L-15"><span class="linenos"> 15</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">clarena.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">HATNetworkCapacity</span>
</span><span id="L-16"><a href="#L-16"><span class="linenos"> 16</span></a>
</span><span id="L-17"><a href="#L-17"><span class="linenos"> 17</span></a><span class="c1"># always get logger for built-in logging in each module</span>
</span><span id="L-18"><a href="#L-18"><span class="linenos"> 18</span></a><span class="n">pylogger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>
</span><span id="L-19"><a href="#L-19"><span class="linenos"> 19</span></a>
</span><span id="L-20"><a href="#L-20"><span class="linenos"> 20</span></a>
</span><span id="L-21"><a href="#L-21"><span class="linenos"> 21</span></a><span class="k">class</span><span class="w"> </span><span class="nc">AdaHAT</span><span class="p">(</span><span class="n">HAT</span><span class="p">):</span>
</span><span id="L-22"><a href="#L-22"><span class="linenos"> 22</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;AdaHAT (Adaptive Hard Attention to the Task) algorithm.</span>
</span><span id="L-23"><a href="#L-23"><span class="linenos"> 23</span></a>
</span><span id="L-24"><a href="#L-24"><span class="linenos"> 24</span></a><span class="sd">    [Adaptive HAT (Adaptive Hard Attention to the Task, 2024)](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9) is an architecture-based continual learning approach that improves [HAT (Hard Attention to the Task, 2018)](http://proceedings.mlr.press/v80/serra18a) by introducing new adaptive soft gradient clipping based on parameter importance and network sparsity.</span>
</span><span id="L-25"><a href="#L-25"><span class="linenos"> 25</span></a>
</span><span id="L-26"><a href="#L-26"><span class="linenos"> 26</span></a><span class="sd">    We implement AdaHAT as a subclass of HAT algorithm, as AdaHAT has the same  `forward()`, `compensate_task_embedding_gradients()`, `training_step()`, `on_train_end()`,`validation_step()`, `test_step()` method as `HAT` class.</span>
</span><span id="L-27"><a href="#L-27"><span class="linenos"> 27</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-28"><a href="#L-28"><span class="linenos"> 28</span></a>
</span><span id="L-29"><a href="#L-29"><span class="linenos"> 29</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-30"><a href="#L-30"><span class="linenos"> 30</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-31"><a href="#L-31"><span class="linenos"> 31</span></a>        <span class="n">backbone</span><span class="p">:</span> <span class="n">HATMaskBackbone</span><span class="p">,</span>
</span><span id="L-32"><a href="#L-32"><span class="linenos"> 32</span></a>        <span class="n">heads</span><span class="p">:</span> <span class="n">HeadsTIL</span> <span class="o">|</span> <span class="n">HeadsCIL</span><span class="p">,</span>
</span><span id="L-33"><a href="#L-33"><span class="linenos"> 33</span></a>        <span class="n">adjustment_mode</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-34"><a href="#L-34"><span class="linenos"> 34</span></a>        <span class="n">adjustment_intensity</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="L-35"><a href="#L-35"><span class="linenos"> 35</span></a>        <span class="n">s_max</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="L-36"><a href="#L-36"><span class="linenos"> 36</span></a>        <span class="n">clamp_threshold</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="L-37"><a href="#L-37"><span class="linenos"> 37</span></a>        <span class="n">mask_sparsity_reg_factor</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="L-38"><a href="#L-38"><span class="linenos"> 38</span></a>        <span class="n">mask_sparsity_reg_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;original&quot;</span><span class="p">,</span>
</span><span id="L-39"><a href="#L-39"><span class="linenos"> 39</span></a>        <span class="n">task_embedding_init_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;N01&quot;</span><span class="p">,</span>
</span><span id="L-40"><a href="#L-40"><span class="linenos"> 40</span></a>        <span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
</span><span id="L-41"><a href="#L-41"><span class="linenos"> 41</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-42"><a href="#L-42"><span class="linenos"> 42</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialise the AdaHAT algorithm with the network.</span>
</span><span id="L-43"><a href="#L-43"><span class="linenos"> 43</span></a>
</span><span id="L-44"><a href="#L-44"><span class="linenos"> 44</span></a><span class="sd">        **Args:**</span>
</span><span id="L-45"><a href="#L-45"><span class="linenos"> 45</span></a><span class="sd">        - **backbone** (`HATMaskBackbone`): must be a backbone network with HAT mask mechanism.</span>
</span><span id="L-46"><a href="#L-46"><span class="linenos"> 46</span></a><span class="sd">        - **heads** (`HeadsTIL` | `HeadsCIL`): output heads.</span>
</span><span id="L-47"><a href="#L-47"><span class="linenos"> 47</span></a><span class="sd">        - **adjustment_mode** (`str`): the strategy of adjustment i.e. the mode of gradient clipping, should be one of the following:</span>
</span><span id="L-48"><a href="#L-48"><span class="linenos"> 48</span></a><span class="sd">            1. &#39;adahat&#39;: set the gradients of parameters linking to masked units to a soft adjustment rate in the original AdaHAT approach. This is the way that AdaHAT does, which allowes the part of network for previous tasks to be updated slightly. See equation (8) and (9) chapter 3.1 in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="L-49"><a href="#L-49"><span class="linenos"> 49</span></a><span class="sd">            2. &#39;adahat_no_sum&#39;: set the gradients of parameters linking to masked units to a soft adjustment rate in the original AdaHAT approach, but without considering the information of parameter importance i.e. summative mask. This is the way that one of the AdaHAT ablation study does. See chapter 4.3 in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="L-50"><a href="#L-50"><span class="linenos"> 50</span></a><span class="sd">            3. &#39;adahat_no_reg&#39;: set the gradients of parameters linking to masked units to a soft adjustment rate in the original AdaHAT approach, but without considering the information of network sparsity i.e. mask sparsity regularisation value. This is the way that one of the AdaHAT ablation study does. See chapter 4.3 in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="L-51"><a href="#L-51"><span class="linenos"> 51</span></a><span class="sd">        - **adjustment_intensity** (`float`): hyperparameter, control the overall intensity of gradient adjustment. It&#39;s the $\alpha$ in equation (9) in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="L-52"><a href="#L-52"><span class="linenos"> 52</span></a><span class="sd">        - **s_max** (`float`): hyperparameter, the maximum scaling factor in the gate function. See chapter 2.4 &quot;Hard Attention Training&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="L-53"><a href="#L-53"><span class="linenos"> 53</span></a><span class="sd">        - **clamp_threshold** (`float`): the threshold for task embedding gradient compensation. See chapter 2.5 &quot;Embedding Gradient Compensation&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="L-54"><a href="#L-54"><span class="linenos"> 54</span></a><span class="sd">        - **mask_sparsity_reg_factor** (`float`): hyperparameter, the regularisation factor for mask sparsity.</span>
</span><span id="L-55"><a href="#L-55"><span class="linenos"> 55</span></a><span class="sd">        - **mask_sparsity_reg_mode** (`str`): the mode of mask sparsity regularisation, should be one of the following:</span>
</span><span id="L-56"><a href="#L-56"><span class="linenos"> 56</span></a><span class="sd">            1. &#39;original&#39; (default): the original mask sparsity regularisation in HAT paper.</span>
</span><span id="L-57"><a href="#L-57"><span class="linenos"> 57</span></a><span class="sd">            2. &#39;cross&#39;: the cross version mask sparsity regularisation.</span>
</span><span id="L-58"><a href="#L-58"><span class="linenos"> 58</span></a><span class="sd">        - **task_embedding_init_mode** (`str`): the initialisation method for task embeddings, should be one of the following:</span>
</span><span id="L-59"><a href="#L-59"><span class="linenos"> 59</span></a><span class="sd">            1. &#39;N01&#39; (default): standard normal distribution $N(0, 1)$.</span>
</span><span id="L-60"><a href="#L-60"><span class="linenos"> 60</span></a><span class="sd">            2. &#39;U-11&#39;: uniform distribution $U(-1, 1)$.</span>
</span><span id="L-61"><a href="#L-61"><span class="linenos"> 61</span></a><span class="sd">            3. &#39;U01&#39;: uniform distribution $U(0, 1)$.</span>
</span><span id="L-62"><a href="#L-62"><span class="linenos"> 62</span></a><span class="sd">            4. &#39;U-10&#39;: uniform distribution $U(-1, 0)$.</span>
</span><span id="L-63"><a href="#L-63"><span class="linenos"> 63</span></a><span class="sd">            5. &#39;last&#39;: inherit task embedding from last task.</span>
</span><span id="L-64"><a href="#L-64"><span class="linenos"> 64</span></a><span class="sd">        - **epsilon** (`float`): the value added to network sparsity to avoid division by zero appeared in equation (9) in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="L-65"><a href="#L-65"><span class="linenos"> 65</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-66"><a href="#L-66"><span class="linenos"> 66</span></a>        <span class="n">HAT</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-67"><a href="#L-67"><span class="linenos"> 67</span></a>            <span class="bp">self</span><span class="p">,</span>
</span><span id="L-68"><a href="#L-68"><span class="linenos"> 68</span></a>            <span class="n">backbone</span><span class="o">=</span><span class="n">backbone</span><span class="p">,</span>
</span><span id="L-69"><a href="#L-69"><span class="linenos"> 69</span></a>            <span class="n">heads</span><span class="o">=</span><span class="n">heads</span><span class="p">,</span>
</span><span id="L-70"><a href="#L-70"><span class="linenos"> 70</span></a>            <span class="n">adjustment_mode</span><span class="o">=</span><span class="n">adjustment_mode</span><span class="p">,</span>
</span><span id="L-71"><a href="#L-71"><span class="linenos"> 71</span></a>            <span class="n">s_max</span><span class="o">=</span><span class="n">s_max</span><span class="p">,</span>
</span><span id="L-72"><a href="#L-72"><span class="linenos"> 72</span></a>            <span class="n">clamp_threshold</span><span class="o">=</span><span class="n">clamp_threshold</span><span class="p">,</span>
</span><span id="L-73"><a href="#L-73"><span class="linenos"> 73</span></a>            <span class="n">mask_sparsity_reg_factor</span><span class="o">=</span><span class="n">mask_sparsity_reg_factor</span><span class="p">,</span>
</span><span id="L-74"><a href="#L-74"><span class="linenos"> 74</span></a>            <span class="n">mask_sparsity_reg_mode</span><span class="o">=</span><span class="n">mask_sparsity_reg_mode</span><span class="p">,</span>
</span><span id="L-75"><a href="#L-75"><span class="linenos"> 75</span></a>            <span class="n">task_embedding_init_mode</span><span class="o">=</span><span class="n">task_embedding_init_mode</span><span class="p">,</span>
</span><span id="L-76"><a href="#L-76"><span class="linenos"> 76</span></a>            <span class="n">alpha</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="L-77"><a href="#L-77"><span class="linenos"> 77</span></a>        <span class="p">)</span>
</span><span id="L-78"><a href="#L-78"><span class="linenos"> 78</span></a>
</span><span id="L-79"><a href="#L-79"><span class="linenos"> 79</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span> <span class="o">=</span> <span class="n">adjustment_intensity</span>
</span><span id="L-80"><a href="#L-80"><span class="linenos"> 80</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the adjustment intensity in equation (9) in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).&quot;&quot;&quot;</span>
</span><span id="L-81"><a href="#L-81"><span class="linenos"> 81</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>
</span><span id="L-82"><a href="#L-82"><span class="linenos"> 82</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Store the small value to avoid division by zero appeared in equation (9) in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).&quot;&quot;&quot;</span>
</span><span id="L-83"><a href="#L-83"><span class="linenos"> 83</span></a>
</span><span id="L-84"><a href="#L-84"><span class="linenos"> 84</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">summative_mask_for_previous_tasks</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="L-85"><a href="#L-85"><span class="linenos"> 85</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the summative binary attention mask $\mathrm{M}^{&lt;t,\text{sum}}$ previous tasks $1,\cdots, t-1$, gated from the task embedding. Keys are task IDs and values are the corresponding summative mask. Each cumulative mask is a dict where keys are layer names and values are the binary mask tensor for the layer. The mask tensor has size (number of units). &quot;&quot;&quot;</span>
</span><span id="L-86"><a href="#L-86"><span class="linenos"> 86</span></a>
</span><span id="L-87"><a href="#L-87"><span class="linenos"> 87</span></a>        <span class="c1"># set manual optimisation</span>
</span><span id="L-88"><a href="#L-88"><span class="linenos"> 88</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">automatic_optimization</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="L-89"><a href="#L-89"><span class="linenos"> 89</span></a>
</span><span id="L-90"><a href="#L-90"><span class="linenos"> 90</span></a>        <span class="n">AdaHAT</span><span class="o">.</span><span class="n">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</span><span id="L-91"><a href="#L-91"><span class="linenos"> 91</span></a>
</span><span id="L-92"><a href="#L-92"><span class="linenos"> 92</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-93"><a href="#L-93"><span class="linenos"> 93</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Check the sanity of the arguments.</span>
</span><span id="L-94"><a href="#L-94"><span class="linenos"> 94</span></a>
</span><span id="L-95"><a href="#L-95"><span class="linenos"> 95</span></a><span class="sd">        **Raises:**</span>
</span><span id="L-96"><a href="#L-96"><span class="linenos"> 96</span></a><span class="sd">        - **ValueError**: If the `adjustment_intensity` is not positive.</span>
</span><span id="L-97"><a href="#L-97"><span class="linenos"> 97</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-98"><a href="#L-98"><span class="linenos"> 98</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="L-99"><a href="#L-99"><span class="linenos"> 99</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="L-100"><a href="#L-100"><span class="linenos">100</span></a>                <span class="sa">f</span><span class="s2">&quot;The adjustment intensity should be positive, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span><span class="si">}</span><span class="s2">.&quot;</span>
</span><span id="L-101"><a href="#L-101"><span class="linenos">101</span></a>            <span class="p">)</span>
</span><span id="L-102"><a href="#L-102"><span class="linenos">102</span></a>
</span><span id="L-103"><a href="#L-103"><span class="linenos">103</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_start</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-104"><a href="#L-104"><span class="linenos">104</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Additionally initialise the summative mask at the beginning of first task.&quot;&quot;&quot;</span>
</span><span id="L-105"><a href="#L-105"><span class="linenos">105</span></a>        <span class="n">HAT</span><span class="o">.</span><span class="n">on_train_start</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</span><span id="L-106"><a href="#L-106"><span class="linenos">106</span></a>
</span><span id="L-107"><a href="#L-107"><span class="linenos">107</span></a>        <span class="c1"># initialise the summative mask at the beginning of first task. This should not be called in `__init__()` method as the `self.device` is not available at that time.</span>
</span><span id="L-108"><a href="#L-108"><span class="linenos">108</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="L-109"><a href="#L-109"><span class="linenos">109</span></a>            <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="L-110"><a href="#L-110"><span class="linenos">110</span></a>                <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span>
</span><span id="L-111"><a href="#L-111"><span class="linenos">111</span></a>                    <span class="n">layer_name</span>
</span><span id="L-112"><a href="#L-112"><span class="linenos">112</span></a>                <span class="p">)</span>  <span class="c1"># get the layer by its name</span>
</span><span id="L-113"><a href="#L-113"><span class="linenos">113</span></a>                <span class="n">num_units</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="L-114"><a href="#L-114"><span class="linenos">114</span></a>
</span><span id="L-115"><a href="#L-115"><span class="linenos">115</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">summative_mask_for_previous_tasks</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
</span><span id="L-116"><a href="#L-116"><span class="linenos">116</span></a>                    <span class="n">num_units</span>
</span><span id="L-117"><a href="#L-117"><span class="linenos">117</span></a>                <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
</span><span id="L-118"><a href="#L-118"><span class="linenos">118</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
</span><span id="L-119"><a href="#L-119"><span class="linenos">119</span></a>                <span class="p">)</span>  <span class="c1"># the summative mask $\mathrm{M}^{&lt;t,\text{sum}}$ is initialised as zeros mask ($t = 1$). See equation (7) in chapter 3.1 in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="L-120"><a href="#L-120"><span class="linenos">120</span></a>
</span><span id="L-121"><a href="#L-121"><span class="linenos">121</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">clip_grad_by_adjustment</span><span class="p">(</span>
</span><span id="L-122"><a href="#L-122"><span class="linenos">122</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-123"><a href="#L-123"><span class="linenos">123</span></a>        <span class="n">network_sparsity</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-124"><a href="#L-124"><span class="linenos">124</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-125"><a href="#L-125"><span class="linenos">125</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Clip the gradients by the adjustment rate.</span>
</span><span id="L-126"><a href="#L-126"><span class="linenos">126</span></a>
</span><span id="L-127"><a href="#L-127"><span class="linenos">127</span></a><span class="sd">        Note that as the task embedding fully covers every layer in the backbone network, no parameters are left out of this system. This applies not only the parameters in between layers with task embedding, but also those before the first layer. We designed it seperately in the codes.</span>
</span><span id="L-128"><a href="#L-128"><span class="linenos">128</span></a>
</span><span id="L-129"><a href="#L-129"><span class="linenos">129</span></a><span class="sd">        Network capacity is measured along with this method. Network capacity is defined as the average adjustment rate over all parameters. See chapter 4.1 in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="L-130"><a href="#L-130"><span class="linenos">130</span></a>
</span><span id="L-131"><a href="#L-131"><span class="linenos">131</span></a><span class="sd">        **Args:**</span>
</span><span id="L-132"><a href="#L-132"><span class="linenos">132</span></a><span class="sd">        - **network_sparsity** (`dict[str, Tensor]` | `None`): The network sparsity i.e. the mask sparsity loss of each layer for the current task. It applies only to AdaHAT modes, as it is used to calculate the adjustment rate for the gradients.</span>
</span><span id="L-133"><a href="#L-133"><span class="linenos">133</span></a>
</span><span id="L-134"><a href="#L-134"><span class="linenos">134</span></a><span class="sd">        **Returns:**</span>
</span><span id="L-135"><a href="#L-135"><span class="linenos">135</span></a><span class="sd">        - **capacity** (`Tensor`): the calculated network capacity.</span>
</span><span id="L-136"><a href="#L-136"><span class="linenos">136</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-137"><a href="#L-137"><span class="linenos">137</span></a>
</span><span id="L-138"><a href="#L-138"><span class="linenos">138</span></a>        <span class="c1"># initialise network capacity metric</span>
</span><span id="L-139"><a href="#L-139"><span class="linenos">139</span></a>        <span class="n">capacity</span> <span class="o">=</span> <span class="n">HATNetworkCapacity</span><span class="p">()</span>
</span><span id="L-140"><a href="#L-140"><span class="linenos">140</span></a>
</span><span id="L-141"><a href="#L-141"><span class="linenos">141</span></a>        <span class="c1"># Calculate the adjustment rate for gradients of the parameters, both weights and biases (if exists)</span>
</span><span id="L-142"><a href="#L-142"><span class="linenos">142</span></a>        <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="L-143"><a href="#L-143"><span class="linenos">143</span></a>
</span><span id="L-144"><a href="#L-144"><span class="linenos">144</span></a>            <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span>
</span><span id="L-145"><a href="#L-145"><span class="linenos">145</span></a>                <span class="n">layer_name</span>
</span><span id="L-146"><a href="#L-146"><span class="linenos">146</span></a>            <span class="p">)</span>  <span class="c1"># get the layer by its name</span>
</span><span id="L-147"><a href="#L-147"><span class="linenos">147</span></a>
</span><span id="L-148"><a href="#L-148"><span class="linenos">148</span></a>            <span class="c1"># placeholder for the adjustment rate to avoid the error of using it before assignment</span>
</span><span id="L-149"><a href="#L-149"><span class="linenos">149</span></a>            <span class="n">adjustment_rate_weight</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="L-150"><a href="#L-150"><span class="linenos">150</span></a>            <span class="n">adjustment_rate_bias</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="L-151"><a href="#L-151"><span class="linenos">151</span></a>
</span><span id="L-152"><a href="#L-152"><span class="linenos">152</span></a>            <span class="n">weight_importance</span><span class="p">,</span> <span class="n">bias_importance</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-153"><a href="#L-153"><span class="linenos">153</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_measure_parameter_wise</span><span class="p">(</span>
</span><span id="L-154"><a href="#L-154"><span class="linenos">154</span></a>                    <span class="n">unit_wise_measure</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">summative_mask_for_previous_tasks</span><span class="p">,</span>
</span><span id="L-155"><a href="#L-155"><span class="linenos">155</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="L-156"><a href="#L-156"><span class="linenos">156</span></a>                    <span class="n">aggregation</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span>
</span><span id="L-157"><a href="#L-157"><span class="linenos">157</span></a>                <span class="p">)</span>
</span><span id="L-158"><a href="#L-158"><span class="linenos">158</span></a>            <span class="p">)</span>  <span class="c1"># AdaHAT depend on parameter importance instead of parameter mask like HAT</span>
</span><span id="L-159"><a href="#L-159"><span class="linenos">159</span></a>
</span><span id="L-160"><a href="#L-160"><span class="linenos">160</span></a>            <span class="n">network_sparsity_layer</span> <span class="o">=</span> <span class="n">network_sparsity</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="L-161"><a href="#L-161"><span class="linenos">161</span></a>
</span><span id="L-162"><a href="#L-162"><span class="linenos">162</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;adahat&quot;</span><span class="p">:</span>
</span><span id="L-163"><a href="#L-163"><span class="linenos">163</span></a>                <span class="n">r_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span> <span class="o">/</span> <span class="p">(</span>
</span><span id="L-164"><a href="#L-164"><span class="linenos">164</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">+</span> <span class="n">network_sparsity_layer</span>
</span><span id="L-165"><a href="#L-165"><span class="linenos">165</span></a>                <span class="p">)</span>
</span><span id="L-166"><a href="#L-166"><span class="linenos">166</span></a>                <span class="n">adjustment_rate_weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span>
</span><span id="L-167"><a href="#L-167"><span class="linenos">167</span></a>                    <span class="n">r_layer</span><span class="p">,</span> <span class="p">(</span><span class="n">weight_importance</span> <span class="o">+</span> <span class="n">r_layer</span><span class="p">)</span>
</span><span id="L-168"><a href="#L-168"><span class="linenos">168</span></a>                <span class="p">)</span>
</span><span id="L-169"><a href="#L-169"><span class="linenos">169</span></a>                <span class="n">adjustment_rate_bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">r_layer</span><span class="p">,</span> <span class="p">(</span><span class="n">bias_importance</span> <span class="o">+</span> <span class="n">r_layer</span><span class="p">))</span>
</span><span id="L-170"><a href="#L-170"><span class="linenos">170</span></a>
</span><span id="L-171"><a href="#L-171"><span class="linenos">171</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;adahat_no_sum&quot;</span><span class="p">:</span>
</span><span id="L-172"><a href="#L-172"><span class="linenos">172</span></a>
</span><span id="L-173"><a href="#L-173"><span class="linenos">173</span></a>                <span class="n">r_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span> <span class="o">/</span> <span class="p">(</span>
</span><span id="L-174"><a href="#L-174"><span class="linenos">174</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">+</span> <span class="n">network_sparsity_layer</span>
</span><span id="L-175"><a href="#L-175"><span class="linenos">175</span></a>                <span class="p">)</span>
</span><span id="L-176"><a href="#L-176"><span class="linenos">176</span></a>                <span class="n">adjustment_rate_weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">r_layer</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">+</span> <span class="n">r_layer</span><span class="p">))</span>
</span><span id="L-177"><a href="#L-177"><span class="linenos">177</span></a>                <span class="n">adjustment_rate_bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">r_layer</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">+</span> <span class="n">r_layer</span><span class="p">))</span>
</span><span id="L-178"><a href="#L-178"><span class="linenos">178</span></a>
</span><span id="L-179"><a href="#L-179"><span class="linenos">179</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;adahat_no_reg&quot;</span><span class="p">:</span>
</span><span id="L-180"><a href="#L-180"><span class="linenos">180</span></a>
</span><span id="L-181"><a href="#L-181"><span class="linenos">181</span></a>                <span class="n">r_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">+</span> <span class="mf">0.0</span><span class="p">)</span>
</span><span id="L-182"><a href="#L-182"><span class="linenos">182</span></a>                <span class="n">adjustment_rate_weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span>
</span><span id="L-183"><a href="#L-183"><span class="linenos">183</span></a>                    <span class="n">r_layer</span><span class="p">,</span> <span class="p">(</span><span class="n">weight_importance</span> <span class="o">+</span> <span class="n">r_layer</span><span class="p">)</span>
</span><span id="L-184"><a href="#L-184"><span class="linenos">184</span></a>                <span class="p">)</span>
</span><span id="L-185"><a href="#L-185"><span class="linenos">185</span></a>                <span class="n">adjustment_rate_bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">r_layer</span><span class="p">,</span> <span class="p">(</span><span class="n">bias_importance</span> <span class="o">+</span> <span class="n">r_layer</span><span class="p">))</span>
</span><span id="L-186"><a href="#L-186"><span class="linenos">186</span></a>
</span><span id="L-187"><a href="#L-187"><span class="linenos">187</span></a>            <span class="c1"># apply the adjustment rate to the gradients</span>
</span><span id="L-188"><a href="#L-188"><span class="linenos">188</span></a>            <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">adjustment_rate_weight</span>
</span><span id="L-189"><a href="#L-189"><span class="linenos">189</span></a>            <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-190"><a href="#L-190"><span class="linenos">190</span></a>                <span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">adjustment_rate_bias</span>
</span><span id="L-191"><a href="#L-191"><span class="linenos">191</span></a>
</span><span id="L-192"><a href="#L-192"><span class="linenos">192</span></a>            <span class="c1"># update network capacity metric</span>
</span><span id="L-193"><a href="#L-193"><span class="linenos">193</span></a>            <span class="n">capacity</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">adjustment_rate_weight</span><span class="p">,</span> <span class="n">adjustment_rate_bias</span><span class="p">)</span>
</span><span id="L-194"><a href="#L-194"><span class="linenos">194</span></a>
</span><span id="L-195"><a href="#L-195"><span class="linenos">195</span></a>        <span class="k">return</span> <span class="n">capacity</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
</span><span id="L-196"><a href="#L-196"><span class="linenos">196</span></a>
</span><span id="L-197"><a href="#L-197"><span class="linenos">197</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_end</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-198"><a href="#L-198"><span class="linenos">198</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Additionally update summative mask after training the task.&quot;&quot;&quot;</span>
</span><span id="L-199"><a href="#L-199"><span class="linenos">199</span></a>
</span><span id="L-200"><a href="#L-200"><span class="linenos">200</span></a>        <span class="n">HAT</span><span class="o">.</span><span class="n">on_train_end</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</span><span id="L-201"><a href="#L-201"><span class="linenos">201</span></a>
</span><span id="L-202"><a href="#L-202"><span class="linenos">202</span></a>        <span class="n">mask_t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">masks</span><span class="p">[</span>
</span><span id="L-203"><a href="#L-203"><span class="linenos">203</span></a>            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="L-204"><a href="#L-204"><span class="linenos">204</span></a>        <span class="p">]</span>  <span class="c1"># get stored mask for the current task again</span>
</span><span id="L-205"><a href="#L-205"><span class="linenos">205</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">summative_mask_for_previous_tasks</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="L-206"><a href="#L-206"><span class="linenos">206</span></a>            <span class="n">layer_name</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">summative_mask_for_previous_tasks</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="L-207"><a href="#L-207"><span class="linenos">207</span></a>            <span class="o">+</span> <span class="n">mask_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="L-208"><a href="#L-208"><span class="linenos">208</span></a>            <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span>
</span><span id="L-209"><a href="#L-209"><span class="linenos">209</span></a>        <span class="p">}</span>
</span></pre></div>


            </section>
                <section id="AdaHAT">
                            <input id="AdaHAT-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">AdaHAT</span><wbr>(<span class="base"><a href="hat.html#HAT">clarena.cl_algorithms.hat.HAT</a></span>):

                <label class="view-source-button" for="AdaHAT-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#AdaHAT"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="AdaHAT-22"><a href="#AdaHAT-22"><span class="linenos"> 22</span></a><span class="k">class</span><span class="w"> </span><span class="nc">AdaHAT</span><span class="p">(</span><span class="n">HAT</span><span class="p">):</span>
</span><span id="AdaHAT-23"><a href="#AdaHAT-23"><span class="linenos"> 23</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;AdaHAT (Adaptive Hard Attention to the Task) algorithm.</span>
</span><span id="AdaHAT-24"><a href="#AdaHAT-24"><span class="linenos"> 24</span></a>
</span><span id="AdaHAT-25"><a href="#AdaHAT-25"><span class="linenos"> 25</span></a><span class="sd">    [Adaptive HAT (Adaptive Hard Attention to the Task, 2024)](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9) is an architecture-based continual learning approach that improves [HAT (Hard Attention to the Task, 2018)](http://proceedings.mlr.press/v80/serra18a) by introducing new adaptive soft gradient clipping based on parameter importance and network sparsity.</span>
</span><span id="AdaHAT-26"><a href="#AdaHAT-26"><span class="linenos"> 26</span></a>
</span><span id="AdaHAT-27"><a href="#AdaHAT-27"><span class="linenos"> 27</span></a><span class="sd">    We implement AdaHAT as a subclass of HAT algorithm, as AdaHAT has the same  `forward()`, `compensate_task_embedding_gradients()`, `training_step()`, `on_train_end()`,`validation_step()`, `test_step()` method as `HAT` class.</span>
</span><span id="AdaHAT-28"><a href="#AdaHAT-28"><span class="linenos"> 28</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="AdaHAT-29"><a href="#AdaHAT-29"><span class="linenos"> 29</span></a>
</span><span id="AdaHAT-30"><a href="#AdaHAT-30"><span class="linenos"> 30</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="AdaHAT-31"><a href="#AdaHAT-31"><span class="linenos"> 31</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="AdaHAT-32"><a href="#AdaHAT-32"><span class="linenos"> 32</span></a>        <span class="n">backbone</span><span class="p">:</span> <span class="n">HATMaskBackbone</span><span class="p">,</span>
</span><span id="AdaHAT-33"><a href="#AdaHAT-33"><span class="linenos"> 33</span></a>        <span class="n">heads</span><span class="p">:</span> <span class="n">HeadsTIL</span> <span class="o">|</span> <span class="n">HeadsCIL</span><span class="p">,</span>
</span><span id="AdaHAT-34"><a href="#AdaHAT-34"><span class="linenos"> 34</span></a>        <span class="n">adjustment_mode</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="AdaHAT-35"><a href="#AdaHAT-35"><span class="linenos"> 35</span></a>        <span class="n">adjustment_intensity</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="AdaHAT-36"><a href="#AdaHAT-36"><span class="linenos"> 36</span></a>        <span class="n">s_max</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="AdaHAT-37"><a href="#AdaHAT-37"><span class="linenos"> 37</span></a>        <span class="n">clamp_threshold</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="AdaHAT-38"><a href="#AdaHAT-38"><span class="linenos"> 38</span></a>        <span class="n">mask_sparsity_reg_factor</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="AdaHAT-39"><a href="#AdaHAT-39"><span class="linenos"> 39</span></a>        <span class="n">mask_sparsity_reg_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;original&quot;</span><span class="p">,</span>
</span><span id="AdaHAT-40"><a href="#AdaHAT-40"><span class="linenos"> 40</span></a>        <span class="n">task_embedding_init_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;N01&quot;</span><span class="p">,</span>
</span><span id="AdaHAT-41"><a href="#AdaHAT-41"><span class="linenos"> 41</span></a>        <span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
</span><span id="AdaHAT-42"><a href="#AdaHAT-42"><span class="linenos"> 42</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="AdaHAT-43"><a href="#AdaHAT-43"><span class="linenos"> 43</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialise the AdaHAT algorithm with the network.</span>
</span><span id="AdaHAT-44"><a href="#AdaHAT-44"><span class="linenos"> 44</span></a>
</span><span id="AdaHAT-45"><a href="#AdaHAT-45"><span class="linenos"> 45</span></a><span class="sd">        **Args:**</span>
</span><span id="AdaHAT-46"><a href="#AdaHAT-46"><span class="linenos"> 46</span></a><span class="sd">        - **backbone** (`HATMaskBackbone`): must be a backbone network with HAT mask mechanism.</span>
</span><span id="AdaHAT-47"><a href="#AdaHAT-47"><span class="linenos"> 47</span></a><span class="sd">        - **heads** (`HeadsTIL` | `HeadsCIL`): output heads.</span>
</span><span id="AdaHAT-48"><a href="#AdaHAT-48"><span class="linenos"> 48</span></a><span class="sd">        - **adjustment_mode** (`str`): the strategy of adjustment i.e. the mode of gradient clipping, should be one of the following:</span>
</span><span id="AdaHAT-49"><a href="#AdaHAT-49"><span class="linenos"> 49</span></a><span class="sd">            1. &#39;adahat&#39;: set the gradients of parameters linking to masked units to a soft adjustment rate in the original AdaHAT approach. This is the way that AdaHAT does, which allowes the part of network for previous tasks to be updated slightly. See equation (8) and (9) chapter 3.1 in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="AdaHAT-50"><a href="#AdaHAT-50"><span class="linenos"> 50</span></a><span class="sd">            2. &#39;adahat_no_sum&#39;: set the gradients of parameters linking to masked units to a soft adjustment rate in the original AdaHAT approach, but without considering the information of parameter importance i.e. summative mask. This is the way that one of the AdaHAT ablation study does. See chapter 4.3 in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="AdaHAT-51"><a href="#AdaHAT-51"><span class="linenos"> 51</span></a><span class="sd">            3. &#39;adahat_no_reg&#39;: set the gradients of parameters linking to masked units to a soft adjustment rate in the original AdaHAT approach, but without considering the information of network sparsity i.e. mask sparsity regularisation value. This is the way that one of the AdaHAT ablation study does. See chapter 4.3 in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="AdaHAT-52"><a href="#AdaHAT-52"><span class="linenos"> 52</span></a><span class="sd">        - **adjustment_intensity** (`float`): hyperparameter, control the overall intensity of gradient adjustment. It&#39;s the $\alpha$ in equation (9) in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="AdaHAT-53"><a href="#AdaHAT-53"><span class="linenos"> 53</span></a><span class="sd">        - **s_max** (`float`): hyperparameter, the maximum scaling factor in the gate function. See chapter 2.4 &quot;Hard Attention Training&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="AdaHAT-54"><a href="#AdaHAT-54"><span class="linenos"> 54</span></a><span class="sd">        - **clamp_threshold** (`float`): the threshold for task embedding gradient compensation. See chapter 2.5 &quot;Embedding Gradient Compensation&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="AdaHAT-55"><a href="#AdaHAT-55"><span class="linenos"> 55</span></a><span class="sd">        - **mask_sparsity_reg_factor** (`float`): hyperparameter, the regularisation factor for mask sparsity.</span>
</span><span id="AdaHAT-56"><a href="#AdaHAT-56"><span class="linenos"> 56</span></a><span class="sd">        - **mask_sparsity_reg_mode** (`str`): the mode of mask sparsity regularisation, should be one of the following:</span>
</span><span id="AdaHAT-57"><a href="#AdaHAT-57"><span class="linenos"> 57</span></a><span class="sd">            1. &#39;original&#39; (default): the original mask sparsity regularisation in HAT paper.</span>
</span><span id="AdaHAT-58"><a href="#AdaHAT-58"><span class="linenos"> 58</span></a><span class="sd">            2. &#39;cross&#39;: the cross version mask sparsity regularisation.</span>
</span><span id="AdaHAT-59"><a href="#AdaHAT-59"><span class="linenos"> 59</span></a><span class="sd">        - **task_embedding_init_mode** (`str`): the initialisation method for task embeddings, should be one of the following:</span>
</span><span id="AdaHAT-60"><a href="#AdaHAT-60"><span class="linenos"> 60</span></a><span class="sd">            1. &#39;N01&#39; (default): standard normal distribution $N(0, 1)$.</span>
</span><span id="AdaHAT-61"><a href="#AdaHAT-61"><span class="linenos"> 61</span></a><span class="sd">            2. &#39;U-11&#39;: uniform distribution $U(-1, 1)$.</span>
</span><span id="AdaHAT-62"><a href="#AdaHAT-62"><span class="linenos"> 62</span></a><span class="sd">            3. &#39;U01&#39;: uniform distribution $U(0, 1)$.</span>
</span><span id="AdaHAT-63"><a href="#AdaHAT-63"><span class="linenos"> 63</span></a><span class="sd">            4. &#39;U-10&#39;: uniform distribution $U(-1, 0)$.</span>
</span><span id="AdaHAT-64"><a href="#AdaHAT-64"><span class="linenos"> 64</span></a><span class="sd">            5. &#39;last&#39;: inherit task embedding from last task.</span>
</span><span id="AdaHAT-65"><a href="#AdaHAT-65"><span class="linenos"> 65</span></a><span class="sd">        - **epsilon** (`float`): the value added to network sparsity to avoid division by zero appeared in equation (9) in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="AdaHAT-66"><a href="#AdaHAT-66"><span class="linenos"> 66</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="AdaHAT-67"><a href="#AdaHAT-67"><span class="linenos"> 67</span></a>        <span class="n">HAT</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="AdaHAT-68"><a href="#AdaHAT-68"><span class="linenos"> 68</span></a>            <span class="bp">self</span><span class="p">,</span>
</span><span id="AdaHAT-69"><a href="#AdaHAT-69"><span class="linenos"> 69</span></a>            <span class="n">backbone</span><span class="o">=</span><span class="n">backbone</span><span class="p">,</span>
</span><span id="AdaHAT-70"><a href="#AdaHAT-70"><span class="linenos"> 70</span></a>            <span class="n">heads</span><span class="o">=</span><span class="n">heads</span><span class="p">,</span>
</span><span id="AdaHAT-71"><a href="#AdaHAT-71"><span class="linenos"> 71</span></a>            <span class="n">adjustment_mode</span><span class="o">=</span><span class="n">adjustment_mode</span><span class="p">,</span>
</span><span id="AdaHAT-72"><a href="#AdaHAT-72"><span class="linenos"> 72</span></a>            <span class="n">s_max</span><span class="o">=</span><span class="n">s_max</span><span class="p">,</span>
</span><span id="AdaHAT-73"><a href="#AdaHAT-73"><span class="linenos"> 73</span></a>            <span class="n">clamp_threshold</span><span class="o">=</span><span class="n">clamp_threshold</span><span class="p">,</span>
</span><span id="AdaHAT-74"><a href="#AdaHAT-74"><span class="linenos"> 74</span></a>            <span class="n">mask_sparsity_reg_factor</span><span class="o">=</span><span class="n">mask_sparsity_reg_factor</span><span class="p">,</span>
</span><span id="AdaHAT-75"><a href="#AdaHAT-75"><span class="linenos"> 75</span></a>            <span class="n">mask_sparsity_reg_mode</span><span class="o">=</span><span class="n">mask_sparsity_reg_mode</span><span class="p">,</span>
</span><span id="AdaHAT-76"><a href="#AdaHAT-76"><span class="linenos"> 76</span></a>            <span class="n">task_embedding_init_mode</span><span class="o">=</span><span class="n">task_embedding_init_mode</span><span class="p">,</span>
</span><span id="AdaHAT-77"><a href="#AdaHAT-77"><span class="linenos"> 77</span></a>            <span class="n">alpha</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="AdaHAT-78"><a href="#AdaHAT-78"><span class="linenos"> 78</span></a>        <span class="p">)</span>
</span><span id="AdaHAT-79"><a href="#AdaHAT-79"><span class="linenos"> 79</span></a>
</span><span id="AdaHAT-80"><a href="#AdaHAT-80"><span class="linenos"> 80</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span> <span class="o">=</span> <span class="n">adjustment_intensity</span>
</span><span id="AdaHAT-81"><a href="#AdaHAT-81"><span class="linenos"> 81</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the adjustment intensity in equation (9) in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).&quot;&quot;&quot;</span>
</span><span id="AdaHAT-82"><a href="#AdaHAT-82"><span class="linenos"> 82</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>
</span><span id="AdaHAT-83"><a href="#AdaHAT-83"><span class="linenos"> 83</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Store the small value to avoid division by zero appeared in equation (9) in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).&quot;&quot;&quot;</span>
</span><span id="AdaHAT-84"><a href="#AdaHAT-84"><span class="linenos"> 84</span></a>
</span><span id="AdaHAT-85"><a href="#AdaHAT-85"><span class="linenos"> 85</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">summative_mask_for_previous_tasks</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="AdaHAT-86"><a href="#AdaHAT-86"><span class="linenos"> 86</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the summative binary attention mask $\mathrm{M}^{&lt;t,\text{sum}}$ previous tasks $1,\cdots, t-1$, gated from the task embedding. Keys are task IDs and values are the corresponding summative mask. Each cumulative mask is a dict where keys are layer names and values are the binary mask tensor for the layer. The mask tensor has size (number of units). &quot;&quot;&quot;</span>
</span><span id="AdaHAT-87"><a href="#AdaHAT-87"><span class="linenos"> 87</span></a>
</span><span id="AdaHAT-88"><a href="#AdaHAT-88"><span class="linenos"> 88</span></a>        <span class="c1"># set manual optimisation</span>
</span><span id="AdaHAT-89"><a href="#AdaHAT-89"><span class="linenos"> 89</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">automatic_optimization</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="AdaHAT-90"><a href="#AdaHAT-90"><span class="linenos"> 90</span></a>
</span><span id="AdaHAT-91"><a href="#AdaHAT-91"><span class="linenos"> 91</span></a>        <span class="n">AdaHAT</span><span class="o">.</span><span class="n">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</span><span id="AdaHAT-92"><a href="#AdaHAT-92"><span class="linenos"> 92</span></a>
</span><span id="AdaHAT-93"><a href="#AdaHAT-93"><span class="linenos"> 93</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="AdaHAT-94"><a href="#AdaHAT-94"><span class="linenos"> 94</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Check the sanity of the arguments.</span>
</span><span id="AdaHAT-95"><a href="#AdaHAT-95"><span class="linenos"> 95</span></a>
</span><span id="AdaHAT-96"><a href="#AdaHAT-96"><span class="linenos"> 96</span></a><span class="sd">        **Raises:**</span>
</span><span id="AdaHAT-97"><a href="#AdaHAT-97"><span class="linenos"> 97</span></a><span class="sd">        - **ValueError**: If the `adjustment_intensity` is not positive.</span>
</span><span id="AdaHAT-98"><a href="#AdaHAT-98"><span class="linenos"> 98</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="AdaHAT-99"><a href="#AdaHAT-99"><span class="linenos"> 99</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="AdaHAT-100"><a href="#AdaHAT-100"><span class="linenos">100</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="AdaHAT-101"><a href="#AdaHAT-101"><span class="linenos">101</span></a>                <span class="sa">f</span><span class="s2">&quot;The adjustment intensity should be positive, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span><span class="si">}</span><span class="s2">.&quot;</span>
</span><span id="AdaHAT-102"><a href="#AdaHAT-102"><span class="linenos">102</span></a>            <span class="p">)</span>
</span><span id="AdaHAT-103"><a href="#AdaHAT-103"><span class="linenos">103</span></a>
</span><span id="AdaHAT-104"><a href="#AdaHAT-104"><span class="linenos">104</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_start</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="AdaHAT-105"><a href="#AdaHAT-105"><span class="linenos">105</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Additionally initialise the summative mask at the beginning of first task.&quot;&quot;&quot;</span>
</span><span id="AdaHAT-106"><a href="#AdaHAT-106"><span class="linenos">106</span></a>        <span class="n">HAT</span><span class="o">.</span><span class="n">on_train_start</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</span><span id="AdaHAT-107"><a href="#AdaHAT-107"><span class="linenos">107</span></a>
</span><span id="AdaHAT-108"><a href="#AdaHAT-108"><span class="linenos">108</span></a>        <span class="c1"># initialise the summative mask at the beginning of first task. This should not be called in `__init__()` method as the `self.device` is not available at that time.</span>
</span><span id="AdaHAT-109"><a href="#AdaHAT-109"><span class="linenos">109</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="AdaHAT-110"><a href="#AdaHAT-110"><span class="linenos">110</span></a>            <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="AdaHAT-111"><a href="#AdaHAT-111"><span class="linenos">111</span></a>                <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span>
</span><span id="AdaHAT-112"><a href="#AdaHAT-112"><span class="linenos">112</span></a>                    <span class="n">layer_name</span>
</span><span id="AdaHAT-113"><a href="#AdaHAT-113"><span class="linenos">113</span></a>                <span class="p">)</span>  <span class="c1"># get the layer by its name</span>
</span><span id="AdaHAT-114"><a href="#AdaHAT-114"><span class="linenos">114</span></a>                <span class="n">num_units</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="AdaHAT-115"><a href="#AdaHAT-115"><span class="linenos">115</span></a>
</span><span id="AdaHAT-116"><a href="#AdaHAT-116"><span class="linenos">116</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">summative_mask_for_previous_tasks</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
</span><span id="AdaHAT-117"><a href="#AdaHAT-117"><span class="linenos">117</span></a>                    <span class="n">num_units</span>
</span><span id="AdaHAT-118"><a href="#AdaHAT-118"><span class="linenos">118</span></a>                <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
</span><span id="AdaHAT-119"><a href="#AdaHAT-119"><span class="linenos">119</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
</span><span id="AdaHAT-120"><a href="#AdaHAT-120"><span class="linenos">120</span></a>                <span class="p">)</span>  <span class="c1"># the summative mask $\mathrm{M}^{&lt;t,\text{sum}}$ is initialised as zeros mask ($t = 1$). See equation (7) in chapter 3.1 in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="AdaHAT-121"><a href="#AdaHAT-121"><span class="linenos">121</span></a>
</span><span id="AdaHAT-122"><a href="#AdaHAT-122"><span class="linenos">122</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">clip_grad_by_adjustment</span><span class="p">(</span>
</span><span id="AdaHAT-123"><a href="#AdaHAT-123"><span class="linenos">123</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="AdaHAT-124"><a href="#AdaHAT-124"><span class="linenos">124</span></a>        <span class="n">network_sparsity</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="AdaHAT-125"><a href="#AdaHAT-125"><span class="linenos">125</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="AdaHAT-126"><a href="#AdaHAT-126"><span class="linenos">126</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Clip the gradients by the adjustment rate.</span>
</span><span id="AdaHAT-127"><a href="#AdaHAT-127"><span class="linenos">127</span></a>
</span><span id="AdaHAT-128"><a href="#AdaHAT-128"><span class="linenos">128</span></a><span class="sd">        Note that as the task embedding fully covers every layer in the backbone network, no parameters are left out of this system. This applies not only the parameters in between layers with task embedding, but also those before the first layer. We designed it seperately in the codes.</span>
</span><span id="AdaHAT-129"><a href="#AdaHAT-129"><span class="linenos">129</span></a>
</span><span id="AdaHAT-130"><a href="#AdaHAT-130"><span class="linenos">130</span></a><span class="sd">        Network capacity is measured along with this method. Network capacity is defined as the average adjustment rate over all parameters. See chapter 4.1 in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="AdaHAT-131"><a href="#AdaHAT-131"><span class="linenos">131</span></a>
</span><span id="AdaHAT-132"><a href="#AdaHAT-132"><span class="linenos">132</span></a><span class="sd">        **Args:**</span>
</span><span id="AdaHAT-133"><a href="#AdaHAT-133"><span class="linenos">133</span></a><span class="sd">        - **network_sparsity** (`dict[str, Tensor]` | `None`): The network sparsity i.e. the mask sparsity loss of each layer for the current task. It applies only to AdaHAT modes, as it is used to calculate the adjustment rate for the gradients.</span>
</span><span id="AdaHAT-134"><a href="#AdaHAT-134"><span class="linenos">134</span></a>
</span><span id="AdaHAT-135"><a href="#AdaHAT-135"><span class="linenos">135</span></a><span class="sd">        **Returns:**</span>
</span><span id="AdaHAT-136"><a href="#AdaHAT-136"><span class="linenos">136</span></a><span class="sd">        - **capacity** (`Tensor`): the calculated network capacity.</span>
</span><span id="AdaHAT-137"><a href="#AdaHAT-137"><span class="linenos">137</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="AdaHAT-138"><a href="#AdaHAT-138"><span class="linenos">138</span></a>
</span><span id="AdaHAT-139"><a href="#AdaHAT-139"><span class="linenos">139</span></a>        <span class="c1"># initialise network capacity metric</span>
</span><span id="AdaHAT-140"><a href="#AdaHAT-140"><span class="linenos">140</span></a>        <span class="n">capacity</span> <span class="o">=</span> <span class="n">HATNetworkCapacity</span><span class="p">()</span>
</span><span id="AdaHAT-141"><a href="#AdaHAT-141"><span class="linenos">141</span></a>
</span><span id="AdaHAT-142"><a href="#AdaHAT-142"><span class="linenos">142</span></a>        <span class="c1"># Calculate the adjustment rate for gradients of the parameters, both weights and biases (if exists)</span>
</span><span id="AdaHAT-143"><a href="#AdaHAT-143"><span class="linenos">143</span></a>        <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="AdaHAT-144"><a href="#AdaHAT-144"><span class="linenos">144</span></a>
</span><span id="AdaHAT-145"><a href="#AdaHAT-145"><span class="linenos">145</span></a>            <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span>
</span><span id="AdaHAT-146"><a href="#AdaHAT-146"><span class="linenos">146</span></a>                <span class="n">layer_name</span>
</span><span id="AdaHAT-147"><a href="#AdaHAT-147"><span class="linenos">147</span></a>            <span class="p">)</span>  <span class="c1"># get the layer by its name</span>
</span><span id="AdaHAT-148"><a href="#AdaHAT-148"><span class="linenos">148</span></a>
</span><span id="AdaHAT-149"><a href="#AdaHAT-149"><span class="linenos">149</span></a>            <span class="c1"># placeholder for the adjustment rate to avoid the error of using it before assignment</span>
</span><span id="AdaHAT-150"><a href="#AdaHAT-150"><span class="linenos">150</span></a>            <span class="n">adjustment_rate_weight</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="AdaHAT-151"><a href="#AdaHAT-151"><span class="linenos">151</span></a>            <span class="n">adjustment_rate_bias</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="AdaHAT-152"><a href="#AdaHAT-152"><span class="linenos">152</span></a>
</span><span id="AdaHAT-153"><a href="#AdaHAT-153"><span class="linenos">153</span></a>            <span class="n">weight_importance</span><span class="p">,</span> <span class="n">bias_importance</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="AdaHAT-154"><a href="#AdaHAT-154"><span class="linenos">154</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_measure_parameter_wise</span><span class="p">(</span>
</span><span id="AdaHAT-155"><a href="#AdaHAT-155"><span class="linenos">155</span></a>                    <span class="n">unit_wise_measure</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">summative_mask_for_previous_tasks</span><span class="p">,</span>
</span><span id="AdaHAT-156"><a href="#AdaHAT-156"><span class="linenos">156</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="AdaHAT-157"><a href="#AdaHAT-157"><span class="linenos">157</span></a>                    <span class="n">aggregation</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span>
</span><span id="AdaHAT-158"><a href="#AdaHAT-158"><span class="linenos">158</span></a>                <span class="p">)</span>
</span><span id="AdaHAT-159"><a href="#AdaHAT-159"><span class="linenos">159</span></a>            <span class="p">)</span>  <span class="c1"># AdaHAT depend on parameter importance instead of parameter mask like HAT</span>
</span><span id="AdaHAT-160"><a href="#AdaHAT-160"><span class="linenos">160</span></a>
</span><span id="AdaHAT-161"><a href="#AdaHAT-161"><span class="linenos">161</span></a>            <span class="n">network_sparsity_layer</span> <span class="o">=</span> <span class="n">network_sparsity</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="AdaHAT-162"><a href="#AdaHAT-162"><span class="linenos">162</span></a>
</span><span id="AdaHAT-163"><a href="#AdaHAT-163"><span class="linenos">163</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;adahat&quot;</span><span class="p">:</span>
</span><span id="AdaHAT-164"><a href="#AdaHAT-164"><span class="linenos">164</span></a>                <span class="n">r_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span> <span class="o">/</span> <span class="p">(</span>
</span><span id="AdaHAT-165"><a href="#AdaHAT-165"><span class="linenos">165</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">+</span> <span class="n">network_sparsity_layer</span>
</span><span id="AdaHAT-166"><a href="#AdaHAT-166"><span class="linenos">166</span></a>                <span class="p">)</span>
</span><span id="AdaHAT-167"><a href="#AdaHAT-167"><span class="linenos">167</span></a>                <span class="n">adjustment_rate_weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span>
</span><span id="AdaHAT-168"><a href="#AdaHAT-168"><span class="linenos">168</span></a>                    <span class="n">r_layer</span><span class="p">,</span> <span class="p">(</span><span class="n">weight_importance</span> <span class="o">+</span> <span class="n">r_layer</span><span class="p">)</span>
</span><span id="AdaHAT-169"><a href="#AdaHAT-169"><span class="linenos">169</span></a>                <span class="p">)</span>
</span><span id="AdaHAT-170"><a href="#AdaHAT-170"><span class="linenos">170</span></a>                <span class="n">adjustment_rate_bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">r_layer</span><span class="p">,</span> <span class="p">(</span><span class="n">bias_importance</span> <span class="o">+</span> <span class="n">r_layer</span><span class="p">))</span>
</span><span id="AdaHAT-171"><a href="#AdaHAT-171"><span class="linenos">171</span></a>
</span><span id="AdaHAT-172"><a href="#AdaHAT-172"><span class="linenos">172</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;adahat_no_sum&quot;</span><span class="p">:</span>
</span><span id="AdaHAT-173"><a href="#AdaHAT-173"><span class="linenos">173</span></a>
</span><span id="AdaHAT-174"><a href="#AdaHAT-174"><span class="linenos">174</span></a>                <span class="n">r_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span> <span class="o">/</span> <span class="p">(</span>
</span><span id="AdaHAT-175"><a href="#AdaHAT-175"><span class="linenos">175</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">+</span> <span class="n">network_sparsity_layer</span>
</span><span id="AdaHAT-176"><a href="#AdaHAT-176"><span class="linenos">176</span></a>                <span class="p">)</span>
</span><span id="AdaHAT-177"><a href="#AdaHAT-177"><span class="linenos">177</span></a>                <span class="n">adjustment_rate_weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">r_layer</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">+</span> <span class="n">r_layer</span><span class="p">))</span>
</span><span id="AdaHAT-178"><a href="#AdaHAT-178"><span class="linenos">178</span></a>                <span class="n">adjustment_rate_bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">r_layer</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">+</span> <span class="n">r_layer</span><span class="p">))</span>
</span><span id="AdaHAT-179"><a href="#AdaHAT-179"><span class="linenos">179</span></a>
</span><span id="AdaHAT-180"><a href="#AdaHAT-180"><span class="linenos">180</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;adahat_no_reg&quot;</span><span class="p">:</span>
</span><span id="AdaHAT-181"><a href="#AdaHAT-181"><span class="linenos">181</span></a>
</span><span id="AdaHAT-182"><a href="#AdaHAT-182"><span class="linenos">182</span></a>                <span class="n">r_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">+</span> <span class="mf">0.0</span><span class="p">)</span>
</span><span id="AdaHAT-183"><a href="#AdaHAT-183"><span class="linenos">183</span></a>                <span class="n">adjustment_rate_weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span>
</span><span id="AdaHAT-184"><a href="#AdaHAT-184"><span class="linenos">184</span></a>                    <span class="n">r_layer</span><span class="p">,</span> <span class="p">(</span><span class="n">weight_importance</span> <span class="o">+</span> <span class="n">r_layer</span><span class="p">)</span>
</span><span id="AdaHAT-185"><a href="#AdaHAT-185"><span class="linenos">185</span></a>                <span class="p">)</span>
</span><span id="AdaHAT-186"><a href="#AdaHAT-186"><span class="linenos">186</span></a>                <span class="n">adjustment_rate_bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">r_layer</span><span class="p">,</span> <span class="p">(</span><span class="n">bias_importance</span> <span class="o">+</span> <span class="n">r_layer</span><span class="p">))</span>
</span><span id="AdaHAT-187"><a href="#AdaHAT-187"><span class="linenos">187</span></a>
</span><span id="AdaHAT-188"><a href="#AdaHAT-188"><span class="linenos">188</span></a>            <span class="c1"># apply the adjustment rate to the gradients</span>
</span><span id="AdaHAT-189"><a href="#AdaHAT-189"><span class="linenos">189</span></a>            <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">adjustment_rate_weight</span>
</span><span id="AdaHAT-190"><a href="#AdaHAT-190"><span class="linenos">190</span></a>            <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="AdaHAT-191"><a href="#AdaHAT-191"><span class="linenos">191</span></a>                <span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">adjustment_rate_bias</span>
</span><span id="AdaHAT-192"><a href="#AdaHAT-192"><span class="linenos">192</span></a>
</span><span id="AdaHAT-193"><a href="#AdaHAT-193"><span class="linenos">193</span></a>            <span class="c1"># update network capacity metric</span>
</span><span id="AdaHAT-194"><a href="#AdaHAT-194"><span class="linenos">194</span></a>            <span class="n">capacity</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">adjustment_rate_weight</span><span class="p">,</span> <span class="n">adjustment_rate_bias</span><span class="p">)</span>
</span><span id="AdaHAT-195"><a href="#AdaHAT-195"><span class="linenos">195</span></a>
</span><span id="AdaHAT-196"><a href="#AdaHAT-196"><span class="linenos">196</span></a>        <span class="k">return</span> <span class="n">capacity</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
</span><span id="AdaHAT-197"><a href="#AdaHAT-197"><span class="linenos">197</span></a>
</span><span id="AdaHAT-198"><a href="#AdaHAT-198"><span class="linenos">198</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_end</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="AdaHAT-199"><a href="#AdaHAT-199"><span class="linenos">199</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Additionally update summative mask after training the task.&quot;&quot;&quot;</span>
</span><span id="AdaHAT-200"><a href="#AdaHAT-200"><span class="linenos">200</span></a>
</span><span id="AdaHAT-201"><a href="#AdaHAT-201"><span class="linenos">201</span></a>        <span class="n">HAT</span><span class="o">.</span><span class="n">on_train_end</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</span><span id="AdaHAT-202"><a href="#AdaHAT-202"><span class="linenos">202</span></a>
</span><span id="AdaHAT-203"><a href="#AdaHAT-203"><span class="linenos">203</span></a>        <span class="n">mask_t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">masks</span><span class="p">[</span>
</span><span id="AdaHAT-204"><a href="#AdaHAT-204"><span class="linenos">204</span></a>            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="AdaHAT-205"><a href="#AdaHAT-205"><span class="linenos">205</span></a>        <span class="p">]</span>  <span class="c1"># get stored mask for the current task again</span>
</span><span id="AdaHAT-206"><a href="#AdaHAT-206"><span class="linenos">206</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">summative_mask_for_previous_tasks</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="AdaHAT-207"><a href="#AdaHAT-207"><span class="linenos">207</span></a>            <span class="n">layer_name</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">summative_mask_for_previous_tasks</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="AdaHAT-208"><a href="#AdaHAT-208"><span class="linenos">208</span></a>            <span class="o">+</span> <span class="n">mask_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="AdaHAT-209"><a href="#AdaHAT-209"><span class="linenos">209</span></a>            <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span>
</span><span id="AdaHAT-210"><a href="#AdaHAT-210"><span class="linenos">210</span></a>        <span class="p">}</span>
</span></pre></div>


            <div class="docstring"><p>AdaHAT (Adaptive Hard Attention to the Task) algorithm.</p>

<p><a href="https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9">Adaptive HAT (Adaptive Hard Attention to the Task, 2024)</a> is an architecture-based continual learning approach that improves <a href="http://proceedings.mlr.press/v80/serra18a">HAT (Hard Attention to the Task, 2018)</a> by introducing new adaptive soft gradient clipping based on parameter importance and network sparsity.</p>

<p>We implement AdaHAT as a subclass of HAT algorithm, as AdaHAT has the same  <code><a href="#AdaHAT.forward">forward()</a></code>, <code><a href="#AdaHAT.compensate_task_embedding_gradients">compensate_task_embedding_gradients()</a></code>, <code><a href="#AdaHAT.training_step">training_step()</a></code>, <code><a href="#AdaHAT.on_train_end">on_train_end()</a></code>,<code><a href="#AdaHAT.validation_step">validation_step()</a></code>, <code><a href="#AdaHAT.test_step">test_step()</a></code> method as <code>HAT</code> class.</p>
</div>


                            <div id="AdaHAT.__init__" class="classattr">
                                        <input id="AdaHAT.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">AdaHAT</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">backbone</span><span class="p">:</span> <span class="n"><a href="../backbones.html#HATMaskBackbone">clarena.backbones.HATMaskBackbone</a></span>,</span><span class="param">	<span class="n">heads</span><span class="p">:</span> <span class="n"><a href="../cl_heads.html#HeadsTIL">clarena.cl_heads.HeadsTIL</a></span> <span class="o">|</span> <span class="n"><a href="../cl_heads.html#HeadsCIL">clarena.cl_heads.HeadsCIL</a></span>,</span><span class="param">	<span class="n">adjustment_mode</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">adjustment_intensity</span><span class="p">:</span> <span class="nb">float</span>,</span><span class="param">	<span class="n">s_max</span><span class="p">:</span> <span class="nb">float</span>,</span><span class="param">	<span class="n">clamp_threshold</span><span class="p">:</span> <span class="nb">float</span>,</span><span class="param">	<span class="n">mask_sparsity_reg_factor</span><span class="p">:</span> <span class="nb">float</span>,</span><span class="param">	<span class="n">mask_sparsity_reg_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;original&#39;</span>,</span><span class="param">	<span class="n">task_embedding_init_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;N01&#39;</span>,</span><span class="param">	<span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span></span>)</span>

                <label class="view-source-button" for="AdaHAT.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#AdaHAT.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="AdaHAT.__init__-30"><a href="#AdaHAT.__init__-30"><span class="linenos">30</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="AdaHAT.__init__-31"><a href="#AdaHAT.__init__-31"><span class="linenos">31</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="AdaHAT.__init__-32"><a href="#AdaHAT.__init__-32"><span class="linenos">32</span></a>        <span class="n">backbone</span><span class="p">:</span> <span class="n">HATMaskBackbone</span><span class="p">,</span>
</span><span id="AdaHAT.__init__-33"><a href="#AdaHAT.__init__-33"><span class="linenos">33</span></a>        <span class="n">heads</span><span class="p">:</span> <span class="n">HeadsTIL</span> <span class="o">|</span> <span class="n">HeadsCIL</span><span class="p">,</span>
</span><span id="AdaHAT.__init__-34"><a href="#AdaHAT.__init__-34"><span class="linenos">34</span></a>        <span class="n">adjustment_mode</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="AdaHAT.__init__-35"><a href="#AdaHAT.__init__-35"><span class="linenos">35</span></a>        <span class="n">adjustment_intensity</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="AdaHAT.__init__-36"><a href="#AdaHAT.__init__-36"><span class="linenos">36</span></a>        <span class="n">s_max</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="AdaHAT.__init__-37"><a href="#AdaHAT.__init__-37"><span class="linenos">37</span></a>        <span class="n">clamp_threshold</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="AdaHAT.__init__-38"><a href="#AdaHAT.__init__-38"><span class="linenos">38</span></a>        <span class="n">mask_sparsity_reg_factor</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="AdaHAT.__init__-39"><a href="#AdaHAT.__init__-39"><span class="linenos">39</span></a>        <span class="n">mask_sparsity_reg_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;original&quot;</span><span class="p">,</span>
</span><span id="AdaHAT.__init__-40"><a href="#AdaHAT.__init__-40"><span class="linenos">40</span></a>        <span class="n">task_embedding_init_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;N01&quot;</span><span class="p">,</span>
</span><span id="AdaHAT.__init__-41"><a href="#AdaHAT.__init__-41"><span class="linenos">41</span></a>        <span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
</span><span id="AdaHAT.__init__-42"><a href="#AdaHAT.__init__-42"><span class="linenos">42</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="AdaHAT.__init__-43"><a href="#AdaHAT.__init__-43"><span class="linenos">43</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialise the AdaHAT algorithm with the network.</span>
</span><span id="AdaHAT.__init__-44"><a href="#AdaHAT.__init__-44"><span class="linenos">44</span></a>
</span><span id="AdaHAT.__init__-45"><a href="#AdaHAT.__init__-45"><span class="linenos">45</span></a><span class="sd">        **Args:**</span>
</span><span id="AdaHAT.__init__-46"><a href="#AdaHAT.__init__-46"><span class="linenos">46</span></a><span class="sd">        - **backbone** (`HATMaskBackbone`): must be a backbone network with HAT mask mechanism.</span>
</span><span id="AdaHAT.__init__-47"><a href="#AdaHAT.__init__-47"><span class="linenos">47</span></a><span class="sd">        - **heads** (`HeadsTIL` | `HeadsCIL`): output heads.</span>
</span><span id="AdaHAT.__init__-48"><a href="#AdaHAT.__init__-48"><span class="linenos">48</span></a><span class="sd">        - **adjustment_mode** (`str`): the strategy of adjustment i.e. the mode of gradient clipping, should be one of the following:</span>
</span><span id="AdaHAT.__init__-49"><a href="#AdaHAT.__init__-49"><span class="linenos">49</span></a><span class="sd">            1. &#39;adahat&#39;: set the gradients of parameters linking to masked units to a soft adjustment rate in the original AdaHAT approach. This is the way that AdaHAT does, which allowes the part of network for previous tasks to be updated slightly. See equation (8) and (9) chapter 3.1 in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="AdaHAT.__init__-50"><a href="#AdaHAT.__init__-50"><span class="linenos">50</span></a><span class="sd">            2. &#39;adahat_no_sum&#39;: set the gradients of parameters linking to masked units to a soft adjustment rate in the original AdaHAT approach, but without considering the information of parameter importance i.e. summative mask. This is the way that one of the AdaHAT ablation study does. See chapter 4.3 in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="AdaHAT.__init__-51"><a href="#AdaHAT.__init__-51"><span class="linenos">51</span></a><span class="sd">            3. &#39;adahat_no_reg&#39;: set the gradients of parameters linking to masked units to a soft adjustment rate in the original AdaHAT approach, but without considering the information of network sparsity i.e. mask sparsity regularisation value. This is the way that one of the AdaHAT ablation study does. See chapter 4.3 in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="AdaHAT.__init__-52"><a href="#AdaHAT.__init__-52"><span class="linenos">52</span></a><span class="sd">        - **adjustment_intensity** (`float`): hyperparameter, control the overall intensity of gradient adjustment. It&#39;s the $\alpha$ in equation (9) in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="AdaHAT.__init__-53"><a href="#AdaHAT.__init__-53"><span class="linenos">53</span></a><span class="sd">        - **s_max** (`float`): hyperparameter, the maximum scaling factor in the gate function. See chapter 2.4 &quot;Hard Attention Training&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="AdaHAT.__init__-54"><a href="#AdaHAT.__init__-54"><span class="linenos">54</span></a><span class="sd">        - **clamp_threshold** (`float`): the threshold for task embedding gradient compensation. See chapter 2.5 &quot;Embedding Gradient Compensation&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="AdaHAT.__init__-55"><a href="#AdaHAT.__init__-55"><span class="linenos">55</span></a><span class="sd">        - **mask_sparsity_reg_factor** (`float`): hyperparameter, the regularisation factor for mask sparsity.</span>
</span><span id="AdaHAT.__init__-56"><a href="#AdaHAT.__init__-56"><span class="linenos">56</span></a><span class="sd">        - **mask_sparsity_reg_mode** (`str`): the mode of mask sparsity regularisation, should be one of the following:</span>
</span><span id="AdaHAT.__init__-57"><a href="#AdaHAT.__init__-57"><span class="linenos">57</span></a><span class="sd">            1. &#39;original&#39; (default): the original mask sparsity regularisation in HAT paper.</span>
</span><span id="AdaHAT.__init__-58"><a href="#AdaHAT.__init__-58"><span class="linenos">58</span></a><span class="sd">            2. &#39;cross&#39;: the cross version mask sparsity regularisation.</span>
</span><span id="AdaHAT.__init__-59"><a href="#AdaHAT.__init__-59"><span class="linenos">59</span></a><span class="sd">        - **task_embedding_init_mode** (`str`): the initialisation method for task embeddings, should be one of the following:</span>
</span><span id="AdaHAT.__init__-60"><a href="#AdaHAT.__init__-60"><span class="linenos">60</span></a><span class="sd">            1. &#39;N01&#39; (default): standard normal distribution $N(0, 1)$.</span>
</span><span id="AdaHAT.__init__-61"><a href="#AdaHAT.__init__-61"><span class="linenos">61</span></a><span class="sd">            2. &#39;U-11&#39;: uniform distribution $U(-1, 1)$.</span>
</span><span id="AdaHAT.__init__-62"><a href="#AdaHAT.__init__-62"><span class="linenos">62</span></a><span class="sd">            3. &#39;U01&#39;: uniform distribution $U(0, 1)$.</span>
</span><span id="AdaHAT.__init__-63"><a href="#AdaHAT.__init__-63"><span class="linenos">63</span></a><span class="sd">            4. &#39;U-10&#39;: uniform distribution $U(-1, 0)$.</span>
</span><span id="AdaHAT.__init__-64"><a href="#AdaHAT.__init__-64"><span class="linenos">64</span></a><span class="sd">            5. &#39;last&#39;: inherit task embedding from last task.</span>
</span><span id="AdaHAT.__init__-65"><a href="#AdaHAT.__init__-65"><span class="linenos">65</span></a><span class="sd">        - **epsilon** (`float`): the value added to network sparsity to avoid division by zero appeared in equation (9) in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="AdaHAT.__init__-66"><a href="#AdaHAT.__init__-66"><span class="linenos">66</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="AdaHAT.__init__-67"><a href="#AdaHAT.__init__-67"><span class="linenos">67</span></a>        <span class="n">HAT</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="AdaHAT.__init__-68"><a href="#AdaHAT.__init__-68"><span class="linenos">68</span></a>            <span class="bp">self</span><span class="p">,</span>
</span><span id="AdaHAT.__init__-69"><a href="#AdaHAT.__init__-69"><span class="linenos">69</span></a>            <span class="n">backbone</span><span class="o">=</span><span class="n">backbone</span><span class="p">,</span>
</span><span id="AdaHAT.__init__-70"><a href="#AdaHAT.__init__-70"><span class="linenos">70</span></a>            <span class="n">heads</span><span class="o">=</span><span class="n">heads</span><span class="p">,</span>
</span><span id="AdaHAT.__init__-71"><a href="#AdaHAT.__init__-71"><span class="linenos">71</span></a>            <span class="n">adjustment_mode</span><span class="o">=</span><span class="n">adjustment_mode</span><span class="p">,</span>
</span><span id="AdaHAT.__init__-72"><a href="#AdaHAT.__init__-72"><span class="linenos">72</span></a>            <span class="n">s_max</span><span class="o">=</span><span class="n">s_max</span><span class="p">,</span>
</span><span id="AdaHAT.__init__-73"><a href="#AdaHAT.__init__-73"><span class="linenos">73</span></a>            <span class="n">clamp_threshold</span><span class="o">=</span><span class="n">clamp_threshold</span><span class="p">,</span>
</span><span id="AdaHAT.__init__-74"><a href="#AdaHAT.__init__-74"><span class="linenos">74</span></a>            <span class="n">mask_sparsity_reg_factor</span><span class="o">=</span><span class="n">mask_sparsity_reg_factor</span><span class="p">,</span>
</span><span id="AdaHAT.__init__-75"><a href="#AdaHAT.__init__-75"><span class="linenos">75</span></a>            <span class="n">mask_sparsity_reg_mode</span><span class="o">=</span><span class="n">mask_sparsity_reg_mode</span><span class="p">,</span>
</span><span id="AdaHAT.__init__-76"><a href="#AdaHAT.__init__-76"><span class="linenos">76</span></a>            <span class="n">task_embedding_init_mode</span><span class="o">=</span><span class="n">task_embedding_init_mode</span><span class="p">,</span>
</span><span id="AdaHAT.__init__-77"><a href="#AdaHAT.__init__-77"><span class="linenos">77</span></a>            <span class="n">alpha</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="AdaHAT.__init__-78"><a href="#AdaHAT.__init__-78"><span class="linenos">78</span></a>        <span class="p">)</span>
</span><span id="AdaHAT.__init__-79"><a href="#AdaHAT.__init__-79"><span class="linenos">79</span></a>
</span><span id="AdaHAT.__init__-80"><a href="#AdaHAT.__init__-80"><span class="linenos">80</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span> <span class="o">=</span> <span class="n">adjustment_intensity</span>
</span><span id="AdaHAT.__init__-81"><a href="#AdaHAT.__init__-81"><span class="linenos">81</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the adjustment intensity in equation (9) in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).&quot;&quot;&quot;</span>
</span><span id="AdaHAT.__init__-82"><a href="#AdaHAT.__init__-82"><span class="linenos">82</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>
</span><span id="AdaHAT.__init__-83"><a href="#AdaHAT.__init__-83"><span class="linenos">83</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Store the small value to avoid division by zero appeared in equation (9) in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).&quot;&quot;&quot;</span>
</span><span id="AdaHAT.__init__-84"><a href="#AdaHAT.__init__-84"><span class="linenos">84</span></a>
</span><span id="AdaHAT.__init__-85"><a href="#AdaHAT.__init__-85"><span class="linenos">85</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">summative_mask_for_previous_tasks</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="AdaHAT.__init__-86"><a href="#AdaHAT.__init__-86"><span class="linenos">86</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the summative binary attention mask $\mathrm{M}^{&lt;t,\text{sum}}$ previous tasks $1,\cdots, t-1$, gated from the task embedding. Keys are task IDs and values are the corresponding summative mask. Each cumulative mask is a dict where keys are layer names and values are the binary mask tensor for the layer. The mask tensor has size (number of units). &quot;&quot;&quot;</span>
</span><span id="AdaHAT.__init__-87"><a href="#AdaHAT.__init__-87"><span class="linenos">87</span></a>
</span><span id="AdaHAT.__init__-88"><a href="#AdaHAT.__init__-88"><span class="linenos">88</span></a>        <span class="c1"># set manual optimisation</span>
</span><span id="AdaHAT.__init__-89"><a href="#AdaHAT.__init__-89"><span class="linenos">89</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">automatic_optimization</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="AdaHAT.__init__-90"><a href="#AdaHAT.__init__-90"><span class="linenos">90</span></a>
</span><span id="AdaHAT.__init__-91"><a href="#AdaHAT.__init__-91"><span class="linenos">91</span></a>        <span class="n">AdaHAT</span><span class="o">.</span><span class="n">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Initialise the AdaHAT algorithm with the network.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>backbone</strong> (<code>HATMaskBackbone</code>): must be a backbone network with HAT mask mechanism.</li>
<li><strong>heads</strong> (<code>HeadsTIL</code> | <code>HeadsCIL</code>): output heads.</li>
<li><strong>adjustment_mode</strong> (<code>str</code>): the strategy of adjustment i.e. the mode of gradient clipping, should be one of the following:
<ol>
<li>'adahat': set the gradients of parameters linking to masked units to a soft adjustment rate in the original AdaHAT approach. This is the way that AdaHAT does, which allowes the part of network for previous tasks to be updated slightly. See equation (8) and (9) chapter 3.1 in <a href="https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9">AdaHAT paper</a>.</li>
<li>'adahat_no_sum': set the gradients of parameters linking to masked units to a soft adjustment rate in the original AdaHAT approach, but without considering the information of parameter importance i.e. summative mask. This is the way that one of the AdaHAT ablation study does. See chapter 4.3 in <a href="https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9">AdaHAT paper</a>.</li>
<li>'adahat_no_reg': set the gradients of parameters linking to masked units to a soft adjustment rate in the original AdaHAT approach, but without considering the information of network sparsity i.e. mask sparsity regularisation value. This is the way that one of the AdaHAT ablation study does. See chapter 4.3 in <a href="https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9">AdaHAT paper</a>.</li>
</ol></li>
<li><strong>adjustment_intensity</strong> (<code><a href="#AdaHAT.float">float</a></code>): hyperparameter, control the overall intensity of gradient adjustment. It's the $\alpha$ in equation (9) in <a href="https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9">AdaHAT paper</a>.</li>
<li><strong>s_max</strong> (<code><a href="#AdaHAT.float">float</a></code>): hyperparameter, the maximum scaling factor in the gate function. See chapter 2.4 "Hard Attention Training" in <a href="http://proceedings.mlr.press/v80/serra18a">HAT paper</a>.</li>
<li><strong>clamp_threshold</strong> (<code><a href="#AdaHAT.float">float</a></code>): the threshold for task embedding gradient compensation. See chapter 2.5 "Embedding Gradient Compensation" in <a href="http://proceedings.mlr.press/v80/serra18a">HAT paper</a>.</li>
<li><strong>mask_sparsity_reg_factor</strong> (<code><a href="#AdaHAT.float">float</a></code>): hyperparameter, the regularisation factor for mask sparsity.</li>
<li><strong>mask_sparsity_reg_mode</strong> (<code>str</code>): the mode of mask sparsity regularisation, should be one of the following:
<ol>
<li>'original' (default): the original mask sparsity regularisation in HAT paper.</li>
<li>'cross': the cross version mask sparsity regularisation.</li>
</ol></li>
<li><strong>task_embedding_init_mode</strong> (<code>str</code>): the initialisation method for task embeddings, should be one of the following:
<ol>
<li>'N01' (default): standard normal distribution $N(0, 1)$.</li>
<li>'U-11': uniform distribution $U(-1, 1)$.</li>
<li>'U01': uniform distribution $U(0, 1)$.</li>
<li>'U-10': uniform distribution $U(-1, 0)$.</li>
<li>'last': inherit task embedding from last task.</li>
</ol></li>
<li><strong>epsilon</strong> (<code><a href="#AdaHAT.float">float</a></code>): the value added to network sparsity to avoid division by zero appeared in equation (9) in <a href="https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9">AdaHAT paper</a>.</li>
</ul>
</div>


                            </div>
                            <div id="AdaHAT.adjustment_intensity" class="classattr">
                                <div class="attr variable">
            <span class="name">adjustment_intensity</span>

        
    </div>
    <a class="headerlink" href="#AdaHAT.adjustment_intensity"></a>
    
            <div class="docstring"><p>Store the adjustment intensity in equation (9) in <a href="https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9">AdaHAT paper</a>.</p>
</div>


                            </div>
                            <div id="AdaHAT.epsilon" class="classattr">
                                <div class="attr variable">
            <span class="name">epsilon</span>

        
    </div>
    <a class="headerlink" href="#AdaHAT.epsilon"></a>
    
            <div class="docstring"><p>Store the small value to avoid division by zero appeared in equation (9) in <a href="https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9">AdaHAT paper</a>.</p>
</div>


                            </div>
                            <div id="AdaHAT.summative_mask_for_previous_tasks" class="classattr">
                                <div class="attr variable">
            <span class="name">summative_mask_for_previous_tasks</span><span class="annotation">: dict[str, torch.Tensor]</span>

        
    </div>
    <a class="headerlink" href="#AdaHAT.summative_mask_for_previous_tasks"></a>
    
            <div class="docstring"><p>Store the summative binary attention mask $\mathrm{M}^{<t,\text{sum}}$ previous tasks $1,\cdots, t-1$, gated from the task embedding. Keys are task IDs and values are the corresponding summative mask. Each cumulative mask is a dict where keys are layer names and values are the binary mask tensor for the layer. The mask tensor has size (number of units).</p>
</div>


                            </div>
                            <div id="AdaHAT.automatic_optimization" class="classattr">
                                        <input id="AdaHAT.automatic_optimization-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr variable">
            <span class="name">automatic_optimization</span><span class="annotation">: bool</span>

                <label class="view-source-button" for="AdaHAT.automatic_optimization-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#AdaHAT.automatic_optimization"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="AdaHAT.automatic_optimization-290"><a href="#AdaHAT.automatic_optimization-290"><span class="linenos">290</span></a>    <span class="nd">@property</span>
</span><span id="AdaHAT.automatic_optimization-291"><a href="#AdaHAT.automatic_optimization-291"><span class="linenos">291</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">automatic_optimization</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
</span><span id="AdaHAT.automatic_optimization-292"><a href="#AdaHAT.automatic_optimization-292"><span class="linenos">292</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``.&quot;&quot;&quot;</span>
</span><span id="AdaHAT.automatic_optimization-293"><a href="#AdaHAT.automatic_optimization-293"><span class="linenos">293</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_automatic_optimization</span>
</span></pre></div>


            <div class="docstring"><p>If set to <code>False</code> you are responsible for calling <code>.backward()</code>, <code>.step()</code>, <code>.zero_grad()</code>.</p>
</div>


                            </div>
                            <div id="AdaHAT.sanity_check" class="classattr">
                                        <input id="AdaHAT.sanity_check-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">sanity_check</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span></span><span class="return-annotation">) -> <span class="kc">None</span>:</span></span>

                <label class="view-source-button" for="AdaHAT.sanity_check-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#AdaHAT.sanity_check"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="AdaHAT.sanity_check-93"><a href="#AdaHAT.sanity_check-93"><span class="linenos"> 93</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="AdaHAT.sanity_check-94"><a href="#AdaHAT.sanity_check-94"><span class="linenos"> 94</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Check the sanity of the arguments.</span>
</span><span id="AdaHAT.sanity_check-95"><a href="#AdaHAT.sanity_check-95"><span class="linenos"> 95</span></a>
</span><span id="AdaHAT.sanity_check-96"><a href="#AdaHAT.sanity_check-96"><span class="linenos"> 96</span></a><span class="sd">        **Raises:**</span>
</span><span id="AdaHAT.sanity_check-97"><a href="#AdaHAT.sanity_check-97"><span class="linenos"> 97</span></a><span class="sd">        - **ValueError**: If the `adjustment_intensity` is not positive.</span>
</span><span id="AdaHAT.sanity_check-98"><a href="#AdaHAT.sanity_check-98"><span class="linenos"> 98</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="AdaHAT.sanity_check-99"><a href="#AdaHAT.sanity_check-99"><span class="linenos"> 99</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="AdaHAT.sanity_check-100"><a href="#AdaHAT.sanity_check-100"><span class="linenos">100</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="AdaHAT.sanity_check-101"><a href="#AdaHAT.sanity_check-101"><span class="linenos">101</span></a>                <span class="sa">f</span><span class="s2">&quot;The adjustment intensity should be positive, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span><span class="si">}</span><span class="s2">.&quot;</span>
</span><span id="AdaHAT.sanity_check-102"><a href="#AdaHAT.sanity_check-102"><span class="linenos">102</span></a>            <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Check the sanity of the arguments.</p>

<p><strong>Raises:</strong></p>

<ul>
<li><strong>ValueError</strong>: If the <code><a href="#AdaHAT.adjustment_intensity">adjustment_intensity</a></code> is not positive.</li>
</ul>
</div>


                            </div>
                            <div id="AdaHAT.on_train_start" class="classattr">
                                        <input id="AdaHAT.on_train_start-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">on_train_start</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span></span><span class="return-annotation">) -> <span class="kc">None</span>:</span></span>

                <label class="view-source-button" for="AdaHAT.on_train_start-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#AdaHAT.on_train_start"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="AdaHAT.on_train_start-104"><a href="#AdaHAT.on_train_start-104"><span class="linenos">104</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_start</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="AdaHAT.on_train_start-105"><a href="#AdaHAT.on_train_start-105"><span class="linenos">105</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Additionally initialise the summative mask at the beginning of first task.&quot;&quot;&quot;</span>
</span><span id="AdaHAT.on_train_start-106"><a href="#AdaHAT.on_train_start-106"><span class="linenos">106</span></a>        <span class="n">HAT</span><span class="o">.</span><span class="n">on_train_start</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</span><span id="AdaHAT.on_train_start-107"><a href="#AdaHAT.on_train_start-107"><span class="linenos">107</span></a>
</span><span id="AdaHAT.on_train_start-108"><a href="#AdaHAT.on_train_start-108"><span class="linenos">108</span></a>        <span class="c1"># initialise the summative mask at the beginning of first task. This should not be called in `__init__()` method as the `self.device` is not available at that time.</span>
</span><span id="AdaHAT.on_train_start-109"><a href="#AdaHAT.on_train_start-109"><span class="linenos">109</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="AdaHAT.on_train_start-110"><a href="#AdaHAT.on_train_start-110"><span class="linenos">110</span></a>            <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="AdaHAT.on_train_start-111"><a href="#AdaHAT.on_train_start-111"><span class="linenos">111</span></a>                <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span>
</span><span id="AdaHAT.on_train_start-112"><a href="#AdaHAT.on_train_start-112"><span class="linenos">112</span></a>                    <span class="n">layer_name</span>
</span><span id="AdaHAT.on_train_start-113"><a href="#AdaHAT.on_train_start-113"><span class="linenos">113</span></a>                <span class="p">)</span>  <span class="c1"># get the layer by its name</span>
</span><span id="AdaHAT.on_train_start-114"><a href="#AdaHAT.on_train_start-114"><span class="linenos">114</span></a>                <span class="n">num_units</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="AdaHAT.on_train_start-115"><a href="#AdaHAT.on_train_start-115"><span class="linenos">115</span></a>
</span><span id="AdaHAT.on_train_start-116"><a href="#AdaHAT.on_train_start-116"><span class="linenos">116</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">summative_mask_for_previous_tasks</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
</span><span id="AdaHAT.on_train_start-117"><a href="#AdaHAT.on_train_start-117"><span class="linenos">117</span></a>                    <span class="n">num_units</span>
</span><span id="AdaHAT.on_train_start-118"><a href="#AdaHAT.on_train_start-118"><span class="linenos">118</span></a>                <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
</span><span id="AdaHAT.on_train_start-119"><a href="#AdaHAT.on_train_start-119"><span class="linenos">119</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
</span><span id="AdaHAT.on_train_start-120"><a href="#AdaHAT.on_train_start-120"><span class="linenos">120</span></a>                <span class="p">)</span>  <span class="c1"># the summative mask $\mathrm{M}^{&lt;t,\text{sum}}$ is initialised as zeros mask ($t = 1$). See equation (7) in chapter 3.1 in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span></pre></div>


            <div class="docstring"><p>Additionally initialise the summative mask at the beginning of first task.</p>
</div>


                            </div>
                            <div id="AdaHAT.clip_grad_by_adjustment" class="classattr">
                                        <input id="AdaHAT.clip_grad_by_adjustment-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">clip_grad_by_adjustment</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">network_sparsity</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="AdaHAT.clip_grad_by_adjustment-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#AdaHAT.clip_grad_by_adjustment"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="AdaHAT.clip_grad_by_adjustment-122"><a href="#AdaHAT.clip_grad_by_adjustment-122"><span class="linenos">122</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">clip_grad_by_adjustment</span><span class="p">(</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-123"><a href="#AdaHAT.clip_grad_by_adjustment-123"><span class="linenos">123</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-124"><a href="#AdaHAT.clip_grad_by_adjustment-124"><span class="linenos">124</span></a>        <span class="n">network_sparsity</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-125"><a href="#AdaHAT.clip_grad_by_adjustment-125"><span class="linenos">125</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-126"><a href="#AdaHAT.clip_grad_by_adjustment-126"><span class="linenos">126</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Clip the gradients by the adjustment rate.</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-127"><a href="#AdaHAT.clip_grad_by_adjustment-127"><span class="linenos">127</span></a>
</span><span id="AdaHAT.clip_grad_by_adjustment-128"><a href="#AdaHAT.clip_grad_by_adjustment-128"><span class="linenos">128</span></a><span class="sd">        Note that as the task embedding fully covers every layer in the backbone network, no parameters are left out of this system. This applies not only the parameters in between layers with task embedding, but also those before the first layer. We designed it seperately in the codes.</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-129"><a href="#AdaHAT.clip_grad_by_adjustment-129"><span class="linenos">129</span></a>
</span><span id="AdaHAT.clip_grad_by_adjustment-130"><a href="#AdaHAT.clip_grad_by_adjustment-130"><span class="linenos">130</span></a><span class="sd">        Network capacity is measured along with this method. Network capacity is defined as the average adjustment rate over all parameters. See chapter 4.1 in [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-131"><a href="#AdaHAT.clip_grad_by_adjustment-131"><span class="linenos">131</span></a>
</span><span id="AdaHAT.clip_grad_by_adjustment-132"><a href="#AdaHAT.clip_grad_by_adjustment-132"><span class="linenos">132</span></a><span class="sd">        **Args:**</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-133"><a href="#AdaHAT.clip_grad_by_adjustment-133"><span class="linenos">133</span></a><span class="sd">        - **network_sparsity** (`dict[str, Tensor]` | `None`): The network sparsity i.e. the mask sparsity loss of each layer for the current task. It applies only to AdaHAT modes, as it is used to calculate the adjustment rate for the gradients.</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-134"><a href="#AdaHAT.clip_grad_by_adjustment-134"><span class="linenos">134</span></a>
</span><span id="AdaHAT.clip_grad_by_adjustment-135"><a href="#AdaHAT.clip_grad_by_adjustment-135"><span class="linenos">135</span></a><span class="sd">        **Returns:**</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-136"><a href="#AdaHAT.clip_grad_by_adjustment-136"><span class="linenos">136</span></a><span class="sd">        - **capacity** (`Tensor`): the calculated network capacity.</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-137"><a href="#AdaHAT.clip_grad_by_adjustment-137"><span class="linenos">137</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-138"><a href="#AdaHAT.clip_grad_by_adjustment-138"><span class="linenos">138</span></a>
</span><span id="AdaHAT.clip_grad_by_adjustment-139"><a href="#AdaHAT.clip_grad_by_adjustment-139"><span class="linenos">139</span></a>        <span class="c1"># initialise network capacity metric</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-140"><a href="#AdaHAT.clip_grad_by_adjustment-140"><span class="linenos">140</span></a>        <span class="n">capacity</span> <span class="o">=</span> <span class="n">HATNetworkCapacity</span><span class="p">()</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-141"><a href="#AdaHAT.clip_grad_by_adjustment-141"><span class="linenos">141</span></a>
</span><span id="AdaHAT.clip_grad_by_adjustment-142"><a href="#AdaHAT.clip_grad_by_adjustment-142"><span class="linenos">142</span></a>        <span class="c1"># Calculate the adjustment rate for gradients of the parameters, both weights and biases (if exists)</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-143"><a href="#AdaHAT.clip_grad_by_adjustment-143"><span class="linenos">143</span></a>        <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-144"><a href="#AdaHAT.clip_grad_by_adjustment-144"><span class="linenos">144</span></a>
</span><span id="AdaHAT.clip_grad_by_adjustment-145"><a href="#AdaHAT.clip_grad_by_adjustment-145"><span class="linenos">145</span></a>            <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-146"><a href="#AdaHAT.clip_grad_by_adjustment-146"><span class="linenos">146</span></a>                <span class="n">layer_name</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-147"><a href="#AdaHAT.clip_grad_by_adjustment-147"><span class="linenos">147</span></a>            <span class="p">)</span>  <span class="c1"># get the layer by its name</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-148"><a href="#AdaHAT.clip_grad_by_adjustment-148"><span class="linenos">148</span></a>
</span><span id="AdaHAT.clip_grad_by_adjustment-149"><a href="#AdaHAT.clip_grad_by_adjustment-149"><span class="linenos">149</span></a>            <span class="c1"># placeholder for the adjustment rate to avoid the error of using it before assignment</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-150"><a href="#AdaHAT.clip_grad_by_adjustment-150"><span class="linenos">150</span></a>            <span class="n">adjustment_rate_weight</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-151"><a href="#AdaHAT.clip_grad_by_adjustment-151"><span class="linenos">151</span></a>            <span class="n">adjustment_rate_bias</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-152"><a href="#AdaHAT.clip_grad_by_adjustment-152"><span class="linenos">152</span></a>
</span><span id="AdaHAT.clip_grad_by_adjustment-153"><a href="#AdaHAT.clip_grad_by_adjustment-153"><span class="linenos">153</span></a>            <span class="n">weight_importance</span><span class="p">,</span> <span class="n">bias_importance</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-154"><a href="#AdaHAT.clip_grad_by_adjustment-154"><span class="linenos">154</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_measure_parameter_wise</span><span class="p">(</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-155"><a href="#AdaHAT.clip_grad_by_adjustment-155"><span class="linenos">155</span></a>                    <span class="n">unit_wise_measure</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">summative_mask_for_previous_tasks</span><span class="p">,</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-156"><a href="#AdaHAT.clip_grad_by_adjustment-156"><span class="linenos">156</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-157"><a href="#AdaHAT.clip_grad_by_adjustment-157"><span class="linenos">157</span></a>                    <span class="n">aggregation</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-158"><a href="#AdaHAT.clip_grad_by_adjustment-158"><span class="linenos">158</span></a>                <span class="p">)</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-159"><a href="#AdaHAT.clip_grad_by_adjustment-159"><span class="linenos">159</span></a>            <span class="p">)</span>  <span class="c1"># AdaHAT depend on parameter importance instead of parameter mask like HAT</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-160"><a href="#AdaHAT.clip_grad_by_adjustment-160"><span class="linenos">160</span></a>
</span><span id="AdaHAT.clip_grad_by_adjustment-161"><a href="#AdaHAT.clip_grad_by_adjustment-161"><span class="linenos">161</span></a>            <span class="n">network_sparsity_layer</span> <span class="o">=</span> <span class="n">network_sparsity</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-162"><a href="#AdaHAT.clip_grad_by_adjustment-162"><span class="linenos">162</span></a>
</span><span id="AdaHAT.clip_grad_by_adjustment-163"><a href="#AdaHAT.clip_grad_by_adjustment-163"><span class="linenos">163</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;adahat&quot;</span><span class="p">:</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-164"><a href="#AdaHAT.clip_grad_by_adjustment-164"><span class="linenos">164</span></a>                <span class="n">r_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span> <span class="o">/</span> <span class="p">(</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-165"><a href="#AdaHAT.clip_grad_by_adjustment-165"><span class="linenos">165</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">+</span> <span class="n">network_sparsity_layer</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-166"><a href="#AdaHAT.clip_grad_by_adjustment-166"><span class="linenos">166</span></a>                <span class="p">)</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-167"><a href="#AdaHAT.clip_grad_by_adjustment-167"><span class="linenos">167</span></a>                <span class="n">adjustment_rate_weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-168"><a href="#AdaHAT.clip_grad_by_adjustment-168"><span class="linenos">168</span></a>                    <span class="n">r_layer</span><span class="p">,</span> <span class="p">(</span><span class="n">weight_importance</span> <span class="o">+</span> <span class="n">r_layer</span><span class="p">)</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-169"><a href="#AdaHAT.clip_grad_by_adjustment-169"><span class="linenos">169</span></a>                <span class="p">)</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-170"><a href="#AdaHAT.clip_grad_by_adjustment-170"><span class="linenos">170</span></a>                <span class="n">adjustment_rate_bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">r_layer</span><span class="p">,</span> <span class="p">(</span><span class="n">bias_importance</span> <span class="o">+</span> <span class="n">r_layer</span><span class="p">))</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-171"><a href="#AdaHAT.clip_grad_by_adjustment-171"><span class="linenos">171</span></a>
</span><span id="AdaHAT.clip_grad_by_adjustment-172"><a href="#AdaHAT.clip_grad_by_adjustment-172"><span class="linenos">172</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;adahat_no_sum&quot;</span><span class="p">:</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-173"><a href="#AdaHAT.clip_grad_by_adjustment-173"><span class="linenos">173</span></a>
</span><span id="AdaHAT.clip_grad_by_adjustment-174"><a href="#AdaHAT.clip_grad_by_adjustment-174"><span class="linenos">174</span></a>                <span class="n">r_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span> <span class="o">/</span> <span class="p">(</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-175"><a href="#AdaHAT.clip_grad_by_adjustment-175"><span class="linenos">175</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">+</span> <span class="n">network_sparsity_layer</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-176"><a href="#AdaHAT.clip_grad_by_adjustment-176"><span class="linenos">176</span></a>                <span class="p">)</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-177"><a href="#AdaHAT.clip_grad_by_adjustment-177"><span class="linenos">177</span></a>                <span class="n">adjustment_rate_weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">r_layer</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">+</span> <span class="n">r_layer</span><span class="p">))</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-178"><a href="#AdaHAT.clip_grad_by_adjustment-178"><span class="linenos">178</span></a>                <span class="n">adjustment_rate_bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">r_layer</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">+</span> <span class="n">r_layer</span><span class="p">))</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-179"><a href="#AdaHAT.clip_grad_by_adjustment-179"><span class="linenos">179</span></a>
</span><span id="AdaHAT.clip_grad_by_adjustment-180"><a href="#AdaHAT.clip_grad_by_adjustment-180"><span class="linenos">180</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;adahat_no_reg&quot;</span><span class="p">:</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-181"><a href="#AdaHAT.clip_grad_by_adjustment-181"><span class="linenos">181</span></a>
</span><span id="AdaHAT.clip_grad_by_adjustment-182"><a href="#AdaHAT.clip_grad_by_adjustment-182"><span class="linenos">182</span></a>                <span class="n">r_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">+</span> <span class="mf">0.0</span><span class="p">)</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-183"><a href="#AdaHAT.clip_grad_by_adjustment-183"><span class="linenos">183</span></a>                <span class="n">adjustment_rate_weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-184"><a href="#AdaHAT.clip_grad_by_adjustment-184"><span class="linenos">184</span></a>                    <span class="n">r_layer</span><span class="p">,</span> <span class="p">(</span><span class="n">weight_importance</span> <span class="o">+</span> <span class="n">r_layer</span><span class="p">)</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-185"><a href="#AdaHAT.clip_grad_by_adjustment-185"><span class="linenos">185</span></a>                <span class="p">)</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-186"><a href="#AdaHAT.clip_grad_by_adjustment-186"><span class="linenos">186</span></a>                <span class="n">adjustment_rate_bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">r_layer</span><span class="p">,</span> <span class="p">(</span><span class="n">bias_importance</span> <span class="o">+</span> <span class="n">r_layer</span><span class="p">))</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-187"><a href="#AdaHAT.clip_grad_by_adjustment-187"><span class="linenos">187</span></a>
</span><span id="AdaHAT.clip_grad_by_adjustment-188"><a href="#AdaHAT.clip_grad_by_adjustment-188"><span class="linenos">188</span></a>            <span class="c1"># apply the adjustment rate to the gradients</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-189"><a href="#AdaHAT.clip_grad_by_adjustment-189"><span class="linenos">189</span></a>            <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">adjustment_rate_weight</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-190"><a href="#AdaHAT.clip_grad_by_adjustment-190"><span class="linenos">190</span></a>            <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-191"><a href="#AdaHAT.clip_grad_by_adjustment-191"><span class="linenos">191</span></a>                <span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">adjustment_rate_bias</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-192"><a href="#AdaHAT.clip_grad_by_adjustment-192"><span class="linenos">192</span></a>
</span><span id="AdaHAT.clip_grad_by_adjustment-193"><a href="#AdaHAT.clip_grad_by_adjustment-193"><span class="linenos">193</span></a>            <span class="c1"># update network capacity metric</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-194"><a href="#AdaHAT.clip_grad_by_adjustment-194"><span class="linenos">194</span></a>            <span class="n">capacity</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">adjustment_rate_weight</span><span class="p">,</span> <span class="n">adjustment_rate_bias</span><span class="p">)</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-195"><a href="#AdaHAT.clip_grad_by_adjustment-195"><span class="linenos">195</span></a>
</span><span id="AdaHAT.clip_grad_by_adjustment-196"><a href="#AdaHAT.clip_grad_by_adjustment-196"><span class="linenos">196</span></a>        <span class="k">return</span> <span class="n">capacity</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
</span></pre></div>


            <div class="docstring"><p>Clip the gradients by the adjustment rate.</p>

<p>Note that as the task embedding fully covers every layer in the backbone network, no parameters are left out of this system. This applies not only the parameters in between layers with task embedding, but also those before the first layer. We designed it seperately in the codes.</p>

<p>Network capacity is measured along with this method. Network capacity is defined as the average adjustment rate over all parameters. See chapter 4.1 in <a href="https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9">AdaHAT paper</a>.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>network_sparsity</strong> (<code>dict[str, Tensor]</code> | <code>None</code>): The network sparsity i.e. the mask sparsity loss of each layer for the current task. It applies only to AdaHAT modes, as it is used to calculate the adjustment rate for the gradients.</li>
</ul>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>capacity</strong> (<code>Tensor</code>): the calculated network capacity.</li>
</ul>
</div>


                            </div>
                            <div id="AdaHAT.on_train_end" class="classattr">
                                        <input id="AdaHAT.on_train_end-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">on_train_end</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span></span><span class="return-annotation">) -> <span class="kc">None</span>:</span></span>

                <label class="view-source-button" for="AdaHAT.on_train_end-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#AdaHAT.on_train_end"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="AdaHAT.on_train_end-198"><a href="#AdaHAT.on_train_end-198"><span class="linenos">198</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_end</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="AdaHAT.on_train_end-199"><a href="#AdaHAT.on_train_end-199"><span class="linenos">199</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Additionally update summative mask after training the task.&quot;&quot;&quot;</span>
</span><span id="AdaHAT.on_train_end-200"><a href="#AdaHAT.on_train_end-200"><span class="linenos">200</span></a>
</span><span id="AdaHAT.on_train_end-201"><a href="#AdaHAT.on_train_end-201"><span class="linenos">201</span></a>        <span class="n">HAT</span><span class="o">.</span><span class="n">on_train_end</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</span><span id="AdaHAT.on_train_end-202"><a href="#AdaHAT.on_train_end-202"><span class="linenos">202</span></a>
</span><span id="AdaHAT.on_train_end-203"><a href="#AdaHAT.on_train_end-203"><span class="linenos">203</span></a>        <span class="n">mask_t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">masks</span><span class="p">[</span>
</span><span id="AdaHAT.on_train_end-204"><a href="#AdaHAT.on_train_end-204"><span class="linenos">204</span></a>            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="AdaHAT.on_train_end-205"><a href="#AdaHAT.on_train_end-205"><span class="linenos">205</span></a>        <span class="p">]</span>  <span class="c1"># get stored mask for the current task again</span>
</span><span id="AdaHAT.on_train_end-206"><a href="#AdaHAT.on_train_end-206"><span class="linenos">206</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">summative_mask_for_previous_tasks</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="AdaHAT.on_train_end-207"><a href="#AdaHAT.on_train_end-207"><span class="linenos">207</span></a>            <span class="n">layer_name</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">summative_mask_for_previous_tasks</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="AdaHAT.on_train_end-208"><a href="#AdaHAT.on_train_end-208"><span class="linenos">208</span></a>            <span class="o">+</span> <span class="n">mask_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="AdaHAT.on_train_end-209"><a href="#AdaHAT.on_train_end-209"><span class="linenos">209</span></a>            <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span>
</span><span id="AdaHAT.on_train_end-210"><a href="#AdaHAT.on_train_end-210"><span class="linenos">210</span></a>        <span class="p">}</span>
</span></pre></div>


            <div class="docstring"><p>Additionally update summative mask after training the task.</p>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt><a href="hat.html#HAT">clarena.cl_algorithms.hat.HAT</a></dt>
                                <dd id="AdaHAT.adjustment_mode" class="variable"><a href="hat.html#HAT.adjustment_mode">adjustment_mode</a></dd>
                <dd id="AdaHAT.s_max" class="variable"><a href="hat.html#HAT.s_max">s_max</a></dd>
                <dd id="AdaHAT.clamp_threshold" class="variable"><a href="hat.html#HAT.clamp_threshold">clamp_threshold</a></dd>
                <dd id="AdaHAT.mask_sparsity_reg_factor" class="variable"><a href="hat.html#HAT.mask_sparsity_reg_factor">mask_sparsity_reg_factor</a></dd>
                <dd id="AdaHAT.mask_sparsity_reg_mode" class="variable"><a href="hat.html#HAT.mask_sparsity_reg_mode">mask_sparsity_reg_mode</a></dd>
                <dd id="AdaHAT.mark_sparsity_reg" class="variable"><a href="hat.html#HAT.mark_sparsity_reg">mark_sparsity_reg</a></dd>
                <dd id="AdaHAT.task_embedding_init_mode" class="variable"><a href="hat.html#HAT.task_embedding_init_mode">task_embedding_init_mode</a></dd>
                <dd id="AdaHAT.alpha" class="variable"><a href="hat.html#HAT.alpha">alpha</a></dd>
                <dd id="AdaHAT.masks" class="variable"><a href="hat.html#HAT.masks">masks</a></dd>
                <dd id="AdaHAT.cumulative_mask_for_previous_tasks" class="variable"><a href="hat.html#HAT.cumulative_mask_for_previous_tasks">cumulative_mask_for_previous_tasks</a></dd>
                <dd id="AdaHAT.compensate_task_embedding_gradients" class="function"><a href="hat.html#HAT.compensate_task_embedding_gradients">compensate_task_embedding_gradients</a></dd>
                <dd id="AdaHAT.forward" class="function"><a href="hat.html#HAT.forward">forward</a></dd>
                <dd id="AdaHAT.training_step" class="function"><a href="hat.html#HAT.training_step">training_step</a></dd>
                <dd id="AdaHAT.validation_step" class="function"><a href="hat.html#HAT.validation_step">validation_step</a></dd>
                <dd id="AdaHAT.test_step" class="function"><a href="hat.html#HAT.test_step">test_step</a></dd>

            </div>
                                </dl>
                            </div>
                </section>
    </main>
<script>
    function escapeHTML(html) {
        return document.createElement('div').appendChild(document.createTextNode(html)).parentNode.innerHTML;
    }

    const originalContent = document.querySelector("main.pdoc");
    let currentContent = originalContent;

    function setContent(innerHTML) {
        let elem;
        if (innerHTML) {
            elem = document.createElement("main");
            elem.classList.add("pdoc");
            elem.innerHTML = innerHTML;
        } else {
            elem = originalContent;
        }
        if (currentContent !== elem) {
            currentContent.replaceWith(elem);
            currentContent = elem;
        }
    }

    function getSearchTerm() {
        return (new URL(window.location)).searchParams.get("search");
    }

    const searchBox = document.querySelector(".pdoc input[type=search]");
    searchBox.addEventListener("input", function () {
        let url = new URL(window.location);
        if (searchBox.value.trim()) {
            url.hash = "";
            url.searchParams.set("search", searchBox.value);
        } else {
            url.searchParams.delete("search");
        }
        history.replaceState("", "", url.toString());
        onInput();
    });
    window.addEventListener("popstate", onInput);


    let search, searchErr;

    async function initialize() {
        try {
            search = await new Promise((resolve, reject) => {
                const script = document.createElement("script");
                script.type = "text/javascript";
                script.async = true;
                script.onload = () => resolve(window.pdocSearch);
                script.onerror = (e) => reject(e);
                script.src = "../../search.js";
                document.getElementsByTagName("head")[0].appendChild(script);
            });
        } catch (e) {
            console.error("Cannot fetch pdoc search index");
            searchErr = "Cannot fetch search index.";
        }
        onInput();

        document.querySelector("nav.pdoc").addEventListener("click", e => {
            if (e.target.hash) {
                searchBox.value = "";
                searchBox.dispatchEvent(new Event("input"));
            }
        });
    }

    function onInput() {
        setContent((() => {
            const term = getSearchTerm();
            if (!term) {
                return null
            }
            if (searchErr) {
                return `<h3>Error: ${searchErr}</h3>`
            }
            if (!search) {
                return "<h3>Searching...</h3>"
            }

            window.scrollTo({top: 0, left: 0, behavior: 'auto'});

            const results = search(term);

            let html;
            if (results.length === 0) {
                html = `No search results for '${escapeHTML(term)}'.`
            } else {
                html = `<h4>${results.length} search result${results.length > 1 ? "s" : ""} for '${escapeHTML(term)}'.</h4>`;
            }
            for (let result of results.slice(0, 10)) {
                let doc = result.doc;
                let url = `../../${doc.modulename.replaceAll(".", "/")}.html`;
                if (doc.qualname) {
                    url += `#${doc.qualname}`;
                }

                let heading;
                switch (result.doc.kind) {
                    case "function":
                        if (doc.fullname.endsWith(".__init__")) {
                            heading = `<span class="name">${doc.fullname.replace(/\.__init__$/, "")}</span>${doc.signature}`;
                        } else {
                            heading = `<span class="def">${doc.funcdef}</span> <span class="name">${doc.fullname}</span>${doc.signature}`;
                        }
                        break;
                    case "class":
                        heading = `<span class="def">class</span> <span class="name">${doc.fullname}</span>`;
                        if (doc.bases)
                            heading += `<wbr>(<span class="base">${doc.bases}</span>)`;
                        heading += `:`;
                        break;
                    case "variable":
                        heading = `<span class="name">${doc.fullname}</span>`;
                        if (doc.annotation)
                            heading += `<span class="annotation">${doc.annotation}</span>`;
                        if (doc.default_value)
                            heading += `<span class="default_value"> = ${doc.default_value}</span>`;
                        break;
                    default:
                        heading = `<span class="name">${doc.fullname}</span>`;
                        break;
                }
                html += `
                        <section class="search-result">
                        <a href="${url}" class="attr ${doc.kind}">${heading}</a>
                        <div class="docstring">${doc.doc}</div>
                        </section>
                    `;

            }
            return html;
        })());
    }

    if (getSearchTerm()) {
        initialize();
        searchBox.value = getSearchTerm();
        onInput();
    } else {
        searchBox.addEventListener("focus", initialize, {once: true});
    }

    searchBox.addEventListener("keydown", e => {
        if (["ArrowDown", "ArrowUp", "Enter"].includes(e.key)) {
            let focused = currentContent.querySelector(".search-result.focused");
            if (!focused) {
                currentContent.querySelector(".search-result").classList.add("focused");
            } else if (
                e.key === "ArrowDown"
                && focused.nextElementSibling
                && focused.nextElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.nextElementSibling.classList.add("focused");
                focused.nextElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "ArrowUp"
                && focused.previousElementSibling
                && focused.previousElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.previousElementSibling.classList.add("focused");
                focused.previousElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "Enter"
            ) {
                focused.querySelector("a").click();
            }
        }
    });
</script></body>
</html>