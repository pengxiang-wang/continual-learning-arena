<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="pdoc 15.0.1"/>
    <title>clarena.cl_algorithms.adahat API documentation</title>

    <style>/*! * Bootstrap Reboot v5.0.0 (https://getbootstrap.com/) * Copyright 2011-2021 The Bootstrap Authors * Copyright 2011-2021 Twitter, Inc. * Licensed under MIT (https://github.com/twbs/bootstrap/blob/main/LICENSE) * Forked from Normalize.css, licensed MIT (https://github.com/necolas/normalize.css/blob/master/LICENSE.md) */*,::after,::before{box-sizing:border-box}@media (prefers-reduced-motion:no-preference){:root{scroll-behavior:smooth}}body{margin:0;font-family:system-ui,-apple-system,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans","Liberation Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";font-size:1rem;font-weight:400;line-height:1.5;color:#212529;background-color:#fff;-webkit-text-size-adjust:100%;-webkit-tap-highlight-color:transparent}hr{margin:1rem 0;color:inherit;background-color:currentColor;border:0;opacity:.25}hr:not([size]){height:1px}h1,h2,h3,h4,h5,h6{margin-top:0;margin-bottom:.5rem;font-weight:500;line-height:1.2}h1{font-size:calc(1.375rem + 1.5vw)}@media (min-width:1200px){h1{font-size:2.5rem}}h2{font-size:calc(1.325rem + .9vw)}@media (min-width:1200px){h2{font-size:2rem}}h3{font-size:calc(1.3rem + .6vw)}@media (min-width:1200px){h3{font-size:1.75rem}}h4{font-size:calc(1.275rem + .3vw)}@media (min-width:1200px){h4{font-size:1.5rem}}h5{font-size:1.25rem}h6{font-size:1rem}p{margin-top:0;margin-bottom:1rem}abbr[data-bs-original-title],abbr[title]{-webkit-text-decoration:underline dotted;text-decoration:underline dotted;cursor:help;-webkit-text-decoration-skip-ink:none;text-decoration-skip-ink:none}address{margin-bottom:1rem;font-style:normal;line-height:inherit}ol,ul{padding-left:2rem}dl,ol,ul{margin-top:0;margin-bottom:1rem}ol ol,ol ul,ul ol,ul ul{margin-bottom:0}dt{font-weight:700}dd{margin-bottom:.5rem;margin-left:0}blockquote{margin:0 0 1rem}b,strong{font-weight:bolder}small{font-size:.875em}mark{padding:.2em;background-color:#fcf8e3}sub,sup{position:relative;font-size:.75em;line-height:0;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}a{color:#0d6efd;text-decoration:underline}a:hover{color:#0a58ca}a:not([href]):not([class]),a:not([href]):not([class]):hover{color:inherit;text-decoration:none}code,kbd,pre,samp{font-family:SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;font-size:1em;direction:ltr;unicode-bidi:bidi-override}pre{display:block;margin-top:0;margin-bottom:1rem;overflow:auto;font-size:.875em}pre code{font-size:inherit;color:inherit;word-break:normal}code{font-size:.875em;color:#d63384;word-wrap:break-word}a>code{color:inherit}kbd{padding:.2rem .4rem;font-size:.875em;color:#fff;background-color:#212529;border-radius:.2rem}kbd kbd{padding:0;font-size:1em;font-weight:700}figure{margin:0 0 1rem}img,svg{vertical-align:middle}table{caption-side:bottom;border-collapse:collapse}caption{padding-top:.5rem;padding-bottom:.5rem;color:#6c757d;text-align:left}th{text-align:inherit;text-align:-webkit-match-parent}tbody,td,tfoot,th,thead,tr{border-color:inherit;border-style:solid;border-width:0}label{display:inline-block}button{border-radius:0}button:focus:not(:focus-visible){outline:0}button,input,optgroup,select,textarea{margin:0;font-family:inherit;font-size:inherit;line-height:inherit}button,select{text-transform:none}[role=button]{cursor:pointer}select{word-wrap:normal}select:disabled{opacity:1}[list]::-webkit-calendar-picker-indicator{display:none}[type=button],[type=reset],[type=submit],button{-webkit-appearance:button}[type=button]:not(:disabled),[type=reset]:not(:disabled),[type=submit]:not(:disabled),button:not(:disabled){cursor:pointer}::-moz-focus-inner{padding:0;border-style:none}textarea{resize:vertical}fieldset{min-width:0;padding:0;margin:0;border:0}legend{float:left;width:100%;padding:0;margin-bottom:.5rem;font-size:calc(1.275rem + .3vw);line-height:inherit}@media (min-width:1200px){legend{font-size:1.5rem}}legend+*{clear:left}::-webkit-datetime-edit-day-field,::-webkit-datetime-edit-fields-wrapper,::-webkit-datetime-edit-hour-field,::-webkit-datetime-edit-minute,::-webkit-datetime-edit-month-field,::-webkit-datetime-edit-text,::-webkit-datetime-edit-year-field{padding:0}::-webkit-inner-spin-button{height:auto}[type=search]{outline-offset:-2px;-webkit-appearance:textfield}::-webkit-search-decoration{-webkit-appearance:none}::-webkit-color-swatch-wrapper{padding:0}::file-selector-button{font:inherit}::-webkit-file-upload-button{font:inherit;-webkit-appearance:button}output{display:inline-block}iframe{border:0}summary{display:list-item;cursor:pointer}progress{vertical-align:baseline}[hidden]{display:none!important}</style>
    <style>/*! syntax-highlighting.css */pre{line-height:125%;}span.linenos{color:inherit; background-color:transparent; padding-left:5px; padding-right:20px;}.pdoc-code .hll{background-color:#ffffcc}.pdoc-code{background:#f8f8f8;}.pdoc-code .c{color:#3D7B7B; font-style:italic}.pdoc-code .err{border:1px solid #FF0000}.pdoc-code .k{color:#008000; font-weight:bold}.pdoc-code .o{color:#666666}.pdoc-code .ch{color:#3D7B7B; font-style:italic}.pdoc-code .cm{color:#3D7B7B; font-style:italic}.pdoc-code .cp{color:#9C6500}.pdoc-code .cpf{color:#3D7B7B; font-style:italic}.pdoc-code .c1{color:#3D7B7B; font-style:italic}.pdoc-code .cs{color:#3D7B7B; font-style:italic}.pdoc-code .gd{color:#A00000}.pdoc-code .ge{font-style:italic}.pdoc-code .gr{color:#E40000}.pdoc-code .gh{color:#000080; font-weight:bold}.pdoc-code .gi{color:#008400}.pdoc-code .go{color:#717171}.pdoc-code .gp{color:#000080; font-weight:bold}.pdoc-code .gs{font-weight:bold}.pdoc-code .gu{color:#800080; font-weight:bold}.pdoc-code .gt{color:#0044DD}.pdoc-code .kc{color:#008000; font-weight:bold}.pdoc-code .kd{color:#008000; font-weight:bold}.pdoc-code .kn{color:#008000; font-weight:bold}.pdoc-code .kp{color:#008000}.pdoc-code .kr{color:#008000; font-weight:bold}.pdoc-code .kt{color:#B00040}.pdoc-code .m{color:#666666}.pdoc-code .s{color:#BA2121}.pdoc-code .na{color:#687822}.pdoc-code .nb{color:#008000}.pdoc-code .nc{color:#0000FF; font-weight:bold}.pdoc-code .no{color:#880000}.pdoc-code .nd{color:#AA22FF}.pdoc-code .ni{color:#717171; font-weight:bold}.pdoc-code .ne{color:#CB3F38; font-weight:bold}.pdoc-code .nf{color:#0000FF}.pdoc-code .nl{color:#767600}.pdoc-code .nn{color:#0000FF; font-weight:bold}.pdoc-code .nt{color:#008000; font-weight:bold}.pdoc-code .nv{color:#19177C}.pdoc-code .ow{color:#AA22FF; font-weight:bold}.pdoc-code .w{color:#bbbbbb}.pdoc-code .mb{color:#666666}.pdoc-code .mf{color:#666666}.pdoc-code .mh{color:#666666}.pdoc-code .mi{color:#666666}.pdoc-code .mo{color:#666666}.pdoc-code .sa{color:#BA2121}.pdoc-code .sb{color:#BA2121}.pdoc-code .sc{color:#BA2121}.pdoc-code .dl{color:#BA2121}.pdoc-code .sd{color:#BA2121; font-style:italic}.pdoc-code .s2{color:#BA2121}.pdoc-code .se{color:#AA5D1F; font-weight:bold}.pdoc-code .sh{color:#BA2121}.pdoc-code .si{color:#A45A77; font-weight:bold}.pdoc-code .sx{color:#008000}.pdoc-code .sr{color:#A45A77}.pdoc-code .s1{color:#BA2121}.pdoc-code .ss{color:#19177C}.pdoc-code .bp{color:#008000}.pdoc-code .fm{color:#0000FF}.pdoc-code .vc{color:#19177C}.pdoc-code .vg{color:#19177C}.pdoc-code .vi{color:#19177C}.pdoc-code .vm{color:#19177C}.pdoc-code .il{color:#666666}</style>
    <style>/*! theme.css */:root{--pdoc-background:#fff;}.pdoc{--text:#212529;--muted:#6c757d;--link:#3660a5;--link-hover:#1659c5;--code:#f8f8f8;--active:#fff598;--accent:#eee;--accent2:#c1c1c1;--nav-hover:rgba(255, 255, 255, 0.5);--name:#0066BB;--def:#008800;--annotation:#007020;}</style>
    <style>/*! layout.css */html, body{width:100%;height:100%;}html, main{scroll-behavior:smooth;}body{background-color:var(--pdoc-background);}@media (max-width:769px){#navtoggle{cursor:pointer;position:absolute;width:50px;height:40px;top:1rem;right:1rem;border-color:var(--text);color:var(--text);display:flex;opacity:0.8;z-index:999;}#navtoggle:hover{opacity:1;}#togglestate + div{display:none;}#togglestate:checked + div{display:inherit;}main, header{padding:2rem 3vw;}header + main{margin-top:-3rem;}.git-button{display:none !important;}nav input[type="search"]{max-width:77%;}nav input[type="search"]:first-child{margin-top:-6px;}nav input[type="search"]:valid ~ *{display:none !important;}}@media (min-width:770px){:root{--sidebar-width:clamp(12.5rem, 28vw, 22rem);}nav{position:fixed;overflow:auto;height:100vh;width:var(--sidebar-width);}main, header{padding:3rem 2rem 3rem calc(var(--sidebar-width) + 3rem);width:calc(54rem + var(--sidebar-width));max-width:100%;}header + main{margin-top:-4rem;}#navtoggle{display:none;}}#togglestate{position:absolute;height:0;opacity:0;}nav.pdoc{--pad:clamp(0.5rem, 2vw, 1.75rem);--indent:1.5rem;background-color:var(--accent);border-right:1px solid var(--accent2);box-shadow:0 0 20px rgba(50, 50, 50, .2) inset;padding:0 0 0 var(--pad);overflow-wrap:anywhere;scrollbar-width:thin; scrollbar-color:var(--accent2) transparent; z-index:1}nav.pdoc::-webkit-scrollbar{width:.4rem; }nav.pdoc::-webkit-scrollbar-thumb{background-color:var(--accent2); }nav.pdoc > div{padding:var(--pad) 0;}nav.pdoc .module-list-button{display:inline-flex;align-items:center;color:var(--text);border-color:var(--muted);margin-bottom:1rem;}nav.pdoc .module-list-button:hover{border-color:var(--text);}nav.pdoc input[type=search]{display:block;outline-offset:0;width:calc(100% - var(--pad));}nav.pdoc .logo{max-width:calc(100% - var(--pad));max-height:35vh;display:block;margin:0 auto 1rem;transform:translate(calc(-.5 * var(--pad)), 0);}nav.pdoc ul{list-style:none;padding-left:0;}nav.pdoc > div > ul{margin-left:calc(0px - var(--pad));}nav.pdoc li a{padding:.2rem 0 .2rem calc(var(--pad) + var(--indent));}nav.pdoc > div > ul > li > a{padding-left:var(--pad);}nav.pdoc li{transition:all 100ms;}nav.pdoc li:hover{background-color:var(--nav-hover);}nav.pdoc a, nav.pdoc a:hover{color:var(--text);}nav.pdoc a{display:block;}nav.pdoc > h2:first-of-type{margin-top:1.5rem;}nav.pdoc .class:before{content:"class ";color:var(--muted);}nav.pdoc .function:after{content:"()";color:var(--muted);}nav.pdoc footer:before{content:"";display:block;width:calc(100% - var(--pad));border-top:solid var(--accent2) 1px;margin-top:1.5rem;padding-top:.5rem;}nav.pdoc footer{font-size:small;}</style>
    <style>/*! content.css */.pdoc{color:var(--text);box-sizing:border-box;line-height:1.5;background:none;}.pdoc .pdoc-button{cursor:pointer;display:inline-block;border:solid black 1px;border-radius:2px;font-size:.75rem;padding:calc(0.5em - 1px) 1em;transition:100ms all;}.pdoc .alert{padding:1rem 1rem 1rem calc(1.5rem + 24px);border:1px solid transparent;border-radius:.25rem;background-repeat:no-repeat;background-position:.75rem center;margin-bottom:1rem;}.pdoc .alert > em{display:none;}.pdoc .alert > *:last-child{margin-bottom:0;}.pdoc .alert.note {color:#084298;background-color:#cfe2ff;border-color:#b6d4fe;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23084298%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8%2016A8%208%200%201%200%208%200a8%208%200%200%200%200%2016zm.93-9.412-1%204.705c-.07.34.029.533.304.533.194%200%20.487-.07.686-.246l-.088.416c-.287.346-.92.598-1.465.598-.703%200-1.002-.422-.808-1.319l.738-3.468c.064-.293.006-.399-.287-.47l-.451-.081.082-.381%202.29-.287zM8%205.5a1%201%200%201%201%200-2%201%201%200%200%201%200%202z%22/%3E%3C/svg%3E");}.pdoc .alert.warning{color:#664d03;background-color:#fff3cd;border-color:#ffecb5;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23664d03%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8.982%201.566a1.13%201.13%200%200%200-1.96%200L.165%2013.233c-.457.778.091%201.767.98%201.767h13.713c.889%200%201.438-.99.98-1.767L8.982%201.566zM8%205c.535%200%20.954.462.9.995l-.35%203.507a.552.552%200%200%201-1.1%200L7.1%205.995A.905.905%200%200%201%208%205zm.002%206a1%201%200%201%201%200%202%201%201%200%200%201%200-2z%22/%3E%3C/svg%3E");}.pdoc .alert.danger{color:#842029;background-color:#f8d7da;border-color:#f5c2c7;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23842029%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M5.52.359A.5.5%200%200%201%206%200h4a.5.5%200%200%201%20.474.658L8.694%206H12.5a.5.5%200%200%201%20.395.807l-7%209a.5.5%200%200%201-.873-.454L6.823%209.5H3.5a.5.5%200%200%201-.48-.641l2.5-8.5z%22/%3E%3C/svg%3E");}.pdoc .visually-hidden{position:absolute !important;width:1px !important;height:1px !important;padding:0 !important;margin:-1px !important;overflow:hidden !important;clip:rect(0, 0, 0, 0) !important;white-space:nowrap !important;border:0 !important;}.pdoc h1, .pdoc h2, .pdoc h3{font-weight:300;margin:.3em 0;padding:.2em 0;}.pdoc > section:not(.module-info) h1{font-size:1.5rem;font-weight:500;}.pdoc > section:not(.module-info) h2{font-size:1.4rem;font-weight:500;}.pdoc > section:not(.module-info) h3{font-size:1.3rem;font-weight:500;}.pdoc > section:not(.module-info) h4{font-size:1.2rem;}.pdoc > section:not(.module-info) h5{font-size:1.1rem;}.pdoc a{text-decoration:none;color:var(--link);}.pdoc a:hover{color:var(--link-hover);}.pdoc blockquote{margin-left:2rem;}.pdoc pre{border-top:1px solid var(--accent2);border-bottom:1px solid var(--accent2);margin-top:0;margin-bottom:1em;padding:.5rem 0 .5rem .5rem;overflow-x:auto;background-color:var(--code);}.pdoc code{color:var(--text);padding:.2em .4em;margin:0;font-size:85%;background-color:var(--accent);border-radius:6px;}.pdoc a > code{color:inherit;}.pdoc pre > code{display:inline-block;font-size:inherit;background:none;border:none;padding:0;}.pdoc > section:not(.module-info){margin-bottom:1.5rem;}.pdoc .modulename{margin-top:0;font-weight:bold;}.pdoc .modulename a{color:var(--link);transition:100ms all;}.pdoc .git-button{float:right;border:solid var(--link) 1px;}.pdoc .git-button:hover{background-color:var(--link);color:var(--pdoc-background);}.view-source-toggle-state,.view-source-toggle-state ~ .pdoc-code{display:none;}.view-source-toggle-state:checked ~ .pdoc-code{display:block;}.view-source-button{display:inline-block;float:right;font-size:.75rem;line-height:1.5rem;color:var(--muted);padding:0 .4rem 0 1.3rem;cursor:pointer;text-indent:-2px;}.view-source-button > span{visibility:hidden;}.module-info .view-source-button{float:none;display:flex;justify-content:flex-end;margin:-1.2rem .4rem -.2rem 0;}.view-source-button::before{position:absolute;content:"View Source";display:list-item;list-style-type:disclosure-closed;}.view-source-toggle-state:checked ~ .attr .view-source-button::before,.view-source-toggle-state:checked ~ .view-source-button::before{list-style-type:disclosure-open;}.pdoc .docstring{margin-bottom:1.5rem;}.pdoc section:not(.module-info) .docstring{margin-left:clamp(0rem, 5vw - 2rem, 1rem);}.pdoc .docstring .pdoc-code{margin-left:1em;margin-right:1em;}.pdoc h1:target,.pdoc h2:target,.pdoc h3:target,.pdoc h4:target,.pdoc h5:target,.pdoc h6:target,.pdoc .pdoc-code > pre > span:target{background-color:var(--active);box-shadow:-1rem 0 0 0 var(--active);}.pdoc .pdoc-code > pre > span:target{display:block;}.pdoc div:target > .attr,.pdoc section:target > .attr,.pdoc dd:target > a{background-color:var(--active);}.pdoc *{scroll-margin:2rem;}.pdoc .pdoc-code .linenos{user-select:none;}.pdoc .attr:hover{filter:contrast(0.95);}.pdoc section, .pdoc .classattr{position:relative;}.pdoc .headerlink{--width:clamp(1rem, 3vw, 2rem);position:absolute;top:0;left:calc(0rem - var(--width));transition:all 100ms ease-in-out;opacity:0;}.pdoc .headerlink::before{content:"#";display:block;text-align:center;width:var(--width);height:2.3rem;line-height:2.3rem;font-size:1.5rem;}.pdoc .attr:hover ~ .headerlink,.pdoc *:target > .headerlink,.pdoc .headerlink:hover{opacity:1;}.pdoc .attr{display:block;margin:.5rem 0 .5rem;padding:.4rem .4rem .4rem 1rem;background-color:var(--accent);overflow-x:auto;}.pdoc .classattr{margin-left:2rem;}.pdoc .decorator-deprecated{color:#842029;}.pdoc .decorator-deprecated ~ span{filter:grayscale(1) opacity(0.8);}.pdoc .name{color:var(--name);font-weight:bold;}.pdoc .def{color:var(--def);font-weight:bold;}.pdoc .signature{background-color:transparent;}.pdoc .param, .pdoc .return-annotation{white-space:pre;}.pdoc .signature.multiline .param{display:block;}.pdoc .signature.condensed .param{display:inline-block;}.pdoc .annotation{color:var(--annotation);}.pdoc .view-value-toggle-state,.pdoc .view-value-toggle-state ~ .default_value{display:none;}.pdoc .view-value-toggle-state:checked ~ .default_value{display:inherit;}.pdoc .view-value-button{font-size:.5rem;vertical-align:middle;border-style:dashed;margin-top:-0.1rem;}.pdoc .view-value-button:hover{background:white;}.pdoc .view-value-button::before{content:"show";text-align:center;width:2.2em;display:inline-block;}.pdoc .view-value-toggle-state:checked ~ .view-value-button::before{content:"hide";}.pdoc .inherited{margin-left:2rem;}.pdoc .inherited dt{font-weight:700;}.pdoc .inherited dt, .pdoc .inherited dd{display:inline;margin-left:0;margin-bottom:.5rem;}.pdoc .inherited dd:not(:last-child):after{content:", ";}.pdoc .inherited .class:before{content:"class ";}.pdoc .inherited .function a:after{content:"()";}.pdoc .search-result .docstring{overflow:auto;max-height:25vh;}.pdoc .search-result.focused > .attr{background-color:var(--active);}.pdoc .attribution{margin-top:2rem;display:block;opacity:0.5;transition:all 200ms;filter:grayscale(100%);}.pdoc .attribution:hover{opacity:1;filter:grayscale(0%);}.pdoc .attribution img{margin-left:5px;height:35px;vertical-align:middle;width:70px;transition:all 200ms;}.pdoc table{display:block;width:max-content;max-width:100%;overflow:auto;margin-bottom:1rem;}.pdoc table th{font-weight:600;}.pdoc table th, .pdoc table td{padding:6px 13px;border:1px solid var(--accent2);}</style>
    <style>/*! custom.css */</style><script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']]
        }
    };
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>
    /* Re-invoke MathJax when DOM content changes, for example during search. */
    document.addEventListener("DOMContentLoaded", () => {
        new MutationObserver(() => MathJax.typeset()).observe(
            document.querySelector("main.pdoc").parentNode,
            {childList: true}
        );
    })
</script>
<style>
    mjx-container {
        overflow-x: auto;
        overflow-y: hidden;
    }
</style></head>
<body>
    <nav class="pdoc">
        <label id="navtoggle" for="togglestate" class="pdoc-button"><svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 30 30'><path stroke-linecap='round' stroke="currentColor" stroke-miterlimit='10' stroke-width='2' d='M4 7h22M4 15h22M4 23h22'/></svg></label>
        <input id="togglestate" type="checkbox" aria-hidden="true" tabindex="-1">
        <div>            <a class="pdoc-button module-list-button" href="../cl_algorithms.html">
<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-box-arrow-in-left" viewBox="0 0 16 16">
  <path fill-rule="evenodd" d="M10 3.5a.5.5 0 0 0-.5-.5h-8a.5.5 0 0 0-.5.5v9a.5.5 0 0 0 .5.5h8a.5.5 0 0 0 .5-.5v-2a.5.5 0 0 1 1 0v2A1.5 1.5 0 0 1 9.5 14h-8A1.5 1.5 0 0 1 0 12.5v-9A1.5 1.5 0 0 1 1.5 2h8A1.5 1.5 0 0 1 11 3.5v2a.5.5 0 0 1-1 0v-2z"/>
  <path fill-rule="evenodd" d="M4.146 8.354a.5.5 0 0 1 0-.708l3-3a.5.5 0 1 1 .708.708L5.707 7.5H14.5a.5.5 0 0 1 0 1H5.707l2.147 2.146a.5.5 0 0 1-.708.708l-3-3z"/>
</svg>                &nbsp;clarena.cl_algorithms</a>


            <input type="search" placeholder="Search..." role="searchbox" aria-label="search"
                   pattern=".+" required>



            <h2>API Documentation</h2>
                <ul class="memberlist">
            <li>
                    <a class="class" href="#AdaHAT">AdaHAT</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#AdaHAT.__init__">AdaHAT</a>
                        </li>
                        <li>
                                <a class="variable" href="#AdaHAT.adjustment_intensity">adjustment_intensity</a>
                        </li>
                        <li>
                                <a class="variable" href="#AdaHAT.epsilon">epsilon</a>
                        </li>
                        <li>
                                <a class="variable" href="#AdaHAT.summative_mask_for_previous_tasks">summative_mask_for_previous_tasks</a>
                        </li>
                        <li>
                                <a class="variable" href="#AdaHAT.automatic_optimization">automatic_optimization</a>
                        </li>
                        <li>
                                <a class="function" href="#AdaHAT.sanity_check">sanity_check</a>
                        </li>
                        <li>
                                <a class="function" href="#AdaHAT.on_train_start">on_train_start</a>
                        </li>
                        <li>
                                <a class="function" href="#AdaHAT.clip_grad_by_adjustment">clip_grad_by_adjustment</a>
                        </li>
                        <li>
                                <a class="function" href="#AdaHAT.on_train_end">on_train_end</a>
                        </li>
                </ul>

            </li>
    </ul>



        <a class="attribution" title="pdoc: Python API documentation generator" href="https://pdoc.dev" target="_blank">
            built with <span class="visually-hidden">pdoc</span><img
                alt="pdoc logo"
                src="data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20role%3D%22img%22%20aria-label%3D%22pdoc%20logo%22%20width%3D%22300%22%20height%3D%22150%22%20viewBox%3D%22-1%200%2060%2030%22%3E%3Ctitle%3Epdoc%3C/title%3E%3Cpath%20d%3D%22M29.621%2021.293c-.011-.273-.214-.475-.511-.481a.5.5%200%200%200-.489.503l-.044%201.393c-.097.551-.695%201.215-1.566%201.704-.577.428-1.306.486-2.193.182-1.426-.617-2.467-1.654-3.304-2.487l-.173-.172a3.43%203.43%200%200%200-.365-.306.49.49%200%200%200-.286-.196c-1.718-1.06-4.931-1.47-7.353.191l-.219.15c-1.707%201.187-3.413%202.131-4.328%201.03-.02-.027-.49-.685-.141-1.763.233-.721.546-2.408.772-4.076.042-.09.067-.187.046-.288.166-1.347.277-2.625.241-3.351%201.378-1.008%202.271-2.586%202.271-4.362%200-.976-.272-1.935-.788-2.774-.057-.094-.122-.18-.184-.268.033-.167.052-.339.052-.516%200-1.477-1.202-2.679-2.679-2.679-.791%200-1.496.352-1.987.9a6.3%206.3%200%200%200-1.001.029c-.492-.564-1.207-.929-2.012-.929-1.477%200-2.679%201.202-2.679%202.679A2.65%202.65%200%200%200%20.97%206.554c-.383.747-.595%201.572-.595%202.41%200%202.311%201.507%204.29%203.635%205.107-.037.699-.147%202.27-.423%203.294l-.137.461c-.622%202.042-2.515%208.257%201.727%2010.643%201.614.908%203.06%201.248%204.317%201.248%202.665%200%204.492-1.524%205.322-2.401%201.476-1.559%202.886-1.854%206.491.82%201.877%201.393%203.514%201.753%204.861%201.068%202.223-1.713%202.811-3.867%203.399-6.374.077-.846.056-1.469.054-1.537zm-4.835%204.313c-.054.305-.156.586-.242.629-.034-.007-.131-.022-.307-.157-.145-.111-.314-.478-.456-.908.221.121.432.25.675.355.115.039.219.051.33.081zm-2.251-1.238c-.05.33-.158.648-.252.694-.022.001-.125-.018-.307-.157-.217-.166-.488-.906-.639-1.573.358.344.754.693%201.198%201.036zm-3.887-2.337c-.006-.116-.018-.231-.041-.342.635.145%201.189.368%201.599.625.097.231.166.481.174.642-.03.049-.055.101-.067.158-.046.013-.128.026-.298.004-.278-.037-.901-.57-1.367-1.087zm-1.127-.497c.116.306.176.625.12.71-.019.014-.117.045-.345.016-.206-.027-.604-.332-.986-.695.41-.051.816-.056%201.211-.031zm-4.535%201.535c.209.22.379.47.358.598-.006.041-.088.138-.351.234-.144.055-.539-.063-.979-.259a11.66%2011.66%200%200%200%20.972-.573zm.983-.664c.359-.237.738-.418%201.126-.554.25.237.479.548.457.694-.006.042-.087.138-.351.235-.174.064-.694-.105-1.232-.375zm-3.381%201.794c-.022.145-.061.29-.149.401-.133.166-.358.248-.69.251h-.002c-.133%200-.306-.26-.45-.621.417.091.854.07%201.291-.031zm-2.066-8.077a4.78%204.78%200%200%201-.775-.584c.172-.115.505-.254.88-.378l-.105.962zm-.331%202.302a10.32%2010.32%200%200%201-.828-.502c.202-.143.576-.328.984-.49l-.156.992zm-.45%202.157l-.701-.403c.214-.115.536-.249.891-.376a11.57%2011.57%200%200%201-.19.779zm-.181%201.716c.064.398.194.702.298.893-.194-.051-.435-.162-.736-.398.061-.119.224-.3.438-.495zM8.87%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zm-.735-.389a1.15%201.15%200%200%200-.314.783%201.16%201.16%200%200%200%201.162%201.162c.457%200%20.842-.27%201.032-.653.026.117.042.238.042.362a1.68%201.68%200%200%201-1.679%201.679%201.68%201.68%200%200%201-1.679-1.679c0-.843.626-1.535%201.436-1.654zM5.059%205.406A1.68%201.68%200%200%201%203.38%207.085a1.68%201.68%200%200%201-1.679-1.679c0-.037.009-.072.011-.109.21.3.541.508.935.508a1.16%201.16%200%200%200%201.162-1.162%201.14%201.14%200%200%200-.474-.912c.015%200%20.03-.005.045-.005.926.001%201.679.754%201.679%201.68zM3.198%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zM1.375%208.964c0-.52.103-1.035.288-1.52.466.394%201.06.64%201.717.64%201.144%200%202.116-.725%202.499-1.738.383%201.012%201.355%201.738%202.499%201.738.867%200%201.631-.421%202.121-1.062.307.605.478%201.267.478%201.942%200%202.486-2.153%204.51-4.801%204.51s-4.801-2.023-4.801-4.51zm24.342%2019.349c-.985.498-2.267.168-3.813-.979-3.073-2.281-5.453-3.199-7.813-.705-1.315%201.391-4.163%203.365-8.423.97-3.174-1.786-2.239-6.266-1.261-9.479l.146-.492c.276-1.02.395-2.457.444-3.268a6.11%206.11%200%200%200%201.18.115%206.01%206.01%200%200%200%202.536-.562l-.006.175c-.802.215-1.848.612-2.021%201.25-.079.295.021.601.274.837.219.203.415.364.598.501-.667.304-1.243.698-1.311%201.179-.02.144-.022.507.393.787.213.144.395.26.564.365-1.285.521-1.361.96-1.381%201.126-.018.142-.011.496.427.746l.854.489c-.473.389-.971.914-.999%201.429-.018.278.095.532.316.713.675.556%201.231.721%201.653.721.059%200%20.104-.014.158-.02.207.707.641%201.64%201.513%201.64h.013c.8-.008%201.236-.345%201.462-.626.173-.216.268-.457.325-.692.424.195.93.374%201.372.374.151%200%20.294-.021.423-.068.732-.27.944-.704.993-1.021.009-.061.003-.119.002-.179.266.086.538.147.789.147.15%200%20.294-.021.423-.069.542-.2.797-.489.914-.754.237.147.478.258.704.288.106.014.205.021.296.021.356%200%20.595-.101.767-.229.438.435%201.094.992%201.656%201.067.106.014.205.021.296.021a1.56%201.56%200%200%200%20.323-.035c.17.575.453%201.289.866%201.605.358.273.665.362.914.362a.99.99%200%200%200%20.421-.093%201.03%201.03%200%200%200%20.245-.164c.168.428.39.846.68%201.068.358.273.665.362.913.362a.99.99%200%200%200%20.421-.093c.317-.148.512-.448.639-.762.251.157.495.257.726.257.127%200%20.25-.024.37-.071.427-.17.706-.617.841-1.314.022-.015.047-.022.068-.038.067-.051.133-.104.196-.159-.443%201.486-1.107%202.761-2.086%203.257zM8.66%209.925a.5.5%200%201%200-1%200c0%20.653-.818%201.205-1.787%201.205s-1.787-.552-1.787-1.205a.5.5%200%201%200-1%200c0%201.216%201.25%202.205%202.787%202.205s2.787-.989%202.787-2.205zm4.4%2015.965l-.208.097c-2.661%201.258-4.708%201.436-6.086.527-1.542-1.017-1.88-3.19-1.844-4.198a.4.4%200%200%200-.385-.414c-.242-.029-.406.164-.414.385-.046%201.249.367%203.686%202.202%204.896.708.467%201.547.7%202.51.7%201.248%200%202.706-.392%204.362-1.174l.185-.086a.4.4%200%200%200%20.205-.527c-.089-.204-.326-.291-.527-.206zM9.547%202.292c.093.077.205.114.317.114a.5.5%200%200%200%20.318-.886L8.817.397a.5.5%200%200%200-.703.068.5.5%200%200%200%20.069.703l1.364%201.124zm-7.661-.065c.086%200%20.173-.022.253-.068l1.523-.893a.5.5%200%200%200-.506-.863l-1.523.892a.5.5%200%200%200-.179.685c.094.158.261.247.432.247z%22%20transform%3D%22matrix%28-1%200%200%201%2058%200%29%22%20fill%3D%22%233bb300%22/%3E%3Cpath%20d%3D%22M.3%2021.86V10.18q0-.46.02-.68.04-.22.18-.5.28-.54%201.34-.54%201.06%200%201.42.28.38.26.44.78.76-1.04%202.38-1.04%201.64%200%203.1%201.54%201.46%201.54%201.46%203.58%200%202.04-1.46%203.58-1.44%201.54-3.08%201.54-1.64%200-2.38-.92v4.04q0%20.46-.04.68-.02.22-.18.5-.14.3-.5.42-.36.12-.98.12-.62%200-1-.12-.36-.12-.52-.4-.14-.28-.18-.5-.02-.22-.02-.68zm3.96-9.42q-.46.54-.46%201.18%200%20.64.46%201.18.48.52%201.2.52.74%200%201.24-.52.52-.52.52-1.18%200-.66-.48-1.18-.48-.54-1.26-.54-.76%200-1.22.54zm14.741-8.36q.16-.3.54-.42.38-.12%201-.12.64%200%201.02.12.38.12.52.42.16.3.18.54.04.22.04.68v11.94q0%20.46-.04.7-.02.22-.18.5-.3.54-1.7.54-1.38%200-1.54-.98-.84.96-2.34.96-1.8%200-3.28-1.56-1.48-1.58-1.48-3.66%200-2.1%201.48-3.68%201.5-1.58%203.28-1.58%201.48%200%202.3%201v-4.2q0-.46.02-.68.04-.24.18-.52zm-3.24%2010.86q.52.54%201.26.54.74%200%201.22-.54.5-.54.5-1.18%200-.66-.48-1.22-.46-.56-1.26-.56-.8%200-1.28.56-.48.54-.48%201.2%200%20.66.52%201.2zm7.833-1.2q0-2.4%201.68-3.96%201.68-1.56%203.84-1.56%202.16%200%203.82%201.56%201.66%201.54%201.66%203.94%200%201.66-.86%202.96-.86%201.28-2.1%201.9-1.22.6-2.54.6-1.32%200-2.56-.64-1.24-.66-2.1-1.92-.84-1.28-.84-2.88zm4.18%201.44q.64.48%201.3.48.66%200%201.32-.5.66-.5.66-1.48%200-.98-.62-1.46-.62-.48-1.34-.48-.72%200-1.34.5-.62.5-.62%201.48%200%20.96.64%201.46zm11.412-1.44q0%20.84.56%201.32.56.46%201.18.46.64%200%201.18-.36.56-.38.9-.38.6%200%201.46%201.06.46.58.46%201.04%200%20.76-1.1%201.42-1.14.8-2.8.8-1.86%200-3.58-1.34-.82-.64-1.34-1.7-.52-1.08-.52-2.36%200-1.3.52-2.34.52-1.06%201.34-1.7%201.66-1.32%203.54-1.32.76%200%201.48.22.72.2%201.06.4l.32.2q.36.24.56.38.52.4.52.92%200%20.5-.42%201.14-.72%201.1-1.38%201.1-.38%200-1.08-.44-.36-.34-1.04-.34-.66%200-1.24.48-.58.48-.58%201.34z%22%20fill%3D%22green%22/%3E%3C/svg%3E"/>
        </a>
</div>
    </nav>
    <main class="pdoc">
            <section class="module-info">
                    <h1 class="modulename">
<a href="./../../clarena.html">clarena</a><wbr>.<a href="./../cl_algorithms.html">cl_algorithms</a><wbr>.adahat    </h1>

                        <div class="docstring"><p>The submodule in <code>cl_algorithms</code> for <a href="https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9">AdaHAT (Adaptive Hard Attention to the Task)</a> algorithm.</p>
</div>

                        <input id="mod-adahat-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">

                        <label class="view-source-button" for="mod-adahat-view-source"><span>View Source</span></label>

                        <div class="pdoc-code codehilite"><pre><span></span><span id="L-1"><a href="#L-1"><span class="linenos">  1</span></a><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-2"><a href="#L-2"><span class="linenos">  2</span></a><span class="sd">The submodule in `cl_algorithms` for [AdaHAT (Adaptive Hard Attention to the Task)](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9) algorithm.</span>
</span><span id="L-3"><a href="#L-3"><span class="linenos">  3</span></a><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-4"><a href="#L-4"><span class="linenos">  4</span></a>
</span><span id="L-5"><a href="#L-5"><span class="linenos">  5</span></a><span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;AdaHAT&quot;</span><span class="p">]</span>
</span><span id="L-6"><a href="#L-6"><span class="linenos">  6</span></a>
</span><span id="L-7"><a href="#L-7"><span class="linenos">  7</span></a><span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
</span><span id="L-8"><a href="#L-8"><span class="linenos">  8</span></a>
</span><span id="L-9"><a href="#L-9"><span class="linenos">  9</span></a><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span id="L-10"><a href="#L-10"><span class="linenos"> 10</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tensor</span>
</span><span id="L-11"><a href="#L-11"><span class="linenos"> 11</span></a>
</span><span id="L-12"><a href="#L-12"><span class="linenos"> 12</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">clarena.backbones</span><span class="w"> </span><span class="kn">import</span> <span class="n">HATMaskBackbone</span>
</span><span id="L-13"><a href="#L-13"><span class="linenos"> 13</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">clarena.cl_algorithms</span><span class="w"> </span><span class="kn">import</span> <span class="n">HAT</span>
</span><span id="L-14"><a href="#L-14"><span class="linenos"> 14</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">clarena.heads</span><span class="w"> </span><span class="kn">import</span> <span class="n">HeadsTIL</span>
</span><span id="L-15"><a href="#L-15"><span class="linenos"> 15</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">clarena.utils.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">HATNetworkCapacityMetric</span>
</span><span id="L-16"><a href="#L-16"><span class="linenos"> 16</span></a>
</span><span id="L-17"><a href="#L-17"><span class="linenos"> 17</span></a><span class="c1"># always get logger for built-in logging in each module</span>
</span><span id="L-18"><a href="#L-18"><span class="linenos"> 18</span></a><span class="n">pylogger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>
</span><span id="L-19"><a href="#L-19"><span class="linenos"> 19</span></a>
</span><span id="L-20"><a href="#L-20"><span class="linenos"> 20</span></a>
</span><span id="L-21"><a href="#L-21"><span class="linenos"> 21</span></a><span class="k">class</span><span class="w"> </span><span class="nc">AdaHAT</span><span class="p">(</span><span class="n">HAT</span><span class="p">):</span>
</span><span id="L-22"><a href="#L-22"><span class="linenos"> 22</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;[AdaHAT (Adaptive Hard Attention to the Task)](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9) algorithm.</span>
</span><span id="L-23"><a href="#L-23"><span class="linenos"> 23</span></a>
</span><span id="L-24"><a href="#L-24"><span class="linenos"> 24</span></a><span class="sd">    An architecture-based continual learning approach that improves [HAT (Hard Attention to the Task)](http://proceedings.mlr.press/v80/serra18a) by introducing adaptive soft gradient clipping based on parameter importance and network sparsity.</span>
</span><span id="L-25"><a href="#L-25"><span class="linenos"> 25</span></a>
</span><span id="L-26"><a href="#L-26"><span class="linenos"> 26</span></a><span class="sd">    We implement AdaHAT as a subclass of HAT, as it shares the same `forward()`, `compensate_task_embedding_gradients()`, `training_step()`, `on_train_end()`, `validation_step()`, and `test_step()` methods as the `HAT` class.</span>
</span><span id="L-27"><a href="#L-27"><span class="linenos"> 27</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-28"><a href="#L-28"><span class="linenos"> 28</span></a>
</span><span id="L-29"><a href="#L-29"><span class="linenos"> 29</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-30"><a href="#L-30"><span class="linenos"> 30</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-31"><a href="#L-31"><span class="linenos"> 31</span></a>        <span class="n">backbone</span><span class="p">:</span> <span class="n">HATMaskBackbone</span><span class="p">,</span>
</span><span id="L-32"><a href="#L-32"><span class="linenos"> 32</span></a>        <span class="n">heads</span><span class="p">:</span> <span class="n">HeadsTIL</span><span class="p">,</span>
</span><span id="L-33"><a href="#L-33"><span class="linenos"> 33</span></a>        <span class="n">adjustment_mode</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-34"><a href="#L-34"><span class="linenos"> 34</span></a>        <span class="n">adjustment_intensity</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="L-35"><a href="#L-35"><span class="linenos"> 35</span></a>        <span class="n">s_max</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="L-36"><a href="#L-36"><span class="linenos"> 36</span></a>        <span class="n">clamp_threshold</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="L-37"><a href="#L-37"><span class="linenos"> 37</span></a>        <span class="n">mask_sparsity_reg_factor</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="L-38"><a href="#L-38"><span class="linenos"> 38</span></a>        <span class="n">mask_sparsity_reg_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;original&quot;</span><span class="p">,</span>
</span><span id="L-39"><a href="#L-39"><span class="linenos"> 39</span></a>        <span class="n">task_embedding_init_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;N01&quot;</span><span class="p">,</span>
</span><span id="L-40"><a href="#L-40"><span class="linenos"> 40</span></a>        <span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
</span><span id="L-41"><a href="#L-41"><span class="linenos"> 41</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-42"><a href="#L-42"><span class="linenos"> 42</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialize the AdaHAT algorithm with the network.</span>
</span><span id="L-43"><a href="#L-43"><span class="linenos"> 43</span></a>
</span><span id="L-44"><a href="#L-44"><span class="linenos"> 44</span></a><span class="sd">        **Args:**</span>
</span><span id="L-45"><a href="#L-45"><span class="linenos"> 45</span></a><span class="sd">        - **backbone** (`HATMaskBackbone`): must be a backbone network with the HAT mask mechanism.</span>
</span><span id="L-46"><a href="#L-46"><span class="linenos"> 46</span></a><span class="sd">        - **heads** (`HeadsTIL`): output heads. AdaHAT supports only TIL (Task-Incremental Learning).</span>
</span><span id="L-47"><a href="#L-47"><span class="linenos"> 47</span></a><span class="sd">        - **adjustment_mode** (`str`): the strategy of adjustment (i.e., the mode of gradient clipping), must be one of:</span>
</span><span id="L-48"><a href="#L-48"><span class="linenos"> 48</span></a><span class="sd">            1. &#39;adahat&#39;: set gradients of parameters linking to masked units to a soft adjustment rate in the original AdaHAT approach (allows slight updates on previous-task parameters). See Eqs. (8) and (9) in Sec. 3.1 of the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="L-49"><a href="#L-49"><span class="linenos"> 49</span></a><span class="sd">            2. &#39;adahat_no_sum&#39;: as above but without parameter-importance (i.e., no summative mask). See Sec. 4.3 (ablation study) in the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="L-50"><a href="#L-50"><span class="linenos"> 50</span></a><span class="sd">            3. &#39;adahat_no_reg&#39;: as above but without network sparsity (i.e., no mask sparsity regularization term). See Sec. 4.3 (ablation study) in the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="L-51"><a href="#L-51"><span class="linenos"> 51</span></a><span class="sd">        - **adjustment_intensity** (`float`): hyperparameter, controls the overall intensity of gradient adjustment (the $\alpha$ in Eq. (9) of the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9)).</span>
</span><span id="L-52"><a href="#L-52"><span class="linenos"> 52</span></a><span class="sd">        - **s_max** (`float`): hyperparameter, the maximum scaling factor in the gate function. See Sec. 2.4 &quot;Hard Attention Training&quot; in the [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="L-53"><a href="#L-53"><span class="linenos"> 53</span></a><span class="sd">        - **clamp_threshold** (`float`): the threshold for task embedding gradient compensation. See Sec. 2.5 &quot;Embedding Gradient Compensation&quot; in the [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="L-54"><a href="#L-54"><span class="linenos"> 54</span></a><span class="sd">        - **mask_sparsity_reg_factor** (`float`): hyperparameter, the regularization factor for mask sparsity.</span>
</span><span id="L-55"><a href="#L-55"><span class="linenos"> 55</span></a><span class="sd">        - **mask_sparsity_reg_mode** (`str`): the mode of mask sparsity regularization, must be one of:</span>
</span><span id="L-56"><a href="#L-56"><span class="linenos"> 56</span></a><span class="sd">            1. &#39;original&#39; (default): the original mask sparsity regularization in the [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="L-57"><a href="#L-57"><span class="linenos"> 57</span></a><span class="sd">            2. &#39;cross&#39;: the cross version of mask sparsity regularization.</span>
</span><span id="L-58"><a href="#L-58"><span class="linenos"> 58</span></a><span class="sd">        - **task_embedding_init_mode** (`str`): the initialization mode for task embeddings, must be one of:</span>
</span><span id="L-59"><a href="#L-59"><span class="linenos"> 59</span></a><span class="sd">            1. &#39;N01&#39; (default): standard normal distribution $N(0, 1)$.</span>
</span><span id="L-60"><a href="#L-60"><span class="linenos"> 60</span></a><span class="sd">            2. &#39;U-11&#39;: uniform distribution $U(-1, 1)$.</span>
</span><span id="L-61"><a href="#L-61"><span class="linenos"> 61</span></a><span class="sd">            3. &#39;U01&#39;: uniform distribution $U(0, 1)$.</span>
</span><span id="L-62"><a href="#L-62"><span class="linenos"> 62</span></a><span class="sd">            4. &#39;U-10&#39;: uniform distribution $U(-1, 0)$.</span>
</span><span id="L-63"><a href="#L-63"><span class="linenos"> 63</span></a><span class="sd">            5. &#39;last&#39;: inherit the task embedding from the last task.</span>
</span><span id="L-64"><a href="#L-64"><span class="linenos"> 64</span></a><span class="sd">        - **epsilon** (`float`): the value added to network sparsity to avoid division by zero (appearing in Eq. (9) of the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9)).</span>
</span><span id="L-65"><a href="#L-65"><span class="linenos"> 65</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-66"><a href="#L-66"><span class="linenos"> 66</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-67"><a href="#L-67"><span class="linenos"> 67</span></a>            <span class="n">backbone</span><span class="o">=</span><span class="n">backbone</span><span class="p">,</span>
</span><span id="L-68"><a href="#L-68"><span class="linenos"> 68</span></a>            <span class="n">heads</span><span class="o">=</span><span class="n">heads</span><span class="p">,</span>
</span><span id="L-69"><a href="#L-69"><span class="linenos"> 69</span></a>            <span class="n">adjustment_mode</span><span class="o">=</span><span class="n">adjustment_mode</span><span class="p">,</span>
</span><span id="L-70"><a href="#L-70"><span class="linenos"> 70</span></a>            <span class="n">s_max</span><span class="o">=</span><span class="n">s_max</span><span class="p">,</span>
</span><span id="L-71"><a href="#L-71"><span class="linenos"> 71</span></a>            <span class="n">clamp_threshold</span><span class="o">=</span><span class="n">clamp_threshold</span><span class="p">,</span>
</span><span id="L-72"><a href="#L-72"><span class="linenos"> 72</span></a>            <span class="n">mask_sparsity_reg_factor</span><span class="o">=</span><span class="n">mask_sparsity_reg_factor</span><span class="p">,</span>
</span><span id="L-73"><a href="#L-73"><span class="linenos"> 73</span></a>            <span class="n">mask_sparsity_reg_mode</span><span class="o">=</span><span class="n">mask_sparsity_reg_mode</span><span class="p">,</span>
</span><span id="L-74"><a href="#L-74"><span class="linenos"> 74</span></a>            <span class="n">task_embedding_init_mode</span><span class="o">=</span><span class="n">task_embedding_init_mode</span><span class="p">,</span>
</span><span id="L-75"><a href="#L-75"><span class="linenos"> 75</span></a>            <span class="n">alpha</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="L-76"><a href="#L-76"><span class="linenos"> 76</span></a>        <span class="p">)</span>
</span><span id="L-77"><a href="#L-77"><span class="linenos"> 77</span></a>
</span><span id="L-78"><a href="#L-78"><span class="linenos"> 78</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">adjustment_intensity</span>
</span><span id="L-79"><a href="#L-79"><span class="linenos"> 79</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The adjustment intensity in Eq. (9) of the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).&quot;&quot;&quot;</span>
</span><span id="L-80"><a href="#L-80"><span class="linenos"> 80</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">epsilon</span>
</span><span id="L-81"><a href="#L-81"><span class="linenos"> 81</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The small value to avoid division by zero (appearing in Eq. (9) of the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9)).&quot;&quot;&quot;</span>
</span><span id="L-82"><a href="#L-82"><span class="linenos"> 82</span></a>
</span><span id="L-83"><a href="#L-83"><span class="linenos"> 83</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">summative_mask_for_previous_tasks</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="L-84"><a href="#L-84"><span class="linenos"> 84</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The summative binary attention mask $\mathrm{M}^{&lt;t,\text{sum}}$ of previous tasks $1,\cdots, t-1$, gated from the task embedding. It is a dict where keys are layer names and values are the binary mask tensors for the layers. The mask tensor has size (number of units, ).&quot;&quot;&quot;</span>
</span><span id="L-85"><a href="#L-85"><span class="linenos"> 85</span></a>
</span><span id="L-86"><a href="#L-86"><span class="linenos"> 86</span></a>        <span class="c1"># set manual optimization</span>
</span><span id="L-87"><a href="#L-87"><span class="linenos"> 87</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">automatic_optimization</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="L-88"><a href="#L-88"><span class="linenos"> 88</span></a>
</span><span id="L-89"><a href="#L-89"><span class="linenos"> 89</span></a>        <span class="n">AdaHAT</span><span class="o">.</span><span class="n">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</span><span id="L-90"><a href="#L-90"><span class="linenos"> 90</span></a>
</span><span id="L-91"><a href="#L-91"><span class="linenos"> 91</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-92"><a href="#L-92"><span class="linenos"> 92</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Check the sanity of the arguments.&quot;&quot;&quot;</span>
</span><span id="L-93"><a href="#L-93"><span class="linenos"> 93</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="L-94"><a href="#L-94"><span class="linenos"> 94</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="L-95"><a href="#L-95"><span class="linenos"> 95</span></a>                <span class="sa">f</span><span class="s2">&quot;The adjustment intensity should be positive, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span><span class="si">}</span><span class="s2">.&quot;</span>
</span><span id="L-96"><a href="#L-96"><span class="linenos"> 96</span></a>            <span class="p">)</span>
</span><span id="L-97"><a href="#L-97"><span class="linenos"> 97</span></a>
</span><span id="L-98"><a href="#L-98"><span class="linenos"> 98</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_start</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-99"><a href="#L-99"><span class="linenos"> 99</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Additionally initialize the summative mask at the beginning of the first task.&quot;&quot;&quot;</span>
</span><span id="L-100"><a href="#L-100"><span class="linenos">100</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">on_train_start</span><span class="p">()</span>
</span><span id="L-101"><a href="#L-101"><span class="linenos">101</span></a>
</span><span id="L-102"><a href="#L-102"><span class="linenos">102</span></a>        <span class="c1"># initialize the summative mask at the beginning of the first task. This should not be called in `__init__()` method because `self.device` is not available at that time</span>
</span><span id="L-103"><a href="#L-103"><span class="linenos">103</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="L-104"><a href="#L-104"><span class="linenos">104</span></a>            <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="L-105"><a href="#L-105"><span class="linenos">105</span></a>                <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span>
</span><span id="L-106"><a href="#L-106"><span class="linenos">106</span></a>                    <span class="n">layer_name</span>
</span><span id="L-107"><a href="#L-107"><span class="linenos">107</span></a>                <span class="p">)</span>  <span class="c1"># get the layer by its name</span>
</span><span id="L-108"><a href="#L-108"><span class="linenos">108</span></a>                <span class="n">num_units</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="L-109"><a href="#L-109"><span class="linenos">109</span></a>
</span><span id="L-110"><a href="#L-110"><span class="linenos">110</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">summative_mask_for_previous_tasks</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
</span><span id="L-111"><a href="#L-111"><span class="linenos">111</span></a>                    <span class="n">num_units</span>
</span><span id="L-112"><a href="#L-112"><span class="linenos">112</span></a>                <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
</span><span id="L-113"><a href="#L-113"><span class="linenos">113</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
</span><span id="L-114"><a href="#L-114"><span class="linenos">114</span></a>                <span class="p">)</span>  <span class="c1"># the summative mask $\mathrm{M}^{&lt;t,\text{sum}}$ is initialized as a zeros mask for $t = 1$. See Eq. (7) in Sec. 3.1 of the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9)</span>
</span><span id="L-115"><a href="#L-115"><span class="linenos">115</span></a>
</span><span id="L-116"><a href="#L-116"><span class="linenos">116</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">clip_grad_by_adjustment</span><span class="p">(</span>
</span><span id="L-117"><a href="#L-117"><span class="linenos">117</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-118"><a href="#L-118"><span class="linenos">118</span></a>        <span class="n">network_sparsity</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-119"><a href="#L-119"><span class="linenos">119</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="n">Tensor</span><span class="p">]:</span>
</span><span id="L-120"><a href="#L-120"><span class="linenos">120</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Clip the gradients by the adjustment rate. See Eq. (8) in the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="L-121"><a href="#L-121"><span class="linenos">121</span></a>
</span><span id="L-122"><a href="#L-122"><span class="linenos">122</span></a><span class="sd">        Note that because the task embedding fully covers every layer in the backbone network, no parameters are left out of this system. This applies not only to parameters between layers with task embeddings, but also to those before the first layer. We design it separately in the code.</span>
</span><span id="L-123"><a href="#L-123"><span class="linenos">123</span></a>
</span><span id="L-124"><a href="#L-124"><span class="linenos">124</span></a><span class="sd">        Network capacity is measured alongside this method. Network capacity is defined as the average adjustment rate over all parameters. See Sec. 4.1 in the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="L-125"><a href="#L-125"><span class="linenos">125</span></a>
</span><span id="L-126"><a href="#L-126"><span class="linenos">126</span></a><span class="sd">        **Args:**</span>
</span><span id="L-127"><a href="#L-127"><span class="linenos">127</span></a><span class="sd">        - **network_sparsity** (`dict[str, Tensor]` | `None`): the network sparsity (i.e., the mask sparsity loss of each layer) for the current task. Keys are layer names and values are the network sparsity values. It is used to calculate the adjustment rate for gradients. Applies only to mode `adahat` and `adahat_no_sum`, not `adahat_no_reg`.</span>
</span><span id="L-128"><a href="#L-128"><span class="linenos">128</span></a>
</span><span id="L-129"><a href="#L-129"><span class="linenos">129</span></a><span class="sd">        **Returns:**</span>
</span><span id="L-130"><a href="#L-130"><span class="linenos">130</span></a><span class="sd">        - **adjustment_rate_weight** (`dict[str, Tensor]`): the adjustment rate for weights. Keys (`str`) are layer names and values (`Tensor`) are the adjustment rate tensors.</span>
</span><span id="L-131"><a href="#L-131"><span class="linenos">131</span></a><span class="sd">        - **adjustment_rate_bias** (`dict[str, Tensor]`): the adjustment rate for biases. Keys (`str`) are layer names and values (`Tensor`) are the adjustment rate tensors.</span>
</span><span id="L-132"><a href="#L-132"><span class="linenos">132</span></a><span class="sd">        - **capacity** (`Tensor`): the calculated network capacity.</span>
</span><span id="L-133"><a href="#L-133"><span class="linenos">133</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-134"><a href="#L-134"><span class="linenos">134</span></a>
</span><span id="L-135"><a href="#L-135"><span class="linenos">135</span></a>        <span class="c1"># initialize network capacity metric</span>
</span><span id="L-136"><a href="#L-136"><span class="linenos">136</span></a>        <span class="n">capacity</span> <span class="o">=</span> <span class="n">HATNetworkCapacityMetric</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="L-137"><a href="#L-137"><span class="linenos">137</span></a>        <span class="n">adjustment_rate_weight</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="L-138"><a href="#L-138"><span class="linenos">138</span></a>        <span class="n">adjustment_rate_bias</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="L-139"><a href="#L-139"><span class="linenos">139</span></a>
</span><span id="L-140"><a href="#L-140"><span class="linenos">140</span></a>        <span class="c1"># calculate the adjustment rate for gradients of the parameters, both weights and biases (if they exist). See Eq. (9) in the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9)</span>
</span><span id="L-141"><a href="#L-141"><span class="linenos">141</span></a>        <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="L-142"><a href="#L-142"><span class="linenos">142</span></a>
</span><span id="L-143"><a href="#L-143"><span class="linenos">143</span></a>            <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span>
</span><span id="L-144"><a href="#L-144"><span class="linenos">144</span></a>                <span class="n">layer_name</span>
</span><span id="L-145"><a href="#L-145"><span class="linenos">145</span></a>            <span class="p">)</span>  <span class="c1"># get the layer by its name</span>
</span><span id="L-146"><a href="#L-146"><span class="linenos">146</span></a>
</span><span id="L-147"><a href="#L-147"><span class="linenos">147</span></a>            <span class="c1"># placeholder for the adjustment rate to avoid the error of using it before assignment</span>
</span><span id="L-148"><a href="#L-148"><span class="linenos">148</span></a>            <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="L-149"><a href="#L-149"><span class="linenos">149</span></a>            <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="L-150"><a href="#L-150"><span class="linenos">150</span></a>
</span><span id="L-151"><a href="#L-151"><span class="linenos">151</span></a>            <span class="n">weight_importance</span><span class="p">,</span> <span class="n">bias_importance</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-152"><a href="#L-152"><span class="linenos">152</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_measure_parameter_wise</span><span class="p">(</span>
</span><span id="L-153"><a href="#L-153"><span class="linenos">153</span></a>                    <span class="n">unit_wise_measure</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">summative_mask_for_previous_tasks</span><span class="p">,</span>
</span><span id="L-154"><a href="#L-154"><span class="linenos">154</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="L-155"><a href="#L-155"><span class="linenos">155</span></a>                    <span class="n">aggregation_mode</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span>
</span><span id="L-156"><a href="#L-156"><span class="linenos">156</span></a>                <span class="p">)</span>
</span><span id="L-157"><a href="#L-157"><span class="linenos">157</span></a>            <span class="p">)</span>  <span class="c1"># AdaHAT depends on parameter importance rather than parameter masks (as in HAT)</span>
</span><span id="L-158"><a href="#L-158"><span class="linenos">158</span></a>
</span><span id="L-159"><a href="#L-159"><span class="linenos">159</span></a>            <span class="n">network_sparsity_layer</span> <span class="o">=</span> <span class="n">network_sparsity</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="L-160"><a href="#L-160"><span class="linenos">160</span></a>
</span><span id="L-161"><a href="#L-161"><span class="linenos">161</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;adahat&quot;</span><span class="p">:</span>
</span><span id="L-162"><a href="#L-162"><span class="linenos">162</span></a>                <span class="n">r_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span> <span class="o">/</span> <span class="p">(</span>
</span><span id="L-163"><a href="#L-163"><span class="linenos">163</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">+</span> <span class="n">network_sparsity_layer</span>
</span><span id="L-164"><a href="#L-164"><span class="linenos">164</span></a>                <span class="p">)</span>
</span><span id="L-165"><a href="#L-165"><span class="linenos">165</span></a>                <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span>
</span><span id="L-166"><a href="#L-166"><span class="linenos">166</span></a>                    <span class="n">r_layer</span><span class="p">,</span> <span class="p">(</span><span class="n">weight_importance</span> <span class="o">+</span> <span class="n">r_layer</span><span class="p">)</span>
</span><span id="L-167"><a href="#L-167"><span class="linenos">167</span></a>                <span class="p">)</span>
</span><span id="L-168"><a href="#L-168"><span class="linenos">168</span></a>                <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span>
</span><span id="L-169"><a href="#L-169"><span class="linenos">169</span></a>                    <span class="n">r_layer</span><span class="p">,</span> <span class="p">(</span><span class="n">bias_importance</span> <span class="o">+</span> <span class="n">r_layer</span><span class="p">)</span>
</span><span id="L-170"><a href="#L-170"><span class="linenos">170</span></a>                <span class="p">)</span>
</span><span id="L-171"><a href="#L-171"><span class="linenos">171</span></a>
</span><span id="L-172"><a href="#L-172"><span class="linenos">172</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;adahat_no_sum&quot;</span><span class="p">:</span>
</span><span id="L-173"><a href="#L-173"><span class="linenos">173</span></a>
</span><span id="L-174"><a href="#L-174"><span class="linenos">174</span></a>                <span class="n">r_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span> <span class="o">/</span> <span class="p">(</span>
</span><span id="L-175"><a href="#L-175"><span class="linenos">175</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">+</span> <span class="n">network_sparsity_layer</span>
</span><span id="L-176"><a href="#L-176"><span class="linenos">176</span></a>                <span class="p">)</span>
</span><span id="L-177"><a href="#L-177"><span class="linenos">177</span></a>                <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span>
</span><span id="L-178"><a href="#L-178"><span class="linenos">178</span></a>                    <span class="n">r_layer</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">+</span> <span class="n">r_layer</span><span class="p">)</span>
</span><span id="L-179"><a href="#L-179"><span class="linenos">179</span></a>                <span class="p">)</span>
</span><span id="L-180"><a href="#L-180"><span class="linenos">180</span></a>                <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span>
</span><span id="L-181"><a href="#L-181"><span class="linenos">181</span></a>                    <span class="n">r_layer</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">+</span> <span class="n">r_layer</span><span class="p">)</span>
</span><span id="L-182"><a href="#L-182"><span class="linenos">182</span></a>                <span class="p">)</span>
</span><span id="L-183"><a href="#L-183"><span class="linenos">183</span></a>
</span><span id="L-184"><a href="#L-184"><span class="linenos">184</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;adahat_no_reg&quot;</span><span class="p">:</span>
</span><span id="L-185"><a href="#L-185"><span class="linenos">185</span></a>
</span><span id="L-186"><a href="#L-186"><span class="linenos">186</span></a>                <span class="n">r_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">+</span> <span class="mf">0.0</span><span class="p">)</span>
</span><span id="L-187"><a href="#L-187"><span class="linenos">187</span></a>                <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span>
</span><span id="L-188"><a href="#L-188"><span class="linenos">188</span></a>                    <span class="n">r_layer</span><span class="p">,</span> <span class="p">(</span><span class="n">weight_importance</span> <span class="o">+</span> <span class="n">r_layer</span><span class="p">)</span>
</span><span id="L-189"><a href="#L-189"><span class="linenos">189</span></a>                <span class="p">)</span>
</span><span id="L-190"><a href="#L-190"><span class="linenos">190</span></a>                <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span>
</span><span id="L-191"><a href="#L-191"><span class="linenos">191</span></a>                    <span class="n">r_layer</span><span class="p">,</span> <span class="p">(</span><span class="n">bias_importance</span> <span class="o">+</span> <span class="n">r_layer</span><span class="p">)</span>
</span><span id="L-192"><a href="#L-192"><span class="linenos">192</span></a>                <span class="p">)</span>
</span><span id="L-193"><a href="#L-193"><span class="linenos">193</span></a>
</span><span id="L-194"><a href="#L-194"><span class="linenos">194</span></a>            <span class="c1"># apply the adjustment rate to the gradients</span>
</span><span id="L-195"><a href="#L-195"><span class="linenos">195</span></a>            <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">adjustment_rate_weight_layer</span>
</span><span id="L-196"><a href="#L-196"><span class="linenos">196</span></a>            <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-197"><a href="#L-197"><span class="linenos">197</span></a>                <span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">adjustment_rate_bias_layer</span>
</span><span id="L-198"><a href="#L-198"><span class="linenos">198</span></a>
</span><span id="L-199"><a href="#L-199"><span class="linenos">199</span></a>            <span class="c1"># store the adjustment rate for logging</span>
</span><span id="L-200"><a href="#L-200"><span class="linenos">200</span></a>            <span class="n">adjustment_rate_weight</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">adjustment_rate_weight_layer</span>
</span><span id="L-201"><a href="#L-201"><span class="linenos">201</span></a>            <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-202"><a href="#L-202"><span class="linenos">202</span></a>                <span class="n">adjustment_rate_bias</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">adjustment_rate_bias_layer</span>
</span><span id="L-203"><a href="#L-203"><span class="linenos">203</span></a>
</span><span id="L-204"><a href="#L-204"><span class="linenos">204</span></a>            <span class="c1"># update network capacity metric</span>
</span><span id="L-205"><a href="#L-205"><span class="linenos">205</span></a>            <span class="n">capacity</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">adjustment_rate_weight_layer</span><span class="p">,</span> <span class="n">adjustment_rate_bias_layer</span><span class="p">)</span>
</span><span id="L-206"><a href="#L-206"><span class="linenos">206</span></a>
</span><span id="L-207"><a href="#L-207"><span class="linenos">207</span></a>        <span class="k">return</span> <span class="n">adjustment_rate_weight</span><span class="p">,</span> <span class="n">adjustment_rate_bias</span><span class="p">,</span> <span class="n">capacity</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
</span><span id="L-208"><a href="#L-208"><span class="linenos">208</span></a>
</span><span id="L-209"><a href="#L-209"><span class="linenos">209</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_end</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-210"><a href="#L-210"><span class="linenos">210</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Additionally update the summative mask after training the task.&quot;&quot;&quot;</span>
</span><span id="L-211"><a href="#L-211"><span class="linenos">211</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">on_train_end</span><span class="p">()</span>
</span><span id="L-212"><a href="#L-212"><span class="linenos">212</span></a>
</span><span id="L-213"><a href="#L-213"><span class="linenos">213</span></a>        <span class="n">mask_t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">masks</span><span class="p">[</span>
</span><span id="L-214"><a href="#L-214"><span class="linenos">214</span></a>            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="L-215"><a href="#L-215"><span class="linenos">215</span></a>        <span class="p">]</span>  <span class="c1"># get stored mask for the current task again</span>
</span><span id="L-216"><a href="#L-216"><span class="linenos">216</span></a>
</span><span id="L-217"><a href="#L-217"><span class="linenos">217</span></a>        <span class="c1"># update the summative mask for previous tasks. See Eq. (7) in Sec. 3.1 of the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9)</span>
</span><span id="L-218"><a href="#L-218"><span class="linenos">218</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">summative_mask_for_previous_tasks</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="L-219"><a href="#L-219"><span class="linenos">219</span></a>            <span class="n">layer_name</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">summative_mask_for_previous_tasks</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="L-220"><a href="#L-220"><span class="linenos">220</span></a>            <span class="o">+</span> <span class="n">mask_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="L-221"><a href="#L-221"><span class="linenos">221</span></a>            <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span>
</span><span id="L-222"><a href="#L-222"><span class="linenos">222</span></a>        <span class="p">}</span>
</span></pre></div>


            </section>
                <section id="AdaHAT">
                            <input id="AdaHAT-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">AdaHAT</span><wbr>(<span class="base"><a href="hat.html#HAT">clarena.cl_algorithms.hat.HAT</a></span>):

                <label class="view-source-button" for="AdaHAT-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#AdaHAT"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="AdaHAT-22"><a href="#AdaHAT-22"><span class="linenos"> 22</span></a><span class="k">class</span><span class="w"> </span><span class="nc">AdaHAT</span><span class="p">(</span><span class="n">HAT</span><span class="p">):</span>
</span><span id="AdaHAT-23"><a href="#AdaHAT-23"><span class="linenos"> 23</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;[AdaHAT (Adaptive Hard Attention to the Task)](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9) algorithm.</span>
</span><span id="AdaHAT-24"><a href="#AdaHAT-24"><span class="linenos"> 24</span></a>
</span><span id="AdaHAT-25"><a href="#AdaHAT-25"><span class="linenos"> 25</span></a><span class="sd">    An architecture-based continual learning approach that improves [HAT (Hard Attention to the Task)](http://proceedings.mlr.press/v80/serra18a) by introducing adaptive soft gradient clipping based on parameter importance and network sparsity.</span>
</span><span id="AdaHAT-26"><a href="#AdaHAT-26"><span class="linenos"> 26</span></a>
</span><span id="AdaHAT-27"><a href="#AdaHAT-27"><span class="linenos"> 27</span></a><span class="sd">    We implement AdaHAT as a subclass of HAT, as it shares the same `forward()`, `compensate_task_embedding_gradients()`, `training_step()`, `on_train_end()`, `validation_step()`, and `test_step()` methods as the `HAT` class.</span>
</span><span id="AdaHAT-28"><a href="#AdaHAT-28"><span class="linenos"> 28</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="AdaHAT-29"><a href="#AdaHAT-29"><span class="linenos"> 29</span></a>
</span><span id="AdaHAT-30"><a href="#AdaHAT-30"><span class="linenos"> 30</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="AdaHAT-31"><a href="#AdaHAT-31"><span class="linenos"> 31</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="AdaHAT-32"><a href="#AdaHAT-32"><span class="linenos"> 32</span></a>        <span class="n">backbone</span><span class="p">:</span> <span class="n">HATMaskBackbone</span><span class="p">,</span>
</span><span id="AdaHAT-33"><a href="#AdaHAT-33"><span class="linenos"> 33</span></a>        <span class="n">heads</span><span class="p">:</span> <span class="n">HeadsTIL</span><span class="p">,</span>
</span><span id="AdaHAT-34"><a href="#AdaHAT-34"><span class="linenos"> 34</span></a>        <span class="n">adjustment_mode</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="AdaHAT-35"><a href="#AdaHAT-35"><span class="linenos"> 35</span></a>        <span class="n">adjustment_intensity</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="AdaHAT-36"><a href="#AdaHAT-36"><span class="linenos"> 36</span></a>        <span class="n">s_max</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="AdaHAT-37"><a href="#AdaHAT-37"><span class="linenos"> 37</span></a>        <span class="n">clamp_threshold</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="AdaHAT-38"><a href="#AdaHAT-38"><span class="linenos"> 38</span></a>        <span class="n">mask_sparsity_reg_factor</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="AdaHAT-39"><a href="#AdaHAT-39"><span class="linenos"> 39</span></a>        <span class="n">mask_sparsity_reg_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;original&quot;</span><span class="p">,</span>
</span><span id="AdaHAT-40"><a href="#AdaHAT-40"><span class="linenos"> 40</span></a>        <span class="n">task_embedding_init_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;N01&quot;</span><span class="p">,</span>
</span><span id="AdaHAT-41"><a href="#AdaHAT-41"><span class="linenos"> 41</span></a>        <span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
</span><span id="AdaHAT-42"><a href="#AdaHAT-42"><span class="linenos"> 42</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="AdaHAT-43"><a href="#AdaHAT-43"><span class="linenos"> 43</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialize the AdaHAT algorithm with the network.</span>
</span><span id="AdaHAT-44"><a href="#AdaHAT-44"><span class="linenos"> 44</span></a>
</span><span id="AdaHAT-45"><a href="#AdaHAT-45"><span class="linenos"> 45</span></a><span class="sd">        **Args:**</span>
</span><span id="AdaHAT-46"><a href="#AdaHAT-46"><span class="linenos"> 46</span></a><span class="sd">        - **backbone** (`HATMaskBackbone`): must be a backbone network with the HAT mask mechanism.</span>
</span><span id="AdaHAT-47"><a href="#AdaHAT-47"><span class="linenos"> 47</span></a><span class="sd">        - **heads** (`HeadsTIL`): output heads. AdaHAT supports only TIL (Task-Incremental Learning).</span>
</span><span id="AdaHAT-48"><a href="#AdaHAT-48"><span class="linenos"> 48</span></a><span class="sd">        - **adjustment_mode** (`str`): the strategy of adjustment (i.e., the mode of gradient clipping), must be one of:</span>
</span><span id="AdaHAT-49"><a href="#AdaHAT-49"><span class="linenos"> 49</span></a><span class="sd">            1. &#39;adahat&#39;: set gradients of parameters linking to masked units to a soft adjustment rate in the original AdaHAT approach (allows slight updates on previous-task parameters). See Eqs. (8) and (9) in Sec. 3.1 of the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="AdaHAT-50"><a href="#AdaHAT-50"><span class="linenos"> 50</span></a><span class="sd">            2. &#39;adahat_no_sum&#39;: as above but without parameter-importance (i.e., no summative mask). See Sec. 4.3 (ablation study) in the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="AdaHAT-51"><a href="#AdaHAT-51"><span class="linenos"> 51</span></a><span class="sd">            3. &#39;adahat_no_reg&#39;: as above but without network sparsity (i.e., no mask sparsity regularization term). See Sec. 4.3 (ablation study) in the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="AdaHAT-52"><a href="#AdaHAT-52"><span class="linenos"> 52</span></a><span class="sd">        - **adjustment_intensity** (`float`): hyperparameter, controls the overall intensity of gradient adjustment (the $\alpha$ in Eq. (9) of the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9)).</span>
</span><span id="AdaHAT-53"><a href="#AdaHAT-53"><span class="linenos"> 53</span></a><span class="sd">        - **s_max** (`float`): hyperparameter, the maximum scaling factor in the gate function. See Sec. 2.4 &quot;Hard Attention Training&quot; in the [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="AdaHAT-54"><a href="#AdaHAT-54"><span class="linenos"> 54</span></a><span class="sd">        - **clamp_threshold** (`float`): the threshold for task embedding gradient compensation. See Sec. 2.5 &quot;Embedding Gradient Compensation&quot; in the [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="AdaHAT-55"><a href="#AdaHAT-55"><span class="linenos"> 55</span></a><span class="sd">        - **mask_sparsity_reg_factor** (`float`): hyperparameter, the regularization factor for mask sparsity.</span>
</span><span id="AdaHAT-56"><a href="#AdaHAT-56"><span class="linenos"> 56</span></a><span class="sd">        - **mask_sparsity_reg_mode** (`str`): the mode of mask sparsity regularization, must be one of:</span>
</span><span id="AdaHAT-57"><a href="#AdaHAT-57"><span class="linenos"> 57</span></a><span class="sd">            1. &#39;original&#39; (default): the original mask sparsity regularization in the [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="AdaHAT-58"><a href="#AdaHAT-58"><span class="linenos"> 58</span></a><span class="sd">            2. &#39;cross&#39;: the cross version of mask sparsity regularization.</span>
</span><span id="AdaHAT-59"><a href="#AdaHAT-59"><span class="linenos"> 59</span></a><span class="sd">        - **task_embedding_init_mode** (`str`): the initialization mode for task embeddings, must be one of:</span>
</span><span id="AdaHAT-60"><a href="#AdaHAT-60"><span class="linenos"> 60</span></a><span class="sd">            1. &#39;N01&#39; (default): standard normal distribution $N(0, 1)$.</span>
</span><span id="AdaHAT-61"><a href="#AdaHAT-61"><span class="linenos"> 61</span></a><span class="sd">            2. &#39;U-11&#39;: uniform distribution $U(-1, 1)$.</span>
</span><span id="AdaHAT-62"><a href="#AdaHAT-62"><span class="linenos"> 62</span></a><span class="sd">            3. &#39;U01&#39;: uniform distribution $U(0, 1)$.</span>
</span><span id="AdaHAT-63"><a href="#AdaHAT-63"><span class="linenos"> 63</span></a><span class="sd">            4. &#39;U-10&#39;: uniform distribution $U(-1, 0)$.</span>
</span><span id="AdaHAT-64"><a href="#AdaHAT-64"><span class="linenos"> 64</span></a><span class="sd">            5. &#39;last&#39;: inherit the task embedding from the last task.</span>
</span><span id="AdaHAT-65"><a href="#AdaHAT-65"><span class="linenos"> 65</span></a><span class="sd">        - **epsilon** (`float`): the value added to network sparsity to avoid division by zero (appearing in Eq. (9) of the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9)).</span>
</span><span id="AdaHAT-66"><a href="#AdaHAT-66"><span class="linenos"> 66</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="AdaHAT-67"><a href="#AdaHAT-67"><span class="linenos"> 67</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="AdaHAT-68"><a href="#AdaHAT-68"><span class="linenos"> 68</span></a>            <span class="n">backbone</span><span class="o">=</span><span class="n">backbone</span><span class="p">,</span>
</span><span id="AdaHAT-69"><a href="#AdaHAT-69"><span class="linenos"> 69</span></a>            <span class="n">heads</span><span class="o">=</span><span class="n">heads</span><span class="p">,</span>
</span><span id="AdaHAT-70"><a href="#AdaHAT-70"><span class="linenos"> 70</span></a>            <span class="n">adjustment_mode</span><span class="o">=</span><span class="n">adjustment_mode</span><span class="p">,</span>
</span><span id="AdaHAT-71"><a href="#AdaHAT-71"><span class="linenos"> 71</span></a>            <span class="n">s_max</span><span class="o">=</span><span class="n">s_max</span><span class="p">,</span>
</span><span id="AdaHAT-72"><a href="#AdaHAT-72"><span class="linenos"> 72</span></a>            <span class="n">clamp_threshold</span><span class="o">=</span><span class="n">clamp_threshold</span><span class="p">,</span>
</span><span id="AdaHAT-73"><a href="#AdaHAT-73"><span class="linenos"> 73</span></a>            <span class="n">mask_sparsity_reg_factor</span><span class="o">=</span><span class="n">mask_sparsity_reg_factor</span><span class="p">,</span>
</span><span id="AdaHAT-74"><a href="#AdaHAT-74"><span class="linenos"> 74</span></a>            <span class="n">mask_sparsity_reg_mode</span><span class="o">=</span><span class="n">mask_sparsity_reg_mode</span><span class="p">,</span>
</span><span id="AdaHAT-75"><a href="#AdaHAT-75"><span class="linenos"> 75</span></a>            <span class="n">task_embedding_init_mode</span><span class="o">=</span><span class="n">task_embedding_init_mode</span><span class="p">,</span>
</span><span id="AdaHAT-76"><a href="#AdaHAT-76"><span class="linenos"> 76</span></a>            <span class="n">alpha</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="AdaHAT-77"><a href="#AdaHAT-77"><span class="linenos"> 77</span></a>        <span class="p">)</span>
</span><span id="AdaHAT-78"><a href="#AdaHAT-78"><span class="linenos"> 78</span></a>
</span><span id="AdaHAT-79"><a href="#AdaHAT-79"><span class="linenos"> 79</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">adjustment_intensity</span>
</span><span id="AdaHAT-80"><a href="#AdaHAT-80"><span class="linenos"> 80</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The adjustment intensity in Eq. (9) of the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).&quot;&quot;&quot;</span>
</span><span id="AdaHAT-81"><a href="#AdaHAT-81"><span class="linenos"> 81</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">epsilon</span>
</span><span id="AdaHAT-82"><a href="#AdaHAT-82"><span class="linenos"> 82</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The small value to avoid division by zero (appearing in Eq. (9) of the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9)).&quot;&quot;&quot;</span>
</span><span id="AdaHAT-83"><a href="#AdaHAT-83"><span class="linenos"> 83</span></a>
</span><span id="AdaHAT-84"><a href="#AdaHAT-84"><span class="linenos"> 84</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">summative_mask_for_previous_tasks</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="AdaHAT-85"><a href="#AdaHAT-85"><span class="linenos"> 85</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The summative binary attention mask $\mathrm{M}^{&lt;t,\text{sum}}$ of previous tasks $1,\cdots, t-1$, gated from the task embedding. It is a dict where keys are layer names and values are the binary mask tensors for the layers. The mask tensor has size (number of units, ).&quot;&quot;&quot;</span>
</span><span id="AdaHAT-86"><a href="#AdaHAT-86"><span class="linenos"> 86</span></a>
</span><span id="AdaHAT-87"><a href="#AdaHAT-87"><span class="linenos"> 87</span></a>        <span class="c1"># set manual optimization</span>
</span><span id="AdaHAT-88"><a href="#AdaHAT-88"><span class="linenos"> 88</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">automatic_optimization</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="AdaHAT-89"><a href="#AdaHAT-89"><span class="linenos"> 89</span></a>
</span><span id="AdaHAT-90"><a href="#AdaHAT-90"><span class="linenos"> 90</span></a>        <span class="n">AdaHAT</span><span class="o">.</span><span class="n">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</span><span id="AdaHAT-91"><a href="#AdaHAT-91"><span class="linenos"> 91</span></a>
</span><span id="AdaHAT-92"><a href="#AdaHAT-92"><span class="linenos"> 92</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="AdaHAT-93"><a href="#AdaHAT-93"><span class="linenos"> 93</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Check the sanity of the arguments.&quot;&quot;&quot;</span>
</span><span id="AdaHAT-94"><a href="#AdaHAT-94"><span class="linenos"> 94</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="AdaHAT-95"><a href="#AdaHAT-95"><span class="linenos"> 95</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="AdaHAT-96"><a href="#AdaHAT-96"><span class="linenos"> 96</span></a>                <span class="sa">f</span><span class="s2">&quot;The adjustment intensity should be positive, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span><span class="si">}</span><span class="s2">.&quot;</span>
</span><span id="AdaHAT-97"><a href="#AdaHAT-97"><span class="linenos"> 97</span></a>            <span class="p">)</span>
</span><span id="AdaHAT-98"><a href="#AdaHAT-98"><span class="linenos"> 98</span></a>
</span><span id="AdaHAT-99"><a href="#AdaHAT-99"><span class="linenos"> 99</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_start</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="AdaHAT-100"><a href="#AdaHAT-100"><span class="linenos">100</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Additionally initialize the summative mask at the beginning of the first task.&quot;&quot;&quot;</span>
</span><span id="AdaHAT-101"><a href="#AdaHAT-101"><span class="linenos">101</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">on_train_start</span><span class="p">()</span>
</span><span id="AdaHAT-102"><a href="#AdaHAT-102"><span class="linenos">102</span></a>
</span><span id="AdaHAT-103"><a href="#AdaHAT-103"><span class="linenos">103</span></a>        <span class="c1"># initialize the summative mask at the beginning of the first task. This should not be called in `__init__()` method because `self.device` is not available at that time</span>
</span><span id="AdaHAT-104"><a href="#AdaHAT-104"><span class="linenos">104</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="AdaHAT-105"><a href="#AdaHAT-105"><span class="linenos">105</span></a>            <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="AdaHAT-106"><a href="#AdaHAT-106"><span class="linenos">106</span></a>                <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span>
</span><span id="AdaHAT-107"><a href="#AdaHAT-107"><span class="linenos">107</span></a>                    <span class="n">layer_name</span>
</span><span id="AdaHAT-108"><a href="#AdaHAT-108"><span class="linenos">108</span></a>                <span class="p">)</span>  <span class="c1"># get the layer by its name</span>
</span><span id="AdaHAT-109"><a href="#AdaHAT-109"><span class="linenos">109</span></a>                <span class="n">num_units</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="AdaHAT-110"><a href="#AdaHAT-110"><span class="linenos">110</span></a>
</span><span id="AdaHAT-111"><a href="#AdaHAT-111"><span class="linenos">111</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">summative_mask_for_previous_tasks</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
</span><span id="AdaHAT-112"><a href="#AdaHAT-112"><span class="linenos">112</span></a>                    <span class="n">num_units</span>
</span><span id="AdaHAT-113"><a href="#AdaHAT-113"><span class="linenos">113</span></a>                <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
</span><span id="AdaHAT-114"><a href="#AdaHAT-114"><span class="linenos">114</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
</span><span id="AdaHAT-115"><a href="#AdaHAT-115"><span class="linenos">115</span></a>                <span class="p">)</span>  <span class="c1"># the summative mask $\mathrm{M}^{&lt;t,\text{sum}}$ is initialized as a zeros mask for $t = 1$. See Eq. (7) in Sec. 3.1 of the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9)</span>
</span><span id="AdaHAT-116"><a href="#AdaHAT-116"><span class="linenos">116</span></a>
</span><span id="AdaHAT-117"><a href="#AdaHAT-117"><span class="linenos">117</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">clip_grad_by_adjustment</span><span class="p">(</span>
</span><span id="AdaHAT-118"><a href="#AdaHAT-118"><span class="linenos">118</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="AdaHAT-119"><a href="#AdaHAT-119"><span class="linenos">119</span></a>        <span class="n">network_sparsity</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="AdaHAT-120"><a href="#AdaHAT-120"><span class="linenos">120</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="n">Tensor</span><span class="p">]:</span>
</span><span id="AdaHAT-121"><a href="#AdaHAT-121"><span class="linenos">121</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Clip the gradients by the adjustment rate. See Eq. (8) in the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="AdaHAT-122"><a href="#AdaHAT-122"><span class="linenos">122</span></a>
</span><span id="AdaHAT-123"><a href="#AdaHAT-123"><span class="linenos">123</span></a><span class="sd">        Note that because the task embedding fully covers every layer in the backbone network, no parameters are left out of this system. This applies not only to parameters between layers with task embeddings, but also to those before the first layer. We design it separately in the code.</span>
</span><span id="AdaHAT-124"><a href="#AdaHAT-124"><span class="linenos">124</span></a>
</span><span id="AdaHAT-125"><a href="#AdaHAT-125"><span class="linenos">125</span></a><span class="sd">        Network capacity is measured alongside this method. Network capacity is defined as the average adjustment rate over all parameters. See Sec. 4.1 in the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="AdaHAT-126"><a href="#AdaHAT-126"><span class="linenos">126</span></a>
</span><span id="AdaHAT-127"><a href="#AdaHAT-127"><span class="linenos">127</span></a><span class="sd">        **Args:**</span>
</span><span id="AdaHAT-128"><a href="#AdaHAT-128"><span class="linenos">128</span></a><span class="sd">        - **network_sparsity** (`dict[str, Tensor]` | `None`): the network sparsity (i.e., the mask sparsity loss of each layer) for the current task. Keys are layer names and values are the network sparsity values. It is used to calculate the adjustment rate for gradients. Applies only to mode `adahat` and `adahat_no_sum`, not `adahat_no_reg`.</span>
</span><span id="AdaHAT-129"><a href="#AdaHAT-129"><span class="linenos">129</span></a>
</span><span id="AdaHAT-130"><a href="#AdaHAT-130"><span class="linenos">130</span></a><span class="sd">        **Returns:**</span>
</span><span id="AdaHAT-131"><a href="#AdaHAT-131"><span class="linenos">131</span></a><span class="sd">        - **adjustment_rate_weight** (`dict[str, Tensor]`): the adjustment rate for weights. Keys (`str`) are layer names and values (`Tensor`) are the adjustment rate tensors.</span>
</span><span id="AdaHAT-132"><a href="#AdaHAT-132"><span class="linenos">132</span></a><span class="sd">        - **adjustment_rate_bias** (`dict[str, Tensor]`): the adjustment rate for biases. Keys (`str`) are layer names and values (`Tensor`) are the adjustment rate tensors.</span>
</span><span id="AdaHAT-133"><a href="#AdaHAT-133"><span class="linenos">133</span></a><span class="sd">        - **capacity** (`Tensor`): the calculated network capacity.</span>
</span><span id="AdaHAT-134"><a href="#AdaHAT-134"><span class="linenos">134</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="AdaHAT-135"><a href="#AdaHAT-135"><span class="linenos">135</span></a>
</span><span id="AdaHAT-136"><a href="#AdaHAT-136"><span class="linenos">136</span></a>        <span class="c1"># initialize network capacity metric</span>
</span><span id="AdaHAT-137"><a href="#AdaHAT-137"><span class="linenos">137</span></a>        <span class="n">capacity</span> <span class="o">=</span> <span class="n">HATNetworkCapacityMetric</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="AdaHAT-138"><a href="#AdaHAT-138"><span class="linenos">138</span></a>        <span class="n">adjustment_rate_weight</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="AdaHAT-139"><a href="#AdaHAT-139"><span class="linenos">139</span></a>        <span class="n">adjustment_rate_bias</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="AdaHAT-140"><a href="#AdaHAT-140"><span class="linenos">140</span></a>
</span><span id="AdaHAT-141"><a href="#AdaHAT-141"><span class="linenos">141</span></a>        <span class="c1"># calculate the adjustment rate for gradients of the parameters, both weights and biases (if they exist). See Eq. (9) in the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9)</span>
</span><span id="AdaHAT-142"><a href="#AdaHAT-142"><span class="linenos">142</span></a>        <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="AdaHAT-143"><a href="#AdaHAT-143"><span class="linenos">143</span></a>
</span><span id="AdaHAT-144"><a href="#AdaHAT-144"><span class="linenos">144</span></a>            <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span>
</span><span id="AdaHAT-145"><a href="#AdaHAT-145"><span class="linenos">145</span></a>                <span class="n">layer_name</span>
</span><span id="AdaHAT-146"><a href="#AdaHAT-146"><span class="linenos">146</span></a>            <span class="p">)</span>  <span class="c1"># get the layer by its name</span>
</span><span id="AdaHAT-147"><a href="#AdaHAT-147"><span class="linenos">147</span></a>
</span><span id="AdaHAT-148"><a href="#AdaHAT-148"><span class="linenos">148</span></a>            <span class="c1"># placeholder for the adjustment rate to avoid the error of using it before assignment</span>
</span><span id="AdaHAT-149"><a href="#AdaHAT-149"><span class="linenos">149</span></a>            <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="AdaHAT-150"><a href="#AdaHAT-150"><span class="linenos">150</span></a>            <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="AdaHAT-151"><a href="#AdaHAT-151"><span class="linenos">151</span></a>
</span><span id="AdaHAT-152"><a href="#AdaHAT-152"><span class="linenos">152</span></a>            <span class="n">weight_importance</span><span class="p">,</span> <span class="n">bias_importance</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="AdaHAT-153"><a href="#AdaHAT-153"><span class="linenos">153</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_measure_parameter_wise</span><span class="p">(</span>
</span><span id="AdaHAT-154"><a href="#AdaHAT-154"><span class="linenos">154</span></a>                    <span class="n">unit_wise_measure</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">summative_mask_for_previous_tasks</span><span class="p">,</span>
</span><span id="AdaHAT-155"><a href="#AdaHAT-155"><span class="linenos">155</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="AdaHAT-156"><a href="#AdaHAT-156"><span class="linenos">156</span></a>                    <span class="n">aggregation_mode</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span>
</span><span id="AdaHAT-157"><a href="#AdaHAT-157"><span class="linenos">157</span></a>                <span class="p">)</span>
</span><span id="AdaHAT-158"><a href="#AdaHAT-158"><span class="linenos">158</span></a>            <span class="p">)</span>  <span class="c1"># AdaHAT depends on parameter importance rather than parameter masks (as in HAT)</span>
</span><span id="AdaHAT-159"><a href="#AdaHAT-159"><span class="linenos">159</span></a>
</span><span id="AdaHAT-160"><a href="#AdaHAT-160"><span class="linenos">160</span></a>            <span class="n">network_sparsity_layer</span> <span class="o">=</span> <span class="n">network_sparsity</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="AdaHAT-161"><a href="#AdaHAT-161"><span class="linenos">161</span></a>
</span><span id="AdaHAT-162"><a href="#AdaHAT-162"><span class="linenos">162</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;adahat&quot;</span><span class="p">:</span>
</span><span id="AdaHAT-163"><a href="#AdaHAT-163"><span class="linenos">163</span></a>                <span class="n">r_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span> <span class="o">/</span> <span class="p">(</span>
</span><span id="AdaHAT-164"><a href="#AdaHAT-164"><span class="linenos">164</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">+</span> <span class="n">network_sparsity_layer</span>
</span><span id="AdaHAT-165"><a href="#AdaHAT-165"><span class="linenos">165</span></a>                <span class="p">)</span>
</span><span id="AdaHAT-166"><a href="#AdaHAT-166"><span class="linenos">166</span></a>                <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span>
</span><span id="AdaHAT-167"><a href="#AdaHAT-167"><span class="linenos">167</span></a>                    <span class="n">r_layer</span><span class="p">,</span> <span class="p">(</span><span class="n">weight_importance</span> <span class="o">+</span> <span class="n">r_layer</span><span class="p">)</span>
</span><span id="AdaHAT-168"><a href="#AdaHAT-168"><span class="linenos">168</span></a>                <span class="p">)</span>
</span><span id="AdaHAT-169"><a href="#AdaHAT-169"><span class="linenos">169</span></a>                <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span>
</span><span id="AdaHAT-170"><a href="#AdaHAT-170"><span class="linenos">170</span></a>                    <span class="n">r_layer</span><span class="p">,</span> <span class="p">(</span><span class="n">bias_importance</span> <span class="o">+</span> <span class="n">r_layer</span><span class="p">)</span>
</span><span id="AdaHAT-171"><a href="#AdaHAT-171"><span class="linenos">171</span></a>                <span class="p">)</span>
</span><span id="AdaHAT-172"><a href="#AdaHAT-172"><span class="linenos">172</span></a>
</span><span id="AdaHAT-173"><a href="#AdaHAT-173"><span class="linenos">173</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;adahat_no_sum&quot;</span><span class="p">:</span>
</span><span id="AdaHAT-174"><a href="#AdaHAT-174"><span class="linenos">174</span></a>
</span><span id="AdaHAT-175"><a href="#AdaHAT-175"><span class="linenos">175</span></a>                <span class="n">r_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span> <span class="o">/</span> <span class="p">(</span>
</span><span id="AdaHAT-176"><a href="#AdaHAT-176"><span class="linenos">176</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">+</span> <span class="n">network_sparsity_layer</span>
</span><span id="AdaHAT-177"><a href="#AdaHAT-177"><span class="linenos">177</span></a>                <span class="p">)</span>
</span><span id="AdaHAT-178"><a href="#AdaHAT-178"><span class="linenos">178</span></a>                <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span>
</span><span id="AdaHAT-179"><a href="#AdaHAT-179"><span class="linenos">179</span></a>                    <span class="n">r_layer</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">+</span> <span class="n">r_layer</span><span class="p">)</span>
</span><span id="AdaHAT-180"><a href="#AdaHAT-180"><span class="linenos">180</span></a>                <span class="p">)</span>
</span><span id="AdaHAT-181"><a href="#AdaHAT-181"><span class="linenos">181</span></a>                <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span>
</span><span id="AdaHAT-182"><a href="#AdaHAT-182"><span class="linenos">182</span></a>                    <span class="n">r_layer</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">+</span> <span class="n">r_layer</span><span class="p">)</span>
</span><span id="AdaHAT-183"><a href="#AdaHAT-183"><span class="linenos">183</span></a>                <span class="p">)</span>
</span><span id="AdaHAT-184"><a href="#AdaHAT-184"><span class="linenos">184</span></a>
</span><span id="AdaHAT-185"><a href="#AdaHAT-185"><span class="linenos">185</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;adahat_no_reg&quot;</span><span class="p">:</span>
</span><span id="AdaHAT-186"><a href="#AdaHAT-186"><span class="linenos">186</span></a>
</span><span id="AdaHAT-187"><a href="#AdaHAT-187"><span class="linenos">187</span></a>                <span class="n">r_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">+</span> <span class="mf">0.0</span><span class="p">)</span>
</span><span id="AdaHAT-188"><a href="#AdaHAT-188"><span class="linenos">188</span></a>                <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span>
</span><span id="AdaHAT-189"><a href="#AdaHAT-189"><span class="linenos">189</span></a>                    <span class="n">r_layer</span><span class="p">,</span> <span class="p">(</span><span class="n">weight_importance</span> <span class="o">+</span> <span class="n">r_layer</span><span class="p">)</span>
</span><span id="AdaHAT-190"><a href="#AdaHAT-190"><span class="linenos">190</span></a>                <span class="p">)</span>
</span><span id="AdaHAT-191"><a href="#AdaHAT-191"><span class="linenos">191</span></a>                <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span>
</span><span id="AdaHAT-192"><a href="#AdaHAT-192"><span class="linenos">192</span></a>                    <span class="n">r_layer</span><span class="p">,</span> <span class="p">(</span><span class="n">bias_importance</span> <span class="o">+</span> <span class="n">r_layer</span><span class="p">)</span>
</span><span id="AdaHAT-193"><a href="#AdaHAT-193"><span class="linenos">193</span></a>                <span class="p">)</span>
</span><span id="AdaHAT-194"><a href="#AdaHAT-194"><span class="linenos">194</span></a>
</span><span id="AdaHAT-195"><a href="#AdaHAT-195"><span class="linenos">195</span></a>            <span class="c1"># apply the adjustment rate to the gradients</span>
</span><span id="AdaHAT-196"><a href="#AdaHAT-196"><span class="linenos">196</span></a>            <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">adjustment_rate_weight_layer</span>
</span><span id="AdaHAT-197"><a href="#AdaHAT-197"><span class="linenos">197</span></a>            <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="AdaHAT-198"><a href="#AdaHAT-198"><span class="linenos">198</span></a>                <span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">adjustment_rate_bias_layer</span>
</span><span id="AdaHAT-199"><a href="#AdaHAT-199"><span class="linenos">199</span></a>
</span><span id="AdaHAT-200"><a href="#AdaHAT-200"><span class="linenos">200</span></a>            <span class="c1"># store the adjustment rate for logging</span>
</span><span id="AdaHAT-201"><a href="#AdaHAT-201"><span class="linenos">201</span></a>            <span class="n">adjustment_rate_weight</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">adjustment_rate_weight_layer</span>
</span><span id="AdaHAT-202"><a href="#AdaHAT-202"><span class="linenos">202</span></a>            <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="AdaHAT-203"><a href="#AdaHAT-203"><span class="linenos">203</span></a>                <span class="n">adjustment_rate_bias</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">adjustment_rate_bias_layer</span>
</span><span id="AdaHAT-204"><a href="#AdaHAT-204"><span class="linenos">204</span></a>
</span><span id="AdaHAT-205"><a href="#AdaHAT-205"><span class="linenos">205</span></a>            <span class="c1"># update network capacity metric</span>
</span><span id="AdaHAT-206"><a href="#AdaHAT-206"><span class="linenos">206</span></a>            <span class="n">capacity</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">adjustment_rate_weight_layer</span><span class="p">,</span> <span class="n">adjustment_rate_bias_layer</span><span class="p">)</span>
</span><span id="AdaHAT-207"><a href="#AdaHAT-207"><span class="linenos">207</span></a>
</span><span id="AdaHAT-208"><a href="#AdaHAT-208"><span class="linenos">208</span></a>        <span class="k">return</span> <span class="n">adjustment_rate_weight</span><span class="p">,</span> <span class="n">adjustment_rate_bias</span><span class="p">,</span> <span class="n">capacity</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
</span><span id="AdaHAT-209"><a href="#AdaHAT-209"><span class="linenos">209</span></a>
</span><span id="AdaHAT-210"><a href="#AdaHAT-210"><span class="linenos">210</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_end</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="AdaHAT-211"><a href="#AdaHAT-211"><span class="linenos">211</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Additionally update the summative mask after training the task.&quot;&quot;&quot;</span>
</span><span id="AdaHAT-212"><a href="#AdaHAT-212"><span class="linenos">212</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">on_train_end</span><span class="p">()</span>
</span><span id="AdaHAT-213"><a href="#AdaHAT-213"><span class="linenos">213</span></a>
</span><span id="AdaHAT-214"><a href="#AdaHAT-214"><span class="linenos">214</span></a>        <span class="n">mask_t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">masks</span><span class="p">[</span>
</span><span id="AdaHAT-215"><a href="#AdaHAT-215"><span class="linenos">215</span></a>            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="AdaHAT-216"><a href="#AdaHAT-216"><span class="linenos">216</span></a>        <span class="p">]</span>  <span class="c1"># get stored mask for the current task again</span>
</span><span id="AdaHAT-217"><a href="#AdaHAT-217"><span class="linenos">217</span></a>
</span><span id="AdaHAT-218"><a href="#AdaHAT-218"><span class="linenos">218</span></a>        <span class="c1"># update the summative mask for previous tasks. See Eq. (7) in Sec. 3.1 of the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9)</span>
</span><span id="AdaHAT-219"><a href="#AdaHAT-219"><span class="linenos">219</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">summative_mask_for_previous_tasks</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="AdaHAT-220"><a href="#AdaHAT-220"><span class="linenos">220</span></a>            <span class="n">layer_name</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">summative_mask_for_previous_tasks</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="AdaHAT-221"><a href="#AdaHAT-221"><span class="linenos">221</span></a>            <span class="o">+</span> <span class="n">mask_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="AdaHAT-222"><a href="#AdaHAT-222"><span class="linenos">222</span></a>            <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span>
</span><span id="AdaHAT-223"><a href="#AdaHAT-223"><span class="linenos">223</span></a>        <span class="p">}</span>
</span></pre></div>


            <div class="docstring"><p><a href="https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9">AdaHAT (Adaptive Hard Attention to the Task)</a> algorithm.</p>

<p>An architecture-based continual learning approach that improves <a href="http://proceedings.mlr.press/v80/serra18a">HAT (Hard Attention to the Task)</a> by introducing adaptive soft gradient clipping based on parameter importance and network sparsity.</p>

<p>We implement AdaHAT as a subclass of HAT, as it shares the same <code><a href="#AdaHAT.forward">forward()</a></code>, <code><a href="#AdaHAT.compensate_task_embedding_gradients">compensate_task_embedding_gradients()</a></code>, <code><a href="#AdaHAT.training_step">training_step()</a></code>, <code><a href="#AdaHAT.on_train_end">on_train_end()</a></code>, <code><a href="#AdaHAT.validation_step">validation_step()</a></code>, and <code><a href="#AdaHAT.test_step">test_step()</a></code> methods as the <code>HAT</code> class.</p>
</div>


                            <div id="AdaHAT.__init__" class="classattr">
                                        <input id="AdaHAT.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">AdaHAT</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">backbone</span><span class="p">:</span> <span class="n"><a href="../backbones.html#HATMaskBackbone">clarena.backbones.HATMaskBackbone</a></span>,</span><span class="param">	<span class="n">heads</span><span class="p">:</span> <span class="n"><a href="../heads.html#HeadsTIL">clarena.heads.HeadsTIL</a></span>,</span><span class="param">	<span class="n">adjustment_mode</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">adjustment_intensity</span><span class="p">:</span> <span class="nb">float</span>,</span><span class="param">	<span class="n">s_max</span><span class="p">:</span> <span class="nb">float</span>,</span><span class="param">	<span class="n">clamp_threshold</span><span class="p">:</span> <span class="nb">float</span>,</span><span class="param">	<span class="n">mask_sparsity_reg_factor</span><span class="p">:</span> <span class="nb">float</span>,</span><span class="param">	<span class="n">mask_sparsity_reg_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;original&#39;</span>,</span><span class="param">	<span class="n">task_embedding_init_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;N01&#39;</span>,</span><span class="param">	<span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span></span>)</span>

                <label class="view-source-button" for="AdaHAT.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#AdaHAT.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="AdaHAT.__init__-30"><a href="#AdaHAT.__init__-30"><span class="linenos">30</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="AdaHAT.__init__-31"><a href="#AdaHAT.__init__-31"><span class="linenos">31</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="AdaHAT.__init__-32"><a href="#AdaHAT.__init__-32"><span class="linenos">32</span></a>        <span class="n">backbone</span><span class="p">:</span> <span class="n">HATMaskBackbone</span><span class="p">,</span>
</span><span id="AdaHAT.__init__-33"><a href="#AdaHAT.__init__-33"><span class="linenos">33</span></a>        <span class="n">heads</span><span class="p">:</span> <span class="n">HeadsTIL</span><span class="p">,</span>
</span><span id="AdaHAT.__init__-34"><a href="#AdaHAT.__init__-34"><span class="linenos">34</span></a>        <span class="n">adjustment_mode</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="AdaHAT.__init__-35"><a href="#AdaHAT.__init__-35"><span class="linenos">35</span></a>        <span class="n">adjustment_intensity</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="AdaHAT.__init__-36"><a href="#AdaHAT.__init__-36"><span class="linenos">36</span></a>        <span class="n">s_max</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="AdaHAT.__init__-37"><a href="#AdaHAT.__init__-37"><span class="linenos">37</span></a>        <span class="n">clamp_threshold</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="AdaHAT.__init__-38"><a href="#AdaHAT.__init__-38"><span class="linenos">38</span></a>        <span class="n">mask_sparsity_reg_factor</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="AdaHAT.__init__-39"><a href="#AdaHAT.__init__-39"><span class="linenos">39</span></a>        <span class="n">mask_sparsity_reg_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;original&quot;</span><span class="p">,</span>
</span><span id="AdaHAT.__init__-40"><a href="#AdaHAT.__init__-40"><span class="linenos">40</span></a>        <span class="n">task_embedding_init_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;N01&quot;</span><span class="p">,</span>
</span><span id="AdaHAT.__init__-41"><a href="#AdaHAT.__init__-41"><span class="linenos">41</span></a>        <span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
</span><span id="AdaHAT.__init__-42"><a href="#AdaHAT.__init__-42"><span class="linenos">42</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="AdaHAT.__init__-43"><a href="#AdaHAT.__init__-43"><span class="linenos">43</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialize the AdaHAT algorithm with the network.</span>
</span><span id="AdaHAT.__init__-44"><a href="#AdaHAT.__init__-44"><span class="linenos">44</span></a>
</span><span id="AdaHAT.__init__-45"><a href="#AdaHAT.__init__-45"><span class="linenos">45</span></a><span class="sd">        **Args:**</span>
</span><span id="AdaHAT.__init__-46"><a href="#AdaHAT.__init__-46"><span class="linenos">46</span></a><span class="sd">        - **backbone** (`HATMaskBackbone`): must be a backbone network with the HAT mask mechanism.</span>
</span><span id="AdaHAT.__init__-47"><a href="#AdaHAT.__init__-47"><span class="linenos">47</span></a><span class="sd">        - **heads** (`HeadsTIL`): output heads. AdaHAT supports only TIL (Task-Incremental Learning).</span>
</span><span id="AdaHAT.__init__-48"><a href="#AdaHAT.__init__-48"><span class="linenos">48</span></a><span class="sd">        - **adjustment_mode** (`str`): the strategy of adjustment (i.e., the mode of gradient clipping), must be one of:</span>
</span><span id="AdaHAT.__init__-49"><a href="#AdaHAT.__init__-49"><span class="linenos">49</span></a><span class="sd">            1. &#39;adahat&#39;: set gradients of parameters linking to masked units to a soft adjustment rate in the original AdaHAT approach (allows slight updates on previous-task parameters). See Eqs. (8) and (9) in Sec. 3.1 of the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="AdaHAT.__init__-50"><a href="#AdaHAT.__init__-50"><span class="linenos">50</span></a><span class="sd">            2. &#39;adahat_no_sum&#39;: as above but without parameter-importance (i.e., no summative mask). See Sec. 4.3 (ablation study) in the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="AdaHAT.__init__-51"><a href="#AdaHAT.__init__-51"><span class="linenos">51</span></a><span class="sd">            3. &#39;adahat_no_reg&#39;: as above but without network sparsity (i.e., no mask sparsity regularization term). See Sec. 4.3 (ablation study) in the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="AdaHAT.__init__-52"><a href="#AdaHAT.__init__-52"><span class="linenos">52</span></a><span class="sd">        - **adjustment_intensity** (`float`): hyperparameter, controls the overall intensity of gradient adjustment (the $\alpha$ in Eq. (9) of the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9)).</span>
</span><span id="AdaHAT.__init__-53"><a href="#AdaHAT.__init__-53"><span class="linenos">53</span></a><span class="sd">        - **s_max** (`float`): hyperparameter, the maximum scaling factor in the gate function. See Sec. 2.4 &quot;Hard Attention Training&quot; in the [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="AdaHAT.__init__-54"><a href="#AdaHAT.__init__-54"><span class="linenos">54</span></a><span class="sd">        - **clamp_threshold** (`float`): the threshold for task embedding gradient compensation. See Sec. 2.5 &quot;Embedding Gradient Compensation&quot; in the [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="AdaHAT.__init__-55"><a href="#AdaHAT.__init__-55"><span class="linenos">55</span></a><span class="sd">        - **mask_sparsity_reg_factor** (`float`): hyperparameter, the regularization factor for mask sparsity.</span>
</span><span id="AdaHAT.__init__-56"><a href="#AdaHAT.__init__-56"><span class="linenos">56</span></a><span class="sd">        - **mask_sparsity_reg_mode** (`str`): the mode of mask sparsity regularization, must be one of:</span>
</span><span id="AdaHAT.__init__-57"><a href="#AdaHAT.__init__-57"><span class="linenos">57</span></a><span class="sd">            1. &#39;original&#39; (default): the original mask sparsity regularization in the [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="AdaHAT.__init__-58"><a href="#AdaHAT.__init__-58"><span class="linenos">58</span></a><span class="sd">            2. &#39;cross&#39;: the cross version of mask sparsity regularization.</span>
</span><span id="AdaHAT.__init__-59"><a href="#AdaHAT.__init__-59"><span class="linenos">59</span></a><span class="sd">        - **task_embedding_init_mode** (`str`): the initialization mode for task embeddings, must be one of:</span>
</span><span id="AdaHAT.__init__-60"><a href="#AdaHAT.__init__-60"><span class="linenos">60</span></a><span class="sd">            1. &#39;N01&#39; (default): standard normal distribution $N(0, 1)$.</span>
</span><span id="AdaHAT.__init__-61"><a href="#AdaHAT.__init__-61"><span class="linenos">61</span></a><span class="sd">            2. &#39;U-11&#39;: uniform distribution $U(-1, 1)$.</span>
</span><span id="AdaHAT.__init__-62"><a href="#AdaHAT.__init__-62"><span class="linenos">62</span></a><span class="sd">            3. &#39;U01&#39;: uniform distribution $U(0, 1)$.</span>
</span><span id="AdaHAT.__init__-63"><a href="#AdaHAT.__init__-63"><span class="linenos">63</span></a><span class="sd">            4. &#39;U-10&#39;: uniform distribution $U(-1, 0)$.</span>
</span><span id="AdaHAT.__init__-64"><a href="#AdaHAT.__init__-64"><span class="linenos">64</span></a><span class="sd">            5. &#39;last&#39;: inherit the task embedding from the last task.</span>
</span><span id="AdaHAT.__init__-65"><a href="#AdaHAT.__init__-65"><span class="linenos">65</span></a><span class="sd">        - **epsilon** (`float`): the value added to network sparsity to avoid division by zero (appearing in Eq. (9) of the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9)).</span>
</span><span id="AdaHAT.__init__-66"><a href="#AdaHAT.__init__-66"><span class="linenos">66</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="AdaHAT.__init__-67"><a href="#AdaHAT.__init__-67"><span class="linenos">67</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="AdaHAT.__init__-68"><a href="#AdaHAT.__init__-68"><span class="linenos">68</span></a>            <span class="n">backbone</span><span class="o">=</span><span class="n">backbone</span><span class="p">,</span>
</span><span id="AdaHAT.__init__-69"><a href="#AdaHAT.__init__-69"><span class="linenos">69</span></a>            <span class="n">heads</span><span class="o">=</span><span class="n">heads</span><span class="p">,</span>
</span><span id="AdaHAT.__init__-70"><a href="#AdaHAT.__init__-70"><span class="linenos">70</span></a>            <span class="n">adjustment_mode</span><span class="o">=</span><span class="n">adjustment_mode</span><span class="p">,</span>
</span><span id="AdaHAT.__init__-71"><a href="#AdaHAT.__init__-71"><span class="linenos">71</span></a>            <span class="n">s_max</span><span class="o">=</span><span class="n">s_max</span><span class="p">,</span>
</span><span id="AdaHAT.__init__-72"><a href="#AdaHAT.__init__-72"><span class="linenos">72</span></a>            <span class="n">clamp_threshold</span><span class="o">=</span><span class="n">clamp_threshold</span><span class="p">,</span>
</span><span id="AdaHAT.__init__-73"><a href="#AdaHAT.__init__-73"><span class="linenos">73</span></a>            <span class="n">mask_sparsity_reg_factor</span><span class="o">=</span><span class="n">mask_sparsity_reg_factor</span><span class="p">,</span>
</span><span id="AdaHAT.__init__-74"><a href="#AdaHAT.__init__-74"><span class="linenos">74</span></a>            <span class="n">mask_sparsity_reg_mode</span><span class="o">=</span><span class="n">mask_sparsity_reg_mode</span><span class="p">,</span>
</span><span id="AdaHAT.__init__-75"><a href="#AdaHAT.__init__-75"><span class="linenos">75</span></a>            <span class="n">task_embedding_init_mode</span><span class="o">=</span><span class="n">task_embedding_init_mode</span><span class="p">,</span>
</span><span id="AdaHAT.__init__-76"><a href="#AdaHAT.__init__-76"><span class="linenos">76</span></a>            <span class="n">alpha</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="AdaHAT.__init__-77"><a href="#AdaHAT.__init__-77"><span class="linenos">77</span></a>        <span class="p">)</span>
</span><span id="AdaHAT.__init__-78"><a href="#AdaHAT.__init__-78"><span class="linenos">78</span></a>
</span><span id="AdaHAT.__init__-79"><a href="#AdaHAT.__init__-79"><span class="linenos">79</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">adjustment_intensity</span>
</span><span id="AdaHAT.__init__-80"><a href="#AdaHAT.__init__-80"><span class="linenos">80</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The adjustment intensity in Eq. (9) of the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).&quot;&quot;&quot;</span>
</span><span id="AdaHAT.__init__-81"><a href="#AdaHAT.__init__-81"><span class="linenos">81</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">epsilon</span>
</span><span id="AdaHAT.__init__-82"><a href="#AdaHAT.__init__-82"><span class="linenos">82</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The small value to avoid division by zero (appearing in Eq. (9) of the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9)).&quot;&quot;&quot;</span>
</span><span id="AdaHAT.__init__-83"><a href="#AdaHAT.__init__-83"><span class="linenos">83</span></a>
</span><span id="AdaHAT.__init__-84"><a href="#AdaHAT.__init__-84"><span class="linenos">84</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">summative_mask_for_previous_tasks</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="AdaHAT.__init__-85"><a href="#AdaHAT.__init__-85"><span class="linenos">85</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The summative binary attention mask $\mathrm{M}^{&lt;t,\text{sum}}$ of previous tasks $1,\cdots, t-1$, gated from the task embedding. It is a dict where keys are layer names and values are the binary mask tensors for the layers. The mask tensor has size (number of units, ).&quot;&quot;&quot;</span>
</span><span id="AdaHAT.__init__-86"><a href="#AdaHAT.__init__-86"><span class="linenos">86</span></a>
</span><span id="AdaHAT.__init__-87"><a href="#AdaHAT.__init__-87"><span class="linenos">87</span></a>        <span class="c1"># set manual optimization</span>
</span><span id="AdaHAT.__init__-88"><a href="#AdaHAT.__init__-88"><span class="linenos">88</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">automatic_optimization</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="AdaHAT.__init__-89"><a href="#AdaHAT.__init__-89"><span class="linenos">89</span></a>
</span><span id="AdaHAT.__init__-90"><a href="#AdaHAT.__init__-90"><span class="linenos">90</span></a>        <span class="n">AdaHAT</span><span class="o">.</span><span class="n">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Initialize the AdaHAT algorithm with the network.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>backbone</strong> (<code>HATMaskBackbone</code>): must be a backbone network with the HAT mask mechanism.</li>
<li><strong>heads</strong> (<code>HeadsTIL</code>): output heads. AdaHAT supports only TIL (Task-Incremental Learning).</li>
<li><strong>adjustment_mode</strong> (<code>str</code>): the strategy of adjustment (i.e., the mode of gradient clipping), must be one of:
<ol>
<li>'adahat': set gradients of parameters linking to masked units to a soft adjustment rate in the original AdaHAT approach (allows slight updates on previous-task parameters). See Eqs. (8) and (9) in Sec. 3.1 of the <a href="https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9">AdaHAT paper</a>.</li>
<li>'adahat_no_sum': as above but without parameter-importance (i.e., no summative mask). See Sec. 4.3 (ablation study) in the <a href="https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9">AdaHAT paper</a>.</li>
<li>'adahat_no_reg': as above but without network sparsity (i.e., no mask sparsity regularization term). See Sec. 4.3 (ablation study) in the <a href="https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9">AdaHAT paper</a>.</li>
</ol></li>
<li><strong>adjustment_intensity</strong> (<code><a href="#AdaHAT.float">float</a></code>): hyperparameter, controls the overall intensity of gradient adjustment (the $\alpha$ in Eq. (9) of the <a href="https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9">AdaHAT paper</a>).</li>
<li><strong>s_max</strong> (<code><a href="#AdaHAT.float">float</a></code>): hyperparameter, the maximum scaling factor in the gate function. See Sec. 2.4 "Hard Attention Training" in the <a href="http://proceedings.mlr.press/v80/serra18a">HAT paper</a>.</li>
<li><strong>clamp_threshold</strong> (<code><a href="#AdaHAT.float">float</a></code>): the threshold for task embedding gradient compensation. See Sec. 2.5 "Embedding Gradient Compensation" in the <a href="http://proceedings.mlr.press/v80/serra18a">HAT paper</a>.</li>
<li><strong>mask_sparsity_reg_factor</strong> (<code><a href="#AdaHAT.float">float</a></code>): hyperparameter, the regularization factor for mask sparsity.</li>
<li><strong>mask_sparsity_reg_mode</strong> (<code>str</code>): the mode of mask sparsity regularization, must be one of:
<ol>
<li>'original' (default): the original mask sparsity regularization in the <a href="http://proceedings.mlr.press/v80/serra18a">HAT paper</a>.</li>
<li>'cross': the cross version of mask sparsity regularization.</li>
</ol></li>
<li><strong>task_embedding_init_mode</strong> (<code>str</code>): the initialization mode for task embeddings, must be one of:
<ol>
<li>'N01' (default): standard normal distribution $N(0, 1)$.</li>
<li>'U-11': uniform distribution $U(-1, 1)$.</li>
<li>'U01': uniform distribution $U(0, 1)$.</li>
<li>'U-10': uniform distribution $U(-1, 0)$.</li>
<li>'last': inherit the task embedding from the last task.</li>
</ol></li>
<li><strong>epsilon</strong> (<code><a href="#AdaHAT.float">float</a></code>): the value added to network sparsity to avoid division by zero (appearing in Eq. (9) of the <a href="https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9">AdaHAT paper</a>).</li>
</ul>
</div>


                            </div>
                            <div id="AdaHAT.adjustment_intensity" class="classattr">
                                <div class="attr variable">
            <span class="name">adjustment_intensity</span><span class="annotation">: float</span>

        
    </div>
    <a class="headerlink" href="#AdaHAT.adjustment_intensity"></a>
    
            <div class="docstring"><p>The adjustment intensity in Eq. (9) of the <a href="https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9">AdaHAT paper</a>.</p>
</div>


                            </div>
                            <div id="AdaHAT.epsilon" class="classattr">
                                <div class="attr variable">
            <span class="name">epsilon</span><span class="annotation">: float | None</span>

        
    </div>
    <a class="headerlink" href="#AdaHAT.epsilon"></a>
    
            <div class="docstring"><p>The small value to avoid division by zero (appearing in Eq. (9) of the <a href="https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9">AdaHAT paper</a>).</p>
</div>


                            </div>
                            <div id="AdaHAT.summative_mask_for_previous_tasks" class="classattr">
                                <div class="attr variable">
            <span class="name">summative_mask_for_previous_tasks</span><span class="annotation">: dict[str, torch.Tensor]</span>

        
    </div>
    <a class="headerlink" href="#AdaHAT.summative_mask_for_previous_tasks"></a>
    
            <div class="docstring"><p>The summative binary attention mask $\mathrm{M}^{<t,\text{sum}}$ of previous tasks $1,\cdots, t-1$, gated from the task embedding. It is a dict where keys are layer names and values are the binary mask tensors for the layers. The mask tensor has size (number of units, ).</p>
</div>


                            </div>
                            <div id="AdaHAT.automatic_optimization" class="classattr">
                                        <input id="AdaHAT.automatic_optimization-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr variable">
            <span class="name">automatic_optimization</span><span class="annotation">: bool</span>

                <label class="view-source-button" for="AdaHAT.automatic_optimization-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#AdaHAT.automatic_optimization"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="AdaHAT.automatic_optimization-290"><a href="#AdaHAT.automatic_optimization-290"><span class="linenos">290</span></a>    <span class="nd">@property</span>
</span><span id="AdaHAT.automatic_optimization-291"><a href="#AdaHAT.automatic_optimization-291"><span class="linenos">291</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">automatic_optimization</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
</span><span id="AdaHAT.automatic_optimization-292"><a href="#AdaHAT.automatic_optimization-292"><span class="linenos">292</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``.&quot;&quot;&quot;</span>
</span><span id="AdaHAT.automatic_optimization-293"><a href="#AdaHAT.automatic_optimization-293"><span class="linenos">293</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_automatic_optimization</span>
</span></pre></div>


            <div class="docstring"><p>If set to <code>False</code> you are responsible for calling <code>.backward()</code>, <code>.step()</code>, <code>.zero_grad()</code>.</p>
</div>


                            </div>
                            <div id="AdaHAT.sanity_check" class="classattr">
                                        <input id="AdaHAT.sanity_check-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">sanity_check</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span></span><span class="return-annotation">) -> <span class="kc">None</span>:</span></span>

                <label class="view-source-button" for="AdaHAT.sanity_check-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#AdaHAT.sanity_check"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="AdaHAT.sanity_check-92"><a href="#AdaHAT.sanity_check-92"><span class="linenos">92</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="AdaHAT.sanity_check-93"><a href="#AdaHAT.sanity_check-93"><span class="linenos">93</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Check the sanity of the arguments.&quot;&quot;&quot;</span>
</span><span id="AdaHAT.sanity_check-94"><a href="#AdaHAT.sanity_check-94"><span class="linenos">94</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="AdaHAT.sanity_check-95"><a href="#AdaHAT.sanity_check-95"><span class="linenos">95</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="AdaHAT.sanity_check-96"><a href="#AdaHAT.sanity_check-96"><span class="linenos">96</span></a>                <span class="sa">f</span><span class="s2">&quot;The adjustment intensity should be positive, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span><span class="si">}</span><span class="s2">.&quot;</span>
</span><span id="AdaHAT.sanity_check-97"><a href="#AdaHAT.sanity_check-97"><span class="linenos">97</span></a>            <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Check the sanity of the arguments.</p>
</div>


                            </div>
                            <div id="AdaHAT.on_train_start" class="classattr">
                                        <input id="AdaHAT.on_train_start-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">on_train_start</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span></span><span class="return-annotation">) -> <span class="kc">None</span>:</span></span>

                <label class="view-source-button" for="AdaHAT.on_train_start-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#AdaHAT.on_train_start"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="AdaHAT.on_train_start-99"><a href="#AdaHAT.on_train_start-99"><span class="linenos"> 99</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_start</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="AdaHAT.on_train_start-100"><a href="#AdaHAT.on_train_start-100"><span class="linenos">100</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Additionally initialize the summative mask at the beginning of the first task.&quot;&quot;&quot;</span>
</span><span id="AdaHAT.on_train_start-101"><a href="#AdaHAT.on_train_start-101"><span class="linenos">101</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">on_train_start</span><span class="p">()</span>
</span><span id="AdaHAT.on_train_start-102"><a href="#AdaHAT.on_train_start-102"><span class="linenos">102</span></a>
</span><span id="AdaHAT.on_train_start-103"><a href="#AdaHAT.on_train_start-103"><span class="linenos">103</span></a>        <span class="c1"># initialize the summative mask at the beginning of the first task. This should not be called in `__init__()` method because `self.device` is not available at that time</span>
</span><span id="AdaHAT.on_train_start-104"><a href="#AdaHAT.on_train_start-104"><span class="linenos">104</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="AdaHAT.on_train_start-105"><a href="#AdaHAT.on_train_start-105"><span class="linenos">105</span></a>            <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="AdaHAT.on_train_start-106"><a href="#AdaHAT.on_train_start-106"><span class="linenos">106</span></a>                <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span>
</span><span id="AdaHAT.on_train_start-107"><a href="#AdaHAT.on_train_start-107"><span class="linenos">107</span></a>                    <span class="n">layer_name</span>
</span><span id="AdaHAT.on_train_start-108"><a href="#AdaHAT.on_train_start-108"><span class="linenos">108</span></a>                <span class="p">)</span>  <span class="c1"># get the layer by its name</span>
</span><span id="AdaHAT.on_train_start-109"><a href="#AdaHAT.on_train_start-109"><span class="linenos">109</span></a>                <span class="n">num_units</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="AdaHAT.on_train_start-110"><a href="#AdaHAT.on_train_start-110"><span class="linenos">110</span></a>
</span><span id="AdaHAT.on_train_start-111"><a href="#AdaHAT.on_train_start-111"><span class="linenos">111</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">summative_mask_for_previous_tasks</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
</span><span id="AdaHAT.on_train_start-112"><a href="#AdaHAT.on_train_start-112"><span class="linenos">112</span></a>                    <span class="n">num_units</span>
</span><span id="AdaHAT.on_train_start-113"><a href="#AdaHAT.on_train_start-113"><span class="linenos">113</span></a>                <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
</span><span id="AdaHAT.on_train_start-114"><a href="#AdaHAT.on_train_start-114"><span class="linenos">114</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
</span><span id="AdaHAT.on_train_start-115"><a href="#AdaHAT.on_train_start-115"><span class="linenos">115</span></a>                <span class="p">)</span>  <span class="c1"># the summative mask $\mathrm{M}^{&lt;t,\text{sum}}$ is initialized as a zeros mask for $t = 1$. See Eq. (7) in Sec. 3.1 of the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9)</span>
</span></pre></div>


            <div class="docstring"><p>Additionally initialize the summative mask at the beginning of the first task.</p>
</div>


                            </div>
                            <div id="AdaHAT.clip_grad_by_adjustment" class="classattr">
                                        <input id="AdaHAT.clip_grad_by_adjustment-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">clip_grad_by_adjustment</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">network_sparsity</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span></span><span class="return-annotation">) -> <span class="nb">tuple</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>:</span></span>

                <label class="view-source-button" for="AdaHAT.clip_grad_by_adjustment-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#AdaHAT.clip_grad_by_adjustment"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="AdaHAT.clip_grad_by_adjustment-117"><a href="#AdaHAT.clip_grad_by_adjustment-117"><span class="linenos">117</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">clip_grad_by_adjustment</span><span class="p">(</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-118"><a href="#AdaHAT.clip_grad_by_adjustment-118"><span class="linenos">118</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-119"><a href="#AdaHAT.clip_grad_by_adjustment-119"><span class="linenos">119</span></a>        <span class="n">network_sparsity</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-120"><a href="#AdaHAT.clip_grad_by_adjustment-120"><span class="linenos">120</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="n">Tensor</span><span class="p">]:</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-121"><a href="#AdaHAT.clip_grad_by_adjustment-121"><span class="linenos">121</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Clip the gradients by the adjustment rate. See Eq. (8) in the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-122"><a href="#AdaHAT.clip_grad_by_adjustment-122"><span class="linenos">122</span></a>
</span><span id="AdaHAT.clip_grad_by_adjustment-123"><a href="#AdaHAT.clip_grad_by_adjustment-123"><span class="linenos">123</span></a><span class="sd">        Note that because the task embedding fully covers every layer in the backbone network, no parameters are left out of this system. This applies not only to parameters between layers with task embeddings, but also to those before the first layer. We design it separately in the code.</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-124"><a href="#AdaHAT.clip_grad_by_adjustment-124"><span class="linenos">124</span></a>
</span><span id="AdaHAT.clip_grad_by_adjustment-125"><a href="#AdaHAT.clip_grad_by_adjustment-125"><span class="linenos">125</span></a><span class="sd">        Network capacity is measured alongside this method. Network capacity is defined as the average adjustment rate over all parameters. See Sec. 4.1 in the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9).</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-126"><a href="#AdaHAT.clip_grad_by_adjustment-126"><span class="linenos">126</span></a>
</span><span id="AdaHAT.clip_grad_by_adjustment-127"><a href="#AdaHAT.clip_grad_by_adjustment-127"><span class="linenos">127</span></a><span class="sd">        **Args:**</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-128"><a href="#AdaHAT.clip_grad_by_adjustment-128"><span class="linenos">128</span></a><span class="sd">        - **network_sparsity** (`dict[str, Tensor]` | `None`): the network sparsity (i.e., the mask sparsity loss of each layer) for the current task. Keys are layer names and values are the network sparsity values. It is used to calculate the adjustment rate for gradients. Applies only to mode `adahat` and `adahat_no_sum`, not `adahat_no_reg`.</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-129"><a href="#AdaHAT.clip_grad_by_adjustment-129"><span class="linenos">129</span></a>
</span><span id="AdaHAT.clip_grad_by_adjustment-130"><a href="#AdaHAT.clip_grad_by_adjustment-130"><span class="linenos">130</span></a><span class="sd">        **Returns:**</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-131"><a href="#AdaHAT.clip_grad_by_adjustment-131"><span class="linenos">131</span></a><span class="sd">        - **adjustment_rate_weight** (`dict[str, Tensor]`): the adjustment rate for weights. Keys (`str`) are layer names and values (`Tensor`) are the adjustment rate tensors.</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-132"><a href="#AdaHAT.clip_grad_by_adjustment-132"><span class="linenos">132</span></a><span class="sd">        - **adjustment_rate_bias** (`dict[str, Tensor]`): the adjustment rate for biases. Keys (`str`) are layer names and values (`Tensor`) are the adjustment rate tensors.</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-133"><a href="#AdaHAT.clip_grad_by_adjustment-133"><span class="linenos">133</span></a><span class="sd">        - **capacity** (`Tensor`): the calculated network capacity.</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-134"><a href="#AdaHAT.clip_grad_by_adjustment-134"><span class="linenos">134</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-135"><a href="#AdaHAT.clip_grad_by_adjustment-135"><span class="linenos">135</span></a>
</span><span id="AdaHAT.clip_grad_by_adjustment-136"><a href="#AdaHAT.clip_grad_by_adjustment-136"><span class="linenos">136</span></a>        <span class="c1"># initialize network capacity metric</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-137"><a href="#AdaHAT.clip_grad_by_adjustment-137"><span class="linenos">137</span></a>        <span class="n">capacity</span> <span class="o">=</span> <span class="n">HATNetworkCapacityMetric</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-138"><a href="#AdaHAT.clip_grad_by_adjustment-138"><span class="linenos">138</span></a>        <span class="n">adjustment_rate_weight</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-139"><a href="#AdaHAT.clip_grad_by_adjustment-139"><span class="linenos">139</span></a>        <span class="n">adjustment_rate_bias</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-140"><a href="#AdaHAT.clip_grad_by_adjustment-140"><span class="linenos">140</span></a>
</span><span id="AdaHAT.clip_grad_by_adjustment-141"><a href="#AdaHAT.clip_grad_by_adjustment-141"><span class="linenos">141</span></a>        <span class="c1"># calculate the adjustment rate for gradients of the parameters, both weights and biases (if they exist). See Eq. (9) in the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9)</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-142"><a href="#AdaHAT.clip_grad_by_adjustment-142"><span class="linenos">142</span></a>        <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-143"><a href="#AdaHAT.clip_grad_by_adjustment-143"><span class="linenos">143</span></a>
</span><span id="AdaHAT.clip_grad_by_adjustment-144"><a href="#AdaHAT.clip_grad_by_adjustment-144"><span class="linenos">144</span></a>            <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-145"><a href="#AdaHAT.clip_grad_by_adjustment-145"><span class="linenos">145</span></a>                <span class="n">layer_name</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-146"><a href="#AdaHAT.clip_grad_by_adjustment-146"><span class="linenos">146</span></a>            <span class="p">)</span>  <span class="c1"># get the layer by its name</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-147"><a href="#AdaHAT.clip_grad_by_adjustment-147"><span class="linenos">147</span></a>
</span><span id="AdaHAT.clip_grad_by_adjustment-148"><a href="#AdaHAT.clip_grad_by_adjustment-148"><span class="linenos">148</span></a>            <span class="c1"># placeholder for the adjustment rate to avoid the error of using it before assignment</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-149"><a href="#AdaHAT.clip_grad_by_adjustment-149"><span class="linenos">149</span></a>            <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-150"><a href="#AdaHAT.clip_grad_by_adjustment-150"><span class="linenos">150</span></a>            <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-151"><a href="#AdaHAT.clip_grad_by_adjustment-151"><span class="linenos">151</span></a>
</span><span id="AdaHAT.clip_grad_by_adjustment-152"><a href="#AdaHAT.clip_grad_by_adjustment-152"><span class="linenos">152</span></a>            <span class="n">weight_importance</span><span class="p">,</span> <span class="n">bias_importance</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-153"><a href="#AdaHAT.clip_grad_by_adjustment-153"><span class="linenos">153</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_layer_measure_parameter_wise</span><span class="p">(</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-154"><a href="#AdaHAT.clip_grad_by_adjustment-154"><span class="linenos">154</span></a>                    <span class="n">unit_wise_measure</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">summative_mask_for_previous_tasks</span><span class="p">,</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-155"><a href="#AdaHAT.clip_grad_by_adjustment-155"><span class="linenos">155</span></a>                    <span class="n">layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-156"><a href="#AdaHAT.clip_grad_by_adjustment-156"><span class="linenos">156</span></a>                    <span class="n">aggregation_mode</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-157"><a href="#AdaHAT.clip_grad_by_adjustment-157"><span class="linenos">157</span></a>                <span class="p">)</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-158"><a href="#AdaHAT.clip_grad_by_adjustment-158"><span class="linenos">158</span></a>            <span class="p">)</span>  <span class="c1"># AdaHAT depends on parameter importance rather than parameter masks (as in HAT)</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-159"><a href="#AdaHAT.clip_grad_by_adjustment-159"><span class="linenos">159</span></a>
</span><span id="AdaHAT.clip_grad_by_adjustment-160"><a href="#AdaHAT.clip_grad_by_adjustment-160"><span class="linenos">160</span></a>            <span class="n">network_sparsity_layer</span> <span class="o">=</span> <span class="n">network_sparsity</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-161"><a href="#AdaHAT.clip_grad_by_adjustment-161"><span class="linenos">161</span></a>
</span><span id="AdaHAT.clip_grad_by_adjustment-162"><a href="#AdaHAT.clip_grad_by_adjustment-162"><span class="linenos">162</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;adahat&quot;</span><span class="p">:</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-163"><a href="#AdaHAT.clip_grad_by_adjustment-163"><span class="linenos">163</span></a>                <span class="n">r_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span> <span class="o">/</span> <span class="p">(</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-164"><a href="#AdaHAT.clip_grad_by_adjustment-164"><span class="linenos">164</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">+</span> <span class="n">network_sparsity_layer</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-165"><a href="#AdaHAT.clip_grad_by_adjustment-165"><span class="linenos">165</span></a>                <span class="p">)</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-166"><a href="#AdaHAT.clip_grad_by_adjustment-166"><span class="linenos">166</span></a>                <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-167"><a href="#AdaHAT.clip_grad_by_adjustment-167"><span class="linenos">167</span></a>                    <span class="n">r_layer</span><span class="p">,</span> <span class="p">(</span><span class="n">weight_importance</span> <span class="o">+</span> <span class="n">r_layer</span><span class="p">)</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-168"><a href="#AdaHAT.clip_grad_by_adjustment-168"><span class="linenos">168</span></a>                <span class="p">)</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-169"><a href="#AdaHAT.clip_grad_by_adjustment-169"><span class="linenos">169</span></a>                <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-170"><a href="#AdaHAT.clip_grad_by_adjustment-170"><span class="linenos">170</span></a>                    <span class="n">r_layer</span><span class="p">,</span> <span class="p">(</span><span class="n">bias_importance</span> <span class="o">+</span> <span class="n">r_layer</span><span class="p">)</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-171"><a href="#AdaHAT.clip_grad_by_adjustment-171"><span class="linenos">171</span></a>                <span class="p">)</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-172"><a href="#AdaHAT.clip_grad_by_adjustment-172"><span class="linenos">172</span></a>
</span><span id="AdaHAT.clip_grad_by_adjustment-173"><a href="#AdaHAT.clip_grad_by_adjustment-173"><span class="linenos">173</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;adahat_no_sum&quot;</span><span class="p">:</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-174"><a href="#AdaHAT.clip_grad_by_adjustment-174"><span class="linenos">174</span></a>
</span><span id="AdaHAT.clip_grad_by_adjustment-175"><a href="#AdaHAT.clip_grad_by_adjustment-175"><span class="linenos">175</span></a>                <span class="n">r_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span> <span class="o">/</span> <span class="p">(</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-176"><a href="#AdaHAT.clip_grad_by_adjustment-176"><span class="linenos">176</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">+</span> <span class="n">network_sparsity_layer</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-177"><a href="#AdaHAT.clip_grad_by_adjustment-177"><span class="linenos">177</span></a>                <span class="p">)</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-178"><a href="#AdaHAT.clip_grad_by_adjustment-178"><span class="linenos">178</span></a>                <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-179"><a href="#AdaHAT.clip_grad_by_adjustment-179"><span class="linenos">179</span></a>                    <span class="n">r_layer</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">+</span> <span class="n">r_layer</span><span class="p">)</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-180"><a href="#AdaHAT.clip_grad_by_adjustment-180"><span class="linenos">180</span></a>                <span class="p">)</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-181"><a href="#AdaHAT.clip_grad_by_adjustment-181"><span class="linenos">181</span></a>                <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-182"><a href="#AdaHAT.clip_grad_by_adjustment-182"><span class="linenos">182</span></a>                    <span class="n">r_layer</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">+</span> <span class="n">r_layer</span><span class="p">)</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-183"><a href="#AdaHAT.clip_grad_by_adjustment-183"><span class="linenos">183</span></a>                <span class="p">)</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-184"><a href="#AdaHAT.clip_grad_by_adjustment-184"><span class="linenos">184</span></a>
</span><span id="AdaHAT.clip_grad_by_adjustment-185"><a href="#AdaHAT.clip_grad_by_adjustment-185"><span class="linenos">185</span></a>            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_mode</span> <span class="o">==</span> <span class="s2">&quot;adahat_no_reg&quot;</span><span class="p">:</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-186"><a href="#AdaHAT.clip_grad_by_adjustment-186"><span class="linenos">186</span></a>
</span><span id="AdaHAT.clip_grad_by_adjustment-187"><a href="#AdaHAT.clip_grad_by_adjustment-187"><span class="linenos">187</span></a>                <span class="n">r_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjustment_intensity</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">+</span> <span class="mf">0.0</span><span class="p">)</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-188"><a href="#AdaHAT.clip_grad_by_adjustment-188"><span class="linenos">188</span></a>                <span class="n">adjustment_rate_weight_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-189"><a href="#AdaHAT.clip_grad_by_adjustment-189"><span class="linenos">189</span></a>                    <span class="n">r_layer</span><span class="p">,</span> <span class="p">(</span><span class="n">weight_importance</span> <span class="o">+</span> <span class="n">r_layer</span><span class="p">)</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-190"><a href="#AdaHAT.clip_grad_by_adjustment-190"><span class="linenos">190</span></a>                <span class="p">)</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-191"><a href="#AdaHAT.clip_grad_by_adjustment-191"><span class="linenos">191</span></a>                <span class="n">adjustment_rate_bias_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-192"><a href="#AdaHAT.clip_grad_by_adjustment-192"><span class="linenos">192</span></a>                    <span class="n">r_layer</span><span class="p">,</span> <span class="p">(</span><span class="n">bias_importance</span> <span class="o">+</span> <span class="n">r_layer</span><span class="p">)</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-193"><a href="#AdaHAT.clip_grad_by_adjustment-193"><span class="linenos">193</span></a>                <span class="p">)</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-194"><a href="#AdaHAT.clip_grad_by_adjustment-194"><span class="linenos">194</span></a>
</span><span id="AdaHAT.clip_grad_by_adjustment-195"><a href="#AdaHAT.clip_grad_by_adjustment-195"><span class="linenos">195</span></a>            <span class="c1"># apply the adjustment rate to the gradients</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-196"><a href="#AdaHAT.clip_grad_by_adjustment-196"><span class="linenos">196</span></a>            <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">adjustment_rate_weight_layer</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-197"><a href="#AdaHAT.clip_grad_by_adjustment-197"><span class="linenos">197</span></a>            <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-198"><a href="#AdaHAT.clip_grad_by_adjustment-198"><span class="linenos">198</span></a>                <span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">adjustment_rate_bias_layer</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-199"><a href="#AdaHAT.clip_grad_by_adjustment-199"><span class="linenos">199</span></a>
</span><span id="AdaHAT.clip_grad_by_adjustment-200"><a href="#AdaHAT.clip_grad_by_adjustment-200"><span class="linenos">200</span></a>            <span class="c1"># store the adjustment rate for logging</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-201"><a href="#AdaHAT.clip_grad_by_adjustment-201"><span class="linenos">201</span></a>            <span class="n">adjustment_rate_weight</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">adjustment_rate_weight_layer</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-202"><a href="#AdaHAT.clip_grad_by_adjustment-202"><span class="linenos">202</span></a>            <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-203"><a href="#AdaHAT.clip_grad_by_adjustment-203"><span class="linenos">203</span></a>                <span class="n">adjustment_rate_bias</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">adjustment_rate_bias_layer</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-204"><a href="#AdaHAT.clip_grad_by_adjustment-204"><span class="linenos">204</span></a>
</span><span id="AdaHAT.clip_grad_by_adjustment-205"><a href="#AdaHAT.clip_grad_by_adjustment-205"><span class="linenos">205</span></a>            <span class="c1"># update network capacity metric</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-206"><a href="#AdaHAT.clip_grad_by_adjustment-206"><span class="linenos">206</span></a>            <span class="n">capacity</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">adjustment_rate_weight_layer</span><span class="p">,</span> <span class="n">adjustment_rate_bias_layer</span><span class="p">)</span>
</span><span id="AdaHAT.clip_grad_by_adjustment-207"><a href="#AdaHAT.clip_grad_by_adjustment-207"><span class="linenos">207</span></a>
</span><span id="AdaHAT.clip_grad_by_adjustment-208"><a href="#AdaHAT.clip_grad_by_adjustment-208"><span class="linenos">208</span></a>        <span class="k">return</span> <span class="n">adjustment_rate_weight</span><span class="p">,</span> <span class="n">adjustment_rate_bias</span><span class="p">,</span> <span class="n">capacity</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
</span></pre></div>


            <div class="docstring"><p>Clip the gradients by the adjustment rate. See Eq. (8) in the <a href="https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9">AdaHAT paper</a>.</p>

<p>Note that because the task embedding fully covers every layer in the backbone network, no parameters are left out of this system. This applies not only to parameters between layers with task embeddings, but also to those before the first layer. We design it separately in the code.</p>

<p>Network capacity is measured alongside this method. Network capacity is defined as the average adjustment rate over all parameters. See Sec. 4.1 in the <a href="https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9">AdaHAT paper</a>.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>network_sparsity</strong> (<code>dict[str, Tensor]</code> | <code>None</code>): the network sparsity (i.e., the mask sparsity loss of each layer) for the current task. Keys are layer names and values are the network sparsity values. It is used to calculate the adjustment rate for gradients. Applies only to mode <code>adahat</code> and <code>adahat_no_sum</code>, not <code>adahat_no_reg</code>.</li>
</ul>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>adjustment_rate_weight</strong> (<code>dict[str, Tensor]</code>): the adjustment rate for weights. Keys (<code>str</code>) are layer names and values (<code>Tensor</code>) are the adjustment rate tensors.</li>
<li><strong>adjustment_rate_bias</strong> (<code>dict[str, Tensor]</code>): the adjustment rate for biases. Keys (<code>str</code>) are layer names and values (<code>Tensor</code>) are the adjustment rate tensors.</li>
<li><strong>capacity</strong> (<code>Tensor</code>): the calculated network capacity.</li>
</ul>
</div>


                            </div>
                            <div id="AdaHAT.on_train_end" class="classattr">
                                        <input id="AdaHAT.on_train_end-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">on_train_end</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span></span><span class="return-annotation">) -> <span class="kc">None</span>:</span></span>

                <label class="view-source-button" for="AdaHAT.on_train_end-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#AdaHAT.on_train_end"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="AdaHAT.on_train_end-210"><a href="#AdaHAT.on_train_end-210"><span class="linenos">210</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_end</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="AdaHAT.on_train_end-211"><a href="#AdaHAT.on_train_end-211"><span class="linenos">211</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Additionally update the summative mask after training the task.&quot;&quot;&quot;</span>
</span><span id="AdaHAT.on_train_end-212"><a href="#AdaHAT.on_train_end-212"><span class="linenos">212</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">on_train_end</span><span class="p">()</span>
</span><span id="AdaHAT.on_train_end-213"><a href="#AdaHAT.on_train_end-213"><span class="linenos">213</span></a>
</span><span id="AdaHAT.on_train_end-214"><a href="#AdaHAT.on_train_end-214"><span class="linenos">214</span></a>        <span class="n">mask_t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">masks</span><span class="p">[</span>
</span><span id="AdaHAT.on_train_end-215"><a href="#AdaHAT.on_train_end-215"><span class="linenos">215</span></a>            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="AdaHAT.on_train_end-216"><a href="#AdaHAT.on_train_end-216"><span class="linenos">216</span></a>        <span class="p">]</span>  <span class="c1"># get stored mask for the current task again</span>
</span><span id="AdaHAT.on_train_end-217"><a href="#AdaHAT.on_train_end-217"><span class="linenos">217</span></a>
</span><span id="AdaHAT.on_train_end-218"><a href="#AdaHAT.on_train_end-218"><span class="linenos">218</span></a>        <span class="c1"># update the summative mask for previous tasks. See Eq. (7) in Sec. 3.1 of the [AdaHAT paper](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9)</span>
</span><span id="AdaHAT.on_train_end-219"><a href="#AdaHAT.on_train_end-219"><span class="linenos">219</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">summative_mask_for_previous_tasks</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="AdaHAT.on_train_end-220"><a href="#AdaHAT.on_train_end-220"><span class="linenos">220</span></a>            <span class="n">layer_name</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">summative_mask_for_previous_tasks</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="AdaHAT.on_train_end-221"><a href="#AdaHAT.on_train_end-221"><span class="linenos">221</span></a>            <span class="o">+</span> <span class="n">mask_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="AdaHAT.on_train_end-222"><a href="#AdaHAT.on_train_end-222"><span class="linenos">222</span></a>            <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">weighted_layer_names</span>
</span><span id="AdaHAT.on_train_end-223"><a href="#AdaHAT.on_train_end-223"><span class="linenos">223</span></a>        <span class="p">}</span>
</span></pre></div>


            <div class="docstring"><p>Additionally update the summative mask after training the task.</p>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt><a href="hat.html#HAT">clarena.cl_algorithms.hat.HAT</a></dt>
                                <dd id="AdaHAT.adjustment_mode" class="variable"><a href="hat.html#HAT.adjustment_mode">adjustment_mode</a></dd>
                <dd id="AdaHAT.s_max" class="variable"><a href="hat.html#HAT.s_max">s_max</a></dd>
                <dd id="AdaHAT.clamp_threshold" class="variable"><a href="hat.html#HAT.clamp_threshold">clamp_threshold</a></dd>
                <dd id="AdaHAT.mask_sparsity_reg_factor" class="variable"><a href="hat.html#HAT.mask_sparsity_reg_factor">mask_sparsity_reg_factor</a></dd>
                <dd id="AdaHAT.mask_sparsity_reg_mode" class="variable"><a href="hat.html#HAT.mask_sparsity_reg_mode">mask_sparsity_reg_mode</a></dd>
                <dd id="AdaHAT.mark_sparsity_reg" class="variable"><a href="hat.html#HAT.mark_sparsity_reg">mark_sparsity_reg</a></dd>
                <dd id="AdaHAT.task_embedding_init_mode" class="variable"><a href="hat.html#HAT.task_embedding_init_mode">task_embedding_init_mode</a></dd>
                <dd id="AdaHAT.alpha" class="variable"><a href="hat.html#HAT.alpha">alpha</a></dd>
                <dd id="AdaHAT.cumulative_mask_for_previous_tasks" class="variable"><a href="hat.html#HAT.cumulative_mask_for_previous_tasks">cumulative_mask_for_previous_tasks</a></dd>
                <dd id="AdaHAT.compensate_task_embedding_gradients" class="function"><a href="hat.html#HAT.compensate_task_embedding_gradients">compensate_task_embedding_gradients</a></dd>
                <dd id="AdaHAT.forward" class="function"><a href="hat.html#HAT.forward">forward</a></dd>
                <dd id="AdaHAT.training_step" class="function"><a href="hat.html#HAT.training_step">training_step</a></dd>
                <dd id="AdaHAT.validation_step" class="function"><a href="hat.html#HAT.validation_step">validation_step</a></dd>
                <dd id="AdaHAT.test_step" class="function"><a href="hat.html#HAT.test_step">test_step</a></dd>

            </div>
                                </dl>
                            </div>
                </section>
    </main>
<script>
    function escapeHTML(html) {
        return document.createElement('div').appendChild(document.createTextNode(html)).parentNode.innerHTML;
    }

    const originalContent = document.querySelector("main.pdoc");
    let currentContent = originalContent;

    function setContent(innerHTML) {
        let elem;
        if (innerHTML) {
            elem = document.createElement("main");
            elem.classList.add("pdoc");
            elem.innerHTML = innerHTML;
        } else {
            elem = originalContent;
        }
        if (currentContent !== elem) {
            currentContent.replaceWith(elem);
            currentContent = elem;
        }
    }

    function getSearchTerm() {
        return (new URL(window.location)).searchParams.get("search");
    }

    const searchBox = document.querySelector(".pdoc input[type=search]");
    searchBox.addEventListener("input", function () {
        let url = new URL(window.location);
        if (searchBox.value.trim()) {
            url.hash = "";
            url.searchParams.set("search", searchBox.value);
        } else {
            url.searchParams.delete("search");
        }
        history.replaceState("", "", url.toString());
        onInput();
    });
    window.addEventListener("popstate", onInput);


    let search, searchErr;

    async function initialize() {
        try {
            search = await new Promise((resolve, reject) => {
                const script = document.createElement("script");
                script.type = "text/javascript";
                script.async = true;
                script.onload = () => resolve(window.pdocSearch);
                script.onerror = (e) => reject(e);
                script.src = "../../search.js";
                document.getElementsByTagName("head")[0].appendChild(script);
            });
        } catch (e) {
            console.error("Cannot fetch pdoc search index");
            searchErr = "Cannot fetch search index.";
        }
        onInput();

        document.querySelector("nav.pdoc").addEventListener("click", e => {
            if (e.target.hash) {
                searchBox.value = "";
                searchBox.dispatchEvent(new Event("input"));
            }
        });
    }

    function onInput() {
        setContent((() => {
            const term = getSearchTerm();
            if (!term) {
                return null
            }
            if (searchErr) {
                return `<h3>Error: ${searchErr}</h3>`
            }
            if (!search) {
                return "<h3>Searching...</h3>"
            }

            window.scrollTo({top: 0, left: 0, behavior: 'auto'});

            const results = search(term);

            let html;
            if (results.length === 0) {
                html = `No search results for '${escapeHTML(term)}'.`
            } else {
                html = `<h4>${results.length} search result${results.length > 1 ? "s" : ""} for '${escapeHTML(term)}'.</h4>`;
            }
            for (let result of results.slice(0, 10)) {
                let doc = result.doc;
                let url = `../../${doc.modulename.replaceAll(".", "/")}.html`;
                if (doc.qualname) {
                    url += `#${doc.qualname}`;
                }

                let heading;
                switch (result.doc.kind) {
                    case "function":
                        if (doc.fullname.endsWith(".__init__")) {
                            heading = `<span class="name">${doc.fullname.replace(/\.__init__$/, "")}</span>${doc.signature}`;
                        } else {
                            heading = `<span class="def">${doc.funcdef}</span> <span class="name">${doc.fullname}</span>${doc.signature}`;
                        }
                        break;
                    case "class":
                        heading = `<span class="def">class</span> <span class="name">${doc.fullname}</span>`;
                        if (doc.bases)
                            heading += `<wbr>(<span class="base">${doc.bases}</span>)`;
                        heading += `:`;
                        break;
                    case "variable":
                        heading = `<span class="name">${doc.fullname}</span>`;
                        if (doc.annotation)
                            heading += `<span class="annotation">${doc.annotation}</span>`;
                        if (doc.default_value)
                            heading += `<span class="default_value"> = ${doc.default_value}</span>`;
                        break;
                    default:
                        heading = `<span class="name">${doc.fullname}</span>`;
                        break;
                }
                html += `
                        <section class="search-result">
                        <a href="${url}" class="attr ${doc.kind}">${heading}</a>
                        <div class="docstring">${doc.doc}</div>
                        </section>
                    `;

            }
            return html;
        })());
    }

    if (getSearchTerm()) {
        initialize();
        searchBox.value = getSearchTerm();
        onInput();
    } else {
        searchBox.addEventListener("focus", initialize, {once: true});
    }

    searchBox.addEventListener("keydown", e => {
        if (["ArrowDown", "ArrowUp", "Enter"].includes(e.key)) {
            let focused = currentContent.querySelector(".search-result.focused");
            if (!focused) {
                currentContent.querySelector(".search-result").classList.add("focused");
            } else if (
                e.key === "ArrowDown"
                && focused.nextElementSibling
                && focused.nextElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.nextElementSibling.classList.add("focused");
                focused.nextElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "ArrowUp"
                && focused.previousElementSibling
                && focused.previousElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.previousElementSibling.classList.add("focused");
                focused.previousElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "Enter"
            ) {
                focused.querySelector("a").click();
            }
        }
    });
</script></body>
</html>