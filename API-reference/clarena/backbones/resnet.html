<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="pdoc 15.0.1"/>
    <title>clarena.backbones.resnet API documentation</title>

    <style>/*! * Bootstrap Reboot v5.0.0 (https://getbootstrap.com/) * Copyright 2011-2021 The Bootstrap Authors * Copyright 2011-2021 Twitter, Inc. * Licensed under MIT (https://github.com/twbs/bootstrap/blob/main/LICENSE) * Forked from Normalize.css, licensed MIT (https://github.com/necolas/normalize.css/blob/master/LICENSE.md) */*,::after,::before{box-sizing:border-box}@media (prefers-reduced-motion:no-preference){:root{scroll-behavior:smooth}}body{margin:0;font-family:system-ui,-apple-system,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans","Liberation Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";font-size:1rem;font-weight:400;line-height:1.5;color:#212529;background-color:#fff;-webkit-text-size-adjust:100%;-webkit-tap-highlight-color:transparent}hr{margin:1rem 0;color:inherit;background-color:currentColor;border:0;opacity:.25}hr:not([size]){height:1px}h1,h2,h3,h4,h5,h6{margin-top:0;margin-bottom:.5rem;font-weight:500;line-height:1.2}h1{font-size:calc(1.375rem + 1.5vw)}@media (min-width:1200px){h1{font-size:2.5rem}}h2{font-size:calc(1.325rem + .9vw)}@media (min-width:1200px){h2{font-size:2rem}}h3{font-size:calc(1.3rem + .6vw)}@media (min-width:1200px){h3{font-size:1.75rem}}h4{font-size:calc(1.275rem + .3vw)}@media (min-width:1200px){h4{font-size:1.5rem}}h5{font-size:1.25rem}h6{font-size:1rem}p{margin-top:0;margin-bottom:1rem}abbr[data-bs-original-title],abbr[title]{-webkit-text-decoration:underline dotted;text-decoration:underline dotted;cursor:help;-webkit-text-decoration-skip-ink:none;text-decoration-skip-ink:none}address{margin-bottom:1rem;font-style:normal;line-height:inherit}ol,ul{padding-left:2rem}dl,ol,ul{margin-top:0;margin-bottom:1rem}ol ol,ol ul,ul ol,ul ul{margin-bottom:0}dt{font-weight:700}dd{margin-bottom:.5rem;margin-left:0}blockquote{margin:0 0 1rem}b,strong{font-weight:bolder}small{font-size:.875em}mark{padding:.2em;background-color:#fcf8e3}sub,sup{position:relative;font-size:.75em;line-height:0;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}a{color:#0d6efd;text-decoration:underline}a:hover{color:#0a58ca}a:not([href]):not([class]),a:not([href]):not([class]):hover{color:inherit;text-decoration:none}code,kbd,pre,samp{font-family:SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;font-size:1em;direction:ltr;unicode-bidi:bidi-override}pre{display:block;margin-top:0;margin-bottom:1rem;overflow:auto;font-size:.875em}pre code{font-size:inherit;color:inherit;word-break:normal}code{font-size:.875em;color:#d63384;word-wrap:break-word}a>code{color:inherit}kbd{padding:.2rem .4rem;font-size:.875em;color:#fff;background-color:#212529;border-radius:.2rem}kbd kbd{padding:0;font-size:1em;font-weight:700}figure{margin:0 0 1rem}img,svg{vertical-align:middle}table{caption-side:bottom;border-collapse:collapse}caption{padding-top:.5rem;padding-bottom:.5rem;color:#6c757d;text-align:left}th{text-align:inherit;text-align:-webkit-match-parent}tbody,td,tfoot,th,thead,tr{border-color:inherit;border-style:solid;border-width:0}label{display:inline-block}button{border-radius:0}button:focus:not(:focus-visible){outline:0}button,input,optgroup,select,textarea{margin:0;font-family:inherit;font-size:inherit;line-height:inherit}button,select{text-transform:none}[role=button]{cursor:pointer}select{word-wrap:normal}select:disabled{opacity:1}[list]::-webkit-calendar-picker-indicator{display:none}[type=button],[type=reset],[type=submit],button{-webkit-appearance:button}[type=button]:not(:disabled),[type=reset]:not(:disabled),[type=submit]:not(:disabled),button:not(:disabled){cursor:pointer}::-moz-focus-inner{padding:0;border-style:none}textarea{resize:vertical}fieldset{min-width:0;padding:0;margin:0;border:0}legend{float:left;width:100%;padding:0;margin-bottom:.5rem;font-size:calc(1.275rem + .3vw);line-height:inherit}@media (min-width:1200px){legend{font-size:1.5rem}}legend+*{clear:left}::-webkit-datetime-edit-day-field,::-webkit-datetime-edit-fields-wrapper,::-webkit-datetime-edit-hour-field,::-webkit-datetime-edit-minute,::-webkit-datetime-edit-month-field,::-webkit-datetime-edit-text,::-webkit-datetime-edit-year-field{padding:0}::-webkit-inner-spin-button{height:auto}[type=search]{outline-offset:-2px;-webkit-appearance:textfield}::-webkit-search-decoration{-webkit-appearance:none}::-webkit-color-swatch-wrapper{padding:0}::file-selector-button{font:inherit}::-webkit-file-upload-button{font:inherit;-webkit-appearance:button}output{display:inline-block}iframe{border:0}summary{display:list-item;cursor:pointer}progress{vertical-align:baseline}[hidden]{display:none!important}</style>
    <style>/*! syntax-highlighting.css */pre{line-height:125%;}span.linenos{color:inherit; background-color:transparent; padding-left:5px; padding-right:20px;}.pdoc-code .hll{background-color:#ffffcc}.pdoc-code{background:#f8f8f8;}.pdoc-code .c{color:#3D7B7B; font-style:italic}.pdoc-code .err{border:1px solid #FF0000}.pdoc-code .k{color:#008000; font-weight:bold}.pdoc-code .o{color:#666666}.pdoc-code .ch{color:#3D7B7B; font-style:italic}.pdoc-code .cm{color:#3D7B7B; font-style:italic}.pdoc-code .cp{color:#9C6500}.pdoc-code .cpf{color:#3D7B7B; font-style:italic}.pdoc-code .c1{color:#3D7B7B; font-style:italic}.pdoc-code .cs{color:#3D7B7B; font-style:italic}.pdoc-code .gd{color:#A00000}.pdoc-code .ge{font-style:italic}.pdoc-code .gr{color:#E40000}.pdoc-code .gh{color:#000080; font-weight:bold}.pdoc-code .gi{color:#008400}.pdoc-code .go{color:#717171}.pdoc-code .gp{color:#000080; font-weight:bold}.pdoc-code .gs{font-weight:bold}.pdoc-code .gu{color:#800080; font-weight:bold}.pdoc-code .gt{color:#0044DD}.pdoc-code .kc{color:#008000; font-weight:bold}.pdoc-code .kd{color:#008000; font-weight:bold}.pdoc-code .kn{color:#008000; font-weight:bold}.pdoc-code .kp{color:#008000}.pdoc-code .kr{color:#008000; font-weight:bold}.pdoc-code .kt{color:#B00040}.pdoc-code .m{color:#666666}.pdoc-code .s{color:#BA2121}.pdoc-code .na{color:#687822}.pdoc-code .nb{color:#008000}.pdoc-code .nc{color:#0000FF; font-weight:bold}.pdoc-code .no{color:#880000}.pdoc-code .nd{color:#AA22FF}.pdoc-code .ni{color:#717171; font-weight:bold}.pdoc-code .ne{color:#CB3F38; font-weight:bold}.pdoc-code .nf{color:#0000FF}.pdoc-code .nl{color:#767600}.pdoc-code .nn{color:#0000FF; font-weight:bold}.pdoc-code .nt{color:#008000; font-weight:bold}.pdoc-code .nv{color:#19177C}.pdoc-code .ow{color:#AA22FF; font-weight:bold}.pdoc-code .w{color:#bbbbbb}.pdoc-code .mb{color:#666666}.pdoc-code .mf{color:#666666}.pdoc-code .mh{color:#666666}.pdoc-code .mi{color:#666666}.pdoc-code .mo{color:#666666}.pdoc-code .sa{color:#BA2121}.pdoc-code .sb{color:#BA2121}.pdoc-code .sc{color:#BA2121}.pdoc-code .dl{color:#BA2121}.pdoc-code .sd{color:#BA2121; font-style:italic}.pdoc-code .s2{color:#BA2121}.pdoc-code .se{color:#AA5D1F; font-weight:bold}.pdoc-code .sh{color:#BA2121}.pdoc-code .si{color:#A45A77; font-weight:bold}.pdoc-code .sx{color:#008000}.pdoc-code .sr{color:#A45A77}.pdoc-code .s1{color:#BA2121}.pdoc-code .ss{color:#19177C}.pdoc-code .bp{color:#008000}.pdoc-code .fm{color:#0000FF}.pdoc-code .vc{color:#19177C}.pdoc-code .vg{color:#19177C}.pdoc-code .vi{color:#19177C}.pdoc-code .vm{color:#19177C}.pdoc-code .il{color:#666666}</style>
    <style>/*! theme.css */:root{--pdoc-background:#fff;}.pdoc{--text:#212529;--muted:#6c757d;--link:#3660a5;--link-hover:#1659c5;--code:#f8f8f8;--active:#fff598;--accent:#eee;--accent2:#c1c1c1;--nav-hover:rgba(255, 255, 255, 0.5);--name:#0066BB;--def:#008800;--annotation:#007020;}</style>
    <style>/*! layout.css */html, body{width:100%;height:100%;}html, main{scroll-behavior:smooth;}body{background-color:var(--pdoc-background);}@media (max-width:769px){#navtoggle{cursor:pointer;position:absolute;width:50px;height:40px;top:1rem;right:1rem;border-color:var(--text);color:var(--text);display:flex;opacity:0.8;z-index:999;}#navtoggle:hover{opacity:1;}#togglestate + div{display:none;}#togglestate:checked + div{display:inherit;}main, header{padding:2rem 3vw;}header + main{margin-top:-3rem;}.git-button{display:none !important;}nav input[type="search"]{max-width:77%;}nav input[type="search"]:first-child{margin-top:-6px;}nav input[type="search"]:valid ~ *{display:none !important;}}@media (min-width:770px){:root{--sidebar-width:clamp(12.5rem, 28vw, 22rem);}nav{position:fixed;overflow:auto;height:100vh;width:var(--sidebar-width);}main, header{padding:3rem 2rem 3rem calc(var(--sidebar-width) + 3rem);width:calc(54rem + var(--sidebar-width));max-width:100%;}header + main{margin-top:-4rem;}#navtoggle{display:none;}}#togglestate{position:absolute;height:0;opacity:0;}nav.pdoc{--pad:clamp(0.5rem, 2vw, 1.75rem);--indent:1.5rem;background-color:var(--accent);border-right:1px solid var(--accent2);box-shadow:0 0 20px rgba(50, 50, 50, .2) inset;padding:0 0 0 var(--pad);overflow-wrap:anywhere;scrollbar-width:thin; scrollbar-color:var(--accent2) transparent; z-index:1}nav.pdoc::-webkit-scrollbar{width:.4rem; }nav.pdoc::-webkit-scrollbar-thumb{background-color:var(--accent2); }nav.pdoc > div{padding:var(--pad) 0;}nav.pdoc .module-list-button{display:inline-flex;align-items:center;color:var(--text);border-color:var(--muted);margin-bottom:1rem;}nav.pdoc .module-list-button:hover{border-color:var(--text);}nav.pdoc input[type=search]{display:block;outline-offset:0;width:calc(100% - var(--pad));}nav.pdoc .logo{max-width:calc(100% - var(--pad));max-height:35vh;display:block;margin:0 auto 1rem;transform:translate(calc(-.5 * var(--pad)), 0);}nav.pdoc ul{list-style:none;padding-left:0;}nav.pdoc > div > ul{margin-left:calc(0px - var(--pad));}nav.pdoc li a{padding:.2rem 0 .2rem calc(var(--pad) + var(--indent));}nav.pdoc > div > ul > li > a{padding-left:var(--pad);}nav.pdoc li{transition:all 100ms;}nav.pdoc li:hover{background-color:var(--nav-hover);}nav.pdoc a, nav.pdoc a:hover{color:var(--text);}nav.pdoc a{display:block;}nav.pdoc > h2:first-of-type{margin-top:1.5rem;}nav.pdoc .class:before{content:"class ";color:var(--muted);}nav.pdoc .function:after{content:"()";color:var(--muted);}nav.pdoc footer:before{content:"";display:block;width:calc(100% - var(--pad));border-top:solid var(--accent2) 1px;margin-top:1.5rem;padding-top:.5rem;}nav.pdoc footer{font-size:small;}</style>
    <style>/*! content.css */.pdoc{color:var(--text);box-sizing:border-box;line-height:1.5;background:none;}.pdoc .pdoc-button{cursor:pointer;display:inline-block;border:solid black 1px;border-radius:2px;font-size:.75rem;padding:calc(0.5em - 1px) 1em;transition:100ms all;}.pdoc .alert{padding:1rem 1rem 1rem calc(1.5rem + 24px);border:1px solid transparent;border-radius:.25rem;background-repeat:no-repeat;background-position:.75rem center;margin-bottom:1rem;}.pdoc .alert > em{display:none;}.pdoc .alert > *:last-child{margin-bottom:0;}.pdoc .alert.note {color:#084298;background-color:#cfe2ff;border-color:#b6d4fe;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23084298%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8%2016A8%208%200%201%200%208%200a8%208%200%200%200%200%2016zm.93-9.412-1%204.705c-.07.34.029.533.304.533.194%200%20.487-.07.686-.246l-.088.416c-.287.346-.92.598-1.465.598-.703%200-1.002-.422-.808-1.319l.738-3.468c.064-.293.006-.399-.287-.47l-.451-.081.082-.381%202.29-.287zM8%205.5a1%201%200%201%201%200-2%201%201%200%200%201%200%202z%22/%3E%3C/svg%3E");}.pdoc .alert.warning{color:#664d03;background-color:#fff3cd;border-color:#ffecb5;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23664d03%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8.982%201.566a1.13%201.13%200%200%200-1.96%200L.165%2013.233c-.457.778.091%201.767.98%201.767h13.713c.889%200%201.438-.99.98-1.767L8.982%201.566zM8%205c.535%200%20.954.462.9.995l-.35%203.507a.552.552%200%200%201-1.1%200L7.1%205.995A.905.905%200%200%201%208%205zm.002%206a1%201%200%201%201%200%202%201%201%200%200%201%200-2z%22/%3E%3C/svg%3E");}.pdoc .alert.danger{color:#842029;background-color:#f8d7da;border-color:#f5c2c7;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23842029%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M5.52.359A.5.5%200%200%201%206%200h4a.5.5%200%200%201%20.474.658L8.694%206H12.5a.5.5%200%200%201%20.395.807l-7%209a.5.5%200%200%201-.873-.454L6.823%209.5H3.5a.5.5%200%200%201-.48-.641l2.5-8.5z%22/%3E%3C/svg%3E");}.pdoc .visually-hidden{position:absolute !important;width:1px !important;height:1px !important;padding:0 !important;margin:-1px !important;overflow:hidden !important;clip:rect(0, 0, 0, 0) !important;white-space:nowrap !important;border:0 !important;}.pdoc h1, .pdoc h2, .pdoc h3{font-weight:300;margin:.3em 0;padding:.2em 0;}.pdoc > section:not(.module-info) h1{font-size:1.5rem;font-weight:500;}.pdoc > section:not(.module-info) h2{font-size:1.4rem;font-weight:500;}.pdoc > section:not(.module-info) h3{font-size:1.3rem;font-weight:500;}.pdoc > section:not(.module-info) h4{font-size:1.2rem;}.pdoc > section:not(.module-info) h5{font-size:1.1rem;}.pdoc a{text-decoration:none;color:var(--link);}.pdoc a:hover{color:var(--link-hover);}.pdoc blockquote{margin-left:2rem;}.pdoc pre{border-top:1px solid var(--accent2);border-bottom:1px solid var(--accent2);margin-top:0;margin-bottom:1em;padding:.5rem 0 .5rem .5rem;overflow-x:auto;background-color:var(--code);}.pdoc code{color:var(--text);padding:.2em .4em;margin:0;font-size:85%;background-color:var(--accent);border-radius:6px;}.pdoc a > code{color:inherit;}.pdoc pre > code{display:inline-block;font-size:inherit;background:none;border:none;padding:0;}.pdoc > section:not(.module-info){margin-bottom:1.5rem;}.pdoc .modulename{margin-top:0;font-weight:bold;}.pdoc .modulename a{color:var(--link);transition:100ms all;}.pdoc .git-button{float:right;border:solid var(--link) 1px;}.pdoc .git-button:hover{background-color:var(--link);color:var(--pdoc-background);}.view-source-toggle-state,.view-source-toggle-state ~ .pdoc-code{display:none;}.view-source-toggle-state:checked ~ .pdoc-code{display:block;}.view-source-button{display:inline-block;float:right;font-size:.75rem;line-height:1.5rem;color:var(--muted);padding:0 .4rem 0 1.3rem;cursor:pointer;text-indent:-2px;}.view-source-button > span{visibility:hidden;}.module-info .view-source-button{float:none;display:flex;justify-content:flex-end;margin:-1.2rem .4rem -.2rem 0;}.view-source-button::before{position:absolute;content:"View Source";display:list-item;list-style-type:disclosure-closed;}.view-source-toggle-state:checked ~ .attr .view-source-button::before,.view-source-toggle-state:checked ~ .view-source-button::before{list-style-type:disclosure-open;}.pdoc .docstring{margin-bottom:1.5rem;}.pdoc section:not(.module-info) .docstring{margin-left:clamp(0rem, 5vw - 2rem, 1rem);}.pdoc .docstring .pdoc-code{margin-left:1em;margin-right:1em;}.pdoc h1:target,.pdoc h2:target,.pdoc h3:target,.pdoc h4:target,.pdoc h5:target,.pdoc h6:target,.pdoc .pdoc-code > pre > span:target{background-color:var(--active);box-shadow:-1rem 0 0 0 var(--active);}.pdoc .pdoc-code > pre > span:target{display:block;}.pdoc div:target > .attr,.pdoc section:target > .attr,.pdoc dd:target > a{background-color:var(--active);}.pdoc *{scroll-margin:2rem;}.pdoc .pdoc-code .linenos{user-select:none;}.pdoc .attr:hover{filter:contrast(0.95);}.pdoc section, .pdoc .classattr{position:relative;}.pdoc .headerlink{--width:clamp(1rem, 3vw, 2rem);position:absolute;top:0;left:calc(0rem - var(--width));transition:all 100ms ease-in-out;opacity:0;}.pdoc .headerlink::before{content:"#";display:block;text-align:center;width:var(--width);height:2.3rem;line-height:2.3rem;font-size:1.5rem;}.pdoc .attr:hover ~ .headerlink,.pdoc *:target > .headerlink,.pdoc .headerlink:hover{opacity:1;}.pdoc .attr{display:block;margin:.5rem 0 .5rem;padding:.4rem .4rem .4rem 1rem;background-color:var(--accent);overflow-x:auto;}.pdoc .classattr{margin-left:2rem;}.pdoc .decorator-deprecated{color:#842029;}.pdoc .decorator-deprecated ~ span{filter:grayscale(1) opacity(0.8);}.pdoc .name{color:var(--name);font-weight:bold;}.pdoc .def{color:var(--def);font-weight:bold;}.pdoc .signature{background-color:transparent;}.pdoc .param, .pdoc .return-annotation{white-space:pre;}.pdoc .signature.multiline .param{display:block;}.pdoc .signature.condensed .param{display:inline-block;}.pdoc .annotation{color:var(--annotation);}.pdoc .view-value-toggle-state,.pdoc .view-value-toggle-state ~ .default_value{display:none;}.pdoc .view-value-toggle-state:checked ~ .default_value{display:inherit;}.pdoc .view-value-button{font-size:.5rem;vertical-align:middle;border-style:dashed;margin-top:-0.1rem;}.pdoc .view-value-button:hover{background:white;}.pdoc .view-value-button::before{content:"show";text-align:center;width:2.2em;display:inline-block;}.pdoc .view-value-toggle-state:checked ~ .view-value-button::before{content:"hide";}.pdoc .inherited{margin-left:2rem;}.pdoc .inherited dt{font-weight:700;}.pdoc .inherited dt, .pdoc .inherited dd{display:inline;margin-left:0;margin-bottom:.5rem;}.pdoc .inherited dd:not(:last-child):after{content:", ";}.pdoc .inherited .class:before{content:"class ";}.pdoc .inherited .function a:after{content:"()";}.pdoc .search-result .docstring{overflow:auto;max-height:25vh;}.pdoc .search-result.focused > .attr{background-color:var(--active);}.pdoc .attribution{margin-top:2rem;display:block;opacity:0.5;transition:all 200ms;filter:grayscale(100%);}.pdoc .attribution:hover{opacity:1;filter:grayscale(0%);}.pdoc .attribution img{margin-left:5px;height:35px;vertical-align:middle;width:70px;transition:all 200ms;}.pdoc table{display:block;width:max-content;max-width:100%;overflow:auto;margin-bottom:1rem;}.pdoc table th{font-weight:600;}.pdoc table th, .pdoc table td{padding:6px 13px;border:1px solid var(--accent2);}</style>
    <style>/*! custom.css */</style></head>
<body>
    <nav class="pdoc">
        <label id="navtoggle" for="togglestate" class="pdoc-button"><svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 30 30'><path stroke-linecap='round' stroke="currentColor" stroke-miterlimit='10' stroke-width='2' d='M4 7h22M4 15h22M4 23h22'/></svg></label>
        <input id="togglestate" type="checkbox" aria-hidden="true" tabindex="-1">
        <div>            <a class="pdoc-button module-list-button" href="../backbones.html">
<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-box-arrow-in-left" viewBox="0 0 16 16">
  <path fill-rule="evenodd" d="M10 3.5a.5.5 0 0 0-.5-.5h-8a.5.5 0 0 0-.5.5v9a.5.5 0 0 0 .5.5h8a.5.5 0 0 0 .5-.5v-2a.5.5 0 0 1 1 0v2A1.5 1.5 0 0 1 9.5 14h-8A1.5 1.5 0 0 1 0 12.5v-9A1.5 1.5 0 0 1 1.5 2h8A1.5 1.5 0 0 1 11 3.5v2a.5.5 0 0 1-1 0v-2z"/>
  <path fill-rule="evenodd" d="M4.146 8.354a.5.5 0 0 1 0-.708l3-3a.5.5 0 1 1 .708.708L5.707 7.5H14.5a.5.5 0 0 1 0 1H5.707l2.147 2.146a.5.5 0 0 1-.708.708l-3-3z"/>
</svg>                &nbsp;clarena.backbones</a>


            <input type="search" placeholder="Search..." role="searchbox" aria-label="search"
                   pattern=".+" required>



            <h2>API Documentation</h2>
                <ul class="memberlist">
            <li>
                    <a class="class" href="#ResNetBlockSmall">ResNetBlockSmall</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#ResNetBlockSmall.__init__">ResNetBlockSmall</a>
                        </li>
                        <li>
                                <a class="variable" href="#ResNetBlockSmall.batch_normalisation">batch_normalisation</a>
                        </li>
                        <li>
                                <a class="variable" href="#ResNetBlockSmall.activation">activation</a>
                        </li>
                        <li>
                                <a class="variable" href="#ResNetBlockSmall.full_1st_layer_name">full_1st_layer_name</a>
                        </li>
                        <li>
                                <a class="variable" href="#ResNetBlockSmall.full_2nd_layer_name">full_2nd_layer_name</a>
                        </li>
                        <li>
                                <a class="variable" href="#ResNetBlockSmall.conv1">conv1</a>
                        </li>
                        <li>
                                <a class="variable" href="#ResNetBlockSmall.conv2">conv2</a>
                        </li>
                        <li>
                                <a class="variable" href="#ResNetBlockSmall.identity_downsample">identity_downsample</a>
                        </li>
                        <li>
                                <a class="function" href="#ResNetBlockSmall.forward">forward</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#ResNetBlockLarge">ResNetBlockLarge</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#ResNetBlockLarge.__init__">ResNetBlockLarge</a>
                        </li>
                        <li>
                                <a class="variable" href="#ResNetBlockLarge.batch_normalisation">batch_normalisation</a>
                        </li>
                        <li>
                                <a class="variable" href="#ResNetBlockLarge.activation">activation</a>
                        </li>
                        <li>
                                <a class="variable" href="#ResNetBlockLarge.full_1st_layer_name">full_1st_layer_name</a>
                        </li>
                        <li>
                                <a class="variable" href="#ResNetBlockLarge.full_2nd_layer_name">full_2nd_layer_name</a>
                        </li>
                        <li>
                                <a class="variable" href="#ResNetBlockLarge.full_3rd_layer_name">full_3rd_layer_name</a>
                        </li>
                        <li>
                                <a class="variable" href="#ResNetBlockLarge.conv1">conv1</a>
                        </li>
                        <li>
                                <a class="variable" href="#ResNetBlockLarge.conv2">conv2</a>
                        </li>
                        <li>
                                <a class="variable" href="#ResNetBlockLarge.conv3">conv3</a>
                        </li>
                        <li>
                                <a class="variable" href="#ResNetBlockLarge.identity_downsample">identity_downsample</a>
                        </li>
                        <li>
                                <a class="function" href="#ResNetBlockLarge.forward">forward</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#ResNetBase">ResNetBase</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#ResNetBase.__init__">ResNetBase</a>
                        </li>
                        <li>
                                <a class="variable" href="#ResNetBase.batch_normalisation">batch_normalisation</a>
                        </li>
                        <li>
                                <a class="variable" href="#ResNetBase.activation">activation</a>
                        </li>
                        <li>
                                <a class="variable" href="#ResNetBase.conv1">conv1</a>
                        </li>
                        <li>
                                <a class="variable" href="#ResNetBase.maxpool">maxpool</a>
                        </li>
                        <li>
                                <a class="variable" href="#ResNetBase.conv2x">conv2x</a>
                        </li>
                        <li>
                                <a class="variable" href="#ResNetBase.conv3x">conv3x</a>
                        </li>
                        <li>
                                <a class="variable" href="#ResNetBase.conv4x">conv4x</a>
                        </li>
                        <li>
                                <a class="variable" href="#ResNetBase.conv5x">conv5x</a>
                        </li>
                        <li>
                                <a class="variable" href="#ResNetBase.avepool">avepool</a>
                        </li>
                        <li>
                                <a class="function" href="#ResNetBase.forward">forward</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#ResNet18">ResNet18</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#ResNet18.__init__">ResNet18</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#ResNet34">ResNet34</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#ResNet34.__init__">ResNet34</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#ResNet50">ResNet50</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#ResNet50.__init__">ResNet50</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#ResNet101">ResNet101</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#ResNet101.__init__">ResNet101</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#ResNet152">ResNet152</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#ResNet152.__init__">ResNet152</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#HATMaskResNetBlockSmall">HATMaskResNetBlockSmall</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#HATMaskResNetBlockSmall.__init__">HATMaskResNetBlockSmall</a>
                        </li>
                        <li>
                                <a class="function" href="#HATMaskResNetBlockSmall.forward">forward</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#HATMaskResNetBlockLarge">HATMaskResNetBlockLarge</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#HATMaskResNetBlockLarge.__init__">HATMaskResNetBlockLarge</a>
                        </li>
                        <li>
                                <a class="function" href="#HATMaskResNetBlockLarge.forward">forward</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#HATMaskResNetBase">HATMaskResNetBase</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#HATMaskResNetBase.__init__">HATMaskResNetBase</a>
                        </li>
                        <li>
                                <a class="function" href="#HATMaskResNetBase.update_multiple_blocks_task_embedding">update_multiple_blocks_task_embedding</a>
                        </li>
                        <li>
                                <a class="function" href="#HATMaskResNetBase.forward">forward</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#HATMaskResNet18">HATMaskResNet18</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#HATMaskResNet18.__init__">HATMaskResNet18</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#HATMaskResNet34">HATMaskResNet34</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#HATMaskResNet34.__init__">HATMaskResNet34</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#HATMaskResNet50">HATMaskResNet50</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#HATMaskResNet50.__init__">HATMaskResNet50</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#HATMaskResNet101">HATMaskResNet101</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#HATMaskResNet101.__init__">HATMaskResNet101</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#HATMaskResNet152">HATMaskResNet152</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#HATMaskResNet152.__init__">HATMaskResNet152</a>
                        </li>
                </ul>

            </li>
    </ul>



        <a class="attribution" title="pdoc: Python API documentation generator" href="https://pdoc.dev" target="_blank">
            built with <span class="visually-hidden">pdoc</span><img
                alt="pdoc logo"
                src="data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20role%3D%22img%22%20aria-label%3D%22pdoc%20logo%22%20width%3D%22300%22%20height%3D%22150%22%20viewBox%3D%22-1%200%2060%2030%22%3E%3Ctitle%3Epdoc%3C/title%3E%3Cpath%20d%3D%22M29.621%2021.293c-.011-.273-.214-.475-.511-.481a.5.5%200%200%200-.489.503l-.044%201.393c-.097.551-.695%201.215-1.566%201.704-.577.428-1.306.486-2.193.182-1.426-.617-2.467-1.654-3.304-2.487l-.173-.172a3.43%203.43%200%200%200-.365-.306.49.49%200%200%200-.286-.196c-1.718-1.06-4.931-1.47-7.353.191l-.219.15c-1.707%201.187-3.413%202.131-4.328%201.03-.02-.027-.49-.685-.141-1.763.233-.721.546-2.408.772-4.076.042-.09.067-.187.046-.288.166-1.347.277-2.625.241-3.351%201.378-1.008%202.271-2.586%202.271-4.362%200-.976-.272-1.935-.788-2.774-.057-.094-.122-.18-.184-.268.033-.167.052-.339.052-.516%200-1.477-1.202-2.679-2.679-2.679-.791%200-1.496.352-1.987.9a6.3%206.3%200%200%200-1.001.029c-.492-.564-1.207-.929-2.012-.929-1.477%200-2.679%201.202-2.679%202.679A2.65%202.65%200%200%200%20.97%206.554c-.383.747-.595%201.572-.595%202.41%200%202.311%201.507%204.29%203.635%205.107-.037.699-.147%202.27-.423%203.294l-.137.461c-.622%202.042-2.515%208.257%201.727%2010.643%201.614.908%203.06%201.248%204.317%201.248%202.665%200%204.492-1.524%205.322-2.401%201.476-1.559%202.886-1.854%206.491.82%201.877%201.393%203.514%201.753%204.861%201.068%202.223-1.713%202.811-3.867%203.399-6.374.077-.846.056-1.469.054-1.537zm-4.835%204.313c-.054.305-.156.586-.242.629-.034-.007-.131-.022-.307-.157-.145-.111-.314-.478-.456-.908.221.121.432.25.675.355.115.039.219.051.33.081zm-2.251-1.238c-.05.33-.158.648-.252.694-.022.001-.125-.018-.307-.157-.217-.166-.488-.906-.639-1.573.358.344.754.693%201.198%201.036zm-3.887-2.337c-.006-.116-.018-.231-.041-.342.635.145%201.189.368%201.599.625.097.231.166.481.174.642-.03.049-.055.101-.067.158-.046.013-.128.026-.298.004-.278-.037-.901-.57-1.367-1.087zm-1.127-.497c.116.306.176.625.12.71-.019.014-.117.045-.345.016-.206-.027-.604-.332-.986-.695.41-.051.816-.056%201.211-.031zm-4.535%201.535c.209.22.379.47.358.598-.006.041-.088.138-.351.234-.144.055-.539-.063-.979-.259a11.66%2011.66%200%200%200%20.972-.573zm.983-.664c.359-.237.738-.418%201.126-.554.25.237.479.548.457.694-.006.042-.087.138-.351.235-.174.064-.694-.105-1.232-.375zm-3.381%201.794c-.022.145-.061.29-.149.401-.133.166-.358.248-.69.251h-.002c-.133%200-.306-.26-.45-.621.417.091.854.07%201.291-.031zm-2.066-8.077a4.78%204.78%200%200%201-.775-.584c.172-.115.505-.254.88-.378l-.105.962zm-.331%202.302a10.32%2010.32%200%200%201-.828-.502c.202-.143.576-.328.984-.49l-.156.992zm-.45%202.157l-.701-.403c.214-.115.536-.249.891-.376a11.57%2011.57%200%200%201-.19.779zm-.181%201.716c.064.398.194.702.298.893-.194-.051-.435-.162-.736-.398.061-.119.224-.3.438-.495zM8.87%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zm-.735-.389a1.15%201.15%200%200%200-.314.783%201.16%201.16%200%200%200%201.162%201.162c.457%200%20.842-.27%201.032-.653.026.117.042.238.042.362a1.68%201.68%200%200%201-1.679%201.679%201.68%201.68%200%200%201-1.679-1.679c0-.843.626-1.535%201.436-1.654zM5.059%205.406A1.68%201.68%200%200%201%203.38%207.085a1.68%201.68%200%200%201-1.679-1.679c0-.037.009-.072.011-.109.21.3.541.508.935.508a1.16%201.16%200%200%200%201.162-1.162%201.14%201.14%200%200%200-.474-.912c.015%200%20.03-.005.045-.005.926.001%201.679.754%201.679%201.68zM3.198%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zM1.375%208.964c0-.52.103-1.035.288-1.52.466.394%201.06.64%201.717.64%201.144%200%202.116-.725%202.499-1.738.383%201.012%201.355%201.738%202.499%201.738.867%200%201.631-.421%202.121-1.062.307.605.478%201.267.478%201.942%200%202.486-2.153%204.51-4.801%204.51s-4.801-2.023-4.801-4.51zm24.342%2019.349c-.985.498-2.267.168-3.813-.979-3.073-2.281-5.453-3.199-7.813-.705-1.315%201.391-4.163%203.365-8.423.97-3.174-1.786-2.239-6.266-1.261-9.479l.146-.492c.276-1.02.395-2.457.444-3.268a6.11%206.11%200%200%200%201.18.115%206.01%206.01%200%200%200%202.536-.562l-.006.175c-.802.215-1.848.612-2.021%201.25-.079.295.021.601.274.837.219.203.415.364.598.501-.667.304-1.243.698-1.311%201.179-.02.144-.022.507.393.787.213.144.395.26.564.365-1.285.521-1.361.96-1.381%201.126-.018.142-.011.496.427.746l.854.489c-.473.389-.971.914-.999%201.429-.018.278.095.532.316.713.675.556%201.231.721%201.653.721.059%200%20.104-.014.158-.02.207.707.641%201.64%201.513%201.64h.013c.8-.008%201.236-.345%201.462-.626.173-.216.268-.457.325-.692.424.195.93.374%201.372.374.151%200%20.294-.021.423-.068.732-.27.944-.704.993-1.021.009-.061.003-.119.002-.179.266.086.538.147.789.147.15%200%20.294-.021.423-.069.542-.2.797-.489.914-.754.237.147.478.258.704.288.106.014.205.021.296.021.356%200%20.595-.101.767-.229.438.435%201.094.992%201.656%201.067.106.014.205.021.296.021a1.56%201.56%200%200%200%20.323-.035c.17.575.453%201.289.866%201.605.358.273.665.362.914.362a.99.99%200%200%200%20.421-.093%201.03%201.03%200%200%200%20.245-.164c.168.428.39.846.68%201.068.358.273.665.362.913.362a.99.99%200%200%200%20.421-.093c.317-.148.512-.448.639-.762.251.157.495.257.726.257.127%200%20.25-.024.37-.071.427-.17.706-.617.841-1.314.022-.015.047-.022.068-.038.067-.051.133-.104.196-.159-.443%201.486-1.107%202.761-2.086%203.257zM8.66%209.925a.5.5%200%201%200-1%200c0%20.653-.818%201.205-1.787%201.205s-1.787-.552-1.787-1.205a.5.5%200%201%200-1%200c0%201.216%201.25%202.205%202.787%202.205s2.787-.989%202.787-2.205zm4.4%2015.965l-.208.097c-2.661%201.258-4.708%201.436-6.086.527-1.542-1.017-1.88-3.19-1.844-4.198a.4.4%200%200%200-.385-.414c-.242-.029-.406.164-.414.385-.046%201.249.367%203.686%202.202%204.896.708.467%201.547.7%202.51.7%201.248%200%202.706-.392%204.362-1.174l.185-.086a.4.4%200%200%200%20.205-.527c-.089-.204-.326-.291-.527-.206zM9.547%202.292c.093.077.205.114.317.114a.5.5%200%200%200%20.318-.886L8.817.397a.5.5%200%200%200-.703.068.5.5%200%200%200%20.069.703l1.364%201.124zm-7.661-.065c.086%200%20.173-.022.253-.068l1.523-.893a.5.5%200%200%200-.506-.863l-1.523.892a.5.5%200%200%200-.179.685c.094.158.261.247.432.247z%22%20transform%3D%22matrix%28-1%200%200%201%2058%200%29%22%20fill%3D%22%233bb300%22/%3E%3Cpath%20d%3D%22M.3%2021.86V10.18q0-.46.02-.68.04-.22.18-.5.28-.54%201.34-.54%201.06%200%201.42.28.38.26.44.78.76-1.04%202.38-1.04%201.64%200%203.1%201.54%201.46%201.54%201.46%203.58%200%202.04-1.46%203.58-1.44%201.54-3.08%201.54-1.64%200-2.38-.92v4.04q0%20.46-.04.68-.02.22-.18.5-.14.3-.5.42-.36.12-.98.12-.62%200-1-.12-.36-.12-.52-.4-.14-.28-.18-.5-.02-.22-.02-.68zm3.96-9.42q-.46.54-.46%201.18%200%20.64.46%201.18.48.52%201.2.52.74%200%201.24-.52.52-.52.52-1.18%200-.66-.48-1.18-.48-.54-1.26-.54-.76%200-1.22.54zm14.741-8.36q.16-.3.54-.42.38-.12%201-.12.64%200%201.02.12.38.12.52.42.16.3.18.54.04.22.04.68v11.94q0%20.46-.04.7-.02.22-.18.5-.3.54-1.7.54-1.38%200-1.54-.98-.84.96-2.34.96-1.8%200-3.28-1.56-1.48-1.58-1.48-3.66%200-2.1%201.48-3.68%201.5-1.58%203.28-1.58%201.48%200%202.3%201v-4.2q0-.46.02-.68.04-.24.18-.52zm-3.24%2010.86q.52.54%201.26.54.74%200%201.22-.54.5-.54.5-1.18%200-.66-.48-1.22-.46-.56-1.26-.56-.8%200-1.28.56-.48.54-.48%201.2%200%20.66.52%201.2zm7.833-1.2q0-2.4%201.68-3.96%201.68-1.56%203.84-1.56%202.16%200%203.82%201.56%201.66%201.54%201.66%203.94%200%201.66-.86%202.96-.86%201.28-2.1%201.9-1.22.6-2.54.6-1.32%200-2.56-.64-1.24-.66-2.1-1.92-.84-1.28-.84-2.88zm4.18%201.44q.64.48%201.3.48.66%200%201.32-.5.66-.5.66-1.48%200-.98-.62-1.46-.62-.48-1.34-.48-.72%200-1.34.5-.62.5-.62%201.48%200%20.96.64%201.46zm11.412-1.44q0%20.84.56%201.32.56.46%201.18.46.64%200%201.18-.36.56-.38.9-.38.6%200%201.46%201.06.46.58.46%201.04%200%20.76-1.1%201.42-1.14.8-2.8.8-1.86%200-3.58-1.34-.82-.64-1.34-1.7-.52-1.08-.52-2.36%200-1.3.52-2.34.52-1.06%201.34-1.7%201.66-1.32%203.54-1.32.76%200%201.48.22.72.2%201.06.4l.32.2q.36.24.56.38.52.4.52.92%200%20.5-.42%201.14-.72%201.1-1.38%201.1-.38%200-1.08-.44-.36-.34-1.04-.34-.66%200-1.24.48-.58.48-.58%201.34z%22%20fill%3D%22green%22/%3E%3C/svg%3E"/>
        </a>
</div>
    </nav>
    <main class="pdoc">
            <section class="module-info">
                    <h1 class="modulename">
<a href="./../../clarena.html">clarena</a><wbr>.<a href="./../backbones.html">backbones</a><wbr>.resnet    </h1>

                        <div class="docstring"><p>The submodule in <code>backbones</code> for ResNet backbone network.</p>
</div>

                        <input id="mod-resnet-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">

                        <label class="view-source-button" for="mod-resnet-view-source"><span>View Source</span></label>

                        <div class="pdoc-code codehilite"><pre><span></span><span id="L-1"><a href="#L-1"><span class="linenos">   1</span></a><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-2"><a href="#L-2"><span class="linenos">   2</span></a><span class="sd">The submodule in `backbones` for ResNet backbone network.</span>
</span><span id="L-3"><a href="#L-3"><span class="linenos">   3</span></a><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-4"><a href="#L-4"><span class="linenos">   4</span></a>
</span><span id="L-5"><a href="#L-5"><span class="linenos">   5</span></a><span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
</span><span id="L-6"><a href="#L-6"><span class="linenos">   6</span></a>    <span class="s2">&quot;ResNetBlockSmall&quot;</span><span class="p">,</span>
</span><span id="L-7"><a href="#L-7"><span class="linenos">   7</span></a>    <span class="s2">&quot;ResNetBlockLarge&quot;</span><span class="p">,</span>
</span><span id="L-8"><a href="#L-8"><span class="linenos">   8</span></a>    <span class="s2">&quot;ResNetBase&quot;</span><span class="p">,</span>
</span><span id="L-9"><a href="#L-9"><span class="linenos">   9</span></a>    <span class="s2">&quot;ResNet18&quot;</span><span class="p">,</span>
</span><span id="L-10"><a href="#L-10"><span class="linenos">  10</span></a>    <span class="s2">&quot;ResNet34&quot;</span><span class="p">,</span>
</span><span id="L-11"><a href="#L-11"><span class="linenos">  11</span></a>    <span class="s2">&quot;ResNet50&quot;</span><span class="p">,</span>
</span><span id="L-12"><a href="#L-12"><span class="linenos">  12</span></a>    <span class="s2">&quot;ResNet101&quot;</span><span class="p">,</span>
</span><span id="L-13"><a href="#L-13"><span class="linenos">  13</span></a>    <span class="s2">&quot;ResNet152&quot;</span><span class="p">,</span>
</span><span id="L-14"><a href="#L-14"><span class="linenos">  14</span></a>    <span class="s2">&quot;HATMaskResNetBlockSmall&quot;</span><span class="p">,</span>
</span><span id="L-15"><a href="#L-15"><span class="linenos">  15</span></a>    <span class="s2">&quot;HATMaskResNetBlockLarge&quot;</span><span class="p">,</span>
</span><span id="L-16"><a href="#L-16"><span class="linenos">  16</span></a>    <span class="s2">&quot;HATMaskResNetBase&quot;</span><span class="p">,</span>
</span><span id="L-17"><a href="#L-17"><span class="linenos">  17</span></a>    <span class="s2">&quot;HATMaskResNet18&quot;</span><span class="p">,</span>
</span><span id="L-18"><a href="#L-18"><span class="linenos">  18</span></a>    <span class="s2">&quot;HATMaskResNet34&quot;</span><span class="p">,</span>
</span><span id="L-19"><a href="#L-19"><span class="linenos">  19</span></a>    <span class="s2">&quot;HATMaskResNet50&quot;</span><span class="p">,</span>
</span><span id="L-20"><a href="#L-20"><span class="linenos">  20</span></a>    <span class="s2">&quot;HATMaskResNet101&quot;</span><span class="p">,</span>
</span><span id="L-21"><a href="#L-21"><span class="linenos">  21</span></a>    <span class="s2">&quot;HATMaskResNet152&quot;</span><span class="p">,</span>
</span><span id="L-22"><a href="#L-22"><span class="linenos">  22</span></a><span class="p">]</span>
</span><span id="L-23"><a href="#L-23"><span class="linenos">  23</span></a>
</span><span id="L-24"><a href="#L-24"><span class="linenos">  24</span></a>
</span><span id="L-25"><a href="#L-25"><span class="linenos">  25</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">nn</span>
</span><span id="L-26"><a href="#L-26"><span class="linenos">  26</span></a>
</span><span id="L-27"><a href="#L-27"><span class="linenos">  27</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">clarena.backbones</span><span class="w"> </span><span class="kn">import</span> <span class="n">CLBackbone</span><span class="p">,</span> <span class="n">HATMaskBackbone</span>
</span><span id="L-28"><a href="#L-28"><span class="linenos">  28</span></a>
</span><span id="L-29"><a href="#L-29"><span class="linenos">  29</span></a>
</span><span id="L-30"><a href="#L-30"><span class="linenos">  30</span></a><span class="k">class</span><span class="w"> </span><span class="nc">ResNetBlockSmall</span><span class="p">(</span><span class="n">CLBackbone</span><span class="p">):</span>
</span><span id="L-31"><a href="#L-31"><span class="linenos">  31</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The smaller building block for ResNet-18/34.</span>
</span><span id="L-32"><a href="#L-32"><span class="linenos">  32</span></a>
</span><span id="L-33"><a href="#L-33"><span class="linenos">  33</span></a><span class="sd">    It consists of 2 weight convolutional layers, each followed by an activation function. See Table 1 or Figure 5 (left) in the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html).</span>
</span><span id="L-34"><a href="#L-34"><span class="linenos">  34</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-35"><a href="#L-35"><span class="linenos">  35</span></a>
</span><span id="L-36"><a href="#L-36"><span class="linenos">  36</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-37"><a href="#L-37"><span class="linenos">  37</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-38"><a href="#L-38"><span class="linenos">  38</span></a>        <span class="n">outer_layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-39"><a href="#L-39"><span class="linenos">  39</span></a>        <span class="n">block_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-40"><a href="#L-40"><span class="linenos">  40</span></a>        <span class="n">preceding_output_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-41"><a href="#L-41"><span class="linenos">  41</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-42"><a href="#L-42"><span class="linenos">  42</span></a>        <span class="n">overall_stride</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-43"><a href="#L-43"><span class="linenos">  43</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="L-44"><a href="#L-44"><span class="linenos">  44</span></a>        <span class="n">batch_normalisation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="L-45"><a href="#L-45"><span class="linenos">  45</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-46"><a href="#L-46"><span class="linenos">  46</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-47"><a href="#L-47"><span class="linenos">  47</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the smaller building block.</span>
</span><span id="L-48"><a href="#L-48"><span class="linenos">  48</span></a>
</span><span id="L-49"><a href="#L-49"><span class="linenos">  49</span></a><span class="sd">        **Args:**</span>
</span><span id="L-50"><a href="#L-50"><span class="linenos">  50</span></a><span class="sd">        - **outer_layer_name** (`str`): pass the name of the multi-building-block layer that contains this building block to construct the full name of each weighted convolutional layer.</span>
</span><span id="L-51"><a href="#L-51"><span class="linenos">  51</span></a><span class="sd">        - **block_idx** (`int`): the index of the building blocks in the multi-building-block layer to construct the full name of each weighted convolutional layer.</span>
</span><span id="L-52"><a href="#L-52"><span class="linenos">  52</span></a><span class="sd">        - **preceding_output_channels** (`int`): the number of channels of preceding output of this particular building block.</span>
</span><span id="L-53"><a href="#L-53"><span class="linenos">  53</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input of this building block.</span>
</span><span id="L-54"><a href="#L-54"><span class="linenos">  54</span></a><span class="sd">        - **overall_stride** (`int`): the overall stride of this building block. This stride is performed at 2nd (last) convolutional layer where the 1st convolutional layer remain stride of 1.</span>
</span><span id="L-55"><a href="#L-55"><span class="linenos">  55</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="L-56"><a href="#L-56"><span class="linenos">  56</span></a><span class="sd">        - **batch_normalisation** (`bool`): whether to use batch normalisation after the weight convolutional layers. Default `True`, same as what the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) does.</span>
</span><span id="L-57"><a href="#L-57"><span class="linenos">  57</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`, because batch normalisation are doing the similar thing with bias.</span>
</span><span id="L-58"><a href="#L-58"><span class="linenos">  58</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-59"><a href="#L-59"><span class="linenos">  59</span></a>        <span class="n">CLBackbone</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</span><span id="L-60"><a href="#L-60"><span class="linenos">  60</span></a>
</span><span id="L-61"><a href="#L-61"><span class="linenos">  61</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">batch_normalisation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">batch_normalisation</span>
</span><span id="L-62"><a href="#L-62"><span class="linenos">  62</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store whether to use batch normalisation after the fully-connected layers.&quot;&quot;&quot;</span>
</span><span id="L-63"><a href="#L-63"><span class="linenos">  63</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">activation_layer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span><span id="L-64"><a href="#L-64"><span class="linenos">  64</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store whether to use activation function after the fully-connected layers.&quot;&quot;&quot;</span>
</span><span id="L-65"><a href="#L-65"><span class="linenos">  65</span></a>
</span><span id="L-66"><a href="#L-66"><span class="linenos">  66</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">full_1st_layer_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">outer_layer_name</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">block_idx</span><span class="si">}</span><span class="s2">/conv1&quot;</span>
</span><span id="L-67"><a href="#L-67"><span class="linenos">  67</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Format and store full name of the 1st weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="L-68"><a href="#L-68"><span class="linenos">  68</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">full_2nd_layer_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">outer_layer_name</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">block_idx</span><span class="si">}</span><span class="s2">/conv2&quot;</span>
</span><span id="L-69"><a href="#L-69"><span class="linenos">  69</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Format and store full name of the 2nd weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="L-70"><a href="#L-70"><span class="linenos">  70</span></a>
</span><span id="L-71"><a href="#L-71"><span class="linenos">  71</span></a>        <span class="c1"># construct the 1st weighted convolutional layer and attached layers (batchnorm, activation, etc)</span>
</span><span id="L-72"><a href="#L-72"><span class="linenos">  72</span></a>        <span class="n">layer_input_channels</span> <span class="o">=</span> <span class="n">preceding_output_channels</span>  <span class="c1"># the input channels of the 1st convolutional layer, which receive the output channels of the preceding module</span>
</span><span id="L-73"><a href="#L-73"><span class="linenos">  73</span></a>        <span class="n">layer_output_channels</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-74"><a href="#L-74"><span class="linenos">  74</span></a>            <span class="n">input_channels</span>  <span class="c1"># the output channels of the 1st convolutional layer</span>
</span><span id="L-75"><a href="#L-75"><span class="linenos">  75</span></a>        <span class="p">)</span>
</span><span id="L-76"><a href="#L-76"><span class="linenos">  76</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
</span><span id="L-77"><a href="#L-77"><span class="linenos">  77</span></a>            <span class="n">in_channels</span><span class="o">=</span><span class="n">layer_input_channels</span><span class="p">,</span>
</span><span id="L-78"><a href="#L-78"><span class="linenos">  78</span></a>            <span class="n">out_channels</span><span class="o">=</span><span class="n">layer_output_channels</span><span class="p">,</span>
</span><span id="L-79"><a href="#L-79"><span class="linenos">  79</span></a>            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
</span><span id="L-80"><a href="#L-80"><span class="linenos">  80</span></a>            <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="L-81"><a href="#L-81"><span class="linenos">  81</span></a>            <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="L-82"><a href="#L-82"><span class="linenos">  82</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="L-83"><a href="#L-83"><span class="linenos">  83</span></a>        <span class="p">)</span>  <span class="c1"># construct the 1st weight convolutional layer of the smaller building block. Overall stride is not performed here</span>
</span><span id="L-84"><a href="#L-84"><span class="linenos">  84</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The 1st weight convolutional layer of the smaller building block. &quot;&quot;&quot;</span>
</span><span id="L-85"><a href="#L-85"><span class="linenos">  85</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="L-86"><a href="#L-86"><span class="linenos">  86</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">full_1st_layer_name</span>
</span><span id="L-87"><a href="#L-87"><span class="linenos">  87</span></a>        <span class="p">)</span>  <span class="c1"># update the weighted layer names</span>
</span><span id="L-88"><a href="#L-88"><span class="linenos">  88</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_normalisation</span><span class="p">:</span>
</span><span id="L-89"><a href="#L-89"><span class="linenos">  89</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv_bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span>
</span><span id="L-90"><a href="#L-90"><span class="linenos">  90</span></a>                <span class="n">num_features</span><span class="o">=</span><span class="n">layer_output_channels</span>
</span><span id="L-91"><a href="#L-91"><span class="linenos">  91</span></a>            <span class="p">)</span>  <span class="c1"># construct the batch normalisation layer</span>
</span><span id="L-92"><a href="#L-92"><span class="linenos">  92</span></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The batch normalisation (`nn.BatchNorm2d`) layer after the 1st weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="L-93"><a href="#L-93"><span class="linenos">  93</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="L-94"><a href="#L-94"><span class="linenos">  94</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation1</span> <span class="o">=</span> <span class="n">activation_layer</span><span class="p">()</span>  <span class="c1"># construct the activation layer</span>
</span><span id="L-95"><a href="#L-95"><span class="linenos">  95</span></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The activation layer after the 1st weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="L-96"><a href="#L-96"><span class="linenos">  96</span></a>
</span><span id="L-97"><a href="#L-97"><span class="linenos">  97</span></a>        <span class="c1"># construct the 2nd weighted convolutional layer and attached layers (batchnorm, activation, etc)</span>
</span><span id="L-98"><a href="#L-98"><span class="linenos">  98</span></a>        <span class="n">layer_input_channels</span> <span class="o">=</span> <span class="n">input_channels</span>  <span class="c1"># the input channels of the 2nd convolutional layer, which is `input_channels`, the same as the output channels of the 1st convolutional layer</span>
</span><span id="L-99"><a href="#L-99"><span class="linenos">  99</span></a>        <span class="n">layer_output_channels</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-100"><a href="#L-100"><span class="linenos"> 100</span></a>            <span class="n">input_channels</span> <span class="o">*</span> <span class="mi">1</span>
</span><span id="L-101"><a href="#L-101"><span class="linenos"> 101</span></a>        <span class="p">)</span>  <span class="c1"># the output channels of the 2nd convolutional layer, which is the same as the input channels (without expansion)</span>
</span><span id="L-102"><a href="#L-102"><span class="linenos"> 102</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
</span><span id="L-103"><a href="#L-103"><span class="linenos"> 103</span></a>            <span class="n">in_channels</span><span class="o">=</span><span class="n">layer_input_channels</span><span class="p">,</span>
</span><span id="L-104"><a href="#L-104"><span class="linenos"> 104</span></a>            <span class="n">out_channels</span><span class="o">=</span><span class="n">layer_output_channels</span><span class="p">,</span>
</span><span id="L-105"><a href="#L-105"><span class="linenos"> 105</span></a>            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
</span><span id="L-106"><a href="#L-106"><span class="linenos"> 106</span></a>            <span class="n">stride</span><span class="o">=</span><span class="n">overall_stride</span><span class="p">,</span>
</span><span id="L-107"><a href="#L-107"><span class="linenos"> 107</span></a>            <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="L-108"><a href="#L-108"><span class="linenos"> 108</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="L-109"><a href="#L-109"><span class="linenos"> 109</span></a>        <span class="p">)</span>  <span class="c1"># construct the 2nd weight convolutional layer of the smaller building block. Overall stride is performed here</span>
</span><span id="L-110"><a href="#L-110"><span class="linenos"> 110</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The 2nd weight convolutional layer of the smaller building block. &quot;&quot;&quot;</span>
</span><span id="L-111"><a href="#L-111"><span class="linenos"> 111</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="L-112"><a href="#L-112"><span class="linenos"> 112</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">full_2nd_layer_name</span>
</span><span id="L-113"><a href="#L-113"><span class="linenos"> 113</span></a>        <span class="p">)</span>  <span class="c1"># update the weighted layer names</span>
</span><span id="L-114"><a href="#L-114"><span class="linenos"> 114</span></a>        <span class="k">if</span> <span class="n">batch_normalisation</span><span class="p">:</span>
</span><span id="L-115"><a href="#L-115"><span class="linenos"> 115</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv_bn2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span>
</span><span id="L-116"><a href="#L-116"><span class="linenos"> 116</span></a>                <span class="n">num_features</span><span class="o">=</span><span class="n">layer_output_channels</span>
</span><span id="L-117"><a href="#L-117"><span class="linenos"> 117</span></a>            <span class="p">)</span>  <span class="c1"># construct the batch normalisation layer</span>
</span><span id="L-118"><a href="#L-118"><span class="linenos"> 118</span></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The batch normalisation (`nn.BatchNorm2d`) layer after the 2nd weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="L-119"><a href="#L-119"><span class="linenos"> 119</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="L-120"><a href="#L-120"><span class="linenos"> 120</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation2</span> <span class="o">=</span> <span class="n">activation_layer</span><span class="p">()</span>  <span class="c1"># construct the activation layer</span>
</span><span id="L-121"><a href="#L-121"><span class="linenos"> 121</span></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The activation layer after the 2nd weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="L-122"><a href="#L-122"><span class="linenos"> 122</span></a>
</span><span id="L-123"><a href="#L-123"><span class="linenos"> 123</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">identity_downsample</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-124"><a href="#L-124"><span class="linenos"> 124</span></a>            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
</span><span id="L-125"><a href="#L-125"><span class="linenos"> 125</span></a>                <span class="n">in_channels</span><span class="o">=</span><span class="n">preceding_output_channels</span><span class="p">,</span>
</span><span id="L-126"><a href="#L-126"><span class="linenos"> 126</span></a>                <span class="n">out_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
</span><span id="L-127"><a href="#L-127"><span class="linenos"> 127</span></a>                <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="L-128"><a href="#L-128"><span class="linenos"> 128</span></a>                <span class="n">stride</span><span class="o">=</span><span class="n">overall_stride</span><span class="p">,</span>
</span><span id="L-129"><a href="#L-129"><span class="linenos"> 129</span></a>                <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="L-130"><a href="#L-130"><span class="linenos"> 130</span></a>            <span class="p">)</span>
</span><span id="L-131"><a href="#L-131"><span class="linenos"> 131</span></a>            <span class="k">if</span> <span class="n">preceding_output_channels</span> <span class="o">!=</span> <span class="n">input_channels</span> <span class="ow">or</span> <span class="n">overall_stride</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="L-132"><a href="#L-132"><span class="linenos"> 132</span></a>            <span class="k">else</span> <span class="kc">None</span>
</span><span id="L-133"><a href="#L-133"><span class="linenos"> 133</span></a>        <span class="p">)</span>  <span class="c1"># construct the identity downsample function</span>
</span><span id="L-134"><a href="#L-134"><span class="linenos"> 134</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The convolutional layer for downsampling identity in the shortcut connection if the dimension of identity from input doesn&#39;t match the output&#39;s. This case only happens when the number of input channels doesn&#39;t equal to the number of preceding output channels or a layer with stride &gt; 1 exists. &quot;&quot;&quot;</span>
</span><span id="L-135"><a href="#L-135"><span class="linenos"> 135</span></a>
</span><span id="L-136"><a href="#L-136"><span class="linenos"> 136</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
</span><span id="L-137"><a href="#L-137"><span class="linenos"> 137</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The forward pass for data.</span>
</span><span id="L-138"><a href="#L-138"><span class="linenos"> 138</span></a>
</span><span id="L-139"><a href="#L-139"><span class="linenos"> 139</span></a><span class="sd">        **Args:**</span>
</span><span id="L-140"><a href="#L-140"><span class="linenos"> 140</span></a><span class="sd">        - **input** (`Tensor`): the input feature maps.</span>
</span><span id="L-141"><a href="#L-141"><span class="linenos"> 141</span></a>
</span><span id="L-142"><a href="#L-142"><span class="linenos"> 142</span></a><span class="sd">        **Returns:**</span>
</span><span id="L-143"><a href="#L-143"><span class="linenos"> 143</span></a><span class="sd">        - **output_feature** (`Tensor`): the output feature maps.</span>
</span><span id="L-144"><a href="#L-144"><span class="linenos"> 144</span></a><span class="sd">        - **hidden_features** (`dict[str, Tensor]`): the hidden features (after activation) in each weighted layer. Key (`str`) is the weighted layer name, value (`Tensor`) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes.</span>
</span><span id="L-145"><a href="#L-145"><span class="linenos"> 145</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-146"><a href="#L-146"><span class="linenos"> 146</span></a>        <span class="n">hidden_features</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="L-147"><a href="#L-147"><span class="linenos"> 147</span></a>
</span><span id="L-148"><a href="#L-148"><span class="linenos"> 148</span></a>        <span class="n">identity</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-149"><a href="#L-149"><span class="linenos"> 149</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">identity_downsample</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span><span id="L-150"><a href="#L-150"><span class="linenos"> 150</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">identity_downsample</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span><span id="L-151"><a href="#L-151"><span class="linenos"> 151</span></a>            <span class="k">else</span> <span class="nb">input</span>
</span><span id="L-152"><a href="#L-152"><span class="linenos"> 152</span></a>        <span class="p">)</span>  <span class="c1"># remember the identity of input for the shortcut connection. Perform downsampling if its dimension doesn&#39;t match the output&#39;s</span>
</span><span id="L-153"><a href="#L-153"><span class="linenos"> 153</span></a>
</span><span id="L-154"><a href="#L-154"><span class="linenos"> 154</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="nb">input</span>
</span><span id="L-155"><a href="#L-155"><span class="linenos"> 155</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-156"><a href="#L-156"><span class="linenos"> 156</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_normalisation</span><span class="p">:</span>
</span><span id="L-157"><a href="#L-157"><span class="linenos"> 157</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_bn1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-158"><a href="#L-158"><span class="linenos"> 158</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="L-159"><a href="#L-159"><span class="linenos"> 159</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-160"><a href="#L-160"><span class="linenos"> 160</span></a>        <span class="n">hidden_features</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_1st_layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>  <span class="c1"># store the hidden feature</span>
</span><span id="L-161"><a href="#L-161"><span class="linenos"> 161</span></a>
</span><span id="L-162"><a href="#L-162"><span class="linenos"> 162</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-163"><a href="#L-163"><span class="linenos"> 163</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_normalisation</span><span class="p">:</span>
</span><span id="L-164"><a href="#L-164"><span class="linenos"> 164</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_bn2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-165"><a href="#L-165"><span class="linenos"> 165</span></a>
</span><span id="L-166"><a href="#L-166"><span class="linenos"> 166</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">identity</span>
</span><span id="L-167"><a href="#L-167"><span class="linenos"> 167</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="L-168"><a href="#L-168"><span class="linenos"> 168</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># activation after the shortcut connection</span>
</span><span id="L-169"><a href="#L-169"><span class="linenos"> 169</span></a>        <span class="n">hidden_features</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_2nd_layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>  <span class="c1"># store the hidden feature</span>
</span><span id="L-170"><a href="#L-170"><span class="linenos"> 170</span></a>
</span><span id="L-171"><a href="#L-171"><span class="linenos"> 171</span></a>        <span class="n">output_feature</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="L-172"><a href="#L-172"><span class="linenos"> 172</span></a>
</span><span id="L-173"><a href="#L-173"><span class="linenos"> 173</span></a>        <span class="k">return</span> <span class="n">output_feature</span><span class="p">,</span> <span class="n">hidden_features</span>
</span><span id="L-174"><a href="#L-174"><span class="linenos"> 174</span></a>
</span><span id="L-175"><a href="#L-175"><span class="linenos"> 175</span></a>
</span><span id="L-176"><a href="#L-176"><span class="linenos"> 176</span></a><span class="k">class</span><span class="w"> </span><span class="nc">ResNetBlockLarge</span><span class="p">(</span><span class="n">CLBackbone</span><span class="p">):</span>
</span><span id="L-177"><a href="#L-177"><span class="linenos"> 177</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The larger building block for ResNet-50/101/152. It is referred to &quot;bottleneck&quot; building block in the paper.</span>
</span><span id="L-178"><a href="#L-178"><span class="linenos"> 178</span></a>
</span><span id="L-179"><a href="#L-179"><span class="linenos"> 179</span></a><span class="sd">    It consists of 3 weight convolutional layers, each followed by an activation function. See Table 1 or Figure 5 (right) in the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html).</span>
</span><span id="L-180"><a href="#L-180"><span class="linenos"> 180</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-181"><a href="#L-181"><span class="linenos"> 181</span></a>
</span><span id="L-182"><a href="#L-182"><span class="linenos"> 182</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-183"><a href="#L-183"><span class="linenos"> 183</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-184"><a href="#L-184"><span class="linenos"> 184</span></a>        <span class="n">outer_layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-185"><a href="#L-185"><span class="linenos"> 185</span></a>        <span class="n">block_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-186"><a href="#L-186"><span class="linenos"> 186</span></a>        <span class="n">preceding_output_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-187"><a href="#L-187"><span class="linenos"> 187</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-188"><a href="#L-188"><span class="linenos"> 188</span></a>        <span class="n">overall_stride</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-189"><a href="#L-189"><span class="linenos"> 189</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="L-190"><a href="#L-190"><span class="linenos"> 190</span></a>        <span class="n">batch_normalisation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="L-191"><a href="#L-191"><span class="linenos"> 191</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-192"><a href="#L-192"><span class="linenos"> 192</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-193"><a href="#L-193"><span class="linenos"> 193</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the larger building block.</span>
</span><span id="L-194"><a href="#L-194"><span class="linenos"> 194</span></a>
</span><span id="L-195"><a href="#L-195"><span class="linenos"> 195</span></a><span class="sd">        **Args:**</span>
</span><span id="L-196"><a href="#L-196"><span class="linenos"> 196</span></a><span class="sd">        - **outer_layer_name** (`str`): pass the name of the multi-building-block layer that contains this building block to construct the full name of each weighted convolutional layer.</span>
</span><span id="L-197"><a href="#L-197"><span class="linenos"> 197</span></a><span class="sd">        - **block_idx** (`int`): the index of the building blocks in the multi-building-block layer to construct the full name of each weighted convolutional layer.</span>
</span><span id="L-198"><a href="#L-198"><span class="linenos"> 198</span></a><span class="sd">        - **preceding_output_channels** (`int`): the number of channels of preceding output of this particular building block.</span>
</span><span id="L-199"><a href="#L-199"><span class="linenos"> 199</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input of this building block.</span>
</span><span id="L-200"><a href="#L-200"><span class="linenos"> 200</span></a><span class="sd">        - **overall_stride** (`int`): the overall stride of this building block. This stride is performed at 2nd (middle) convolutional layer where 1st and 3rd convolutional layers remain stride of 1.</span>
</span><span id="L-201"><a href="#L-201"><span class="linenos"> 201</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="L-202"><a href="#L-202"><span class="linenos"> 202</span></a><span class="sd">        - **batch_normalisation** (`bool`): whether to use batch normalisation after the weight convolutional layers. Default `True`, same as what the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) does.</span>
</span><span id="L-203"><a href="#L-203"><span class="linenos"> 203</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`, because batch normalisation are doing the similar thing with bias.</span>
</span><span id="L-204"><a href="#L-204"><span class="linenos"> 204</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-205"><a href="#L-205"><span class="linenos"> 205</span></a>        <span class="n">CLBackbone</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</span><span id="L-206"><a href="#L-206"><span class="linenos"> 206</span></a>
</span><span id="L-207"><a href="#L-207"><span class="linenos"> 207</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">batch_normalisation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">batch_normalisation</span>
</span><span id="L-208"><a href="#L-208"><span class="linenos"> 208</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store whether to use batch normalisation after the fully-connected layers.&quot;&quot;&quot;</span>
</span><span id="L-209"><a href="#L-209"><span class="linenos"> 209</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">activation_layer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span><span id="L-210"><a href="#L-210"><span class="linenos"> 210</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store whether to use activation function after the fully-connected layers.&quot;&quot;&quot;</span>
</span><span id="L-211"><a href="#L-211"><span class="linenos"> 211</span></a>
</span><span id="L-212"><a href="#L-212"><span class="linenos"> 212</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">full_1st_layer_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">outer_layer_name</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">block_idx</span><span class="si">}</span><span class="s2">/conv1&quot;</span>
</span><span id="L-213"><a href="#L-213"><span class="linenos"> 213</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Format and store full name of the 1st weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="L-214"><a href="#L-214"><span class="linenos"> 214</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">full_2nd_layer_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">outer_layer_name</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">block_idx</span><span class="si">}</span><span class="s2">_conv2&quot;</span>
</span><span id="L-215"><a href="#L-215"><span class="linenos"> 215</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Format and store full name of the 2nd weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="L-216"><a href="#L-216"><span class="linenos"> 216</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">full_3rd_layer_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">outer_layer_name</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">block_idx</span><span class="si">}</span><span class="s2">_conv3&quot;</span>
</span><span id="L-217"><a href="#L-217"><span class="linenos"> 217</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Format and store full name of the 3rd weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="L-218"><a href="#L-218"><span class="linenos"> 218</span></a>
</span><span id="L-219"><a href="#L-219"><span class="linenos"> 219</span></a>        <span class="c1"># construct the 1st weighted convolutional layer and attached layers (batchnorm, activation, etc)</span>
</span><span id="L-220"><a href="#L-220"><span class="linenos"> 220</span></a>        <span class="n">layer_input_channels</span> <span class="o">=</span> <span class="n">preceding_output_channels</span>  <span class="c1"># the input channels of the 1st convolutional layer, which receive the output channels of the preceding module</span>
</span><span id="L-221"><a href="#L-221"><span class="linenos"> 221</span></a>        <span class="n">layer_output_channels</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-222"><a href="#L-222"><span class="linenos"> 222</span></a>            <span class="n">input_channels</span>  <span class="c1"># the output channels of the 1st convolutional layer</span>
</span><span id="L-223"><a href="#L-223"><span class="linenos"> 223</span></a>        <span class="p">)</span>
</span><span id="L-224"><a href="#L-224"><span class="linenos"> 224</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
</span><span id="L-225"><a href="#L-225"><span class="linenos"> 225</span></a>            <span class="n">in_channels</span><span class="o">=</span><span class="n">layer_input_channels</span><span class="p">,</span>
</span><span id="L-226"><a href="#L-226"><span class="linenos"> 226</span></a>            <span class="n">out_channels</span><span class="o">=</span><span class="n">layer_output_channels</span><span class="p">,</span>
</span><span id="L-227"><a href="#L-227"><span class="linenos"> 227</span></a>            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="L-228"><a href="#L-228"><span class="linenos"> 228</span></a>            <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="L-229"><a href="#L-229"><span class="linenos"> 229</span></a>            <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
</span><span id="L-230"><a href="#L-230"><span class="linenos"> 230</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="L-231"><a href="#L-231"><span class="linenos"> 231</span></a>        <span class="p">)</span>  <span class="c1"># construct the 1st weight convolutional layer of the larger building block. Overall stride is not performed here</span>
</span><span id="L-232"><a href="#L-232"><span class="linenos"> 232</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The 1st weight convolutional layer of the larger building block. &quot;&quot;&quot;</span>
</span><span id="L-233"><a href="#L-233"><span class="linenos"> 233</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="L-234"><a href="#L-234"><span class="linenos"> 234</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">full_1st_layer_name</span>
</span><span id="L-235"><a href="#L-235"><span class="linenos"> 235</span></a>        <span class="p">)</span>  <span class="c1"># update the weighted layer names</span>
</span><span id="L-236"><a href="#L-236"><span class="linenos"> 236</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_normalisation</span><span class="p">:</span>
</span><span id="L-237"><a href="#L-237"><span class="linenos"> 237</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv_bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span>
</span><span id="L-238"><a href="#L-238"><span class="linenos"> 238</span></a>                <span class="n">num_features</span><span class="o">=</span><span class="n">layer_output_channels</span>
</span><span id="L-239"><a href="#L-239"><span class="linenos"> 239</span></a>            <span class="p">)</span>  <span class="c1"># construct the batch normalisation layer</span>
</span><span id="L-240"><a href="#L-240"><span class="linenos"> 240</span></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The batch normalisation (`nn.BatchNorm2d`) layer after the 1st weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="L-241"><a href="#L-241"><span class="linenos"> 241</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="L-242"><a href="#L-242"><span class="linenos"> 242</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation1</span> <span class="o">=</span> <span class="n">activation_layer</span><span class="p">()</span>  <span class="c1"># construct the activation layer</span>
</span><span id="L-243"><a href="#L-243"><span class="linenos"> 243</span></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The activation layer after the 1st weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="L-244"><a href="#L-244"><span class="linenos"> 244</span></a>
</span><span id="L-245"><a href="#L-245"><span class="linenos"> 245</span></a>        <span class="c1"># construct the 2nd weighted convolutional layer and attached layers (batchnorm, activation, etc)</span>
</span><span id="L-246"><a href="#L-246"><span class="linenos"> 246</span></a>        <span class="n">layer_input_channels</span> <span class="o">=</span> <span class="n">input_channels</span>  <span class="c1"># the input channels of the 2nd convolutional layer, which is `input_channels`, the same as the output channels of the 1st convolutional layer</span>
</span><span id="L-247"><a href="#L-247"><span class="linenos"> 247</span></a>        <span class="n">layer_output_channels</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-248"><a href="#L-248"><span class="linenos"> 248</span></a>            <span class="n">input_channels</span>
</span><span id="L-249"><a href="#L-249"><span class="linenos"> 249</span></a>            <span class="o">*</span> <span class="mi">1</span>  <span class="c1"># the output channels of the 2nd convolutional layer, which is the same as the input channels (without expansion)</span>
</span><span id="L-250"><a href="#L-250"><span class="linenos"> 250</span></a>        <span class="p">)</span>
</span><span id="L-251"><a href="#L-251"><span class="linenos"> 251</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
</span><span id="L-252"><a href="#L-252"><span class="linenos"> 252</span></a>            <span class="n">in_channels</span><span class="o">=</span><span class="n">layer_input_channels</span><span class="p">,</span>
</span><span id="L-253"><a href="#L-253"><span class="linenos"> 253</span></a>            <span class="n">out_channels</span><span class="o">=</span><span class="n">layer_output_channels</span><span class="p">,</span>
</span><span id="L-254"><a href="#L-254"><span class="linenos"> 254</span></a>            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
</span><span id="L-255"><a href="#L-255"><span class="linenos"> 255</span></a>            <span class="n">stride</span><span class="o">=</span><span class="n">overall_stride</span><span class="p">,</span>
</span><span id="L-256"><a href="#L-256"><span class="linenos"> 256</span></a>            <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="L-257"><a href="#L-257"><span class="linenos"> 257</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="L-258"><a href="#L-258"><span class="linenos"> 258</span></a>        <span class="p">)</span>  <span class="c1"># construct the 2nd weight convolutional layer of the larger building block. Overall stride is performed here</span>
</span><span id="L-259"><a href="#L-259"><span class="linenos"> 259</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The 2nd weight convolutional layer of the larger building block. &quot;&quot;&quot;</span>
</span><span id="L-260"><a href="#L-260"><span class="linenos"> 260</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="L-261"><a href="#L-261"><span class="linenos"> 261</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">full_2nd_layer_name</span>
</span><span id="L-262"><a href="#L-262"><span class="linenos"> 262</span></a>        <span class="p">)</span>  <span class="c1"># update the weighted layer names</span>
</span><span id="L-263"><a href="#L-263"><span class="linenos"> 263</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_normalisation</span><span class="p">:</span>
</span><span id="L-264"><a href="#L-264"><span class="linenos"> 264</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv_bn2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span>
</span><span id="L-265"><a href="#L-265"><span class="linenos"> 265</span></a>                <span class="n">num_features</span><span class="o">=</span><span class="n">layer_output_channels</span>
</span><span id="L-266"><a href="#L-266"><span class="linenos"> 266</span></a>            <span class="p">)</span>  <span class="c1"># construct the batch normalisation layer</span>
</span><span id="L-267"><a href="#L-267"><span class="linenos"> 267</span></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The batch normalisation (`nn.BatchNorm2d`) layer after the 2nd weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="L-268"><a href="#L-268"><span class="linenos"> 268</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="L-269"><a href="#L-269"><span class="linenos"> 269</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation2</span> <span class="o">=</span> <span class="n">activation_layer</span><span class="p">()</span>  <span class="c1"># construct the activation layer</span>
</span><span id="L-270"><a href="#L-270"><span class="linenos"> 270</span></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The activation layer after the 2nd weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="L-271"><a href="#L-271"><span class="linenos"> 271</span></a>
</span><span id="L-272"><a href="#L-272"><span class="linenos"> 272</span></a>        <span class="c1"># construct the 3rd weighted convolutional layer and attached layers (batchnorm, activation, etc)</span>
</span><span id="L-273"><a href="#L-273"><span class="linenos"> 273</span></a>        <span class="n">layer_input_channels</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-274"><a href="#L-274"><span class="linenos"> 274</span></a>            <span class="n">input_channels</span> <span class="o">*</span> <span class="mi">1</span>
</span><span id="L-275"><a href="#L-275"><span class="linenos"> 275</span></a>        <span class="p">)</span>  <span class="c1"># the input channels of the 2nd convolutional layer, which is `input_channels * 1`, the same as the output channels of the 1st convolutional layer</span>
</span><span id="L-276"><a href="#L-276"><span class="linenos"> 276</span></a>        <span class="n">layer_output_channels</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-277"><a href="#L-277"><span class="linenos"> 277</span></a>            <span class="n">input_channels</span>
</span><span id="L-278"><a href="#L-278"><span class="linenos"> 278</span></a>            <span class="o">*</span> <span class="mi">4</span>  <span class="c1"># the output channels of the 2nd convolutional layer, which is 4 times expanded as the input channels</span>
</span><span id="L-279"><a href="#L-279"><span class="linenos"> 279</span></a>        <span class="p">)</span>
</span><span id="L-280"><a href="#L-280"><span class="linenos"> 280</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
</span><span id="L-281"><a href="#L-281"><span class="linenos"> 281</span></a>            <span class="n">in_channels</span><span class="o">=</span><span class="n">layer_input_channels</span><span class="p">,</span>
</span><span id="L-282"><a href="#L-282"><span class="linenos"> 282</span></a>            <span class="n">out_channels</span><span class="o">=</span><span class="n">layer_output_channels</span><span class="p">,</span>
</span><span id="L-283"><a href="#L-283"><span class="linenos"> 283</span></a>            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="L-284"><a href="#L-284"><span class="linenos"> 284</span></a>            <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="L-285"><a href="#L-285"><span class="linenos"> 285</span></a>            <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
</span><span id="L-286"><a href="#L-286"><span class="linenos"> 286</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="L-287"><a href="#L-287"><span class="linenos"> 287</span></a>        <span class="p">)</span>  <span class="c1"># construct the 3rd weight convolutional layer of the larger building block. Overall stride is not performed here</span>
</span><span id="L-288"><a href="#L-288"><span class="linenos"> 288</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The 3rd weight convolutional layer of the larger building block. &quot;&quot;&quot;</span>
</span><span id="L-289"><a href="#L-289"><span class="linenos"> 289</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="L-290"><a href="#L-290"><span class="linenos"> 290</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">full_3rd_layer_name</span>
</span><span id="L-291"><a href="#L-291"><span class="linenos"> 291</span></a>        <span class="p">)</span>  <span class="c1"># update the weighted layer names</span>
</span><span id="L-292"><a href="#L-292"><span class="linenos"> 292</span></a>        <span class="k">if</span> <span class="n">batch_normalisation</span><span class="p">:</span>
</span><span id="L-293"><a href="#L-293"><span class="linenos"> 293</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv_bn3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span>
</span><span id="L-294"><a href="#L-294"><span class="linenos"> 294</span></a>                <span class="n">num_features</span><span class="o">=</span><span class="n">layer_output_channels</span>
</span><span id="L-295"><a href="#L-295"><span class="linenos"> 295</span></a>            <span class="p">)</span>  <span class="c1"># construct the batch normalisation layer</span>
</span><span id="L-296"><a href="#L-296"><span class="linenos"> 296</span></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The batch normalisation (`nn.BatchNorm2d`) layer after the 3rd weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="L-297"><a href="#L-297"><span class="linenos"> 297</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="L-298"><a href="#L-298"><span class="linenos"> 298</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation3</span> <span class="o">=</span> <span class="n">activation_layer</span><span class="p">()</span>  <span class="c1"># construct the activation layer</span>
</span><span id="L-299"><a href="#L-299"><span class="linenos"> 299</span></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The activation layer after the 3rd weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="L-300"><a href="#L-300"><span class="linenos"> 300</span></a>
</span><span id="L-301"><a href="#L-301"><span class="linenos"> 301</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">identity_downsample</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-302"><a href="#L-302"><span class="linenos"> 302</span></a>            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
</span><span id="L-303"><a href="#L-303"><span class="linenos"> 303</span></a>                <span class="n">in_channels</span><span class="o">=</span><span class="n">preceding_output_channels</span><span class="p">,</span>
</span><span id="L-304"><a href="#L-304"><span class="linenos"> 304</span></a>                <span class="n">out_channels</span><span class="o">=</span><span class="n">input_channels</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span>
</span><span id="L-305"><a href="#L-305"><span class="linenos"> 305</span></a>                <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="L-306"><a href="#L-306"><span class="linenos"> 306</span></a>                <span class="n">stride</span><span class="o">=</span><span class="n">overall_stride</span><span class="p">,</span>
</span><span id="L-307"><a href="#L-307"><span class="linenos"> 307</span></a>                <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="L-308"><a href="#L-308"><span class="linenos"> 308</span></a>            <span class="p">)</span>
</span><span id="L-309"><a href="#L-309"><span class="linenos"> 309</span></a>            <span class="k">if</span> <span class="n">preceding_output_channels</span> <span class="o">!=</span> <span class="n">input_channels</span> <span class="o">*</span> <span class="mi">4</span> <span class="ow">or</span> <span class="n">overall_stride</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="L-310"><a href="#L-310"><span class="linenos"> 310</span></a>            <span class="k">else</span> <span class="kc">None</span>
</span><span id="L-311"><a href="#L-311"><span class="linenos"> 311</span></a>        <span class="p">)</span>
</span><span id="L-312"><a href="#L-312"><span class="linenos"> 312</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The convolutional layer for downsampling identity in the shortcut connection if the dimension of identity from input doesn&#39;t match the output&#39;s. This case only happens when the number of input channels doesn&#39;t equal to the number of preceding output channels or a layer with stride &gt; 1 exists. &quot;&quot;&quot;</span>
</span><span id="L-313"><a href="#L-313"><span class="linenos"> 313</span></a>
</span><span id="L-314"><a href="#L-314"><span class="linenos"> 314</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
</span><span id="L-315"><a href="#L-315"><span class="linenos"> 315</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The forward pass for data.</span>
</span><span id="L-316"><a href="#L-316"><span class="linenos"> 316</span></a>
</span><span id="L-317"><a href="#L-317"><span class="linenos"> 317</span></a><span class="sd">        **Args:**</span>
</span><span id="L-318"><a href="#L-318"><span class="linenos"> 318</span></a><span class="sd">        - **input** (`Tensor`): the input feature maps.</span>
</span><span id="L-319"><a href="#L-319"><span class="linenos"> 319</span></a>
</span><span id="L-320"><a href="#L-320"><span class="linenos"> 320</span></a><span class="sd">        **Returns:**</span>
</span><span id="L-321"><a href="#L-321"><span class="linenos"> 321</span></a><span class="sd">        - **output_feature** (`Tensor`): the output feature maps.</span>
</span><span id="L-322"><a href="#L-322"><span class="linenos"> 322</span></a><span class="sd">        - **hidden_features** (`dict[str, Tensor]`): the hidden features (after activation) in each weighted layer. Key (`str`) is the weighted layer name, value (`Tensor`) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes.</span>
</span><span id="L-323"><a href="#L-323"><span class="linenos"> 323</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-324"><a href="#L-324"><span class="linenos"> 324</span></a>        <span class="n">hidden_features</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="L-325"><a href="#L-325"><span class="linenos"> 325</span></a>
</span><span id="L-326"><a href="#L-326"><span class="linenos"> 326</span></a>        <span class="n">identity</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-327"><a href="#L-327"><span class="linenos"> 327</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">identity_downsample</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span><span id="L-328"><a href="#L-328"><span class="linenos"> 328</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">identity_downsample</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span><span id="L-329"><a href="#L-329"><span class="linenos"> 329</span></a>            <span class="k">else</span> <span class="nb">input</span>
</span><span id="L-330"><a href="#L-330"><span class="linenos"> 330</span></a>        <span class="p">)</span>  <span class="c1"># remember the identity of input for the shortcut connection. Perform downsampling if its dimension doesn&#39;t match the output&#39;s</span>
</span><span id="L-331"><a href="#L-331"><span class="linenos"> 331</span></a>
</span><span id="L-332"><a href="#L-332"><span class="linenos"> 332</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="nb">input</span>
</span><span id="L-333"><a href="#L-333"><span class="linenos"> 333</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-334"><a href="#L-334"><span class="linenos"> 334</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_normalisation</span><span class="p">:</span>
</span><span id="L-335"><a href="#L-335"><span class="linenos"> 335</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_bn1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-336"><a href="#L-336"><span class="linenos"> 336</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="L-337"><a href="#L-337"><span class="linenos"> 337</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-338"><a href="#L-338"><span class="linenos"> 338</span></a>        <span class="n">hidden_features</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_1st_layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>  <span class="c1"># store the hidden feature</span>
</span><span id="L-339"><a href="#L-339"><span class="linenos"> 339</span></a>
</span><span id="L-340"><a href="#L-340"><span class="linenos"> 340</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-341"><a href="#L-341"><span class="linenos"> 341</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_normalisation</span><span class="p">:</span>
</span><span id="L-342"><a href="#L-342"><span class="linenos"> 342</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_bn2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-343"><a href="#L-343"><span class="linenos"> 343</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="L-344"><a href="#L-344"><span class="linenos"> 344</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-345"><a href="#L-345"><span class="linenos"> 345</span></a>        <span class="n">hidden_features</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_2nd_layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>  <span class="c1"># store the hidden feature</span>
</span><span id="L-346"><a href="#L-346"><span class="linenos"> 346</span></a>
</span><span id="L-347"><a href="#L-347"><span class="linenos"> 347</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-348"><a href="#L-348"><span class="linenos"> 348</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_normalisation</span><span class="p">:</span>
</span><span id="L-349"><a href="#L-349"><span class="linenos"> 349</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_bn3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-350"><a href="#L-350"><span class="linenos"> 350</span></a>
</span><span id="L-351"><a href="#L-351"><span class="linenos"> 351</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">identity</span>
</span><span id="L-352"><a href="#L-352"><span class="linenos"> 352</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="L-353"><a href="#L-353"><span class="linenos"> 353</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># activation after the shortcut connection</span>
</span><span id="L-354"><a href="#L-354"><span class="linenos"> 354</span></a>        <span class="n">hidden_features</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_3rd_layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>  <span class="c1"># store the hidden feature</span>
</span><span id="L-355"><a href="#L-355"><span class="linenos"> 355</span></a>
</span><span id="L-356"><a href="#L-356"><span class="linenos"> 356</span></a>        <span class="n">output_feature</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="L-357"><a href="#L-357"><span class="linenos"> 357</span></a>
</span><span id="L-358"><a href="#L-358"><span class="linenos"> 358</span></a>        <span class="k">return</span> <span class="n">output_feature</span><span class="p">,</span> <span class="n">hidden_features</span>
</span><span id="L-359"><a href="#L-359"><span class="linenos"> 359</span></a>
</span><span id="L-360"><a href="#L-360"><span class="linenos"> 360</span></a>
</span><span id="L-361"><a href="#L-361"><span class="linenos"> 361</span></a><span class="k">class</span><span class="w"> </span><span class="nc">ResNetBase</span><span class="p">(</span><span class="n">CLBackbone</span><span class="p">):</span>
</span><span id="L-362"><a href="#L-362"><span class="linenos"> 362</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The base class of [residual network (ResNet)](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html).</span>
</span><span id="L-363"><a href="#L-363"><span class="linenos"> 363</span></a>
</span><span id="L-364"><a href="#L-364"><span class="linenos"> 364</span></a><span class="sd">    ResNet is a convolutional network architecture, which has 1st convolutional parameter layer and a maxpooling layer, connecting to 4 convolutional layers which contains multiple convolutional parameter layer. Each layer of the 4 are constructed from basic building blocks which are either small (`ResNetBlockSmall`) or large (`ResNetBlockLarge`). Each building block contains several convolutional parameter layers. The building blocks are connected by a skip connection which is a direct connection from the input of the block to the output of the block, and this is why it&#39;s called residual (find &quot;shortcut connections&quot; in the paper for more details). After the 5th convolutional layer, there are average pooling layer and a fully connected layer which connects to the CL output heads.</span>
</span><span id="L-365"><a href="#L-365"><span class="linenos"> 365</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-366"><a href="#L-366"><span class="linenos"> 366</span></a>
</span><span id="L-367"><a href="#L-367"><span class="linenos"> 367</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-368"><a href="#L-368"><span class="linenos"> 368</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-369"><a href="#L-369"><span class="linenos"> 369</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-370"><a href="#L-370"><span class="linenos"> 370</span></a>        <span class="n">building_block_type</span><span class="p">:</span> <span class="n">ResNetBlockSmall</span> <span class="o">|</span> <span class="n">ResNetBlockLarge</span><span class="p">,</span>
</span><span id="L-371"><a href="#L-371"><span class="linenos"> 371</span></a>        <span class="n">building_block_nums</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
</span><span id="L-372"><a href="#L-372"><span class="linenos"> 372</span></a>        <span class="n">building_block_preceding_output_channels</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
</span><span id="L-373"><a href="#L-373"><span class="linenos"> 373</span></a>        <span class="n">building_block_input_channels</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
</span><span id="L-374"><a href="#L-374"><span class="linenos"> 374</span></a>        <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-375"><a href="#L-375"><span class="linenos"> 375</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="L-376"><a href="#L-376"><span class="linenos"> 376</span></a>        <span class="n">batch_normalisation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="L-377"><a href="#L-377"><span class="linenos"> 377</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-378"><a href="#L-378"><span class="linenos"> 378</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-379"><a href="#L-379"><span class="linenos"> 379</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the ResNet backbone network.</span>
</span><span id="L-380"><a href="#L-380"><span class="linenos"> 380</span></a>
</span><span id="L-381"><a href="#L-381"><span class="linenos"> 381</span></a><span class="sd">        **Args:**</span>
</span><span id="L-382"><a href="#L-382"><span class="linenos"> 382</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input. Image data are kept channels when going in ResNet. Note that convolutional networks require number of input channels instead of dimension.</span>
</span><span id="L-383"><a href="#L-383"><span class="linenos"> 383</span></a><span class="sd">        - **building_block_type** (`ResNetBlockSmall` | `ResNetBlockLarge`): the type of building block used in the ResNet.</span>
</span><span id="L-384"><a href="#L-384"><span class="linenos"> 384</span></a><span class="sd">        - **building_block_nums** (`tuple[int, int, int, int]`): the number of building blocks in the 2-5 convolutional layer correspondingly.</span>
</span><span id="L-385"><a href="#L-385"><span class="linenos"> 385</span></a><span class="sd">        - **building_block_preceding_output_channels** (`tuple[int, int, int, int]`): the number of channels of preceding output of each building block in the 2-5 convolutional layer correspondingly.</span>
</span><span id="L-386"><a href="#L-386"><span class="linenos"> 386</span></a><span class="sd">        - **building_block_input_channels** (`tuple[int, int, int, int]`): the number of channels of input of each building block in the 2-5 convolutional layer correspondingly.</span>
</span><span id="L-387"><a href="#L-387"><span class="linenos"> 387</span></a><span class="sd">        - **output_dim** (`int`): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</span>
</span><span id="L-388"><a href="#L-388"><span class="linenos"> 388</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="L-389"><a href="#L-389"><span class="linenos"> 389</span></a><span class="sd">        - **batch_normalisation** (`bool`): whether to use batch normalisation after the weight convolutional layers. Default `True`, same as what the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) does.</span>
</span><span id="L-390"><a href="#L-390"><span class="linenos"> 390</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`, because batch normalisation are doing the similar thing with bias.</span>
</span><span id="L-391"><a href="#L-391"><span class="linenos"> 391</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-392"><a href="#L-392"><span class="linenos"> 392</span></a>        <span class="n">CLBackbone</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">)</span>
</span><span id="L-393"><a href="#L-393"><span class="linenos"> 393</span></a>
</span><span id="L-394"><a href="#L-394"><span class="linenos"> 394</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">batch_normalisation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">batch_normalisation</span>
</span><span id="L-395"><a href="#L-395"><span class="linenos"> 395</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store whether to use batch normalisation after the fully-connected layers.&quot;&quot;&quot;</span>
</span><span id="L-396"><a href="#L-396"><span class="linenos"> 396</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">activation_layer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span><span id="L-397"><a href="#L-397"><span class="linenos"> 397</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store whether to use activation function after the fully-connected layers.&quot;&quot;&quot;</span>
</span><span id="L-398"><a href="#L-398"><span class="linenos"> 398</span></a>
</span><span id="L-399"><a href="#L-399"><span class="linenos"> 399</span></a>        <span class="c1"># construct the 1st weighted convolutional layer and attached layers (batchnorm, activation, etc)</span>
</span><span id="L-400"><a href="#L-400"><span class="linenos"> 400</span></a>        <span class="n">layer_input_channels</span> <span class="o">=</span> <span class="n">input_channels</span>  <span class="c1"># the input channels of the 1st convolutional layer, which receive the input of the entire network</span>
</span><span id="L-401"><a href="#L-401"><span class="linenos"> 401</span></a>        <span class="n">layer_output_channels</span> <span class="o">=</span> <span class="mi">64</span>  <span class="c1"># the output channels of the 1st convolutional layer</span>
</span><span id="L-402"><a href="#L-402"><span class="linenos"> 402</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
</span><span id="L-403"><a href="#L-403"><span class="linenos"> 403</span></a>            <span class="n">in_channels</span><span class="o">=</span><span class="n">layer_input_channels</span><span class="p">,</span>
</span><span id="L-404"><a href="#L-404"><span class="linenos"> 404</span></a>            <span class="n">out_channels</span><span class="o">=</span><span class="n">layer_output_channels</span><span class="p">,</span>
</span><span id="L-405"><a href="#L-405"><span class="linenos"> 405</span></a>            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
</span><span id="L-406"><a href="#L-406"><span class="linenos"> 406</span></a>            <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
</span><span id="L-407"><a href="#L-407"><span class="linenos"> 407</span></a>            <span class="n">padding</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
</span><span id="L-408"><a href="#L-408"><span class="linenos"> 408</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="L-409"><a href="#L-409"><span class="linenos"> 409</span></a>        <span class="p">)</span>  <span class="c1"># construct the 1st weight convolutional layer of the entire ResNet</span>
</span><span id="L-410"><a href="#L-410"><span class="linenos"> 410</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The 1st weight convolutional layer of the entire ResNet. It  is always with fixed kernel size 7x7, stride 2, and padding 3. &quot;&quot;&quot;</span>
</span><span id="L-411"><a href="#L-411"><span class="linenos"> 411</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;conv1&quot;</span><span class="p">)</span>  <span class="c1"># collect the layer name to be masked</span>
</span><span id="L-412"><a href="#L-412"><span class="linenos"> 412</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_normalisation</span><span class="p">:</span>
</span><span id="L-413"><a href="#L-413"><span class="linenos"> 413</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv_bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span>
</span><span id="L-414"><a href="#L-414"><span class="linenos"> 414</span></a>                <span class="n">num_features</span><span class="o">=</span><span class="n">layer_output_channels</span>
</span><span id="L-415"><a href="#L-415"><span class="linenos"> 415</span></a>            <span class="p">)</span>  <span class="c1"># construct the batch normalisation layer</span>
</span><span id="L-416"><a href="#L-416"><span class="linenos"> 416</span></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The batch normalisation (`nn.BatchNorm2d`) layer after the 1st weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="L-417"><a href="#L-417"><span class="linenos"> 417</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="L-418"><a href="#L-418"><span class="linenos"> 418</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation1</span> <span class="o">=</span> <span class="n">activation_layer</span><span class="p">()</span>  <span class="c1"># construct the activation layer</span>
</span><span id="L-419"><a href="#L-419"><span class="linenos"> 419</span></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The activation layer after the 1st weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="L-420"><a href="#L-420"><span class="linenos"> 420</span></a>
</span><span id="L-421"><a href="#L-421"><span class="linenos"> 421</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1">#</span>
</span><span id="L-422"><a href="#L-422"><span class="linenos"> 422</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The max pooling layer which is laid in between 1st and 2nd convolutional layers with kernel size 3x3, stride 2. &quot;&quot;&quot;</span>
</span><span id="L-423"><a href="#L-423"><span class="linenos"> 423</span></a>
</span><span id="L-424"><a href="#L-424"><span class="linenos"> 424</span></a>        <span class="c1"># construct the 2nd convolutional layer with multiple blocks, and its attached layers (batchnorm, activation, etc)</span>
</span><span id="L-425"><a href="#L-425"><span class="linenos"> 425</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv2x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_multiple_blocks</span><span class="p">(</span>
</span><span id="L-426"><a href="#L-426"><span class="linenos"> 426</span></a>            <span class="n">layer_name</span><span class="o">=</span><span class="s2">&quot;conv2x&quot;</span><span class="p">,</span>
</span><span id="L-427"><a href="#L-427"><span class="linenos"> 427</span></a>            <span class="n">building_block_type</span><span class="o">=</span><span class="n">building_block_type</span><span class="p">,</span>
</span><span id="L-428"><a href="#L-428"><span class="linenos"> 428</span></a>            <span class="n">building_block_num</span><span class="o">=</span><span class="n">building_block_nums</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
</span><span id="L-429"><a href="#L-429"><span class="linenos"> 429</span></a>            <span class="n">preceding_output_channels</span><span class="o">=</span><span class="n">building_block_preceding_output_channels</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
</span><span id="L-430"><a href="#L-430"><span class="linenos"> 430</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">building_block_input_channels</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
</span><span id="L-431"><a href="#L-431"><span class="linenos"> 431</span></a>            <span class="n">overall_stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>  <span class="c1"># the overall stride of the 2nd convolutional layer should be 1, as the preceding maxpooling layer has stride 2, which already made 112x112 -&gt; 56x56. See Table 2 in the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) for details.</span>
</span><span id="L-432"><a href="#L-432"><span class="linenos"> 432</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="L-433"><a href="#L-433"><span class="linenos"> 433</span></a>            <span class="n">batch_normalisation</span><span class="o">=</span><span class="n">batch_normalisation</span><span class="p">,</span>
</span><span id="L-434"><a href="#L-434"><span class="linenos"> 434</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="L-435"><a href="#L-435"><span class="linenos"> 435</span></a>        <span class="p">)</span>
</span><span id="L-436"><a href="#L-436"><span class="linenos"> 436</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The 2nd convolutional layer of the ResNet, which contains multiple blocks. &quot;&quot;&quot;</span>
</span><span id="L-437"><a href="#L-437"><span class="linenos"> 437</span></a>
</span><span id="L-438"><a href="#L-438"><span class="linenos"> 438</span></a>        <span class="c1"># construct the 3rd convolutional layer with multiple blocks, and its attached layers (batchnorm, activation, etc)</span>
</span><span id="L-439"><a href="#L-439"><span class="linenos"> 439</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv3x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_multiple_blocks</span><span class="p">(</span>
</span><span id="L-440"><a href="#L-440"><span class="linenos"> 440</span></a>            <span class="n">layer_name</span><span class="o">=</span><span class="s2">&quot;conv3x&quot;</span><span class="p">,</span>
</span><span id="L-441"><a href="#L-441"><span class="linenos"> 441</span></a>            <span class="n">building_block_type</span><span class="o">=</span><span class="n">building_block_type</span><span class="p">,</span>
</span><span id="L-442"><a href="#L-442"><span class="linenos"> 442</span></a>            <span class="n">building_block_num</span><span class="o">=</span><span class="n">building_block_nums</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
</span><span id="L-443"><a href="#L-443"><span class="linenos"> 443</span></a>            <span class="n">preceding_output_channels</span><span class="o">=</span><span class="n">building_block_preceding_output_channels</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
</span><span id="L-444"><a href="#L-444"><span class="linenos"> 444</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">building_block_input_channels</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
</span><span id="L-445"><a href="#L-445"><span class="linenos"> 445</span></a>            <span class="n">overall_stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># the overall stride of the 3rd convolutional layer should be 2, making 56x56 -&gt; 28x28. See Table 2 in the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) for details.</span>
</span><span id="L-446"><a href="#L-446"><span class="linenos"> 446</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="L-447"><a href="#L-447"><span class="linenos"> 447</span></a>            <span class="n">batch_normalisation</span><span class="o">=</span><span class="n">batch_normalisation</span><span class="p">,</span>
</span><span id="L-448"><a href="#L-448"><span class="linenos"> 448</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="L-449"><a href="#L-449"><span class="linenos"> 449</span></a>        <span class="p">)</span>
</span><span id="L-450"><a href="#L-450"><span class="linenos"> 450</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The 3rd convolutional layer of the ResNet, which contains multiple blocks. &quot;&quot;&quot;</span>
</span><span id="L-451"><a href="#L-451"><span class="linenos"> 451</span></a>
</span><span id="L-452"><a href="#L-452"><span class="linenos"> 452</span></a>        <span class="c1"># construct the 4th convolutional layer with multiple blocks, and its attached layers (batchnorm, activation, etc)</span>
</span><span id="L-453"><a href="#L-453"><span class="linenos"> 453</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv4x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_multiple_blocks</span><span class="p">(</span>
</span><span id="L-454"><a href="#L-454"><span class="linenos"> 454</span></a>            <span class="n">layer_name</span><span class="o">=</span><span class="s2">&quot;conv4x&quot;</span><span class="p">,</span>
</span><span id="L-455"><a href="#L-455"><span class="linenos"> 455</span></a>            <span class="n">building_block_type</span><span class="o">=</span><span class="n">building_block_type</span><span class="p">,</span>
</span><span id="L-456"><a href="#L-456"><span class="linenos"> 456</span></a>            <span class="n">building_block_num</span><span class="o">=</span><span class="n">building_block_nums</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
</span><span id="L-457"><a href="#L-457"><span class="linenos"> 457</span></a>            <span class="n">preceding_output_channels</span><span class="o">=</span><span class="n">building_block_preceding_output_channels</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
</span><span id="L-458"><a href="#L-458"><span class="linenos"> 458</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">building_block_input_channels</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
</span><span id="L-459"><a href="#L-459"><span class="linenos"> 459</span></a>            <span class="n">overall_stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># the overall stride of the 4th convolutional layer should be 2, making 28x28 -&gt; 14x14. See Table 2 in the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) for details.</span>
</span><span id="L-460"><a href="#L-460"><span class="linenos"> 460</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="L-461"><a href="#L-461"><span class="linenos"> 461</span></a>            <span class="n">batch_normalisation</span><span class="o">=</span><span class="n">batch_normalisation</span><span class="p">,</span>
</span><span id="L-462"><a href="#L-462"><span class="linenos"> 462</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="L-463"><a href="#L-463"><span class="linenos"> 463</span></a>        <span class="p">)</span>
</span><span id="L-464"><a href="#L-464"><span class="linenos"> 464</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The 4th convolutional layer of the ResNet, which contains multiple blocks. &quot;&quot;&quot;</span>
</span><span id="L-465"><a href="#L-465"><span class="linenos"> 465</span></a>
</span><span id="L-466"><a href="#L-466"><span class="linenos"> 466</span></a>        <span class="c1"># construct the 5th convolutional layer with multiple blocks, and its attached layers (batchnorm, activation, etc)</span>
</span><span id="L-467"><a href="#L-467"><span class="linenos"> 467</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv5x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_multiple_blocks</span><span class="p">(</span>
</span><span id="L-468"><a href="#L-468"><span class="linenos"> 468</span></a>            <span class="n">layer_name</span><span class="o">=</span><span class="s2">&quot;conv5x&quot;</span><span class="p">,</span>
</span><span id="L-469"><a href="#L-469"><span class="linenos"> 469</span></a>            <span class="n">building_block_type</span><span class="o">=</span><span class="n">building_block_type</span><span class="p">,</span>
</span><span id="L-470"><a href="#L-470"><span class="linenos"> 470</span></a>            <span class="n">building_block_num</span><span class="o">=</span><span class="n">building_block_nums</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span>
</span><span id="L-471"><a href="#L-471"><span class="linenos"> 471</span></a>            <span class="n">preceding_output_channels</span><span class="o">=</span><span class="n">building_block_preceding_output_channels</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span>
</span><span id="L-472"><a href="#L-472"><span class="linenos"> 472</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">building_block_input_channels</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span>
</span><span id="L-473"><a href="#L-473"><span class="linenos"> 473</span></a>            <span class="n">overall_stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># the overall stride of the 2nd convolutional layer should be 2, making 14x14 -&gt; 7x7. See Table 2 in the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) for details.</span>
</span><span id="L-474"><a href="#L-474"><span class="linenos"> 474</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="L-475"><a href="#L-475"><span class="linenos"> 475</span></a>            <span class="n">batch_normalisation</span><span class="o">=</span><span class="n">batch_normalisation</span><span class="p">,</span>
</span><span id="L-476"><a href="#L-476"><span class="linenos"> 476</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="L-477"><a href="#L-477"><span class="linenos"> 477</span></a>        <span class="p">)</span>
</span><span id="L-478"><a href="#L-478"><span class="linenos"> 478</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The 5th convolutional layer of the ResNet, which contains multiple blocks. &quot;&quot;&quot;</span>
</span><span id="L-479"><a href="#L-479"><span class="linenos"> 479</span></a>
</span><span id="L-480"><a href="#L-480"><span class="linenos"> 480</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">avepool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">(</span><span class="n">output_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span><span id="L-481"><a href="#L-481"><span class="linenos"> 481</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The average pooling layer which is laid after the convolutional layers and before feature maps are flattened. &quot;&quot;&quot;</span>
</span><span id="L-482"><a href="#L-482"><span class="linenos"> 482</span></a>
</span><span id="L-483"><a href="#L-483"><span class="linenos"> 483</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_multiple_blocks</span><span class="p">(</span>
</span><span id="L-484"><a href="#L-484"><span class="linenos"> 484</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-485"><a href="#L-485"><span class="linenos"> 485</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-486"><a href="#L-486"><span class="linenos"> 486</span></a>        <span class="n">building_block_type</span><span class="p">:</span> <span class="n">ResNetBlockSmall</span> <span class="o">|</span> <span class="n">ResNetBlockLarge</span><span class="p">,</span>
</span><span id="L-487"><a href="#L-487"><span class="linenos"> 487</span></a>        <span class="n">building_block_num</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-488"><a href="#L-488"><span class="linenos"> 488</span></a>        <span class="n">preceding_output_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-489"><a href="#L-489"><span class="linenos"> 489</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-490"><a href="#L-490"><span class="linenos"> 490</span></a>        <span class="n">overall_stride</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-491"><a href="#L-491"><span class="linenos"> 491</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="L-492"><a href="#L-492"><span class="linenos"> 492</span></a>        <span class="n">batch_normalisation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="L-493"><a href="#L-493"><span class="linenos"> 493</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-494"><a href="#L-494"><span class="linenos"> 494</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">:</span>
</span><span id="L-495"><a href="#L-495"><span class="linenos"> 495</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct a layer consisting of multiple building blocks. It&#39;s used to construct the 2-5 convolutional layers of the ResNet.</span>
</span><span id="L-496"><a href="#L-496"><span class="linenos"> 496</span></a>
</span><span id="L-497"><a href="#L-497"><span class="linenos"> 497</span></a><span class="sd">        The &quot;shortcut connections&quot; are performed between the input and output of each building block:</span>
</span><span id="L-498"><a href="#L-498"><span class="linenos"> 498</span></a><span class="sd">        1. If the input and output of the building block have exactly the same dimensions (including number of channels and size), add the input to the output.</span>
</span><span id="L-499"><a href="#L-499"><span class="linenos"> 499</span></a><span class="sd">        2. If the input and output of the building block have different dimensions (including number of channels and size), add the input to the output after a convolutional layer to make the dimensions match.</span>
</span><span id="L-500"><a href="#L-500"><span class="linenos"> 500</span></a>
</span><span id="L-501"><a href="#L-501"><span class="linenos"> 501</span></a><span class="sd">        **Args:**</span>
</span><span id="L-502"><a href="#L-502"><span class="linenos"> 502</span></a><span class="sd">        - **layer_name** (`str`): pass the name of this multi-building-block layer to construct the full name of each weighted convolutional layer.</span>
</span><span id="L-503"><a href="#L-503"><span class="linenos"> 503</span></a><span class="sd">        - **building_block_type** (`ResNetBlockSmall` | `ResNetBlockLarge`): the type of the building block.</span>
</span><span id="L-504"><a href="#L-504"><span class="linenos"> 504</span></a><span class="sd">        - **building_block_num** (`int`): the number of building blocks in this multi-building-block layer.</span>
</span><span id="L-505"><a href="#L-505"><span class="linenos"> 505</span></a><span class="sd">        - **preceding_output_channels** (`int`): the number of channels of preceding output of this entire multi-building-block layer.</span>
</span><span id="L-506"><a href="#L-506"><span class="linenos"> 506</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input of this multi-building-block layer.</span>
</span><span id="L-507"><a href="#L-507"><span class="linenos"> 507</span></a><span class="sd">        - **overall_stride** (`int`): the overall stride of the building blocks. This stride is performed at the 1st building block where other building blocks remain their own overall stride of 1. Inside that building block, this stride is performed at certain convolutional layer in the building block where other convolutional layers remain stride of 1:</span>
</span><span id="L-508"><a href="#L-508"><span class="linenos"> 508</span></a><span class="sd">            - For `ResNetBlockSmall`, it performs at the 2nd (last) layer.</span>
</span><span id="L-509"><a href="#L-509"><span class="linenos"> 509</span></a><span class="sd">            - For `ResNetBlockLarge`, it performs at the 2nd (middle) layer.</span>
</span><span id="L-510"><a href="#L-510"><span class="linenos"> 510</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="L-511"><a href="#L-511"><span class="linenos"> 511</span></a><span class="sd">        - **batch_normalisation** (`bool`): whether to use batch normalisation after the weight convolutional layers. Default `True`, same as what the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) does.</span>
</span><span id="L-512"><a href="#L-512"><span class="linenos"> 512</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`, because batch normalisation are doing the similar thing with bias.</span>
</span><span id="L-513"><a href="#L-513"><span class="linenos"> 513</span></a>
</span><span id="L-514"><a href="#L-514"><span class="linenos"> 514</span></a><span class="sd">        **Returns:**</span>
</span><span id="L-515"><a href="#L-515"><span class="linenos"> 515</span></a><span class="sd">        - **layer** (`nn.Sequential`): the constructed layer consisting of multiple building blocks.</span>
</span><span id="L-516"><a href="#L-516"><span class="linenos"> 516</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-517"><a href="#L-517"><span class="linenos"> 517</span></a>
</span><span id="L-518"><a href="#L-518"><span class="linenos"> 518</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="L-519"><a href="#L-519"><span class="linenos"> 519</span></a>
</span><span id="L-520"><a href="#L-520"><span class="linenos"> 520</span></a>        <span class="k">for</span> <span class="n">block_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">building_block_num</span><span class="p">):</span>
</span><span id="L-521"><a href="#L-521"><span class="linenos"> 521</span></a>            <span class="n">layer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="L-522"><a href="#L-522"><span class="linenos"> 522</span></a>                <span class="n">building_block_type</span><span class="p">(</span>
</span><span id="L-523"><a href="#L-523"><span class="linenos"> 523</span></a>                    <span class="n">outer_layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="L-524"><a href="#L-524"><span class="linenos"> 524</span></a>                    <span class="n">block_idx</span><span class="o">=</span><span class="n">block_idx</span><span class="p">,</span>
</span><span id="L-525"><a href="#L-525"><span class="linenos"> 525</span></a>                    <span class="n">preceding_output_channels</span><span class="o">=</span><span class="p">(</span>
</span><span id="L-526"><a href="#L-526"><span class="linenos"> 526</span></a>                        <span class="n">preceding_output_channels</span>
</span><span id="L-527"><a href="#L-527"><span class="linenos"> 527</span></a>                        <span class="k">if</span> <span class="n">block_idx</span> <span class="o">==</span> <span class="mi">0</span>
</span><span id="L-528"><a href="#L-528"><span class="linenos"> 528</span></a>                        <span class="k">else</span> <span class="p">(</span>
</span><span id="L-529"><a href="#L-529"><span class="linenos"> 529</span></a>                            <span class="n">input_channels</span>
</span><span id="L-530"><a href="#L-530"><span class="linenos"> 530</span></a>                            <span class="k">if</span> <span class="n">building_block_type</span> <span class="o">==</span> <span class="n">ResNetBlockSmall</span>
</span><span id="L-531"><a href="#L-531"><span class="linenos"> 531</span></a>                            <span class="k">else</span> <span class="n">input_channels</span> <span class="o">*</span> <span class="mi">4</span>
</span><span id="L-532"><a href="#L-532"><span class="linenos"> 532</span></a>                        <span class="p">)</span>
</span><span id="L-533"><a href="#L-533"><span class="linenos"> 533</span></a>                    <span class="p">),</span>  <span class="c1"># if it&#39;s the 1st block in this multi-building-block layer, it should be the number of channels of the preceding output of this entire multi-building-block layer. Otherwise, it should be the number of channels from last building block where the number of channels is 4 times expanded as the input channels for `ResNetBlockLarge` than `ResNetBlockSmall`.</span>
</span><span id="L-534"><a href="#L-534"><span class="linenos"> 534</span></a>                    <span class="n">input_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
</span><span id="L-535"><a href="#L-535"><span class="linenos"> 535</span></a>                    <span class="n">overall_stride</span><span class="o">=</span><span class="p">(</span>
</span><span id="L-536"><a href="#L-536"><span class="linenos"> 536</span></a>                        <span class="n">overall_stride</span> <span class="k">if</span> <span class="n">block_idx</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">1</span>
</span><span id="L-537"><a href="#L-537"><span class="linenos"> 537</span></a>                    <span class="p">),</span>  <span class="c1"># only perform the overall stride at the 1st block in this multi-building-block layer</span>
</span><span id="L-538"><a href="#L-538"><span class="linenos"> 538</span></a>                    <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="L-539"><a href="#L-539"><span class="linenos"> 539</span></a>                    <span class="n">batch_normalisation</span><span class="o">=</span><span class="n">batch_normalisation</span><span class="p">,</span>
</span><span id="L-540"><a href="#L-540"><span class="linenos"> 540</span></a>                    <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="L-541"><a href="#L-541"><span class="linenos"> 541</span></a>                <span class="p">)</span>
</span><span id="L-542"><a href="#L-542"><span class="linenos"> 542</span></a>            <span class="p">)</span>
</span><span id="L-543"><a href="#L-543"><span class="linenos"> 543</span></a>
</span><span id="L-544"><a href="#L-544"><span class="linenos"> 544</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span> <span class="o">+=</span> <span class="n">layer</span><span class="p">[</span>
</span><span id="L-545"><a href="#L-545"><span class="linenos"> 545</span></a>                <span class="o">-</span><span class="mi">1</span>
</span><span id="L-546"><a href="#L-546"><span class="linenos"> 546</span></a>            <span class="p">]</span><span class="o">.</span><span class="n">weighted_layer_names</span>  <span class="c1"># collect the weighted layer names in the blocks and sync to the weighted layer names list in the outer network</span>
</span><span id="L-547"><a href="#L-547"><span class="linenos"> 547</span></a>
</span><span id="L-548"><a href="#L-548"><span class="linenos"> 548</span></a>        <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layer</span><span class="p">)</span>
</span><span id="L-549"><a href="#L-549"><span class="linenos"> 549</span></a>
</span><span id="L-550"><a href="#L-550"><span class="linenos"> 550</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="L-551"><a href="#L-551"><span class="linenos"> 551</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">stage</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">task_id</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="L-552"><a href="#L-552"><span class="linenos"> 552</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
</span><span id="L-553"><a href="#L-553"><span class="linenos"> 553</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The forward pass for data. It is the same for all tasks.</span>
</span><span id="L-554"><a href="#L-554"><span class="linenos"> 554</span></a>
</span><span id="L-555"><a href="#L-555"><span class="linenos"> 555</span></a><span class="sd">        **Args:**</span>
</span><span id="L-556"><a href="#L-556"><span class="linenos"> 556</span></a><span class="sd">        - **input** (`Tensor`): the input tensor from data.</span>
</span><span id="L-557"><a href="#L-557"><span class="linenos"> 557</span></a>
</span><span id="L-558"><a href="#L-558"><span class="linenos"> 558</span></a><span class="sd">        **Returns:**</span>
</span><span id="L-559"><a href="#L-559"><span class="linenos"> 559</span></a><span class="sd">        - **output_feature** (`Tensor`): the output feature tensor to be passed into heads. This is the main target of backpropagation.</span>
</span><span id="L-560"><a href="#L-560"><span class="linenos"> 560</span></a><span class="sd">        - **hidden_features** (`dict[str, Tensor]`): the hidden features (after activation) in each weighted layer. Key (`str`) is the weighted layer name, value (`Tensor`) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes.</span>
</span><span id="L-561"><a href="#L-561"><span class="linenos"> 561</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-562"><a href="#L-562"><span class="linenos"> 562</span></a>        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="L-563"><a href="#L-563"><span class="linenos"> 563</span></a>        <span class="n">hidden_features</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="L-564"><a href="#L-564"><span class="linenos"> 564</span></a>
</span><span id="L-565"><a href="#L-565"><span class="linenos"> 565</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="nb">input</span>
</span><span id="L-566"><a href="#L-566"><span class="linenos"> 566</span></a>
</span><span id="L-567"><a href="#L-567"><span class="linenos"> 567</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-568"><a href="#L-568"><span class="linenos"> 568</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_normalisation</span><span class="p">:</span>
</span><span id="L-569"><a href="#L-569"><span class="linenos"> 569</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_bn1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-570"><a href="#L-570"><span class="linenos"> 570</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="L-571"><a href="#L-571"><span class="linenos"> 571</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-572"><a href="#L-572"><span class="linenos"> 572</span></a>        <span class="n">hidden_features</span><span class="p">[</span><span class="s2">&quot;conv1&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="L-573"><a href="#L-573"><span class="linenos"> 573</span></a>
</span><span id="L-574"><a href="#L-574"><span class="linenos"> 574</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-575"><a href="#L-575"><span class="linenos"> 575</span></a>
</span><span id="L-576"><a href="#L-576"><span class="linenos"> 576</span></a>        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2x</span><span class="p">:</span>
</span><span id="L-577"><a href="#L-577"><span class="linenos"> 577</span></a>            <span class="n">x</span><span class="p">,</span> <span class="n">hidden_features_block</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-578"><a href="#L-578"><span class="linenos"> 578</span></a>            <span class="n">hidden_features</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">hidden_features_block</span><span class="p">)</span>  <span class="c1"># store the hidden feature</span>
</span><span id="L-579"><a href="#L-579"><span class="linenos"> 579</span></a>        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv3x</span><span class="p">:</span>
</span><span id="L-580"><a href="#L-580"><span class="linenos"> 580</span></a>            <span class="n">x</span><span class="p">,</span> <span class="n">hidden_features_block</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-581"><a href="#L-581"><span class="linenos"> 581</span></a>            <span class="n">hidden_features</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">hidden_features_block</span><span class="p">)</span>  <span class="c1"># store the hidden feature</span>
</span><span id="L-582"><a href="#L-582"><span class="linenos"> 582</span></a>        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv4x</span><span class="p">:</span>
</span><span id="L-583"><a href="#L-583"><span class="linenos"> 583</span></a>            <span class="n">x</span><span class="p">,</span> <span class="n">hidden_features_block</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-584"><a href="#L-584"><span class="linenos"> 584</span></a>            <span class="n">hidden_features</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">hidden_features_block</span><span class="p">)</span>  <span class="c1"># store the hidden feature</span>
</span><span id="L-585"><a href="#L-585"><span class="linenos"> 585</span></a>        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv5x</span><span class="p">:</span>
</span><span id="L-586"><a href="#L-586"><span class="linenos"> 586</span></a>            <span class="n">x</span><span class="p">,</span> <span class="n">hidden_features_block</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-587"><a href="#L-587"><span class="linenos"> 587</span></a>            <span class="n">hidden_features</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">hidden_features_block</span><span class="p">)</span>  <span class="c1"># store the hidden feature</span>
</span><span id="L-588"><a href="#L-588"><span class="linenos"> 588</span></a>
</span><span id="L-589"><a href="#L-589"><span class="linenos"> 589</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">avepool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-590"><a href="#L-590"><span class="linenos"> 590</span></a>
</span><span id="L-591"><a href="#L-591"><span class="linenos"> 591</span></a>        <span class="n">output_feature</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># flatten before going through heads</span>
</span><span id="L-592"><a href="#L-592"><span class="linenos"> 592</span></a>
</span><span id="L-593"><a href="#L-593"><span class="linenos"> 593</span></a>        <span class="k">return</span> <span class="n">output_feature</span><span class="p">,</span> <span class="n">hidden_features</span>
</span><span id="L-594"><a href="#L-594"><span class="linenos"> 594</span></a>
</span><span id="L-595"><a href="#L-595"><span class="linenos"> 595</span></a>
</span><span id="L-596"><a href="#L-596"><span class="linenos"> 596</span></a><span class="k">class</span><span class="w"> </span><span class="nc">ResNet18</span><span class="p">(</span><span class="n">ResNetBase</span><span class="p">):</span>
</span><span id="L-597"><a href="#L-597"><span class="linenos"> 597</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;ResNet-18 backbone network.</span>
</span><span id="L-598"><a href="#L-598"><span class="linenos"> 598</span></a>
</span><span id="L-599"><a href="#L-599"><span class="linenos"> 599</span></a><span class="sd">    This is a smaller architecture proposed in the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html). It consists of 18 weight convolutional layers in total. See Table 1 in the paper for details.</span>
</span><span id="L-600"><a href="#L-600"><span class="linenos"> 600</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-601"><a href="#L-601"><span class="linenos"> 601</span></a>
</span><span id="L-602"><a href="#L-602"><span class="linenos"> 602</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-603"><a href="#L-603"><span class="linenos"> 603</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-604"><a href="#L-604"><span class="linenos"> 604</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-605"><a href="#L-605"><span class="linenos"> 605</span></a>        <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-606"><a href="#L-606"><span class="linenos"> 606</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="L-607"><a href="#L-607"><span class="linenos"> 607</span></a>        <span class="n">batch_normalisation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="L-608"><a href="#L-608"><span class="linenos"> 608</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-609"><a href="#L-609"><span class="linenos"> 609</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-610"><a href="#L-610"><span class="linenos"> 610</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the ResNet-18 backbone network.</span>
</span><span id="L-611"><a href="#L-611"><span class="linenos"> 611</span></a>
</span><span id="L-612"><a href="#L-612"><span class="linenos"> 612</span></a><span class="sd">        **Args:**</span>
</span><span id="L-613"><a href="#L-613"><span class="linenos"> 613</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</span>
</span><span id="L-614"><a href="#L-614"><span class="linenos"> 614</span></a><span class="sd">        - **output_dim** (`int`): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</span>
</span><span id="L-615"><a href="#L-615"><span class="linenos"> 615</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="L-616"><a href="#L-616"><span class="linenos"> 616</span></a><span class="sd">        - **batch_normalisation** (`bool`): whether to use batch normalisation after the weight convolutional layers. Default `True`, same as what the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) does.</span>
</span><span id="L-617"><a href="#L-617"><span class="linenos"> 617</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`, because batch normalisation are doing the similar thing with bias.</span>
</span><span id="L-618"><a href="#L-618"><span class="linenos"> 618</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-619"><a href="#L-619"><span class="linenos"> 619</span></a>        <span class="n">ResNetBase</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-620"><a href="#L-620"><span class="linenos"> 620</span></a>            <span class="bp">self</span><span class="p">,</span>
</span><span id="L-621"><a href="#L-621"><span class="linenos"> 621</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
</span><span id="L-622"><a href="#L-622"><span class="linenos"> 622</span></a>            <span class="n">building_block_type</span><span class="o">=</span><span class="n">ResNetBlockSmall</span><span class="p">,</span>  <span class="c1"># use the smaller building block for ResNet-18</span>
</span><span id="L-623"><a href="#L-623"><span class="linenos"> 623</span></a>            <span class="n">building_block_nums</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
</span><span id="L-624"><a href="#L-624"><span class="linenos"> 624</span></a>            <span class="n">building_block_preceding_output_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>
</span><span id="L-625"><a href="#L-625"><span class="linenos"> 625</span></a>            <span class="n">building_block_input_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
</span><span id="L-626"><a href="#L-626"><span class="linenos"> 626</span></a>            <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span>
</span><span id="L-627"><a href="#L-627"><span class="linenos"> 627</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="L-628"><a href="#L-628"><span class="linenos"> 628</span></a>            <span class="n">batch_normalisation</span><span class="o">=</span><span class="n">batch_normalisation</span><span class="p">,</span>
</span><span id="L-629"><a href="#L-629"><span class="linenos"> 629</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="L-630"><a href="#L-630"><span class="linenos"> 630</span></a>        <span class="p">)</span>
</span><span id="L-631"><a href="#L-631"><span class="linenos"> 631</span></a>
</span><span id="L-632"><a href="#L-632"><span class="linenos"> 632</span></a>
</span><span id="L-633"><a href="#L-633"><span class="linenos"> 633</span></a><span class="k">class</span><span class="w"> </span><span class="nc">ResNet34</span><span class="p">(</span><span class="n">ResNetBase</span><span class="p">):</span>
</span><span id="L-634"><a href="#L-634"><span class="linenos"> 634</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;ResNet-34 backbone network.</span>
</span><span id="L-635"><a href="#L-635"><span class="linenos"> 635</span></a>
</span><span id="L-636"><a href="#L-636"><span class="linenos"> 636</span></a><span class="sd">    This is a smaller architecture proposed in the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html). It consists of 34 weight convolutional layers in total. See Table 1 in the paper for details.</span>
</span><span id="L-637"><a href="#L-637"><span class="linenos"> 637</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-638"><a href="#L-638"><span class="linenos"> 638</span></a>
</span><span id="L-639"><a href="#L-639"><span class="linenos"> 639</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-640"><a href="#L-640"><span class="linenos"> 640</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-641"><a href="#L-641"><span class="linenos"> 641</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-642"><a href="#L-642"><span class="linenos"> 642</span></a>        <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-643"><a href="#L-643"><span class="linenos"> 643</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="L-644"><a href="#L-644"><span class="linenos"> 644</span></a>        <span class="n">batch_normalisation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="L-645"><a href="#L-645"><span class="linenos"> 645</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-646"><a href="#L-646"><span class="linenos"> 646</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-647"><a href="#L-647"><span class="linenos"> 647</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the ResNet-34 backbone network.</span>
</span><span id="L-648"><a href="#L-648"><span class="linenos"> 648</span></a>
</span><span id="L-649"><a href="#L-649"><span class="linenos"> 649</span></a><span class="sd">        **Args:**</span>
</span><span id="L-650"><a href="#L-650"><span class="linenos"> 650</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</span>
</span><span id="L-651"><a href="#L-651"><span class="linenos"> 651</span></a><span class="sd">        - **output_dim** (`int`): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</span>
</span><span id="L-652"><a href="#L-652"><span class="linenos"> 652</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="L-653"><a href="#L-653"><span class="linenos"> 653</span></a><span class="sd">        - **batch_normalisation** (`bool`): whether to use batch normalisation after the weight convolutional layers. Default `True`, same as what the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) does.</span>
</span><span id="L-654"><a href="#L-654"><span class="linenos"> 654</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`, because batch normalisation are doing the similar thing with bias.</span>
</span><span id="L-655"><a href="#L-655"><span class="linenos"> 655</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-656"><a href="#L-656"><span class="linenos"> 656</span></a>        <span class="n">ResNetBase</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-657"><a href="#L-657"><span class="linenos"> 657</span></a>            <span class="bp">self</span><span class="p">,</span>
</span><span id="L-658"><a href="#L-658"><span class="linenos"> 658</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
</span><span id="L-659"><a href="#L-659"><span class="linenos"> 659</span></a>            <span class="n">building_block_type</span><span class="o">=</span><span class="n">ResNetBlockSmall</span><span class="p">,</span>  <span class="c1"># use the smaller building block for ResNet-34</span>
</span><span id="L-660"><a href="#L-660"><span class="linenos"> 660</span></a>            <span class="n">building_block_nums</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
</span><span id="L-661"><a href="#L-661"><span class="linenos"> 661</span></a>            <span class="n">building_block_preceding_output_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>
</span><span id="L-662"><a href="#L-662"><span class="linenos"> 662</span></a>            <span class="n">building_block_input_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
</span><span id="L-663"><a href="#L-663"><span class="linenos"> 663</span></a>            <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span>
</span><span id="L-664"><a href="#L-664"><span class="linenos"> 664</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="L-665"><a href="#L-665"><span class="linenos"> 665</span></a>            <span class="n">batch_normalisation</span><span class="o">=</span><span class="n">batch_normalisation</span><span class="p">,</span>
</span><span id="L-666"><a href="#L-666"><span class="linenos"> 666</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="L-667"><a href="#L-667"><span class="linenos"> 667</span></a>        <span class="p">)</span>
</span><span id="L-668"><a href="#L-668"><span class="linenos"> 668</span></a>
</span><span id="L-669"><a href="#L-669"><span class="linenos"> 669</span></a>
</span><span id="L-670"><a href="#L-670"><span class="linenos"> 670</span></a><span class="k">class</span><span class="w"> </span><span class="nc">ResNet50</span><span class="p">(</span><span class="n">ResNetBase</span><span class="p">):</span>
</span><span id="L-671"><a href="#L-671"><span class="linenos"> 671</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;ResNet-50 backbone network.</span>
</span><span id="L-672"><a href="#L-672"><span class="linenos"> 672</span></a>
</span><span id="L-673"><a href="#L-673"><span class="linenos"> 673</span></a><span class="sd">    This is a larger architecture proposed in the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html). It consists of 50 weight convolutional layers in total. See Table 1 in the paper for details.</span>
</span><span id="L-674"><a href="#L-674"><span class="linenos"> 674</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-675"><a href="#L-675"><span class="linenos"> 675</span></a>
</span><span id="L-676"><a href="#L-676"><span class="linenos"> 676</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-677"><a href="#L-677"><span class="linenos"> 677</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-678"><a href="#L-678"><span class="linenos"> 678</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-679"><a href="#L-679"><span class="linenos"> 679</span></a>        <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-680"><a href="#L-680"><span class="linenos"> 680</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="L-681"><a href="#L-681"><span class="linenos"> 681</span></a>        <span class="n">batch_normalisation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="L-682"><a href="#L-682"><span class="linenos"> 682</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-683"><a href="#L-683"><span class="linenos"> 683</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-684"><a href="#L-684"><span class="linenos"> 684</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the ResNet-50 backbone network.</span>
</span><span id="L-685"><a href="#L-685"><span class="linenos"> 685</span></a>
</span><span id="L-686"><a href="#L-686"><span class="linenos"> 686</span></a><span class="sd">        **Args:**</span>
</span><span id="L-687"><a href="#L-687"><span class="linenos"> 687</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</span>
</span><span id="L-688"><a href="#L-688"><span class="linenos"> 688</span></a><span class="sd">        - **output_dim** (`int`): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</span>
</span><span id="L-689"><a href="#L-689"><span class="linenos"> 689</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="L-690"><a href="#L-690"><span class="linenos"> 690</span></a><span class="sd">        - **batch_normalisation** (`bool`): whether to use batch normalisation after the weight convolutional layers. Default `True`, same as what the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) does.</span>
</span><span id="L-691"><a href="#L-691"><span class="linenos"> 691</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`, because batch normalisation are doing the similar thing with bias.</span>
</span><span id="L-692"><a href="#L-692"><span class="linenos"> 692</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-693"><a href="#L-693"><span class="linenos"> 693</span></a>        <span class="n">ResNetBase</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-694"><a href="#L-694"><span class="linenos"> 694</span></a>            <span class="bp">self</span><span class="p">,</span>
</span><span id="L-695"><a href="#L-695"><span class="linenos"> 695</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
</span><span id="L-696"><a href="#L-696"><span class="linenos"> 696</span></a>            <span class="n">building_block_type</span><span class="o">=</span><span class="n">ResNetBlockLarge</span><span class="p">,</span>  <span class="c1"># use the larger building block for ResNet-50</span>
</span><span id="L-697"><a href="#L-697"><span class="linenos"> 697</span></a>            <span class="n">building_block_nums</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
</span><span id="L-698"><a href="#L-698"><span class="linenos"> 698</span></a>            <span class="n">building_block_preceding_output_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span>
</span><span id="L-699"><a href="#L-699"><span class="linenos"> 699</span></a>            <span class="n">building_block_input_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
</span><span id="L-700"><a href="#L-700"><span class="linenos"> 700</span></a>            <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span>
</span><span id="L-701"><a href="#L-701"><span class="linenos"> 701</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="L-702"><a href="#L-702"><span class="linenos"> 702</span></a>            <span class="n">batch_normalisation</span><span class="o">=</span><span class="n">batch_normalisation</span><span class="p">,</span>
</span><span id="L-703"><a href="#L-703"><span class="linenos"> 703</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="L-704"><a href="#L-704"><span class="linenos"> 704</span></a>        <span class="p">)</span>
</span><span id="L-705"><a href="#L-705"><span class="linenos"> 705</span></a>
</span><span id="L-706"><a href="#L-706"><span class="linenos"> 706</span></a>
</span><span id="L-707"><a href="#L-707"><span class="linenos"> 707</span></a><span class="k">class</span><span class="w"> </span><span class="nc">ResNet101</span><span class="p">(</span><span class="n">ResNetBase</span><span class="p">):</span>
</span><span id="L-708"><a href="#L-708"><span class="linenos"> 708</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;ResNet-101 backbone network.</span>
</span><span id="L-709"><a href="#L-709"><span class="linenos"> 709</span></a>
</span><span id="L-710"><a href="#L-710"><span class="linenos"> 710</span></a><span class="sd">    This is a larger architecture proposed in the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html). It consists of 101 weight convolutional layers in total. See Table 1 in the paper for details.</span>
</span><span id="L-711"><a href="#L-711"><span class="linenos"> 711</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-712"><a href="#L-712"><span class="linenos"> 712</span></a>
</span><span id="L-713"><a href="#L-713"><span class="linenos"> 713</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-714"><a href="#L-714"><span class="linenos"> 714</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-715"><a href="#L-715"><span class="linenos"> 715</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-716"><a href="#L-716"><span class="linenos"> 716</span></a>        <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-717"><a href="#L-717"><span class="linenos"> 717</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="L-718"><a href="#L-718"><span class="linenos"> 718</span></a>        <span class="n">batch_normalisation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="L-719"><a href="#L-719"><span class="linenos"> 719</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-720"><a href="#L-720"><span class="linenos"> 720</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-721"><a href="#L-721"><span class="linenos"> 721</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the ResNet-101 backbone network.</span>
</span><span id="L-722"><a href="#L-722"><span class="linenos"> 722</span></a>
</span><span id="L-723"><a href="#L-723"><span class="linenos"> 723</span></a><span class="sd">        **Args:**</span>
</span><span id="L-724"><a href="#L-724"><span class="linenos"> 724</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</span>
</span><span id="L-725"><a href="#L-725"><span class="linenos"> 725</span></a><span class="sd">        - **output_dim** (`int`): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</span>
</span><span id="L-726"><a href="#L-726"><span class="linenos"> 726</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="L-727"><a href="#L-727"><span class="linenos"> 727</span></a><span class="sd">        - **batch_normalisation** (`bool`): whether to use batch normalisation after the weight convolutional layers. Default `True`, same as what the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) does.</span>
</span><span id="L-728"><a href="#L-728"><span class="linenos"> 728</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`, because batch normalisation are doing the similar thing with bias.</span>
</span><span id="L-729"><a href="#L-729"><span class="linenos"> 729</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-730"><a href="#L-730"><span class="linenos"> 730</span></a>        <span class="n">ResNetBase</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-731"><a href="#L-731"><span class="linenos"> 731</span></a>            <span class="bp">self</span><span class="p">,</span>
</span><span id="L-732"><a href="#L-732"><span class="linenos"> 732</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
</span><span id="L-733"><a href="#L-733"><span class="linenos"> 733</span></a>            <span class="n">building_block_type</span><span class="o">=</span><span class="n">ResNetBlockLarge</span><span class="p">,</span>  <span class="c1"># use the larger building block for ResNet-101</span>
</span><span id="L-734"><a href="#L-734"><span class="linenos"> 734</span></a>            <span class="n">building_block_nums</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
</span><span id="L-735"><a href="#L-735"><span class="linenos"> 735</span></a>            <span class="n">building_block_preceding_output_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span>
</span><span id="L-736"><a href="#L-736"><span class="linenos"> 736</span></a>            <span class="n">building_block_input_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
</span><span id="L-737"><a href="#L-737"><span class="linenos"> 737</span></a>            <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span>
</span><span id="L-738"><a href="#L-738"><span class="linenos"> 738</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="L-739"><a href="#L-739"><span class="linenos"> 739</span></a>            <span class="n">batch_normalisation</span><span class="o">=</span><span class="n">batch_normalisation</span><span class="p">,</span>
</span><span id="L-740"><a href="#L-740"><span class="linenos"> 740</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="L-741"><a href="#L-741"><span class="linenos"> 741</span></a>        <span class="p">)</span>
</span><span id="L-742"><a href="#L-742"><span class="linenos"> 742</span></a>
</span><span id="L-743"><a href="#L-743"><span class="linenos"> 743</span></a>
</span><span id="L-744"><a href="#L-744"><span class="linenos"> 744</span></a><span class="k">class</span><span class="w"> </span><span class="nc">ResNet152</span><span class="p">(</span><span class="n">ResNetBase</span><span class="p">):</span>
</span><span id="L-745"><a href="#L-745"><span class="linenos"> 745</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;ResNet-152 backbone network.</span>
</span><span id="L-746"><a href="#L-746"><span class="linenos"> 746</span></a>
</span><span id="L-747"><a href="#L-747"><span class="linenos"> 747</span></a><span class="sd">    This is the largest architecture proposed in the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html). It consists of 152 weight convolutional layers in total. See Table 1 in the paper for details.</span>
</span><span id="L-748"><a href="#L-748"><span class="linenos"> 748</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-749"><a href="#L-749"><span class="linenos"> 749</span></a>
</span><span id="L-750"><a href="#L-750"><span class="linenos"> 750</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-751"><a href="#L-751"><span class="linenos"> 751</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-752"><a href="#L-752"><span class="linenos"> 752</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-753"><a href="#L-753"><span class="linenos"> 753</span></a>        <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-754"><a href="#L-754"><span class="linenos"> 754</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="L-755"><a href="#L-755"><span class="linenos"> 755</span></a>        <span class="n">batch_normalisation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="L-756"><a href="#L-756"><span class="linenos"> 756</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-757"><a href="#L-757"><span class="linenos"> 757</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-758"><a href="#L-758"><span class="linenos"> 758</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the ResNet-50 backbone network.</span>
</span><span id="L-759"><a href="#L-759"><span class="linenos"> 759</span></a>
</span><span id="L-760"><a href="#L-760"><span class="linenos"> 760</span></a><span class="sd">        **Args:**</span>
</span><span id="L-761"><a href="#L-761"><span class="linenos"> 761</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</span>
</span><span id="L-762"><a href="#L-762"><span class="linenos"> 762</span></a><span class="sd">        - **output_dim** (`int`): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</span>
</span><span id="L-763"><a href="#L-763"><span class="linenos"> 763</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="L-764"><a href="#L-764"><span class="linenos"> 764</span></a><span class="sd">        - **batch_normalisation** (`bool`): whether to use batch normalisation after the weight convolutional layers. Default `True`, same as what the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) does.</span>
</span><span id="L-765"><a href="#L-765"><span class="linenos"> 765</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`, because batch normalisation are doing the similar thing with bias.</span>
</span><span id="L-766"><a href="#L-766"><span class="linenos"> 766</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-767"><a href="#L-767"><span class="linenos"> 767</span></a>        <span class="n">ResNetBase</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-768"><a href="#L-768"><span class="linenos"> 768</span></a>            <span class="bp">self</span><span class="p">,</span>
</span><span id="L-769"><a href="#L-769"><span class="linenos"> 769</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
</span><span id="L-770"><a href="#L-770"><span class="linenos"> 770</span></a>            <span class="n">building_block_type</span><span class="o">=</span><span class="n">ResNetBlockLarge</span><span class="p">,</span>  <span class="c1"># use the larger building block for ResNet-152</span>
</span><span id="L-771"><a href="#L-771"><span class="linenos"> 771</span></a>            <span class="n">building_block_nums</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">36</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
</span><span id="L-772"><a href="#L-772"><span class="linenos"> 772</span></a>            <span class="n">building_block_preceding_output_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span>
</span><span id="L-773"><a href="#L-773"><span class="linenos"> 773</span></a>            <span class="n">building_block_input_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
</span><span id="L-774"><a href="#L-774"><span class="linenos"> 774</span></a>            <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span>
</span><span id="L-775"><a href="#L-775"><span class="linenos"> 775</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="L-776"><a href="#L-776"><span class="linenos"> 776</span></a>            <span class="n">batch_normalisation</span><span class="o">=</span><span class="n">batch_normalisation</span><span class="p">,</span>
</span><span id="L-777"><a href="#L-777"><span class="linenos"> 777</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="L-778"><a href="#L-778"><span class="linenos"> 778</span></a>        <span class="p">)</span>
</span><span id="L-779"><a href="#L-779"><span class="linenos"> 779</span></a>
</span><span id="L-780"><a href="#L-780"><span class="linenos"> 780</span></a>
</span><span id="L-781"><a href="#L-781"><span class="linenos"> 781</span></a><span class="k">class</span><span class="w"> </span><span class="nc">HATMaskResNetBlockSmall</span><span class="p">(</span><span class="n">HATMaskBackbone</span><span class="p">,</span> <span class="n">ResNetBlockSmall</span><span class="p">):</span>
</span><span id="L-782"><a href="#L-782"><span class="linenos"> 782</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The smaller building block for HAT masked ResNet-18/34.</span>
</span><span id="L-783"><a href="#L-783"><span class="linenos"> 783</span></a>
</span><span id="L-784"><a href="#L-784"><span class="linenos"> 784</span></a><span class="sd">    It consists of 2 weight convolutional layers, each followed by an activation function. See Table 1 or Figure 5 (left) in the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html).</span>
</span><span id="L-785"><a href="#L-785"><span class="linenos"> 785</span></a>
</span><span id="L-786"><a href="#L-786"><span class="linenos"> 786</span></a><span class="sd">    Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the unit-wise task embedding and gate function.</span>
</span><span id="L-787"><a href="#L-787"><span class="linenos"> 787</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-788"><a href="#L-788"><span class="linenos"> 788</span></a>
</span><span id="L-789"><a href="#L-789"><span class="linenos"> 789</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-790"><a href="#L-790"><span class="linenos"> 790</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-791"><a href="#L-791"><span class="linenos"> 791</span></a>        <span class="n">outer_layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-792"><a href="#L-792"><span class="linenos"> 792</span></a>        <span class="n">block_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-793"><a href="#L-793"><span class="linenos"> 793</span></a>        <span class="n">preceding_output_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-794"><a href="#L-794"><span class="linenos"> 794</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-795"><a href="#L-795"><span class="linenos"> 795</span></a>        <span class="n">overall_stride</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-796"><a href="#L-796"><span class="linenos"> 796</span></a>        <span class="n">gate</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-797"><a href="#L-797"><span class="linenos"> 797</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="L-798"><a href="#L-798"><span class="linenos"> 798</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-799"><a href="#L-799"><span class="linenos"> 799</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-800"><a href="#L-800"><span class="linenos"> 800</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the smaller building block with task embedding.</span>
</span><span id="L-801"><a href="#L-801"><span class="linenos"> 801</span></a>
</span><span id="L-802"><a href="#L-802"><span class="linenos"> 802</span></a><span class="sd">        **Args:**</span>
</span><span id="L-803"><a href="#L-803"><span class="linenos"> 803</span></a><span class="sd">        - **outer_layer_name** (`str`): pass the name of the multi-building-block layer that contains this building block to construct the full name of each weighted convolutional layer.</span>
</span><span id="L-804"><a href="#L-804"><span class="linenos"> 804</span></a><span class="sd">        - **block_idx** (`int`): the index of the building blocks in the multi-building-block layer to construct the full name of each weighted convolutional layer.</span>
</span><span id="L-805"><a href="#L-805"><span class="linenos"> 805</span></a><span class="sd">        - **preceding_output_channels** (`int`): the number of channels of preceding output of this particular building block.</span>
</span><span id="L-806"><a href="#L-806"><span class="linenos"> 806</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input of this building block.</span>
</span><span id="L-807"><a href="#L-807"><span class="linenos"> 807</span></a><span class="sd">        - **overall_stride** (`int`): the overall stride of this building block. This stride is performed at 2nd (last) convolutional layer where the 1st convolutional layer remain stride of 1.</span>
</span><span id="L-808"><a href="#L-808"><span class="linenos"> 808</span></a><span class="sd">        - **gate** (`str`): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:</span>
</span><span id="L-809"><a href="#L-809"><span class="linenos"> 809</span></a><span class="sd">            - `sigmoid`: the sigmoid function.</span>
</span><span id="L-810"><a href="#L-810"><span class="linenos"> 810</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="L-811"><a href="#L-811"><span class="linenos"> 811</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`.</span>
</span><span id="L-812"><a href="#L-812"><span class="linenos"> 812</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-813"><a href="#L-813"><span class="linenos"> 813</span></a>        <span class="n">HATMaskBackbone</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">gate</span><span class="o">=</span><span class="n">gate</span><span class="p">)</span>
</span><span id="L-814"><a href="#L-814"><span class="linenos"> 814</span></a>        <span class="n">ResNetBlockSmall</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-815"><a href="#L-815"><span class="linenos"> 815</span></a>            <span class="bp">self</span><span class="p">,</span>
</span><span id="L-816"><a href="#L-816"><span class="linenos"> 816</span></a>            <span class="n">outer_layer_name</span><span class="o">=</span><span class="n">outer_layer_name</span><span class="p">,</span>
</span><span id="L-817"><a href="#L-817"><span class="linenos"> 817</span></a>            <span class="n">block_idx</span><span class="o">=</span><span class="n">block_idx</span><span class="p">,</span>
</span><span id="L-818"><a href="#L-818"><span class="linenos"> 818</span></a>            <span class="n">preceding_output_channels</span><span class="o">=</span><span class="n">preceding_output_channels</span><span class="p">,</span>
</span><span id="L-819"><a href="#L-819"><span class="linenos"> 819</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
</span><span id="L-820"><a href="#L-820"><span class="linenos"> 820</span></a>            <span class="n">overall_stride</span><span class="o">=</span><span class="n">overall_stride</span><span class="p">,</span>
</span><span id="L-821"><a href="#L-821"><span class="linenos"> 821</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="L-822"><a href="#L-822"><span class="linenos"> 822</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="L-823"><a href="#L-823"><span class="linenos"> 823</span></a>        <span class="p">)</span>
</span><span id="L-824"><a href="#L-824"><span class="linenos"> 824</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">register_hat_mask_module_explicitly</span><span class="p">(</span><span class="n">gate</span><span class="o">=</span><span class="n">gate</span><span class="p">)</span>
</span><span id="L-825"><a href="#L-825"><span class="linenos"> 825</span></a>
</span><span id="L-826"><a href="#L-826"><span class="linenos"> 826</span></a>        <span class="c1"># construct the task embedding over the 1st weighted convolutional layer. It is channel-wise</span>
</span><span id="L-827"><a href="#L-827"><span class="linenos"> 827</span></a>        <span class="n">layer_output_channels</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-828"><a href="#L-828"><span class="linenos"> 828</span></a>            <span class="n">input_channels</span>  <span class="c1"># the output channels of the 1st convolutional layer</span>
</span><span id="L-829"><a href="#L-829"><span class="linenos"> 829</span></a>        <span class="p">)</span>
</span><span id="L-830"><a href="#L-830"><span class="linenos"> 830</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_1st_layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
</span><span id="L-831"><a href="#L-831"><span class="linenos"> 831</span></a>            <span class="n">num_embeddings</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="n">layer_output_channels</span>
</span><span id="L-832"><a href="#L-832"><span class="linenos"> 832</span></a>        <span class="p">)</span>
</span><span id="L-833"><a href="#L-833"><span class="linenos"> 833</span></a>
</span><span id="L-834"><a href="#L-834"><span class="linenos"> 834</span></a>        <span class="c1"># construct the task embedding over the 2nd weighted convolutional layer. It is channel-wise</span>
</span><span id="L-835"><a href="#L-835"><span class="linenos"> 835</span></a>        <span class="n">layer_output_channels</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-836"><a href="#L-836"><span class="linenos"> 836</span></a>            <span class="n">input_channels</span> <span class="o">*</span> <span class="mi">1</span>
</span><span id="L-837"><a href="#L-837"><span class="linenos"> 837</span></a>        <span class="p">)</span>  <span class="c1"># the output channels of the 2nd convolutional layer, which is the same as the input channels (without expansion)</span>
</span><span id="L-838"><a href="#L-838"><span class="linenos"> 838</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_2nd_layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
</span><span id="L-839"><a href="#L-839"><span class="linenos"> 839</span></a>            <span class="n">num_embeddings</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="n">layer_output_channels</span>
</span><span id="L-840"><a href="#L-840"><span class="linenos"> 840</span></a>        <span class="p">)</span>
</span><span id="L-841"><a href="#L-841"><span class="linenos"> 841</span></a>
</span><span id="L-842"><a href="#L-842"><span class="linenos"> 842</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="L-843"><a href="#L-843"><span class="linenos"> 843</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-844"><a href="#L-844"><span class="linenos"> 844</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-845"><a href="#L-845"><span class="linenos"> 845</span></a>        <span class="n">stage</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-846"><a href="#L-846"><span class="linenos"> 846</span></a>        <span class="n">s_max</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-847"><a href="#L-847"><span class="linenos"> 847</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-848"><a href="#L-848"><span class="linenos"> 848</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-849"><a href="#L-849"><span class="linenos"> 849</span></a>        <span class="n">test_mask</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-850"><a href="#L-850"><span class="linenos"> 850</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
</span><span id="L-851"><a href="#L-851"><span class="linenos"> 851</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The forward pass for data from task `task_id`. Task-specific mask for `task_id` are applied to the units which are channels in each weighted convolutional layer.</span>
</span><span id="L-852"><a href="#L-852"><span class="linenos"> 852</span></a>
</span><span id="L-853"><a href="#L-853"><span class="linenos"> 853</span></a><span class="sd">        **Args:**</span>
</span><span id="L-854"><a href="#L-854"><span class="linenos"> 854</span></a><span class="sd">        - **input** (`Tensor`): The input tensor from data.</span>
</span><span id="L-855"><a href="#L-855"><span class="linenos"> 855</span></a><span class="sd">        - **stage** (`str`): the stage of the forward pass, should be one of the following:</span>
</span><span id="L-856"><a href="#L-856"><span class="linenos"> 856</span></a><span class="sd">            1. &#39;train&#39;: training stage.</span>
</span><span id="L-857"><a href="#L-857"><span class="linenos"> 857</span></a><span class="sd">            2. &#39;validation&#39;: validation stage.</span>
</span><span id="L-858"><a href="#L-858"><span class="linenos"> 858</span></a><span class="sd">            3. &#39;test&#39;: testing stage.</span>
</span><span id="L-859"><a href="#L-859"><span class="linenos"> 859</span></a><span class="sd">        - **s_max** (`float` | `None`): the maximum scaling factor in the gate function. Doesn&#39;t apply to testing stage. See chapter 2.4 &quot;Hard Attention Training&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="L-860"><a href="#L-860"><span class="linenos"> 860</span></a><span class="sd">        - **batch_idx** (`int` | `None`): the current batch index. Applies only to training stage. For other stages, it is default `None`.</span>
</span><span id="L-861"><a href="#L-861"><span class="linenos"> 861</span></a><span class="sd">        - **num_batches** (`int` | `None`): the total number of batches. Applies only to training stage. For other stages, it is default `None`.</span>
</span><span id="L-862"><a href="#L-862"><span class="linenos"> 862</span></a><span class="sd">        - **test_mask** (`dict[str, Tensor]` | `None`): the binary mask used for test. Applies only to testing stage. For other stages, it is default `None`.</span>
</span><span id="L-863"><a href="#L-863"><span class="linenos"> 863</span></a>
</span><span id="L-864"><a href="#L-864"><span class="linenos"> 864</span></a><span class="sd">        **Returns:**</span>
</span><span id="L-865"><a href="#L-865"><span class="linenos"> 865</span></a><span class="sd">        - **output_feature** (`Tensor`): the output feature maps.</span>
</span><span id="L-866"><a href="#L-866"><span class="linenos"> 866</span></a><span class="sd">        - **mask** (`dict[str, Tensor]`): the mask for the current task. Key (`str`) is layer name, value (`Tensor`) is the mask tensor. The mask tensor has size (number of units).</span>
</span><span id="L-867"><a href="#L-867"><span class="linenos"> 867</span></a><span class="sd">        - **hidden_features** (`dict[str, Tensor]`): the hidden features (after activation) in each weighted layer. Key (`str`) is the weighted layer name, value (`Tensor`) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes. Although HAT algorithm does not need this, it is still provided for API consistence for other HAT-based algorithms inherited this `forward()` method of `HAT` class.</span>
</span><span id="L-868"><a href="#L-868"><span class="linenos"> 868</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-869"><a href="#L-869"><span class="linenos"> 869</span></a>        <span class="n">hidden_features</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="L-870"><a href="#L-870"><span class="linenos"> 870</span></a>
</span><span id="L-871"><a href="#L-871"><span class="linenos"> 871</span></a>        <span class="c1"># get the mask for the current task from the task embedding in this stage</span>
</span><span id="L-872"><a href="#L-872"><span class="linenos"> 872</span></a>        <span class="n">mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_mask</span><span class="p">(</span>
</span><span id="L-873"><a href="#L-873"><span class="linenos"> 873</span></a>            <span class="n">stage</span><span class="o">=</span><span class="n">stage</span><span class="p">,</span>
</span><span id="L-874"><a href="#L-874"><span class="linenos"> 874</span></a>            <span class="n">s_max</span><span class="o">=</span><span class="n">s_max</span><span class="p">,</span>
</span><span id="L-875"><a href="#L-875"><span class="linenos"> 875</span></a>            <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="L-876"><a href="#L-876"><span class="linenos"> 876</span></a>            <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="L-877"><a href="#L-877"><span class="linenos"> 877</span></a>            <span class="n">test_mask</span><span class="o">=</span><span class="n">test_mask</span><span class="p">,</span>
</span><span id="L-878"><a href="#L-878"><span class="linenos"> 878</span></a>        <span class="p">)</span>
</span><span id="L-879"><a href="#L-879"><span class="linenos"> 879</span></a>
</span><span id="L-880"><a href="#L-880"><span class="linenos"> 880</span></a>        <span class="n">identity</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-881"><a href="#L-881"><span class="linenos"> 881</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">identity_downsample</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span><span id="L-882"><a href="#L-882"><span class="linenos"> 882</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">identity_downsample</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span><span id="L-883"><a href="#L-883"><span class="linenos"> 883</span></a>            <span class="k">else</span> <span class="nb">input</span>
</span><span id="L-884"><a href="#L-884"><span class="linenos"> 884</span></a>        <span class="p">)</span>  <span class="c1"># remember the identity of input for the shortcut connection. Perform downsampling if its dimension doesn&#39;t match the output&#39;s</span>
</span><span id="L-885"><a href="#L-885"><span class="linenos"> 885</span></a>
</span><span id="L-886"><a href="#L-886"><span class="linenos"> 886</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="nb">input</span>
</span><span id="L-887"><a href="#L-887"><span class="linenos"> 887</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># weighted convolutional layer first</span>
</span><span id="L-888"><a href="#L-888"><span class="linenos"> 888</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span>
</span><span id="L-889"><a href="#L-889"><span class="linenos"> 889</span></a>            <span class="n">mask</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_1st_layer_name</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="L-890"><a href="#L-890"><span class="linenos"> 890</span></a>        <span class="p">)</span>  <span class="c1"># apply the mask to the 1st convolutional layer. Broadcast the dimension of mask to match the input</span>
</span><span id="L-891"><a href="#L-891"><span class="linenos"> 891</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="L-892"><a href="#L-892"><span class="linenos"> 892</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># activation function third</span>
</span><span id="L-893"><a href="#L-893"><span class="linenos"> 893</span></a>        <span class="n">hidden_features</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_1st_layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>  <span class="c1"># store the hidden feature</span>
</span><span id="L-894"><a href="#L-894"><span class="linenos"> 894</span></a>
</span><span id="L-895"><a href="#L-895"><span class="linenos"> 895</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># weighted convolutional layer first</span>
</span><span id="L-896"><a href="#L-896"><span class="linenos"> 896</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">identity</span>
</span><span id="L-897"><a href="#L-897"><span class="linenos"> 897</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span>
</span><span id="L-898"><a href="#L-898"><span class="linenos"> 898</span></a>            <span class="n">mask</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_2nd_layer_name</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="L-899"><a href="#L-899"><span class="linenos"> 899</span></a>        <span class="p">)</span>  <span class="c1"># apply the mask to the 2nd convolutional layer after the shortcut connection. Broadcast the dimension of mask to match the input</span>
</span><span id="L-900"><a href="#L-900"><span class="linenos"> 900</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="L-901"><a href="#L-901"><span class="linenos"> 901</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># activation after the shortcut connection</span>
</span><span id="L-902"><a href="#L-902"><span class="linenos"> 902</span></a>        <span class="n">hidden_features</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_2nd_layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>  <span class="c1"># store the hidden feature</span>
</span><span id="L-903"><a href="#L-903"><span class="linenos"> 903</span></a>
</span><span id="L-904"><a href="#L-904"><span class="linenos"> 904</span></a>        <span class="n">output_feature</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="L-905"><a href="#L-905"><span class="linenos"> 905</span></a>
</span><span id="L-906"><a href="#L-906"><span class="linenos"> 906</span></a>        <span class="k">return</span> <span class="n">output_feature</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">hidden_features</span>
</span><span id="L-907"><a href="#L-907"><span class="linenos"> 907</span></a>
</span><span id="L-908"><a href="#L-908"><span class="linenos"> 908</span></a>
</span><span id="L-909"><a href="#L-909"><span class="linenos"> 909</span></a><span class="k">class</span><span class="w"> </span><span class="nc">HATMaskResNetBlockLarge</span><span class="p">(</span><span class="n">HATMaskBackbone</span><span class="p">,</span> <span class="n">ResNetBlockLarge</span><span class="p">):</span>
</span><span id="L-910"><a href="#L-910"><span class="linenos"> 910</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The larger building block for ResNet-50/101/152. It is referred to &quot;bottleneck&quot; building block in the ResNet paper.</span>
</span><span id="L-911"><a href="#L-911"><span class="linenos"> 911</span></a>
</span><span id="L-912"><a href="#L-912"><span class="linenos"> 912</span></a><span class="sd">    It consists of 3 weight convolutional layers, each followed by an activation function. See Table 1 or Figure 5 (right) in the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html).</span>
</span><span id="L-913"><a href="#L-913"><span class="linenos"> 913</span></a>
</span><span id="L-914"><a href="#L-914"><span class="linenos"> 914</span></a><span class="sd">    Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the unit-wise task embedding and gate function.</span>
</span><span id="L-915"><a href="#L-915"><span class="linenos"> 915</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-916"><a href="#L-916"><span class="linenos"> 916</span></a>
</span><span id="L-917"><a href="#L-917"><span class="linenos"> 917</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-918"><a href="#L-918"><span class="linenos"> 918</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-919"><a href="#L-919"><span class="linenos"> 919</span></a>        <span class="n">outer_layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-920"><a href="#L-920"><span class="linenos"> 920</span></a>        <span class="n">block_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-921"><a href="#L-921"><span class="linenos"> 921</span></a>        <span class="n">preceding_output_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-922"><a href="#L-922"><span class="linenos"> 922</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-923"><a href="#L-923"><span class="linenos"> 923</span></a>        <span class="n">overall_stride</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-924"><a href="#L-924"><span class="linenos"> 924</span></a>        <span class="n">gate</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-925"><a href="#L-925"><span class="linenos"> 925</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="L-926"><a href="#L-926"><span class="linenos"> 926</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-927"><a href="#L-927"><span class="linenos"> 927</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-928"><a href="#L-928"><span class="linenos"> 928</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the larger building block with task embedding.</span>
</span><span id="L-929"><a href="#L-929"><span class="linenos"> 929</span></a>
</span><span id="L-930"><a href="#L-930"><span class="linenos"> 930</span></a><span class="sd">        **Args:**</span>
</span><span id="L-931"><a href="#L-931"><span class="linenos"> 931</span></a><span class="sd">        - **outer_layer_name** (`str`): pass the name of the multi-building-block layer that contains this building block to construct the full name of each weighted convolutional layer.</span>
</span><span id="L-932"><a href="#L-932"><span class="linenos"> 932</span></a><span class="sd">        - **block_idx** (`int`): the index of the building blocks in the multi-building-block layer to construct the full name of each weighted convolutional layer.</span>
</span><span id="L-933"><a href="#L-933"><span class="linenos"> 933</span></a><span class="sd">        - **preceding_output_channels** (`int`): the number of channels of preceding output of this particular building block.</span>
</span><span id="L-934"><a href="#L-934"><span class="linenos"> 934</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input of this building block.</span>
</span><span id="L-935"><a href="#L-935"><span class="linenos"> 935</span></a><span class="sd">        - **overall_stride** (`int`): the overall stride of this building block. This stride is performed at 2nd (middle) convolutional layer where 1st and 3rd convolutional layers remain stride of 1.</span>
</span><span id="L-936"><a href="#L-936"><span class="linenos"> 936</span></a><span class="sd">        - **gate** (`str`): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:</span>
</span><span id="L-937"><a href="#L-937"><span class="linenos"> 937</span></a><span class="sd">            - `sigmoid`: the sigmoid function.</span>
</span><span id="L-938"><a href="#L-938"><span class="linenos"> 938</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="L-939"><a href="#L-939"><span class="linenos"> 939</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`.</span>
</span><span id="L-940"><a href="#L-940"><span class="linenos"> 940</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-941"><a href="#L-941"><span class="linenos"> 941</span></a>        <span class="n">HATMaskBackbone</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">gate</span><span class="o">=</span><span class="n">gate</span><span class="p">)</span>
</span><span id="L-942"><a href="#L-942"><span class="linenos"> 942</span></a>        <span class="n">ResNetBlockLarge</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-943"><a href="#L-943"><span class="linenos"> 943</span></a>            <span class="bp">self</span><span class="p">,</span>
</span><span id="L-944"><a href="#L-944"><span class="linenos"> 944</span></a>            <span class="n">outer_layer_name</span><span class="o">=</span><span class="n">outer_layer_name</span><span class="p">,</span>
</span><span id="L-945"><a href="#L-945"><span class="linenos"> 945</span></a>            <span class="n">block_idx</span><span class="o">=</span><span class="n">block_idx</span><span class="p">,</span>
</span><span id="L-946"><a href="#L-946"><span class="linenos"> 946</span></a>            <span class="n">preceding_output_channels</span><span class="o">=</span><span class="n">preceding_output_channels</span><span class="p">,</span>
</span><span id="L-947"><a href="#L-947"><span class="linenos"> 947</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
</span><span id="L-948"><a href="#L-948"><span class="linenos"> 948</span></a>            <span class="n">overall_stride</span><span class="o">=</span><span class="n">overall_stride</span><span class="p">,</span>
</span><span id="L-949"><a href="#L-949"><span class="linenos"> 949</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="L-950"><a href="#L-950"><span class="linenos"> 950</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="L-951"><a href="#L-951"><span class="linenos"> 951</span></a>        <span class="p">)</span>
</span><span id="L-952"><a href="#L-952"><span class="linenos"> 952</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">register_hat_mask_module_explicitly</span><span class="p">(</span><span class="n">gate</span><span class="o">=</span><span class="n">gate</span><span class="p">)</span>
</span><span id="L-953"><a href="#L-953"><span class="linenos"> 953</span></a>
</span><span id="L-954"><a href="#L-954"><span class="linenos"> 954</span></a>        <span class="c1"># construct the task embedding over the 1st weighted convolutional layer. It is channel-wise</span>
</span><span id="L-955"><a href="#L-955"><span class="linenos"> 955</span></a>        <span class="n">layer_output_channels</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-956"><a href="#L-956"><span class="linenos"> 956</span></a>            <span class="n">input_channels</span>  <span class="c1"># the output channels of the 1st convolutional layer</span>
</span><span id="L-957"><a href="#L-957"><span class="linenos"> 957</span></a>        <span class="p">)</span>
</span><span id="L-958"><a href="#L-958"><span class="linenos"> 958</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_1st_layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
</span><span id="L-959"><a href="#L-959"><span class="linenos"> 959</span></a>            <span class="n">num_embeddings</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="n">layer_output_channels</span>
</span><span id="L-960"><a href="#L-960"><span class="linenos"> 960</span></a>        <span class="p">)</span>
</span><span id="L-961"><a href="#L-961"><span class="linenos"> 961</span></a>
</span><span id="L-962"><a href="#L-962"><span class="linenos"> 962</span></a>        <span class="c1"># construct the task embedding over the 2nd weighted convolutional layer. It is channel-wise</span>
</span><span id="L-963"><a href="#L-963"><span class="linenos"> 963</span></a>        <span class="n">layer_output_channels</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-964"><a href="#L-964"><span class="linenos"> 964</span></a>            <span class="n">input_channels</span> <span class="o">*</span> <span class="mi">1</span>
</span><span id="L-965"><a href="#L-965"><span class="linenos"> 965</span></a>        <span class="p">)</span>  <span class="c1"># the output channels of the 2nd convolutional layer, which is the same as the input channels (without expansion)</span>
</span><span id="L-966"><a href="#L-966"><span class="linenos"> 966</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_2nd_layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
</span><span id="L-967"><a href="#L-967"><span class="linenos"> 967</span></a>            <span class="n">num_embeddings</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="n">layer_output_channels</span>
</span><span id="L-968"><a href="#L-968"><span class="linenos"> 968</span></a>        <span class="p">)</span>
</span><span id="L-969"><a href="#L-969"><span class="linenos"> 969</span></a>
</span><span id="L-970"><a href="#L-970"><span class="linenos"> 970</span></a>        <span class="c1"># construct the task embedding over the 3rd weighted convolutional layer. It is channel-wise</span>
</span><span id="L-971"><a href="#L-971"><span class="linenos"> 971</span></a>        <span class="n">layer_output_channels</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-972"><a href="#L-972"><span class="linenos"> 972</span></a>            <span class="n">input_channels</span>
</span><span id="L-973"><a href="#L-973"><span class="linenos"> 973</span></a>            <span class="o">*</span> <span class="mi">4</span>  <span class="c1"># the output channels of the 2nd convolutional layer, which is 4 times expanded as the input channels</span>
</span><span id="L-974"><a href="#L-974"><span class="linenos"> 974</span></a>        <span class="p">)</span>
</span><span id="L-975"><a href="#L-975"><span class="linenos"> 975</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_3rd_layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
</span><span id="L-976"><a href="#L-976"><span class="linenos"> 976</span></a>            <span class="n">num_embeddings</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="n">layer_output_channels</span>
</span><span id="L-977"><a href="#L-977"><span class="linenos"> 977</span></a>        <span class="p">)</span>
</span><span id="L-978"><a href="#L-978"><span class="linenos"> 978</span></a>
</span><span id="L-979"><a href="#L-979"><span class="linenos"> 979</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="L-980"><a href="#L-980"><span class="linenos"> 980</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-981"><a href="#L-981"><span class="linenos"> 981</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-982"><a href="#L-982"><span class="linenos"> 982</span></a>        <span class="n">stage</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-983"><a href="#L-983"><span class="linenos"> 983</span></a>        <span class="n">s_max</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-984"><a href="#L-984"><span class="linenos"> 984</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-985"><a href="#L-985"><span class="linenos"> 985</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-986"><a href="#L-986"><span class="linenos"> 986</span></a>        <span class="n">test_mask</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-987"><a href="#L-987"><span class="linenos"> 987</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
</span><span id="L-988"><a href="#L-988"><span class="linenos"> 988</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The forward pass for data from task `task_id`. Task-specific mask for `task_id` are applied to the units which are channels in each weighted convolutional layer.</span>
</span><span id="L-989"><a href="#L-989"><span class="linenos"> 989</span></a>
</span><span id="L-990"><a href="#L-990"><span class="linenos"> 990</span></a><span class="sd">        **Args:**</span>
</span><span id="L-991"><a href="#L-991"><span class="linenos"> 991</span></a><span class="sd">        - **input** (`Tensor`): The input tensor from data.</span>
</span><span id="L-992"><a href="#L-992"><span class="linenos"> 992</span></a><span class="sd">        - **stage** (`str`): the stage of the forward pass, should be one of the following:</span>
</span><span id="L-993"><a href="#L-993"><span class="linenos"> 993</span></a><span class="sd">            1. &#39;train&#39;: training stage.</span>
</span><span id="L-994"><a href="#L-994"><span class="linenos"> 994</span></a><span class="sd">            2. &#39;validation&#39;: validation stage.</span>
</span><span id="L-995"><a href="#L-995"><span class="linenos"> 995</span></a><span class="sd">            3. &#39;test&#39;: testing stage.</span>
</span><span id="L-996"><a href="#L-996"><span class="linenos"> 996</span></a><span class="sd">        - **s_max** (`float` | `None`): the maximum scaling factor in the gate function. Doesn&#39;t apply to testing stage. See chapter 2.4 &quot;Hard Attention Training&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="L-997"><a href="#L-997"><span class="linenos"> 997</span></a><span class="sd">        - **batch_idx** (`int` | `None`): the current batch index. Applies only to training stage. For other stages, it is default `None`.</span>
</span><span id="L-998"><a href="#L-998"><span class="linenos"> 998</span></a><span class="sd">        - **num_batches** (`int` | `None`): the total number of batches. Applies only to training stage. For other stages, it is default `None`.</span>
</span><span id="L-999"><a href="#L-999"><span class="linenos"> 999</span></a><span class="sd">        - **test_mask** (`dict[str, Tensor]` | `None`): the binary mask used for test. Applies only to testing stage. For other stages, it is default `None`.</span>
</span><span id="L-1000"><a href="#L-1000"><span class="linenos">1000</span></a>
</span><span id="L-1001"><a href="#L-1001"><span class="linenos">1001</span></a><span class="sd">        **Returns:**</span>
</span><span id="L-1002"><a href="#L-1002"><span class="linenos">1002</span></a><span class="sd">        - **output_feature** (`Tensor`): the output feature maps.</span>
</span><span id="L-1003"><a href="#L-1003"><span class="linenos">1003</span></a><span class="sd">        - **mask** (`dict[str, Tensor]`): the mask for the current task. Key (`str`) is layer name, value (`Tensor`) is the mask tensor. The mask tensor has size (number of units).</span>
</span><span id="L-1004"><a href="#L-1004"><span class="linenos">1004</span></a><span class="sd">        - **hidden_features** (`dict[str, Tensor]`): the hidden features (after activation) in each weighted layer. Key (`str`) is the weighted layer name, value (`Tensor`) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes. Although HAT algorithm does not need this, it is still provided for API consistence for other HAT-based algorithms inherited this `forward()` method of `HAT` class.</span>
</span><span id="L-1005"><a href="#L-1005"><span class="linenos">1005</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-1006"><a href="#L-1006"><span class="linenos">1006</span></a>        <span class="n">hidden_features</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="L-1007"><a href="#L-1007"><span class="linenos">1007</span></a>
</span><span id="L-1008"><a href="#L-1008"><span class="linenos">1008</span></a>        <span class="c1"># get the mask for the current task from the task embedding in this stage</span>
</span><span id="L-1009"><a href="#L-1009"><span class="linenos">1009</span></a>        <span class="n">mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_mask</span><span class="p">(</span>
</span><span id="L-1010"><a href="#L-1010"><span class="linenos">1010</span></a>            <span class="n">stage</span><span class="o">=</span><span class="n">stage</span><span class="p">,</span>
</span><span id="L-1011"><a href="#L-1011"><span class="linenos">1011</span></a>            <span class="n">s_max</span><span class="o">=</span><span class="n">s_max</span><span class="p">,</span>
</span><span id="L-1012"><a href="#L-1012"><span class="linenos">1012</span></a>            <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="L-1013"><a href="#L-1013"><span class="linenos">1013</span></a>            <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="L-1014"><a href="#L-1014"><span class="linenos">1014</span></a>            <span class="n">test_mask</span><span class="o">=</span><span class="n">test_mask</span><span class="p">,</span>
</span><span id="L-1015"><a href="#L-1015"><span class="linenos">1015</span></a>        <span class="p">)</span>
</span><span id="L-1016"><a href="#L-1016"><span class="linenos">1016</span></a>
</span><span id="L-1017"><a href="#L-1017"><span class="linenos">1017</span></a>        <span class="n">identity</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-1018"><a href="#L-1018"><span class="linenos">1018</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">identity_downsample</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span><span id="L-1019"><a href="#L-1019"><span class="linenos">1019</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">identity_downsample</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span><span id="L-1020"><a href="#L-1020"><span class="linenos">1020</span></a>            <span class="k">else</span> <span class="nb">input</span>
</span><span id="L-1021"><a href="#L-1021"><span class="linenos">1021</span></a>        <span class="p">)</span>  <span class="c1"># remember the identity of input for the shortcut connection. Perform downsampling if its dimension doesn&#39;t match the output&#39;s</span>
</span><span id="L-1022"><a href="#L-1022"><span class="linenos">1022</span></a>
</span><span id="L-1023"><a href="#L-1023"><span class="linenos">1023</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="nb">input</span>
</span><span id="L-1024"><a href="#L-1024"><span class="linenos">1024</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># weighted convolutional layer first</span>
</span><span id="L-1025"><a href="#L-1025"><span class="linenos">1025</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span>
</span><span id="L-1026"><a href="#L-1026"><span class="linenos">1026</span></a>            <span class="n">mask</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_1st_layer_name</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="L-1027"><a href="#L-1027"><span class="linenos">1027</span></a>        <span class="p">)</span>  <span class="c1"># apply the mask to the 1st convolutional layer. Broadcast the dimension of mask to match the input</span>
</span><span id="L-1028"><a href="#L-1028"><span class="linenos">1028</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="L-1029"><a href="#L-1029"><span class="linenos">1029</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># activation function third</span>
</span><span id="L-1030"><a href="#L-1030"><span class="linenos">1030</span></a>        <span class="n">hidden_features</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_1st_layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>  <span class="c1"># store the hidden feature</span>
</span><span id="L-1031"><a href="#L-1031"><span class="linenos">1031</span></a>
</span><span id="L-1032"><a href="#L-1032"><span class="linenos">1032</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># weighted convolutional layer first</span>
</span><span id="L-1033"><a href="#L-1033"><span class="linenos">1033</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span>
</span><span id="L-1034"><a href="#L-1034"><span class="linenos">1034</span></a>            <span class="n">mask</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_2nd_layer_name</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="L-1035"><a href="#L-1035"><span class="linenos">1035</span></a>        <span class="p">)</span>  <span class="c1"># apply the mask to the 2nd convolutional layer. Broadcast the dimension of mask to match the input</span>
</span><span id="L-1036"><a href="#L-1036"><span class="linenos">1036</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="L-1037"><a href="#L-1037"><span class="linenos">1037</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># activation function third</span>
</span><span id="L-1038"><a href="#L-1038"><span class="linenos">1038</span></a>        <span class="n">hidden_features</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_2nd_layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>  <span class="c1"># store the hidden feature</span>
</span><span id="L-1039"><a href="#L-1039"><span class="linenos">1039</span></a>
</span><span id="L-1040"><a href="#L-1040"><span class="linenos">1040</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># weighted convolutional layer first</span>
</span><span id="L-1041"><a href="#L-1041"><span class="linenos">1041</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">identity</span>
</span><span id="L-1042"><a href="#L-1042"><span class="linenos">1042</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span>
</span><span id="L-1043"><a href="#L-1043"><span class="linenos">1043</span></a>            <span class="n">mask</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_3rd_layer_name</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="L-1044"><a href="#L-1044"><span class="linenos">1044</span></a>        <span class="p">)</span>  <span class="c1"># apply the mask to the 3rd convolutional layer after the shortcut connection. Broadcast the dimension of mask to match the input</span>
</span><span id="L-1045"><a href="#L-1045"><span class="linenos">1045</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="L-1046"><a href="#L-1046"><span class="linenos">1046</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># activation after the shortcut connection</span>
</span><span id="L-1047"><a href="#L-1047"><span class="linenos">1047</span></a>        <span class="n">hidden_features</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_3rd_layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>  <span class="c1"># store the hidden feature</span>
</span><span id="L-1048"><a href="#L-1048"><span class="linenos">1048</span></a>
</span><span id="L-1049"><a href="#L-1049"><span class="linenos">1049</span></a>        <span class="n">output_feature</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="L-1050"><a href="#L-1050"><span class="linenos">1050</span></a>
</span><span id="L-1051"><a href="#L-1051"><span class="linenos">1051</span></a>        <span class="k">return</span> <span class="n">output_feature</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">hidden_features</span>
</span><span id="L-1052"><a href="#L-1052"><span class="linenos">1052</span></a>
</span><span id="L-1053"><a href="#L-1053"><span class="linenos">1053</span></a>
</span><span id="L-1054"><a href="#L-1054"><span class="linenos">1054</span></a><span class="k">class</span><span class="w"> </span><span class="nc">HATMaskResNetBase</span><span class="p">(</span><span class="n">ResNetBase</span><span class="p">,</span> <span class="n">HATMaskBackbone</span><span class="p">):</span>
</span><span id="L-1055"><a href="#L-1055"><span class="linenos">1055</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The base class of HAT masked [residual network (ResNet)](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html).</span>
</span><span id="L-1056"><a href="#L-1056"><span class="linenos">1056</span></a>
</span><span id="L-1057"><a href="#L-1057"><span class="linenos">1057</span></a><span class="sd">    [HAT (Hard Attention to the Task, 2018)](http://proceedings.mlr.press/v80/serra18a) is an architecture-based continual learning approach that uses learnable hard attention masks to select the task-specific parameters.</span>
</span><span id="L-1058"><a href="#L-1058"><span class="linenos">1058</span></a>
</span><span id="L-1059"><a href="#L-1059"><span class="linenos">1059</span></a><span class="sd">    ResNet is a convolutional network architecture, which has 1st convolutional parameter layer and a maxpooling layer, connecting to 4 convolutional layers which contains multiple convolutional parameter layer. Each layer of the 4 are constructed from basic building blocks which are either small (`ResNetBlockSmall`) or large (`ResNetBlockLarge`). Each building block contains several convolutional parameter layers. The building blocks are connected by a skip connection which is a direct connection from the input of the block to the output of the block, and this is why it&#39;s called residual (find &quot;shortcut connections&quot; in the paper for more details). After the 5th convolutional layer, there are average pooling layer and a fully connected layer which connects to the CL output heads.</span>
</span><span id="L-1060"><a href="#L-1060"><span class="linenos">1060</span></a>
</span><span id="L-1061"><a href="#L-1061"><span class="linenos">1061</span></a><span class="sd">    Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the unit-wise task embedding and gate function.</span>
</span><span id="L-1062"><a href="#L-1062"><span class="linenos">1062</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-1063"><a href="#L-1063"><span class="linenos">1063</span></a>
</span><span id="L-1064"><a href="#L-1064"><span class="linenos">1064</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-1065"><a href="#L-1065"><span class="linenos">1065</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-1066"><a href="#L-1066"><span class="linenos">1066</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-1067"><a href="#L-1067"><span class="linenos">1067</span></a>        <span class="n">building_block_type</span><span class="p">:</span> <span class="n">HATMaskResNetBlockSmall</span> <span class="o">|</span> <span class="n">HATMaskResNetBlockLarge</span><span class="p">,</span>
</span><span id="L-1068"><a href="#L-1068"><span class="linenos">1068</span></a>        <span class="n">building_block_nums</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
</span><span id="L-1069"><a href="#L-1069"><span class="linenos">1069</span></a>        <span class="n">building_block_preceding_output_channels</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
</span><span id="L-1070"><a href="#L-1070"><span class="linenos">1070</span></a>        <span class="n">building_block_input_channels</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
</span><span id="L-1071"><a href="#L-1071"><span class="linenos">1071</span></a>        <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-1072"><a href="#L-1072"><span class="linenos">1072</span></a>        <span class="n">gate</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-1073"><a href="#L-1073"><span class="linenos">1073</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="L-1074"><a href="#L-1074"><span class="linenos">1074</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-1075"><a href="#L-1075"><span class="linenos">1075</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-1076"><a href="#L-1076"><span class="linenos">1076</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the HAT masked ResNet backbone network with task embedding. Note that batch normalisation is incompatible with HAT mechanism.</span>
</span><span id="L-1077"><a href="#L-1077"><span class="linenos">1077</span></a>
</span><span id="L-1078"><a href="#L-1078"><span class="linenos">1078</span></a><span class="sd">        **Args:**</span>
</span><span id="L-1079"><a href="#L-1079"><span class="linenos">1079</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input. Image data are kept channels when going in ResNet. Note that convolutional networks require number of input channels instead of dimension.</span>
</span><span id="L-1080"><a href="#L-1080"><span class="linenos">1080</span></a><span class="sd">        - **building_block_type** (`HATMaskResNetBlockSmall` | `HATMaskResNetBlockLarge`): the type of building block used in the ResNet.</span>
</span><span id="L-1081"><a href="#L-1081"><span class="linenos">1081</span></a><span class="sd">        - **building_block_nums** (`tuple[int, int, int, int]`): the number of building blocks in the 2-5 convolutional layer correspondingly.</span>
</span><span id="L-1082"><a href="#L-1082"><span class="linenos">1082</span></a><span class="sd">        - **building_block_preceding_output_channels** (`tuple[int, int, int, int]`): the number of channels of preceding output of each building block in the 2-5 convolutional layer correspondingly.</span>
</span><span id="L-1083"><a href="#L-1083"><span class="linenos">1083</span></a><span class="sd">        - **building_block_input_channels** (`tuple[int, int, int, int]`): the number of channels of input of each building block in the 2-5 convolutional layer correspondingly.</span>
</span><span id="L-1084"><a href="#L-1084"><span class="linenos">1084</span></a><span class="sd">        - **output_dim** (`int`): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</span>
</span><span id="L-1085"><a href="#L-1085"><span class="linenos">1085</span></a><span class="sd">        - **gate** (`str`): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:</span>
</span><span id="L-1086"><a href="#L-1086"><span class="linenos">1086</span></a><span class="sd">            - `sigmoid`: the sigmoid function.</span>
</span><span id="L-1087"><a href="#L-1087"><span class="linenos">1087</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="L-1088"><a href="#L-1088"><span class="linenos">1088</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`, because batch normalisation are doing the similar thing with bias.</span>
</span><span id="L-1089"><a href="#L-1089"><span class="linenos">1089</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-1090"><a href="#L-1090"><span class="linenos">1090</span></a>        <span class="c1"># init from both inherited classes</span>
</span><span id="L-1091"><a href="#L-1091"><span class="linenos">1091</span></a>        <span class="n">HATMaskBackbone</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span> <span class="n">gate</span><span class="o">=</span><span class="n">gate</span><span class="p">)</span>
</span><span id="L-1092"><a href="#L-1092"><span class="linenos">1092</span></a>        <span class="n">ResNetBase</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-1093"><a href="#L-1093"><span class="linenos">1093</span></a>            <span class="bp">self</span><span class="p">,</span>
</span><span id="L-1094"><a href="#L-1094"><span class="linenos">1094</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
</span><span id="L-1095"><a href="#L-1095"><span class="linenos">1095</span></a>            <span class="n">building_block_type</span><span class="o">=</span><span class="n">building_block_type</span><span class="p">,</span>
</span><span id="L-1096"><a href="#L-1096"><span class="linenos">1096</span></a>            <span class="n">building_block_nums</span><span class="o">=</span><span class="n">building_block_nums</span><span class="p">,</span>
</span><span id="L-1097"><a href="#L-1097"><span class="linenos">1097</span></a>            <span class="n">building_block_preceding_output_channels</span><span class="o">=</span><span class="n">building_block_preceding_output_channels</span><span class="p">,</span>
</span><span id="L-1098"><a href="#L-1098"><span class="linenos">1098</span></a>            <span class="n">building_block_input_channels</span><span class="o">=</span><span class="n">building_block_input_channels</span><span class="p">,</span>
</span><span id="L-1099"><a href="#L-1099"><span class="linenos">1099</span></a>            <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span>
</span><span id="L-1100"><a href="#L-1100"><span class="linenos">1100</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="L-1101"><a href="#L-1101"><span class="linenos">1101</span></a>            <span class="n">batch_normalisation</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>  <span class="c1"># batch normalisation is incompatible with HAT mechanism</span>
</span><span id="L-1102"><a href="#L-1102"><span class="linenos">1102</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="L-1103"><a href="#L-1103"><span class="linenos">1103</span></a>        <span class="p">)</span>
</span><span id="L-1104"><a href="#L-1104"><span class="linenos">1104</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">register_hat_mask_module_explicitly</span><span class="p">(</span>
</span><span id="L-1105"><a href="#L-1105"><span class="linenos">1105</span></a>            <span class="n">gate</span><span class="o">=</span><span class="n">gate</span>
</span><span id="L-1106"><a href="#L-1106"><span class="linenos">1106</span></a>        <span class="p">)</span>  <span class="c1"># register all `nn.Module`s for HATMaskBackbone explicitly because the second `__init__()` wipes out them inited by the first `__init__()`</span>
</span><span id="L-1107"><a href="#L-1107"><span class="linenos">1107</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">update_multiple_blocks_task_embedding</span><span class="p">()</span>
</span><span id="L-1108"><a href="#L-1108"><span class="linenos">1108</span></a>
</span><span id="L-1109"><a href="#L-1109"><span class="linenos">1109</span></a>        <span class="c1"># construct the task embedding over the 1st weighted convolutional layers. It is channel-wise</span>
</span><span id="L-1110"><a href="#L-1110"><span class="linenos">1110</span></a>        <span class="n">layer_output_channels</span> <span class="o">=</span> <span class="mi">64</span>  <span class="c1"># the output channels of the 1st convolutional layer</span>
</span><span id="L-1111"><a href="#L-1111"><span class="linenos">1111</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="p">[</span><span class="s2">&quot;conv1&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
</span><span id="L-1112"><a href="#L-1112"><span class="linenos">1112</span></a>            <span class="n">num_embeddings</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="n">layer_output_channels</span>
</span><span id="L-1113"><a href="#L-1113"><span class="linenos">1113</span></a>        <span class="p">)</span>
</span><span id="L-1114"><a href="#L-1114"><span class="linenos">1114</span></a>
</span><span id="L-1115"><a href="#L-1115"><span class="linenos">1115</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_multiple_blocks</span><span class="p">(</span>
</span><span id="L-1116"><a href="#L-1116"><span class="linenos">1116</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-1117"><a href="#L-1117"><span class="linenos">1117</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-1118"><a href="#L-1118"><span class="linenos">1118</span></a>        <span class="n">building_block_type</span><span class="p">:</span> <span class="n">HATMaskResNetBlockSmall</span> <span class="o">|</span> <span class="n">HATMaskResNetBlockLarge</span><span class="p">,</span>
</span><span id="L-1119"><a href="#L-1119"><span class="linenos">1119</span></a>        <span class="n">building_block_num</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-1120"><a href="#L-1120"><span class="linenos">1120</span></a>        <span class="n">preceding_output_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-1121"><a href="#L-1121"><span class="linenos">1121</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-1122"><a href="#L-1122"><span class="linenos">1122</span></a>        <span class="n">overall_stride</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-1123"><a href="#L-1123"><span class="linenos">1123</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="L-1124"><a href="#L-1124"><span class="linenos">1124</span></a>        <span class="n">batch_normalisation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-1125"><a href="#L-1125"><span class="linenos">1125</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-1126"><a href="#L-1126"><span class="linenos">1126</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-1127"><a href="#L-1127"><span class="linenos">1127</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct a layer consisting of multiple building blocks with task embedding. It&#39;s used to construct the 2-5 convolutional layers of the HAT masked ResNet.</span>
</span><span id="L-1128"><a href="#L-1128"><span class="linenos">1128</span></a>
</span><span id="L-1129"><a href="#L-1129"><span class="linenos">1129</span></a><span class="sd">        The &quot;shortcut connections&quot; are performed between the input and output of each building block:</span>
</span><span id="L-1130"><a href="#L-1130"><span class="linenos">1130</span></a><span class="sd">        1. If the input and output of the building block have exactly the same dimensions (including number of channels and size), add the input to the output.</span>
</span><span id="L-1131"><a href="#L-1131"><span class="linenos">1131</span></a><span class="sd">        2. If the input and output of the building block have different dimensions (including number of channels and size), add the input to the output after a convolutional layer to make the dimensions match.</span>
</span><span id="L-1132"><a href="#L-1132"><span class="linenos">1132</span></a>
</span><span id="L-1133"><a href="#L-1133"><span class="linenos">1133</span></a><span class="sd">        **Args:**</span>
</span><span id="L-1134"><a href="#L-1134"><span class="linenos">1134</span></a><span class="sd">        - **layer_name** (`str`): pass the name of this multi-building-block layer to construct the full name of each weighted convolutional layer.</span>
</span><span id="L-1135"><a href="#L-1135"><span class="linenos">1135</span></a><span class="sd">        - **building_block_type** (`HATMaskResNetBlockSmall` | `HATMaskResNetBlockLarge`): the type of the building block.</span>
</span><span id="L-1136"><a href="#L-1136"><span class="linenos">1136</span></a><span class="sd">        - **building_block_num** (`int`): the number of building blocks in this multi-building-block layer.</span>
</span><span id="L-1137"><a href="#L-1137"><span class="linenos">1137</span></a><span class="sd">        - **preceding_output_channels** (`int`): the number of channels of preceding output of this entire multi-building-block layer.</span>
</span><span id="L-1138"><a href="#L-1138"><span class="linenos">1138</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input of this multi-building-block layer.</span>
</span><span id="L-1139"><a href="#L-1139"><span class="linenos">1139</span></a><span class="sd">        - **overall_stride** (`int`): the overall stride of the building blocks. This stride is performed at the 1st building block where other building blocks remain their own overall stride of 1. Inside that building block, this stride is performed at certain convolutional layer in the building block where other convolutional layers remain stride of 1:</span>
</span><span id="L-1140"><a href="#L-1140"><span class="linenos">1140</span></a><span class="sd">            - For `ResNetBlockSmall`, it performs at the 2nd (last) layer.</span>
</span><span id="L-1141"><a href="#L-1141"><span class="linenos">1141</span></a><span class="sd">            - For `ResNetBlockLarge`, it performs at the 2nd (middle) layer.</span>
</span><span id="L-1142"><a href="#L-1142"><span class="linenos">1142</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="L-1143"><a href="#L-1143"><span class="linenos">1143</span></a><span class="sd">        - **batch_normalisation** (`bool`): whether to use batch normalisation after the weight convolutional layers. In HATMaskResNet, batch normalisation is incompatible with HAT mechanism and shoule be always set `False`. We include this argument for compatibility with the original ResNet API.</span>
</span><span id="L-1144"><a href="#L-1144"><span class="linenos">1144</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`.</span>
</span><span id="L-1145"><a href="#L-1145"><span class="linenos">1145</span></a>
</span><span id="L-1146"><a href="#L-1146"><span class="linenos">1146</span></a><span class="sd">        **Returns:**</span>
</span><span id="L-1147"><a href="#L-1147"><span class="linenos">1147</span></a><span class="sd">        - **layer** (`nn.Sequential`): the constructed layer consisting of multiple building blocks.</span>
</span><span id="L-1148"><a href="#L-1148"><span class="linenos">1148</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-1149"><a href="#L-1149"><span class="linenos">1149</span></a>
</span><span id="L-1150"><a href="#L-1150"><span class="linenos">1150</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="L-1151"><a href="#L-1151"><span class="linenos">1151</span></a>
</span><span id="L-1152"><a href="#L-1152"><span class="linenos">1152</span></a>        <span class="k">for</span> <span class="n">block_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">building_block_num</span><span class="p">):</span>
</span><span id="L-1153"><a href="#L-1153"><span class="linenos">1153</span></a>            <span class="n">layer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="L-1154"><a href="#L-1154"><span class="linenos">1154</span></a>                <span class="n">building_block_type</span><span class="p">(</span>
</span><span id="L-1155"><a href="#L-1155"><span class="linenos">1155</span></a>                    <span class="n">outer_layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="L-1156"><a href="#L-1156"><span class="linenos">1156</span></a>                    <span class="n">block_idx</span><span class="o">=</span><span class="n">block_idx</span><span class="p">,</span>
</span><span id="L-1157"><a href="#L-1157"><span class="linenos">1157</span></a>                    <span class="n">preceding_output_channels</span><span class="o">=</span><span class="p">(</span>
</span><span id="L-1158"><a href="#L-1158"><span class="linenos">1158</span></a>                        <span class="n">preceding_output_channels</span>
</span><span id="L-1159"><a href="#L-1159"><span class="linenos">1159</span></a>                        <span class="k">if</span> <span class="n">block_idx</span> <span class="o">==</span> <span class="mi">0</span>
</span><span id="L-1160"><a href="#L-1160"><span class="linenos">1160</span></a>                        <span class="k">else</span> <span class="p">(</span>
</span><span id="L-1161"><a href="#L-1161"><span class="linenos">1161</span></a>                            <span class="n">input_channels</span>
</span><span id="L-1162"><a href="#L-1162"><span class="linenos">1162</span></a>                            <span class="k">if</span> <span class="n">building_block_type</span> <span class="o">==</span> <span class="n">HATMaskResNetBlockSmall</span>
</span><span id="L-1163"><a href="#L-1163"><span class="linenos">1163</span></a>                            <span class="k">else</span> <span class="n">input_channels</span> <span class="o">*</span> <span class="mi">4</span>
</span><span id="L-1164"><a href="#L-1164"><span class="linenos">1164</span></a>                        <span class="p">)</span>
</span><span id="L-1165"><a href="#L-1165"><span class="linenos">1165</span></a>                    <span class="p">),</span>  <span class="c1"># if it&#39;s the 1st block in this multi-building-block layer, it should be the number of channels of the preceding output of this entire multi-building-block layer. Otherwise, it should be the number of channels from last building block where the number of channels is 4 times of the input channels for `ResNetBlockLarge` than `ResNetBlockSmall`.</span>
</span><span id="L-1166"><a href="#L-1166"><span class="linenos">1166</span></a>                    <span class="n">input_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
</span><span id="L-1167"><a href="#L-1167"><span class="linenos">1167</span></a>                    <span class="n">overall_stride</span><span class="o">=</span><span class="p">(</span>
</span><span id="L-1168"><a href="#L-1168"><span class="linenos">1168</span></a>                        <span class="n">overall_stride</span> <span class="k">if</span> <span class="n">block_idx</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">1</span>
</span><span id="L-1169"><a href="#L-1169"><span class="linenos">1169</span></a>                    <span class="p">),</span>  <span class="c1"># only perform the overall stride at the 1st block in this multi-building-block layer</span>
</span><span id="L-1170"><a href="#L-1170"><span class="linenos">1170</span></a>                    <span class="n">gate</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">gate</span><span class="p">,</span>
</span><span id="L-1171"><a href="#L-1171"><span class="linenos">1171</span></a>                    <span class="c1"># no batch normalisation in HAT masked blocks</span>
</span><span id="L-1172"><a href="#L-1172"><span class="linenos">1172</span></a>                    <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="L-1173"><a href="#L-1173"><span class="linenos">1173</span></a>                    <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="L-1174"><a href="#L-1174"><span class="linenos">1174</span></a>                <span class="p">)</span>
</span><span id="L-1175"><a href="#L-1175"><span class="linenos">1175</span></a>            <span class="p">)</span>
</span><span id="L-1176"><a href="#L-1176"><span class="linenos">1176</span></a>
</span><span id="L-1177"><a href="#L-1177"><span class="linenos">1177</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span> <span class="o">+=</span> <span class="n">layer</span><span class="p">[</span>
</span><span id="L-1178"><a href="#L-1178"><span class="linenos">1178</span></a>                <span class="o">-</span><span class="mi">1</span>
</span><span id="L-1179"><a href="#L-1179"><span class="linenos">1179</span></a>            <span class="p">]</span><span class="o">.</span><span class="n">weighted_layer_names</span>  <span class="c1"># collect the weighted layer names in the blocks and sync to the weighted layer names list in the outer network</span>
</span><span id="L-1180"><a href="#L-1180"><span class="linenos">1180</span></a>
</span><span id="L-1181"><a href="#L-1181"><span class="linenos">1181</span></a>        <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layer</span><span class="p">)</span>
</span><span id="L-1182"><a href="#L-1182"><span class="linenos">1182</span></a>
</span><span id="L-1183"><a href="#L-1183"><span class="linenos">1183</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">update_multiple_blocks_task_embedding</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-1184"><a href="#L-1184"><span class="linenos">1184</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Collect the task embeddings in the multiple building blocks (2-5 convolutional layers) and sync to the weighted layer names list in the outer network.</span>
</span><span id="L-1185"><a href="#L-1185"><span class="linenos">1185</span></a>
</span><span id="L-1186"><a href="#L-1186"><span class="linenos">1186</span></a><span class="sd">        This should only be called explicitly after the `__init__()` method, just because task embedding as `nn.Module` instance was wiped out at the beginning of it.</span>
</span><span id="L-1187"><a href="#L-1187"><span class="linenos">1187</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-1188"><a href="#L-1188"><span class="linenos">1188</span></a>        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2x</span><span class="p">:</span>
</span><span id="L-1189"><a href="#L-1189"><span class="linenos">1189</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">block</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="p">)</span>
</span><span id="L-1190"><a href="#L-1190"><span class="linenos">1190</span></a>        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv3x</span><span class="p">:</span>
</span><span id="L-1191"><a href="#L-1191"><span class="linenos">1191</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">block</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="p">)</span>
</span><span id="L-1192"><a href="#L-1192"><span class="linenos">1192</span></a>        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv4x</span><span class="p">:</span>
</span><span id="L-1193"><a href="#L-1193"><span class="linenos">1193</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">block</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="p">)</span>
</span><span id="L-1194"><a href="#L-1194"><span class="linenos">1194</span></a>        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv5x</span><span class="p">:</span>
</span><span id="L-1195"><a href="#L-1195"><span class="linenos">1195</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">block</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="p">)</span>
</span><span id="L-1196"><a href="#L-1196"><span class="linenos">1196</span></a>
</span><span id="L-1197"><a href="#L-1197"><span class="linenos">1197</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="L-1198"><a href="#L-1198"><span class="linenos">1198</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-1199"><a href="#L-1199"><span class="linenos">1199</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-1200"><a href="#L-1200"><span class="linenos">1200</span></a>        <span class="n">stage</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-1201"><a href="#L-1201"><span class="linenos">1201</span></a>        <span class="n">s_max</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-1202"><a href="#L-1202"><span class="linenos">1202</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-1203"><a href="#L-1203"><span class="linenos">1203</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-1204"><a href="#L-1204"><span class="linenos">1204</span></a>        <span class="n">test_mask</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-1205"><a href="#L-1205"><span class="linenos">1205</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
</span><span id="L-1206"><a href="#L-1206"><span class="linenos">1206</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The forward pass for data from task `task_id`. Task-specific mask for `task_id` are applied to the units which are channels in each weighted convolutional layer.</span>
</span><span id="L-1207"><a href="#L-1207"><span class="linenos">1207</span></a>
</span><span id="L-1208"><a href="#L-1208"><span class="linenos">1208</span></a><span class="sd">        **Args:**</span>
</span><span id="L-1209"><a href="#L-1209"><span class="linenos">1209</span></a><span class="sd">        - **input** (`Tensor`): the input tensor from data.</span>
</span><span id="L-1210"><a href="#L-1210"><span class="linenos">1210</span></a><span class="sd">        - **stage** (`str`): the stage of the forward pass, should be one of the following:</span>
</span><span id="L-1211"><a href="#L-1211"><span class="linenos">1211</span></a><span class="sd">            1. &#39;train&#39;: training stage.</span>
</span><span id="L-1212"><a href="#L-1212"><span class="linenos">1212</span></a><span class="sd">            2. &#39;validation&#39;: validation stage.</span>
</span><span id="L-1213"><a href="#L-1213"><span class="linenos">1213</span></a><span class="sd">            3. &#39;test&#39;: testing stage.</span>
</span><span id="L-1214"><a href="#L-1214"><span class="linenos">1214</span></a><span class="sd">        - **s_max** (`float` | `None`): the maximum scaling factor in the gate function. Doesn&#39;t apply to testing stage. See chapter 2.4 &quot;Hard Attention Training&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="L-1215"><a href="#L-1215"><span class="linenos">1215</span></a><span class="sd">        - **batch_idx** (`int` | `None`): the current batch index. Applies only to training stage. For other stages, it is default `None`.</span>
</span><span id="L-1216"><a href="#L-1216"><span class="linenos">1216</span></a><span class="sd">        - **num_batches** (`int` | `None`): the total number of batches. Applies only to training stage. For other stages, it is default `None`.</span>
</span><span id="L-1217"><a href="#L-1217"><span class="linenos">1217</span></a><span class="sd">        - **test_mask** (`dict[str, Tensor]` | `None`): the binary mask used for test. Applies only to testing stage. For other stages, it is default `None`.</span>
</span><span id="L-1218"><a href="#L-1218"><span class="linenos">1218</span></a>
</span><span id="L-1219"><a href="#L-1219"><span class="linenos">1219</span></a><span class="sd">        **Returns:**</span>
</span><span id="L-1220"><a href="#L-1220"><span class="linenos">1220</span></a><span class="sd">        - **output_feature** (`Tensor`): the output feature tensor to be passed to the heads.</span>
</span><span id="L-1221"><a href="#L-1221"><span class="linenos">1221</span></a><span class="sd">        - **mask** (`dict[str, Tensor]`): the mask for the current task. Key (`str`) is layer name, value (`Tensor`) is the mask tensor. The mask tensor has size (number of units).</span>
</span><span id="L-1222"><a href="#L-1222"><span class="linenos">1222</span></a><span class="sd">        - **hidden_features** (`dict[str, Tensor]`): the hidden features (after activation) in each weighted layer. Key (`str`) is the weighted layer name, value (`Tensor`) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes. Although HAT algorithm does not need this, it is still provided for API consistence for other HAT-based algorithms inherited this `forward()` method of `HAT` class.</span>
</span><span id="L-1223"><a href="#L-1223"><span class="linenos">1223</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-1224"><a href="#L-1224"><span class="linenos">1224</span></a>        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="L-1225"><a href="#L-1225"><span class="linenos">1225</span></a>        <span class="n">hidden_features</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="L-1226"><a href="#L-1226"><span class="linenos">1226</span></a>
</span><span id="L-1227"><a href="#L-1227"><span class="linenos">1227</span></a>        <span class="c1"># get the mask for the current task from the task embedding in this stage</span>
</span><span id="L-1228"><a href="#L-1228"><span class="linenos">1228</span></a>        <span class="n">mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_mask</span><span class="p">(</span>
</span><span id="L-1229"><a href="#L-1229"><span class="linenos">1229</span></a>            <span class="n">stage</span><span class="o">=</span><span class="n">stage</span><span class="p">,</span>
</span><span id="L-1230"><a href="#L-1230"><span class="linenos">1230</span></a>            <span class="n">s_max</span><span class="o">=</span><span class="n">s_max</span><span class="p">,</span>
</span><span id="L-1231"><a href="#L-1231"><span class="linenos">1231</span></a>            <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="L-1232"><a href="#L-1232"><span class="linenos">1232</span></a>            <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="L-1233"><a href="#L-1233"><span class="linenos">1233</span></a>            <span class="n">test_mask</span><span class="o">=</span><span class="n">test_mask</span><span class="p">,</span>
</span><span id="L-1234"><a href="#L-1234"><span class="linenos">1234</span></a>        <span class="p">)</span>
</span><span id="L-1235"><a href="#L-1235"><span class="linenos">1235</span></a>
</span><span id="L-1236"><a href="#L-1236"><span class="linenos">1236</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="nb">input</span>
</span><span id="L-1237"><a href="#L-1237"><span class="linenos">1237</span></a>
</span><span id="L-1238"><a href="#L-1238"><span class="linenos">1238</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-1239"><a href="#L-1239"><span class="linenos">1239</span></a>
</span><span id="L-1240"><a href="#L-1240"><span class="linenos">1240</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span>
</span><span id="L-1241"><a href="#L-1241"><span class="linenos">1241</span></a>            <span class="n">mask</span><span class="p">[</span><span class="s2">&quot;conv1&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="L-1242"><a href="#L-1242"><span class="linenos">1242</span></a>        <span class="p">)</span>  <span class="c1"># apply the mask to the 1st convolutional layer. Broadcast the dimension of mask to match the input</span>
</span><span id="L-1243"><a href="#L-1243"><span class="linenos">1243</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="L-1244"><a href="#L-1244"><span class="linenos">1244</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-1245"><a href="#L-1245"><span class="linenos">1245</span></a>        <span class="n">hidden_features</span><span class="p">[</span><span class="s2">&quot;conv1&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="L-1246"><a href="#L-1246"><span class="linenos">1246</span></a>
</span><span id="L-1247"><a href="#L-1247"><span class="linenos">1247</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-1248"><a href="#L-1248"><span class="linenos">1248</span></a>
</span><span id="L-1249"><a href="#L-1249"><span class="linenos">1249</span></a>        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2x</span><span class="p">:</span>
</span><span id="L-1250"><a href="#L-1250"><span class="linenos">1250</span></a>            <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">hidden_features_block</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span>
</span><span id="L-1251"><a href="#L-1251"><span class="linenos">1251</span></a>                <span class="n">x</span><span class="p">,</span>
</span><span id="L-1252"><a href="#L-1252"><span class="linenos">1252</span></a>                <span class="n">stage</span><span class="o">=</span><span class="n">stage</span><span class="p">,</span>
</span><span id="L-1253"><a href="#L-1253"><span class="linenos">1253</span></a>                <span class="n">s_max</span><span class="o">=</span><span class="n">s_max</span><span class="p">,</span>
</span><span id="L-1254"><a href="#L-1254"><span class="linenos">1254</span></a>                <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="L-1255"><a href="#L-1255"><span class="linenos">1255</span></a>                <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="L-1256"><a href="#L-1256"><span class="linenos">1256</span></a>                <span class="n">test_mask</span><span class="o">=</span><span class="n">test_mask</span><span class="p">,</span>
</span><span id="L-1257"><a href="#L-1257"><span class="linenos">1257</span></a>            <span class="p">)</span>
</span><span id="L-1258"><a href="#L-1258"><span class="linenos">1258</span></a>            <span class="n">hidden_features</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">hidden_features_block</span><span class="p">)</span>  <span class="c1"># store the hidden feature</span>
</span><span id="L-1259"><a href="#L-1259"><span class="linenos">1259</span></a>        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv3x</span><span class="p">:</span>
</span><span id="L-1260"><a href="#L-1260"><span class="linenos">1260</span></a>            <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">hidden_features_block</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span>
</span><span id="L-1261"><a href="#L-1261"><span class="linenos">1261</span></a>                <span class="n">x</span><span class="p">,</span>
</span><span id="L-1262"><a href="#L-1262"><span class="linenos">1262</span></a>                <span class="n">stage</span><span class="o">=</span><span class="n">stage</span><span class="p">,</span>
</span><span id="L-1263"><a href="#L-1263"><span class="linenos">1263</span></a>                <span class="n">s_max</span><span class="o">=</span><span class="n">s_max</span><span class="p">,</span>
</span><span id="L-1264"><a href="#L-1264"><span class="linenos">1264</span></a>                <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="L-1265"><a href="#L-1265"><span class="linenos">1265</span></a>                <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="L-1266"><a href="#L-1266"><span class="linenos">1266</span></a>                <span class="n">test_mask</span><span class="o">=</span><span class="n">test_mask</span><span class="p">,</span>
</span><span id="L-1267"><a href="#L-1267"><span class="linenos">1267</span></a>            <span class="p">)</span>
</span><span id="L-1268"><a href="#L-1268"><span class="linenos">1268</span></a>            <span class="n">hidden_features</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">hidden_features_block</span><span class="p">)</span>  <span class="c1"># store the hidden feature</span>
</span><span id="L-1269"><a href="#L-1269"><span class="linenos">1269</span></a>        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv4x</span><span class="p">:</span>
</span><span id="L-1270"><a href="#L-1270"><span class="linenos">1270</span></a>            <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">hidden_features_block</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span>
</span><span id="L-1271"><a href="#L-1271"><span class="linenos">1271</span></a>                <span class="n">x</span><span class="p">,</span>
</span><span id="L-1272"><a href="#L-1272"><span class="linenos">1272</span></a>                <span class="n">stage</span><span class="o">=</span><span class="n">stage</span><span class="p">,</span>
</span><span id="L-1273"><a href="#L-1273"><span class="linenos">1273</span></a>                <span class="n">s_max</span><span class="o">=</span><span class="n">s_max</span><span class="p">,</span>
</span><span id="L-1274"><a href="#L-1274"><span class="linenos">1274</span></a>                <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="L-1275"><a href="#L-1275"><span class="linenos">1275</span></a>                <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="L-1276"><a href="#L-1276"><span class="linenos">1276</span></a>                <span class="n">test_mask</span><span class="o">=</span><span class="n">test_mask</span><span class="p">,</span>
</span><span id="L-1277"><a href="#L-1277"><span class="linenos">1277</span></a>            <span class="p">)</span>
</span><span id="L-1278"><a href="#L-1278"><span class="linenos">1278</span></a>            <span class="n">hidden_features</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">hidden_features_block</span><span class="p">)</span>  <span class="c1"># store the hidden feature</span>
</span><span id="L-1279"><a href="#L-1279"><span class="linenos">1279</span></a>        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv5x</span><span class="p">:</span>
</span><span id="L-1280"><a href="#L-1280"><span class="linenos">1280</span></a>            <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">hidden_features_block</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span>
</span><span id="L-1281"><a href="#L-1281"><span class="linenos">1281</span></a>                <span class="n">x</span><span class="p">,</span>
</span><span id="L-1282"><a href="#L-1282"><span class="linenos">1282</span></a>                <span class="n">stage</span><span class="o">=</span><span class="n">stage</span><span class="p">,</span>
</span><span id="L-1283"><a href="#L-1283"><span class="linenos">1283</span></a>                <span class="n">s_max</span><span class="o">=</span><span class="n">s_max</span><span class="p">,</span>
</span><span id="L-1284"><a href="#L-1284"><span class="linenos">1284</span></a>                <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="L-1285"><a href="#L-1285"><span class="linenos">1285</span></a>                <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="L-1286"><a href="#L-1286"><span class="linenos">1286</span></a>                <span class="n">test_mask</span><span class="o">=</span><span class="n">test_mask</span><span class="p">,</span>
</span><span id="L-1287"><a href="#L-1287"><span class="linenos">1287</span></a>            <span class="p">)</span>
</span><span id="L-1288"><a href="#L-1288"><span class="linenos">1288</span></a>            <span class="n">hidden_features</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">hidden_features_block</span><span class="p">)</span>  <span class="c1"># store the hidden feature</span>
</span><span id="L-1289"><a href="#L-1289"><span class="linenos">1289</span></a>
</span><span id="L-1290"><a href="#L-1290"><span class="linenos">1290</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">avepool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-1291"><a href="#L-1291"><span class="linenos">1291</span></a>
</span><span id="L-1292"><a href="#L-1292"><span class="linenos">1292</span></a>        <span class="n">output_feature</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># flatten before going through heads</span>
</span><span id="L-1293"><a href="#L-1293"><span class="linenos">1293</span></a>
</span><span id="L-1294"><a href="#L-1294"><span class="linenos">1294</span></a>        <span class="k">return</span> <span class="n">output_feature</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">hidden_features</span>
</span><span id="L-1295"><a href="#L-1295"><span class="linenos">1295</span></a>
</span><span id="L-1296"><a href="#L-1296"><span class="linenos">1296</span></a>
</span><span id="L-1297"><a href="#L-1297"><span class="linenos">1297</span></a><span class="k">class</span><span class="w"> </span><span class="nc">HATMaskResNet18</span><span class="p">(</span><span class="n">HATMaskResNetBase</span><span class="p">):</span>
</span><span id="L-1298"><a href="#L-1298"><span class="linenos">1298</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;HAT masked ResNet-18 backbone network.</span>
</span><span id="L-1299"><a href="#L-1299"><span class="linenos">1299</span></a>
</span><span id="L-1300"><a href="#L-1300"><span class="linenos">1300</span></a><span class="sd">    [HAT (Hard Attention to the Task, 2018)](http://proceedings.mlr.press/v80/serra18a) is an architecture-based continual learning approach that uses learnable hard attention masks to select the task-specific parameters.</span>
</span><span id="L-1301"><a href="#L-1301"><span class="linenos">1301</span></a>
</span><span id="L-1302"><a href="#L-1302"><span class="linenos">1302</span></a><span class="sd">    ResNet-18 is a smaller architecture proposed in the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html). It consists of 18 weight convolutional layers in total. See Table 1 in the paper for details.</span>
</span><span id="L-1303"><a href="#L-1303"><span class="linenos">1303</span></a>
</span><span id="L-1304"><a href="#L-1304"><span class="linenos">1304</span></a><span class="sd">    Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the unit-wise task embedding and gate function.</span>
</span><span id="L-1305"><a href="#L-1305"><span class="linenos">1305</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-1306"><a href="#L-1306"><span class="linenos">1306</span></a>
</span><span id="L-1307"><a href="#L-1307"><span class="linenos">1307</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-1308"><a href="#L-1308"><span class="linenos">1308</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-1309"><a href="#L-1309"><span class="linenos">1309</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-1310"><a href="#L-1310"><span class="linenos">1310</span></a>        <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-1311"><a href="#L-1311"><span class="linenos">1311</span></a>        <span class="n">gate</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-1312"><a href="#L-1312"><span class="linenos">1312</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="L-1313"><a href="#L-1313"><span class="linenos">1313</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-1314"><a href="#L-1314"><span class="linenos">1314</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-1315"><a href="#L-1315"><span class="linenos">1315</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the ResNet-18 backbone network with task embedding. Note that batch normalisation is incompatible with HAT mechanism.</span>
</span><span id="L-1316"><a href="#L-1316"><span class="linenos">1316</span></a>
</span><span id="L-1317"><a href="#L-1317"><span class="linenos">1317</span></a><span class="sd">        **Args:**</span>
</span><span id="L-1318"><a href="#L-1318"><span class="linenos">1318</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</span>
</span><span id="L-1319"><a href="#L-1319"><span class="linenos">1319</span></a><span class="sd">        - **output_dim** (`int`): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</span>
</span><span id="L-1320"><a href="#L-1320"><span class="linenos">1320</span></a><span class="sd">        - **gate** (`str`): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:</span>
</span><span id="L-1321"><a href="#L-1321"><span class="linenos">1321</span></a><span class="sd">            - `sigmoid`: the sigmoid function.</span>
</span><span id="L-1322"><a href="#L-1322"><span class="linenos">1322</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="L-1323"><a href="#L-1323"><span class="linenos">1323</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`.</span>
</span><span id="L-1324"><a href="#L-1324"><span class="linenos">1324</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-1325"><a href="#L-1325"><span class="linenos">1325</span></a>        <span class="n">HATMaskResNetBase</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-1326"><a href="#L-1326"><span class="linenos">1326</span></a>            <span class="bp">self</span><span class="p">,</span>
</span><span id="L-1327"><a href="#L-1327"><span class="linenos">1327</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
</span><span id="L-1328"><a href="#L-1328"><span class="linenos">1328</span></a>            <span class="n">building_block_type</span><span class="o">=</span><span class="n">HATMaskResNetBlockSmall</span><span class="p">,</span>  <span class="c1"># use the smaller building block for ResNet-18</span>
</span><span id="L-1329"><a href="#L-1329"><span class="linenos">1329</span></a>            <span class="n">building_block_nums</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
</span><span id="L-1330"><a href="#L-1330"><span class="linenos">1330</span></a>            <span class="n">building_block_preceding_output_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>
</span><span id="L-1331"><a href="#L-1331"><span class="linenos">1331</span></a>            <span class="n">building_block_input_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
</span><span id="L-1332"><a href="#L-1332"><span class="linenos">1332</span></a>            <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span>
</span><span id="L-1333"><a href="#L-1333"><span class="linenos">1333</span></a>            <span class="n">gate</span><span class="o">=</span><span class="n">gate</span><span class="p">,</span>
</span><span id="L-1334"><a href="#L-1334"><span class="linenos">1334</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="L-1335"><a href="#L-1335"><span class="linenos">1335</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="L-1336"><a href="#L-1336"><span class="linenos">1336</span></a>        <span class="p">)</span>
</span><span id="L-1337"><a href="#L-1337"><span class="linenos">1337</span></a>
</span><span id="L-1338"><a href="#L-1338"><span class="linenos">1338</span></a>
</span><span id="L-1339"><a href="#L-1339"><span class="linenos">1339</span></a><span class="k">class</span><span class="w"> </span><span class="nc">HATMaskResNet34</span><span class="p">(</span><span class="n">HATMaskResNetBase</span><span class="p">):</span>
</span><span id="L-1340"><a href="#L-1340"><span class="linenos">1340</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;HAT masked ResNet-34 backbone network.</span>
</span><span id="L-1341"><a href="#L-1341"><span class="linenos">1341</span></a>
</span><span id="L-1342"><a href="#L-1342"><span class="linenos">1342</span></a><span class="sd">    [HAT (Hard Attention to the Task, 2018)](http://proceedings.mlr.press/v80/serra18a) is an architecture-based continual learning approach that uses learnable hard attention masks to select the task-specific parameters.</span>
</span><span id="L-1343"><a href="#L-1343"><span class="linenos">1343</span></a>
</span><span id="L-1344"><a href="#L-1344"><span class="linenos">1344</span></a><span class="sd">    ResNet-34 is a smaller architecture proposed in the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html). It consists of 34 weight convolutional layers in total. See Table 1 in the paper for details.</span>
</span><span id="L-1345"><a href="#L-1345"><span class="linenos">1345</span></a>
</span><span id="L-1346"><a href="#L-1346"><span class="linenos">1346</span></a><span class="sd">    Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the unit-wise task embedding and gate function.</span>
</span><span id="L-1347"><a href="#L-1347"><span class="linenos">1347</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-1348"><a href="#L-1348"><span class="linenos">1348</span></a>
</span><span id="L-1349"><a href="#L-1349"><span class="linenos">1349</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-1350"><a href="#L-1350"><span class="linenos">1350</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-1351"><a href="#L-1351"><span class="linenos">1351</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-1352"><a href="#L-1352"><span class="linenos">1352</span></a>        <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-1353"><a href="#L-1353"><span class="linenos">1353</span></a>        <span class="n">gate</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-1354"><a href="#L-1354"><span class="linenos">1354</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="L-1355"><a href="#L-1355"><span class="linenos">1355</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-1356"><a href="#L-1356"><span class="linenos">1356</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-1357"><a href="#L-1357"><span class="linenos">1357</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the ResNet-34 backbone network with task embedding. Note that batch normalisation is incompatible with HAT mechanism.</span>
</span><span id="L-1358"><a href="#L-1358"><span class="linenos">1358</span></a>
</span><span id="L-1359"><a href="#L-1359"><span class="linenos">1359</span></a><span class="sd">        **Args:**</span>
</span><span id="L-1360"><a href="#L-1360"><span class="linenos">1360</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</span>
</span><span id="L-1361"><a href="#L-1361"><span class="linenos">1361</span></a><span class="sd">        - **output_dim** (`int`): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</span>
</span><span id="L-1362"><a href="#L-1362"><span class="linenos">1362</span></a><span class="sd">        - **gate** (`str`): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:</span>
</span><span id="L-1363"><a href="#L-1363"><span class="linenos">1363</span></a><span class="sd">            - `sigmoid`: the sigmoid function.</span>
</span><span id="L-1364"><a href="#L-1364"><span class="linenos">1364</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="L-1365"><a href="#L-1365"><span class="linenos">1365</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`.</span>
</span><span id="L-1366"><a href="#L-1366"><span class="linenos">1366</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-1367"><a href="#L-1367"><span class="linenos">1367</span></a>        <span class="n">HATMaskResNetBase</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-1368"><a href="#L-1368"><span class="linenos">1368</span></a>            <span class="bp">self</span><span class="p">,</span>
</span><span id="L-1369"><a href="#L-1369"><span class="linenos">1369</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
</span><span id="L-1370"><a href="#L-1370"><span class="linenos">1370</span></a>            <span class="n">building_block_type</span><span class="o">=</span><span class="n">HATMaskResNetBlockSmall</span><span class="p">,</span>  <span class="c1"># use the smaller building block for ResNet-34</span>
</span><span id="L-1371"><a href="#L-1371"><span class="linenos">1371</span></a>            <span class="n">building_block_nums</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
</span><span id="L-1372"><a href="#L-1372"><span class="linenos">1372</span></a>            <span class="n">building_block_preceding_output_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>
</span><span id="L-1373"><a href="#L-1373"><span class="linenos">1373</span></a>            <span class="n">building_block_input_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
</span><span id="L-1374"><a href="#L-1374"><span class="linenos">1374</span></a>            <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span>
</span><span id="L-1375"><a href="#L-1375"><span class="linenos">1375</span></a>            <span class="n">gate</span><span class="o">=</span><span class="n">gate</span><span class="p">,</span>
</span><span id="L-1376"><a href="#L-1376"><span class="linenos">1376</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="L-1377"><a href="#L-1377"><span class="linenos">1377</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="L-1378"><a href="#L-1378"><span class="linenos">1378</span></a>        <span class="p">)</span>
</span><span id="L-1379"><a href="#L-1379"><span class="linenos">1379</span></a>
</span><span id="L-1380"><a href="#L-1380"><span class="linenos">1380</span></a>
</span><span id="L-1381"><a href="#L-1381"><span class="linenos">1381</span></a><span class="k">class</span><span class="w"> </span><span class="nc">HATMaskResNet50</span><span class="p">(</span><span class="n">HATMaskResNetBase</span><span class="p">):</span>
</span><span id="L-1382"><a href="#L-1382"><span class="linenos">1382</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;HAT masked ResNet-50 backbone network.</span>
</span><span id="L-1383"><a href="#L-1383"><span class="linenos">1383</span></a>
</span><span id="L-1384"><a href="#L-1384"><span class="linenos">1384</span></a><span class="sd">    [HAT (Hard Attention to the Task, 2018)](http://proceedings.mlr.press/v80/serra18a) is an architecture-based continual learning approach that uses learnable hard attention masks to select the task-specific parameters.</span>
</span><span id="L-1385"><a href="#L-1385"><span class="linenos">1385</span></a>
</span><span id="L-1386"><a href="#L-1386"><span class="linenos">1386</span></a><span class="sd">    ResNet-50 is a larger architecture proposed in the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html). It consists of 50 weight convolutional layers in total. See Table 1 in the paper for details.</span>
</span><span id="L-1387"><a href="#L-1387"><span class="linenos">1387</span></a>
</span><span id="L-1388"><a href="#L-1388"><span class="linenos">1388</span></a><span class="sd">    Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the unit-wise task embedding and gate function.</span>
</span><span id="L-1389"><a href="#L-1389"><span class="linenos">1389</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-1390"><a href="#L-1390"><span class="linenos">1390</span></a>
</span><span id="L-1391"><a href="#L-1391"><span class="linenos">1391</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-1392"><a href="#L-1392"><span class="linenos">1392</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-1393"><a href="#L-1393"><span class="linenos">1393</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-1394"><a href="#L-1394"><span class="linenos">1394</span></a>        <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-1395"><a href="#L-1395"><span class="linenos">1395</span></a>        <span class="n">gate</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-1396"><a href="#L-1396"><span class="linenos">1396</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="L-1397"><a href="#L-1397"><span class="linenos">1397</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-1398"><a href="#L-1398"><span class="linenos">1398</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-1399"><a href="#L-1399"><span class="linenos">1399</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the ResNet-50 backbone network with task embedding. Note that batch normalisation is incompatible with HAT mechanism.</span>
</span><span id="L-1400"><a href="#L-1400"><span class="linenos">1400</span></a>
</span><span id="L-1401"><a href="#L-1401"><span class="linenos">1401</span></a><span class="sd">        **Args:**</span>
</span><span id="L-1402"><a href="#L-1402"><span class="linenos">1402</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</span>
</span><span id="L-1403"><a href="#L-1403"><span class="linenos">1403</span></a><span class="sd">        - **output_dim** (`int`): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</span>
</span><span id="L-1404"><a href="#L-1404"><span class="linenos">1404</span></a><span class="sd">        - **gate** (`str`): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:</span>
</span><span id="L-1405"><a href="#L-1405"><span class="linenos">1405</span></a><span class="sd">            - `sigmoid`: the sigmoid function.</span>
</span><span id="L-1406"><a href="#L-1406"><span class="linenos">1406</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="L-1407"><a href="#L-1407"><span class="linenos">1407</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`.</span>
</span><span id="L-1408"><a href="#L-1408"><span class="linenos">1408</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-1409"><a href="#L-1409"><span class="linenos">1409</span></a>        <span class="n">HATMaskResNetBase</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-1410"><a href="#L-1410"><span class="linenos">1410</span></a>            <span class="bp">self</span><span class="p">,</span>
</span><span id="L-1411"><a href="#L-1411"><span class="linenos">1411</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
</span><span id="L-1412"><a href="#L-1412"><span class="linenos">1412</span></a>            <span class="n">building_block_type</span><span class="o">=</span><span class="n">HATMaskResNetBlockLarge</span><span class="p">,</span>  <span class="c1"># use the smaller building block for ResNet-50</span>
</span><span id="L-1413"><a href="#L-1413"><span class="linenos">1413</span></a>            <span class="n">building_block_nums</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
</span><span id="L-1414"><a href="#L-1414"><span class="linenos">1414</span></a>            <span class="n">building_block_preceding_output_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span>
</span><span id="L-1415"><a href="#L-1415"><span class="linenos">1415</span></a>            <span class="n">building_block_input_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
</span><span id="L-1416"><a href="#L-1416"><span class="linenos">1416</span></a>            <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span>
</span><span id="L-1417"><a href="#L-1417"><span class="linenos">1417</span></a>            <span class="n">gate</span><span class="o">=</span><span class="n">gate</span><span class="p">,</span>
</span><span id="L-1418"><a href="#L-1418"><span class="linenos">1418</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="L-1419"><a href="#L-1419"><span class="linenos">1419</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="L-1420"><a href="#L-1420"><span class="linenos">1420</span></a>        <span class="p">)</span>
</span><span id="L-1421"><a href="#L-1421"><span class="linenos">1421</span></a>
</span><span id="L-1422"><a href="#L-1422"><span class="linenos">1422</span></a>
</span><span id="L-1423"><a href="#L-1423"><span class="linenos">1423</span></a><span class="k">class</span><span class="w"> </span><span class="nc">HATMaskResNet101</span><span class="p">(</span><span class="n">HATMaskResNetBase</span><span class="p">):</span>
</span><span id="L-1424"><a href="#L-1424"><span class="linenos">1424</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;HAT masked ResNet-101 backbone network.</span>
</span><span id="L-1425"><a href="#L-1425"><span class="linenos">1425</span></a>
</span><span id="L-1426"><a href="#L-1426"><span class="linenos">1426</span></a><span class="sd">    [HAT (Hard Attention to the Task, 2018)](http://proceedings.mlr.press/v80/serra18a) is an architecture-based continual learning approach that uses learnable hard attention masks to select the task-specific parameters.</span>
</span><span id="L-1427"><a href="#L-1427"><span class="linenos">1427</span></a>
</span><span id="L-1428"><a href="#L-1428"><span class="linenos">1428</span></a><span class="sd">    ResNet-101 is a larger architecture proposed in the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html). It consists of 101 weight convolutional layers in total. See Table 1 in the paper for details.</span>
</span><span id="L-1429"><a href="#L-1429"><span class="linenos">1429</span></a>
</span><span id="L-1430"><a href="#L-1430"><span class="linenos">1430</span></a><span class="sd">    Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the unit-wise task embedding and gate function.</span>
</span><span id="L-1431"><a href="#L-1431"><span class="linenos">1431</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-1432"><a href="#L-1432"><span class="linenos">1432</span></a>
</span><span id="L-1433"><a href="#L-1433"><span class="linenos">1433</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-1434"><a href="#L-1434"><span class="linenos">1434</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-1435"><a href="#L-1435"><span class="linenos">1435</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-1436"><a href="#L-1436"><span class="linenos">1436</span></a>        <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-1437"><a href="#L-1437"><span class="linenos">1437</span></a>        <span class="n">gate</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-1438"><a href="#L-1438"><span class="linenos">1438</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="L-1439"><a href="#L-1439"><span class="linenos">1439</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-1440"><a href="#L-1440"><span class="linenos">1440</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-1441"><a href="#L-1441"><span class="linenos">1441</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the ResNet-101 backbone network with task embedding. Note that batch normalisation is incompatible with HAT mechanism.</span>
</span><span id="L-1442"><a href="#L-1442"><span class="linenos">1442</span></a>
</span><span id="L-1443"><a href="#L-1443"><span class="linenos">1443</span></a><span class="sd">        **Args:**</span>
</span><span id="L-1444"><a href="#L-1444"><span class="linenos">1444</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</span>
</span><span id="L-1445"><a href="#L-1445"><span class="linenos">1445</span></a><span class="sd">        - **output_dim** (`int`): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</span>
</span><span id="L-1446"><a href="#L-1446"><span class="linenos">1446</span></a><span class="sd">        - **gate** (`str`): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:</span>
</span><span id="L-1447"><a href="#L-1447"><span class="linenos">1447</span></a><span class="sd">            - `sigmoid`: the sigmoid function.</span>
</span><span id="L-1448"><a href="#L-1448"><span class="linenos">1448</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="L-1449"><a href="#L-1449"><span class="linenos">1449</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`.</span>
</span><span id="L-1450"><a href="#L-1450"><span class="linenos">1450</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-1451"><a href="#L-1451"><span class="linenos">1451</span></a>        <span class="n">HATMaskResNetBase</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-1452"><a href="#L-1452"><span class="linenos">1452</span></a>            <span class="bp">self</span><span class="p">,</span>
</span><span id="L-1453"><a href="#L-1453"><span class="linenos">1453</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
</span><span id="L-1454"><a href="#L-1454"><span class="linenos">1454</span></a>            <span class="n">building_block_type</span><span class="o">=</span><span class="n">HATMaskResNetBlockLarge</span><span class="p">,</span>  <span class="c1"># use the smaller building block for ResNet-18</span>
</span><span id="L-1455"><a href="#L-1455"><span class="linenos">1455</span></a>            <span class="n">building_block_nums</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
</span><span id="L-1456"><a href="#L-1456"><span class="linenos">1456</span></a>            <span class="n">building_block_preceding_output_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span>
</span><span id="L-1457"><a href="#L-1457"><span class="linenos">1457</span></a>            <span class="n">building_block_input_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
</span><span id="L-1458"><a href="#L-1458"><span class="linenos">1458</span></a>            <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span>
</span><span id="L-1459"><a href="#L-1459"><span class="linenos">1459</span></a>            <span class="n">gate</span><span class="o">=</span><span class="n">gate</span><span class="p">,</span>
</span><span id="L-1460"><a href="#L-1460"><span class="linenos">1460</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="L-1461"><a href="#L-1461"><span class="linenos">1461</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="L-1462"><a href="#L-1462"><span class="linenos">1462</span></a>        <span class="p">)</span>
</span><span id="L-1463"><a href="#L-1463"><span class="linenos">1463</span></a>
</span><span id="L-1464"><a href="#L-1464"><span class="linenos">1464</span></a>
</span><span id="L-1465"><a href="#L-1465"><span class="linenos">1465</span></a><span class="k">class</span><span class="w"> </span><span class="nc">HATMaskResNet152</span><span class="p">(</span><span class="n">HATMaskResNetBase</span><span class="p">):</span>
</span><span id="L-1466"><a href="#L-1466"><span class="linenos">1466</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;HAT masked ResNet-152 backbone network.</span>
</span><span id="L-1467"><a href="#L-1467"><span class="linenos">1467</span></a>
</span><span id="L-1468"><a href="#L-1468"><span class="linenos">1468</span></a><span class="sd">    [HAT (Hard Attention to the Task, 2018)](http://proceedings.mlr.press/v80/serra18a) is an architecture-based continual learning approach that uses learnable hard attention masks to select the task-specific parameters.</span>
</span><span id="L-1469"><a href="#L-1469"><span class="linenos">1469</span></a>
</span><span id="L-1470"><a href="#L-1470"><span class="linenos">1470</span></a><span class="sd">    ResNet-152 is the largest architecture proposed in the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html). It consists of 152 weight convolutional layers in total. See Table 1 in the paper for details.</span>
</span><span id="L-1471"><a href="#L-1471"><span class="linenos">1471</span></a>
</span><span id="L-1472"><a href="#L-1472"><span class="linenos">1472</span></a><span class="sd">    Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the unit-wise task embedding and gate function.</span>
</span><span id="L-1473"><a href="#L-1473"><span class="linenos">1473</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-1474"><a href="#L-1474"><span class="linenos">1474</span></a>
</span><span id="L-1475"><a href="#L-1475"><span class="linenos">1475</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-1476"><a href="#L-1476"><span class="linenos">1476</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-1477"><a href="#L-1477"><span class="linenos">1477</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-1478"><a href="#L-1478"><span class="linenos">1478</span></a>        <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-1479"><a href="#L-1479"><span class="linenos">1479</span></a>        <span class="n">gate</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-1480"><a href="#L-1480"><span class="linenos">1480</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="L-1481"><a href="#L-1481"><span class="linenos">1481</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-1482"><a href="#L-1482"><span class="linenos">1482</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-1483"><a href="#L-1483"><span class="linenos">1483</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the ResNet-152 backbone network with task embedding. Note that batch normalisation is incompatible with HAT mechanism.</span>
</span><span id="L-1484"><a href="#L-1484"><span class="linenos">1484</span></a>
</span><span id="L-1485"><a href="#L-1485"><span class="linenos">1485</span></a><span class="sd">        **Args:**</span>
</span><span id="L-1486"><a href="#L-1486"><span class="linenos">1486</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</span>
</span><span id="L-1487"><a href="#L-1487"><span class="linenos">1487</span></a><span class="sd">        - **output_dim** (`int`): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</span>
</span><span id="L-1488"><a href="#L-1488"><span class="linenos">1488</span></a><span class="sd">        - **gate** (`str`): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:</span>
</span><span id="L-1489"><a href="#L-1489"><span class="linenos">1489</span></a><span class="sd">            - `sigmoid`: the sigmoid function.</span>
</span><span id="L-1490"><a href="#L-1490"><span class="linenos">1490</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="L-1491"><a href="#L-1491"><span class="linenos">1491</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`.</span>
</span><span id="L-1492"><a href="#L-1492"><span class="linenos">1492</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-1493"><a href="#L-1493"><span class="linenos">1493</span></a>        <span class="n">HATMaskResNetBase</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-1494"><a href="#L-1494"><span class="linenos">1494</span></a>            <span class="bp">self</span><span class="p">,</span>
</span><span id="L-1495"><a href="#L-1495"><span class="linenos">1495</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
</span><span id="L-1496"><a href="#L-1496"><span class="linenos">1496</span></a>            <span class="n">building_block_type</span><span class="o">=</span><span class="n">HATMaskResNetBlockLarge</span><span class="p">,</span>  <span class="c1"># use the smaller building block for ResNet-152</span>
</span><span id="L-1497"><a href="#L-1497"><span class="linenos">1497</span></a>            <span class="n">building_block_nums</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">36</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
</span><span id="L-1498"><a href="#L-1498"><span class="linenos">1498</span></a>            <span class="n">building_block_preceding_output_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span>
</span><span id="L-1499"><a href="#L-1499"><span class="linenos">1499</span></a>            <span class="n">building_block_input_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
</span><span id="L-1500"><a href="#L-1500"><span class="linenos">1500</span></a>            <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span>
</span><span id="L-1501"><a href="#L-1501"><span class="linenos">1501</span></a>            <span class="n">gate</span><span class="o">=</span><span class="n">gate</span><span class="p">,</span>
</span><span id="L-1502"><a href="#L-1502"><span class="linenos">1502</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="L-1503"><a href="#L-1503"><span class="linenos">1503</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="L-1504"><a href="#L-1504"><span class="linenos">1504</span></a>        <span class="p">)</span>
</span></pre></div>


            </section>
                <section id="ResNetBlockSmall">
                            <input id="ResNetBlockSmall-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">ResNetBlockSmall</span><wbr>(<span class="base">clarena.backbones.base.CLBackbone</span>):

                <label class="view-source-button" for="ResNetBlockSmall-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#ResNetBlockSmall"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ResNetBlockSmall-31"><a href="#ResNetBlockSmall-31"><span class="linenos"> 31</span></a><span class="k">class</span><span class="w"> </span><span class="nc">ResNetBlockSmall</span><span class="p">(</span><span class="n">CLBackbone</span><span class="p">):</span>
</span><span id="ResNetBlockSmall-32"><a href="#ResNetBlockSmall-32"><span class="linenos"> 32</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The smaller building block for ResNet-18/34.</span>
</span><span id="ResNetBlockSmall-33"><a href="#ResNetBlockSmall-33"><span class="linenos"> 33</span></a>
</span><span id="ResNetBlockSmall-34"><a href="#ResNetBlockSmall-34"><span class="linenos"> 34</span></a><span class="sd">    It consists of 2 weight convolutional layers, each followed by an activation function. See Table 1 or Figure 5 (left) in the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html).</span>
</span><span id="ResNetBlockSmall-35"><a href="#ResNetBlockSmall-35"><span class="linenos"> 35</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="ResNetBlockSmall-36"><a href="#ResNetBlockSmall-36"><span class="linenos"> 36</span></a>
</span><span id="ResNetBlockSmall-37"><a href="#ResNetBlockSmall-37"><span class="linenos"> 37</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="ResNetBlockSmall-38"><a href="#ResNetBlockSmall-38"><span class="linenos"> 38</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="ResNetBlockSmall-39"><a href="#ResNetBlockSmall-39"><span class="linenos"> 39</span></a>        <span class="n">outer_layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="ResNetBlockSmall-40"><a href="#ResNetBlockSmall-40"><span class="linenos"> 40</span></a>        <span class="n">block_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNetBlockSmall-41"><a href="#ResNetBlockSmall-41"><span class="linenos"> 41</span></a>        <span class="n">preceding_output_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNetBlockSmall-42"><a href="#ResNetBlockSmall-42"><span class="linenos"> 42</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNetBlockSmall-43"><a href="#ResNetBlockSmall-43"><span class="linenos"> 43</span></a>        <span class="n">overall_stride</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNetBlockSmall-44"><a href="#ResNetBlockSmall-44"><span class="linenos"> 44</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="ResNetBlockSmall-45"><a href="#ResNetBlockSmall-45"><span class="linenos"> 45</span></a>        <span class="n">batch_normalisation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="ResNetBlockSmall-46"><a href="#ResNetBlockSmall-46"><span class="linenos"> 46</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="ResNetBlockSmall-47"><a href="#ResNetBlockSmall-47"><span class="linenos"> 47</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ResNetBlockSmall-48"><a href="#ResNetBlockSmall-48"><span class="linenos"> 48</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the smaller building block.</span>
</span><span id="ResNetBlockSmall-49"><a href="#ResNetBlockSmall-49"><span class="linenos"> 49</span></a>
</span><span id="ResNetBlockSmall-50"><a href="#ResNetBlockSmall-50"><span class="linenos"> 50</span></a><span class="sd">        **Args:**</span>
</span><span id="ResNetBlockSmall-51"><a href="#ResNetBlockSmall-51"><span class="linenos"> 51</span></a><span class="sd">        - **outer_layer_name** (`str`): pass the name of the multi-building-block layer that contains this building block to construct the full name of each weighted convolutional layer.</span>
</span><span id="ResNetBlockSmall-52"><a href="#ResNetBlockSmall-52"><span class="linenos"> 52</span></a><span class="sd">        - **block_idx** (`int`): the index of the building blocks in the multi-building-block layer to construct the full name of each weighted convolutional layer.</span>
</span><span id="ResNetBlockSmall-53"><a href="#ResNetBlockSmall-53"><span class="linenos"> 53</span></a><span class="sd">        - **preceding_output_channels** (`int`): the number of channels of preceding output of this particular building block.</span>
</span><span id="ResNetBlockSmall-54"><a href="#ResNetBlockSmall-54"><span class="linenos"> 54</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input of this building block.</span>
</span><span id="ResNetBlockSmall-55"><a href="#ResNetBlockSmall-55"><span class="linenos"> 55</span></a><span class="sd">        - **overall_stride** (`int`): the overall stride of this building block. This stride is performed at 2nd (last) convolutional layer where the 1st convolutional layer remain stride of 1.</span>
</span><span id="ResNetBlockSmall-56"><a href="#ResNetBlockSmall-56"><span class="linenos"> 56</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="ResNetBlockSmall-57"><a href="#ResNetBlockSmall-57"><span class="linenos"> 57</span></a><span class="sd">        - **batch_normalisation** (`bool`): whether to use batch normalisation after the weight convolutional layers. Default `True`, same as what the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) does.</span>
</span><span id="ResNetBlockSmall-58"><a href="#ResNetBlockSmall-58"><span class="linenos"> 58</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`, because batch normalisation are doing the similar thing with bias.</span>
</span><span id="ResNetBlockSmall-59"><a href="#ResNetBlockSmall-59"><span class="linenos"> 59</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="ResNetBlockSmall-60"><a href="#ResNetBlockSmall-60"><span class="linenos"> 60</span></a>        <span class="n">CLBackbone</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</span><span id="ResNetBlockSmall-61"><a href="#ResNetBlockSmall-61"><span class="linenos"> 61</span></a>
</span><span id="ResNetBlockSmall-62"><a href="#ResNetBlockSmall-62"><span class="linenos"> 62</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">batch_normalisation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">batch_normalisation</span>
</span><span id="ResNetBlockSmall-63"><a href="#ResNetBlockSmall-63"><span class="linenos"> 63</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store whether to use batch normalisation after the fully-connected layers.&quot;&quot;&quot;</span>
</span><span id="ResNetBlockSmall-64"><a href="#ResNetBlockSmall-64"><span class="linenos"> 64</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">activation_layer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span><span id="ResNetBlockSmall-65"><a href="#ResNetBlockSmall-65"><span class="linenos"> 65</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store whether to use activation function after the fully-connected layers.&quot;&quot;&quot;</span>
</span><span id="ResNetBlockSmall-66"><a href="#ResNetBlockSmall-66"><span class="linenos"> 66</span></a>
</span><span id="ResNetBlockSmall-67"><a href="#ResNetBlockSmall-67"><span class="linenos"> 67</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">full_1st_layer_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">outer_layer_name</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">block_idx</span><span class="si">}</span><span class="s2">/conv1&quot;</span>
</span><span id="ResNetBlockSmall-68"><a href="#ResNetBlockSmall-68"><span class="linenos"> 68</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Format and store full name of the 1st weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="ResNetBlockSmall-69"><a href="#ResNetBlockSmall-69"><span class="linenos"> 69</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">full_2nd_layer_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">outer_layer_name</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">block_idx</span><span class="si">}</span><span class="s2">/conv2&quot;</span>
</span><span id="ResNetBlockSmall-70"><a href="#ResNetBlockSmall-70"><span class="linenos"> 70</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Format and store full name of the 2nd weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="ResNetBlockSmall-71"><a href="#ResNetBlockSmall-71"><span class="linenos"> 71</span></a>
</span><span id="ResNetBlockSmall-72"><a href="#ResNetBlockSmall-72"><span class="linenos"> 72</span></a>        <span class="c1"># construct the 1st weighted convolutional layer and attached layers (batchnorm, activation, etc)</span>
</span><span id="ResNetBlockSmall-73"><a href="#ResNetBlockSmall-73"><span class="linenos"> 73</span></a>        <span class="n">layer_input_channels</span> <span class="o">=</span> <span class="n">preceding_output_channels</span>  <span class="c1"># the input channels of the 1st convolutional layer, which receive the output channels of the preceding module</span>
</span><span id="ResNetBlockSmall-74"><a href="#ResNetBlockSmall-74"><span class="linenos"> 74</span></a>        <span class="n">layer_output_channels</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="ResNetBlockSmall-75"><a href="#ResNetBlockSmall-75"><span class="linenos"> 75</span></a>            <span class="n">input_channels</span>  <span class="c1"># the output channels of the 1st convolutional layer</span>
</span><span id="ResNetBlockSmall-76"><a href="#ResNetBlockSmall-76"><span class="linenos"> 76</span></a>        <span class="p">)</span>
</span><span id="ResNetBlockSmall-77"><a href="#ResNetBlockSmall-77"><span class="linenos"> 77</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
</span><span id="ResNetBlockSmall-78"><a href="#ResNetBlockSmall-78"><span class="linenos"> 78</span></a>            <span class="n">in_channels</span><span class="o">=</span><span class="n">layer_input_channels</span><span class="p">,</span>
</span><span id="ResNetBlockSmall-79"><a href="#ResNetBlockSmall-79"><span class="linenos"> 79</span></a>            <span class="n">out_channels</span><span class="o">=</span><span class="n">layer_output_channels</span><span class="p">,</span>
</span><span id="ResNetBlockSmall-80"><a href="#ResNetBlockSmall-80"><span class="linenos"> 80</span></a>            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
</span><span id="ResNetBlockSmall-81"><a href="#ResNetBlockSmall-81"><span class="linenos"> 81</span></a>            <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="ResNetBlockSmall-82"><a href="#ResNetBlockSmall-82"><span class="linenos"> 82</span></a>            <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="ResNetBlockSmall-83"><a href="#ResNetBlockSmall-83"><span class="linenos"> 83</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="ResNetBlockSmall-84"><a href="#ResNetBlockSmall-84"><span class="linenos"> 84</span></a>        <span class="p">)</span>  <span class="c1"># construct the 1st weight convolutional layer of the smaller building block. Overall stride is not performed here</span>
</span><span id="ResNetBlockSmall-85"><a href="#ResNetBlockSmall-85"><span class="linenos"> 85</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The 1st weight convolutional layer of the smaller building block. &quot;&quot;&quot;</span>
</span><span id="ResNetBlockSmall-86"><a href="#ResNetBlockSmall-86"><span class="linenos"> 86</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="ResNetBlockSmall-87"><a href="#ResNetBlockSmall-87"><span class="linenos"> 87</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">full_1st_layer_name</span>
</span><span id="ResNetBlockSmall-88"><a href="#ResNetBlockSmall-88"><span class="linenos"> 88</span></a>        <span class="p">)</span>  <span class="c1"># update the weighted layer names</span>
</span><span id="ResNetBlockSmall-89"><a href="#ResNetBlockSmall-89"><span class="linenos"> 89</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_normalisation</span><span class="p">:</span>
</span><span id="ResNetBlockSmall-90"><a href="#ResNetBlockSmall-90"><span class="linenos"> 90</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv_bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span>
</span><span id="ResNetBlockSmall-91"><a href="#ResNetBlockSmall-91"><span class="linenos"> 91</span></a>                <span class="n">num_features</span><span class="o">=</span><span class="n">layer_output_channels</span>
</span><span id="ResNetBlockSmall-92"><a href="#ResNetBlockSmall-92"><span class="linenos"> 92</span></a>            <span class="p">)</span>  <span class="c1"># construct the batch normalisation layer</span>
</span><span id="ResNetBlockSmall-93"><a href="#ResNetBlockSmall-93"><span class="linenos"> 93</span></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The batch normalisation (`nn.BatchNorm2d`) layer after the 1st weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="ResNetBlockSmall-94"><a href="#ResNetBlockSmall-94"><span class="linenos"> 94</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="ResNetBlockSmall-95"><a href="#ResNetBlockSmall-95"><span class="linenos"> 95</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation1</span> <span class="o">=</span> <span class="n">activation_layer</span><span class="p">()</span>  <span class="c1"># construct the activation layer</span>
</span><span id="ResNetBlockSmall-96"><a href="#ResNetBlockSmall-96"><span class="linenos"> 96</span></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The activation layer after the 1st weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="ResNetBlockSmall-97"><a href="#ResNetBlockSmall-97"><span class="linenos"> 97</span></a>
</span><span id="ResNetBlockSmall-98"><a href="#ResNetBlockSmall-98"><span class="linenos"> 98</span></a>        <span class="c1"># construct the 2nd weighted convolutional layer and attached layers (batchnorm, activation, etc)</span>
</span><span id="ResNetBlockSmall-99"><a href="#ResNetBlockSmall-99"><span class="linenos"> 99</span></a>        <span class="n">layer_input_channels</span> <span class="o">=</span> <span class="n">input_channels</span>  <span class="c1"># the input channels of the 2nd convolutional layer, which is `input_channels`, the same as the output channels of the 1st convolutional layer</span>
</span><span id="ResNetBlockSmall-100"><a href="#ResNetBlockSmall-100"><span class="linenos">100</span></a>        <span class="n">layer_output_channels</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="ResNetBlockSmall-101"><a href="#ResNetBlockSmall-101"><span class="linenos">101</span></a>            <span class="n">input_channels</span> <span class="o">*</span> <span class="mi">1</span>
</span><span id="ResNetBlockSmall-102"><a href="#ResNetBlockSmall-102"><span class="linenos">102</span></a>        <span class="p">)</span>  <span class="c1"># the output channels of the 2nd convolutional layer, which is the same as the input channels (without expansion)</span>
</span><span id="ResNetBlockSmall-103"><a href="#ResNetBlockSmall-103"><span class="linenos">103</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
</span><span id="ResNetBlockSmall-104"><a href="#ResNetBlockSmall-104"><span class="linenos">104</span></a>            <span class="n">in_channels</span><span class="o">=</span><span class="n">layer_input_channels</span><span class="p">,</span>
</span><span id="ResNetBlockSmall-105"><a href="#ResNetBlockSmall-105"><span class="linenos">105</span></a>            <span class="n">out_channels</span><span class="o">=</span><span class="n">layer_output_channels</span><span class="p">,</span>
</span><span id="ResNetBlockSmall-106"><a href="#ResNetBlockSmall-106"><span class="linenos">106</span></a>            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
</span><span id="ResNetBlockSmall-107"><a href="#ResNetBlockSmall-107"><span class="linenos">107</span></a>            <span class="n">stride</span><span class="o">=</span><span class="n">overall_stride</span><span class="p">,</span>
</span><span id="ResNetBlockSmall-108"><a href="#ResNetBlockSmall-108"><span class="linenos">108</span></a>            <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="ResNetBlockSmall-109"><a href="#ResNetBlockSmall-109"><span class="linenos">109</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="ResNetBlockSmall-110"><a href="#ResNetBlockSmall-110"><span class="linenos">110</span></a>        <span class="p">)</span>  <span class="c1"># construct the 2nd weight convolutional layer of the smaller building block. Overall stride is performed here</span>
</span><span id="ResNetBlockSmall-111"><a href="#ResNetBlockSmall-111"><span class="linenos">111</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The 2nd weight convolutional layer of the smaller building block. &quot;&quot;&quot;</span>
</span><span id="ResNetBlockSmall-112"><a href="#ResNetBlockSmall-112"><span class="linenos">112</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="ResNetBlockSmall-113"><a href="#ResNetBlockSmall-113"><span class="linenos">113</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">full_2nd_layer_name</span>
</span><span id="ResNetBlockSmall-114"><a href="#ResNetBlockSmall-114"><span class="linenos">114</span></a>        <span class="p">)</span>  <span class="c1"># update the weighted layer names</span>
</span><span id="ResNetBlockSmall-115"><a href="#ResNetBlockSmall-115"><span class="linenos">115</span></a>        <span class="k">if</span> <span class="n">batch_normalisation</span><span class="p">:</span>
</span><span id="ResNetBlockSmall-116"><a href="#ResNetBlockSmall-116"><span class="linenos">116</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv_bn2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span>
</span><span id="ResNetBlockSmall-117"><a href="#ResNetBlockSmall-117"><span class="linenos">117</span></a>                <span class="n">num_features</span><span class="o">=</span><span class="n">layer_output_channels</span>
</span><span id="ResNetBlockSmall-118"><a href="#ResNetBlockSmall-118"><span class="linenos">118</span></a>            <span class="p">)</span>  <span class="c1"># construct the batch normalisation layer</span>
</span><span id="ResNetBlockSmall-119"><a href="#ResNetBlockSmall-119"><span class="linenos">119</span></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The batch normalisation (`nn.BatchNorm2d`) layer after the 2nd weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="ResNetBlockSmall-120"><a href="#ResNetBlockSmall-120"><span class="linenos">120</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="ResNetBlockSmall-121"><a href="#ResNetBlockSmall-121"><span class="linenos">121</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation2</span> <span class="o">=</span> <span class="n">activation_layer</span><span class="p">()</span>  <span class="c1"># construct the activation layer</span>
</span><span id="ResNetBlockSmall-122"><a href="#ResNetBlockSmall-122"><span class="linenos">122</span></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The activation layer after the 2nd weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="ResNetBlockSmall-123"><a href="#ResNetBlockSmall-123"><span class="linenos">123</span></a>
</span><span id="ResNetBlockSmall-124"><a href="#ResNetBlockSmall-124"><span class="linenos">124</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">identity_downsample</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="ResNetBlockSmall-125"><a href="#ResNetBlockSmall-125"><span class="linenos">125</span></a>            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
</span><span id="ResNetBlockSmall-126"><a href="#ResNetBlockSmall-126"><span class="linenos">126</span></a>                <span class="n">in_channels</span><span class="o">=</span><span class="n">preceding_output_channels</span><span class="p">,</span>
</span><span id="ResNetBlockSmall-127"><a href="#ResNetBlockSmall-127"><span class="linenos">127</span></a>                <span class="n">out_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
</span><span id="ResNetBlockSmall-128"><a href="#ResNetBlockSmall-128"><span class="linenos">128</span></a>                <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="ResNetBlockSmall-129"><a href="#ResNetBlockSmall-129"><span class="linenos">129</span></a>                <span class="n">stride</span><span class="o">=</span><span class="n">overall_stride</span><span class="p">,</span>
</span><span id="ResNetBlockSmall-130"><a href="#ResNetBlockSmall-130"><span class="linenos">130</span></a>                <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="ResNetBlockSmall-131"><a href="#ResNetBlockSmall-131"><span class="linenos">131</span></a>            <span class="p">)</span>
</span><span id="ResNetBlockSmall-132"><a href="#ResNetBlockSmall-132"><span class="linenos">132</span></a>            <span class="k">if</span> <span class="n">preceding_output_channels</span> <span class="o">!=</span> <span class="n">input_channels</span> <span class="ow">or</span> <span class="n">overall_stride</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="ResNetBlockSmall-133"><a href="#ResNetBlockSmall-133"><span class="linenos">133</span></a>            <span class="k">else</span> <span class="kc">None</span>
</span><span id="ResNetBlockSmall-134"><a href="#ResNetBlockSmall-134"><span class="linenos">134</span></a>        <span class="p">)</span>  <span class="c1"># construct the identity downsample function</span>
</span><span id="ResNetBlockSmall-135"><a href="#ResNetBlockSmall-135"><span class="linenos">135</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The convolutional layer for downsampling identity in the shortcut connection if the dimension of identity from input doesn&#39;t match the output&#39;s. This case only happens when the number of input channels doesn&#39;t equal to the number of preceding output channels or a layer with stride &gt; 1 exists. &quot;&quot;&quot;</span>
</span><span id="ResNetBlockSmall-136"><a href="#ResNetBlockSmall-136"><span class="linenos">136</span></a>
</span><span id="ResNetBlockSmall-137"><a href="#ResNetBlockSmall-137"><span class="linenos">137</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
</span><span id="ResNetBlockSmall-138"><a href="#ResNetBlockSmall-138"><span class="linenos">138</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The forward pass for data.</span>
</span><span id="ResNetBlockSmall-139"><a href="#ResNetBlockSmall-139"><span class="linenos">139</span></a>
</span><span id="ResNetBlockSmall-140"><a href="#ResNetBlockSmall-140"><span class="linenos">140</span></a><span class="sd">        **Args:**</span>
</span><span id="ResNetBlockSmall-141"><a href="#ResNetBlockSmall-141"><span class="linenos">141</span></a><span class="sd">        - **input** (`Tensor`): the input feature maps.</span>
</span><span id="ResNetBlockSmall-142"><a href="#ResNetBlockSmall-142"><span class="linenos">142</span></a>
</span><span id="ResNetBlockSmall-143"><a href="#ResNetBlockSmall-143"><span class="linenos">143</span></a><span class="sd">        **Returns:**</span>
</span><span id="ResNetBlockSmall-144"><a href="#ResNetBlockSmall-144"><span class="linenos">144</span></a><span class="sd">        - **output_feature** (`Tensor`): the output feature maps.</span>
</span><span id="ResNetBlockSmall-145"><a href="#ResNetBlockSmall-145"><span class="linenos">145</span></a><span class="sd">        - **hidden_features** (`dict[str, Tensor]`): the hidden features (after activation) in each weighted layer. Key (`str`) is the weighted layer name, value (`Tensor`) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes.</span>
</span><span id="ResNetBlockSmall-146"><a href="#ResNetBlockSmall-146"><span class="linenos">146</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="ResNetBlockSmall-147"><a href="#ResNetBlockSmall-147"><span class="linenos">147</span></a>        <span class="n">hidden_features</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="ResNetBlockSmall-148"><a href="#ResNetBlockSmall-148"><span class="linenos">148</span></a>
</span><span id="ResNetBlockSmall-149"><a href="#ResNetBlockSmall-149"><span class="linenos">149</span></a>        <span class="n">identity</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="ResNetBlockSmall-150"><a href="#ResNetBlockSmall-150"><span class="linenos">150</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">identity_downsample</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span><span id="ResNetBlockSmall-151"><a href="#ResNetBlockSmall-151"><span class="linenos">151</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">identity_downsample</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span><span id="ResNetBlockSmall-152"><a href="#ResNetBlockSmall-152"><span class="linenos">152</span></a>            <span class="k">else</span> <span class="nb">input</span>
</span><span id="ResNetBlockSmall-153"><a href="#ResNetBlockSmall-153"><span class="linenos">153</span></a>        <span class="p">)</span>  <span class="c1"># remember the identity of input for the shortcut connection. Perform downsampling if its dimension doesn&#39;t match the output&#39;s</span>
</span><span id="ResNetBlockSmall-154"><a href="#ResNetBlockSmall-154"><span class="linenos">154</span></a>
</span><span id="ResNetBlockSmall-155"><a href="#ResNetBlockSmall-155"><span class="linenos">155</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="nb">input</span>
</span><span id="ResNetBlockSmall-156"><a href="#ResNetBlockSmall-156"><span class="linenos">156</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNetBlockSmall-157"><a href="#ResNetBlockSmall-157"><span class="linenos">157</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_normalisation</span><span class="p">:</span>
</span><span id="ResNetBlockSmall-158"><a href="#ResNetBlockSmall-158"><span class="linenos">158</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_bn1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNetBlockSmall-159"><a href="#ResNetBlockSmall-159"><span class="linenos">159</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="ResNetBlockSmall-160"><a href="#ResNetBlockSmall-160"><span class="linenos">160</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNetBlockSmall-161"><a href="#ResNetBlockSmall-161"><span class="linenos">161</span></a>        <span class="n">hidden_features</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_1st_layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>  <span class="c1"># store the hidden feature</span>
</span><span id="ResNetBlockSmall-162"><a href="#ResNetBlockSmall-162"><span class="linenos">162</span></a>
</span><span id="ResNetBlockSmall-163"><a href="#ResNetBlockSmall-163"><span class="linenos">163</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNetBlockSmall-164"><a href="#ResNetBlockSmall-164"><span class="linenos">164</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_normalisation</span><span class="p">:</span>
</span><span id="ResNetBlockSmall-165"><a href="#ResNetBlockSmall-165"><span class="linenos">165</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_bn2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNetBlockSmall-166"><a href="#ResNetBlockSmall-166"><span class="linenos">166</span></a>
</span><span id="ResNetBlockSmall-167"><a href="#ResNetBlockSmall-167"><span class="linenos">167</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">identity</span>
</span><span id="ResNetBlockSmall-168"><a href="#ResNetBlockSmall-168"><span class="linenos">168</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="ResNetBlockSmall-169"><a href="#ResNetBlockSmall-169"><span class="linenos">169</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># activation after the shortcut connection</span>
</span><span id="ResNetBlockSmall-170"><a href="#ResNetBlockSmall-170"><span class="linenos">170</span></a>        <span class="n">hidden_features</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_2nd_layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>  <span class="c1"># store the hidden feature</span>
</span><span id="ResNetBlockSmall-171"><a href="#ResNetBlockSmall-171"><span class="linenos">171</span></a>
</span><span id="ResNetBlockSmall-172"><a href="#ResNetBlockSmall-172"><span class="linenos">172</span></a>        <span class="n">output_feature</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="ResNetBlockSmall-173"><a href="#ResNetBlockSmall-173"><span class="linenos">173</span></a>
</span><span id="ResNetBlockSmall-174"><a href="#ResNetBlockSmall-174"><span class="linenos">174</span></a>        <span class="k">return</span> <span class="n">output_feature</span><span class="p">,</span> <span class="n">hidden_features</span>
</span></pre></div>


            <div class="docstring"><p>The smaller building block for ResNet-18/34.</p>

<p>It consists of 2 weight convolutional layers, each followed by an activation function. See Table 1 or Figure 5 (left) in the <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">original ResNet paper</a>.</p>
</div>


                            <div id="ResNetBlockSmall.__init__" class="classattr">
                                        <input id="ResNetBlockSmall.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">ResNetBlockSmall</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">outer_layer_name</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">block_idx</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">preceding_output_channels</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">overall_stride</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	activation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class="param">	<span class="n">batch_normalisation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>,</span><span class="param">	<span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span></span>)</span>

                <label class="view-source-button" for="ResNetBlockSmall.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#ResNetBlockSmall.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ResNetBlockSmall.__init__-37"><a href="#ResNetBlockSmall.__init__-37"><span class="linenos"> 37</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="ResNetBlockSmall.__init__-38"><a href="#ResNetBlockSmall.__init__-38"><span class="linenos"> 38</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="ResNetBlockSmall.__init__-39"><a href="#ResNetBlockSmall.__init__-39"><span class="linenos"> 39</span></a>        <span class="n">outer_layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="ResNetBlockSmall.__init__-40"><a href="#ResNetBlockSmall.__init__-40"><span class="linenos"> 40</span></a>        <span class="n">block_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNetBlockSmall.__init__-41"><a href="#ResNetBlockSmall.__init__-41"><span class="linenos"> 41</span></a>        <span class="n">preceding_output_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNetBlockSmall.__init__-42"><a href="#ResNetBlockSmall.__init__-42"><span class="linenos"> 42</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNetBlockSmall.__init__-43"><a href="#ResNetBlockSmall.__init__-43"><span class="linenos"> 43</span></a>        <span class="n">overall_stride</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNetBlockSmall.__init__-44"><a href="#ResNetBlockSmall.__init__-44"><span class="linenos"> 44</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="ResNetBlockSmall.__init__-45"><a href="#ResNetBlockSmall.__init__-45"><span class="linenos"> 45</span></a>        <span class="n">batch_normalisation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="ResNetBlockSmall.__init__-46"><a href="#ResNetBlockSmall.__init__-46"><span class="linenos"> 46</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="ResNetBlockSmall.__init__-47"><a href="#ResNetBlockSmall.__init__-47"><span class="linenos"> 47</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ResNetBlockSmall.__init__-48"><a href="#ResNetBlockSmall.__init__-48"><span class="linenos"> 48</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the smaller building block.</span>
</span><span id="ResNetBlockSmall.__init__-49"><a href="#ResNetBlockSmall.__init__-49"><span class="linenos"> 49</span></a>
</span><span id="ResNetBlockSmall.__init__-50"><a href="#ResNetBlockSmall.__init__-50"><span class="linenos"> 50</span></a><span class="sd">        **Args:**</span>
</span><span id="ResNetBlockSmall.__init__-51"><a href="#ResNetBlockSmall.__init__-51"><span class="linenos"> 51</span></a><span class="sd">        - **outer_layer_name** (`str`): pass the name of the multi-building-block layer that contains this building block to construct the full name of each weighted convolutional layer.</span>
</span><span id="ResNetBlockSmall.__init__-52"><a href="#ResNetBlockSmall.__init__-52"><span class="linenos"> 52</span></a><span class="sd">        - **block_idx** (`int`): the index of the building blocks in the multi-building-block layer to construct the full name of each weighted convolutional layer.</span>
</span><span id="ResNetBlockSmall.__init__-53"><a href="#ResNetBlockSmall.__init__-53"><span class="linenos"> 53</span></a><span class="sd">        - **preceding_output_channels** (`int`): the number of channels of preceding output of this particular building block.</span>
</span><span id="ResNetBlockSmall.__init__-54"><a href="#ResNetBlockSmall.__init__-54"><span class="linenos"> 54</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input of this building block.</span>
</span><span id="ResNetBlockSmall.__init__-55"><a href="#ResNetBlockSmall.__init__-55"><span class="linenos"> 55</span></a><span class="sd">        - **overall_stride** (`int`): the overall stride of this building block. This stride is performed at 2nd (last) convolutional layer where the 1st convolutional layer remain stride of 1.</span>
</span><span id="ResNetBlockSmall.__init__-56"><a href="#ResNetBlockSmall.__init__-56"><span class="linenos"> 56</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="ResNetBlockSmall.__init__-57"><a href="#ResNetBlockSmall.__init__-57"><span class="linenos"> 57</span></a><span class="sd">        - **batch_normalisation** (`bool`): whether to use batch normalisation after the weight convolutional layers. Default `True`, same as what the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) does.</span>
</span><span id="ResNetBlockSmall.__init__-58"><a href="#ResNetBlockSmall.__init__-58"><span class="linenos"> 58</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`, because batch normalisation are doing the similar thing with bias.</span>
</span><span id="ResNetBlockSmall.__init__-59"><a href="#ResNetBlockSmall.__init__-59"><span class="linenos"> 59</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="ResNetBlockSmall.__init__-60"><a href="#ResNetBlockSmall.__init__-60"><span class="linenos"> 60</span></a>        <span class="n">CLBackbone</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</span><span id="ResNetBlockSmall.__init__-61"><a href="#ResNetBlockSmall.__init__-61"><span class="linenos"> 61</span></a>
</span><span id="ResNetBlockSmall.__init__-62"><a href="#ResNetBlockSmall.__init__-62"><span class="linenos"> 62</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">batch_normalisation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">batch_normalisation</span>
</span><span id="ResNetBlockSmall.__init__-63"><a href="#ResNetBlockSmall.__init__-63"><span class="linenos"> 63</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store whether to use batch normalisation after the fully-connected layers.&quot;&quot;&quot;</span>
</span><span id="ResNetBlockSmall.__init__-64"><a href="#ResNetBlockSmall.__init__-64"><span class="linenos"> 64</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">activation_layer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span><span id="ResNetBlockSmall.__init__-65"><a href="#ResNetBlockSmall.__init__-65"><span class="linenos"> 65</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store whether to use activation function after the fully-connected layers.&quot;&quot;&quot;</span>
</span><span id="ResNetBlockSmall.__init__-66"><a href="#ResNetBlockSmall.__init__-66"><span class="linenos"> 66</span></a>
</span><span id="ResNetBlockSmall.__init__-67"><a href="#ResNetBlockSmall.__init__-67"><span class="linenos"> 67</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">full_1st_layer_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">outer_layer_name</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">block_idx</span><span class="si">}</span><span class="s2">/conv1&quot;</span>
</span><span id="ResNetBlockSmall.__init__-68"><a href="#ResNetBlockSmall.__init__-68"><span class="linenos"> 68</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Format and store full name of the 1st weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="ResNetBlockSmall.__init__-69"><a href="#ResNetBlockSmall.__init__-69"><span class="linenos"> 69</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">full_2nd_layer_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">outer_layer_name</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">block_idx</span><span class="si">}</span><span class="s2">/conv2&quot;</span>
</span><span id="ResNetBlockSmall.__init__-70"><a href="#ResNetBlockSmall.__init__-70"><span class="linenos"> 70</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Format and store full name of the 2nd weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="ResNetBlockSmall.__init__-71"><a href="#ResNetBlockSmall.__init__-71"><span class="linenos"> 71</span></a>
</span><span id="ResNetBlockSmall.__init__-72"><a href="#ResNetBlockSmall.__init__-72"><span class="linenos"> 72</span></a>        <span class="c1"># construct the 1st weighted convolutional layer and attached layers (batchnorm, activation, etc)</span>
</span><span id="ResNetBlockSmall.__init__-73"><a href="#ResNetBlockSmall.__init__-73"><span class="linenos"> 73</span></a>        <span class="n">layer_input_channels</span> <span class="o">=</span> <span class="n">preceding_output_channels</span>  <span class="c1"># the input channels of the 1st convolutional layer, which receive the output channels of the preceding module</span>
</span><span id="ResNetBlockSmall.__init__-74"><a href="#ResNetBlockSmall.__init__-74"><span class="linenos"> 74</span></a>        <span class="n">layer_output_channels</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="ResNetBlockSmall.__init__-75"><a href="#ResNetBlockSmall.__init__-75"><span class="linenos"> 75</span></a>            <span class="n">input_channels</span>  <span class="c1"># the output channels of the 1st convolutional layer</span>
</span><span id="ResNetBlockSmall.__init__-76"><a href="#ResNetBlockSmall.__init__-76"><span class="linenos"> 76</span></a>        <span class="p">)</span>
</span><span id="ResNetBlockSmall.__init__-77"><a href="#ResNetBlockSmall.__init__-77"><span class="linenos"> 77</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
</span><span id="ResNetBlockSmall.__init__-78"><a href="#ResNetBlockSmall.__init__-78"><span class="linenos"> 78</span></a>            <span class="n">in_channels</span><span class="o">=</span><span class="n">layer_input_channels</span><span class="p">,</span>
</span><span id="ResNetBlockSmall.__init__-79"><a href="#ResNetBlockSmall.__init__-79"><span class="linenos"> 79</span></a>            <span class="n">out_channels</span><span class="o">=</span><span class="n">layer_output_channels</span><span class="p">,</span>
</span><span id="ResNetBlockSmall.__init__-80"><a href="#ResNetBlockSmall.__init__-80"><span class="linenos"> 80</span></a>            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
</span><span id="ResNetBlockSmall.__init__-81"><a href="#ResNetBlockSmall.__init__-81"><span class="linenos"> 81</span></a>            <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="ResNetBlockSmall.__init__-82"><a href="#ResNetBlockSmall.__init__-82"><span class="linenos"> 82</span></a>            <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="ResNetBlockSmall.__init__-83"><a href="#ResNetBlockSmall.__init__-83"><span class="linenos"> 83</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="ResNetBlockSmall.__init__-84"><a href="#ResNetBlockSmall.__init__-84"><span class="linenos"> 84</span></a>        <span class="p">)</span>  <span class="c1"># construct the 1st weight convolutional layer of the smaller building block. Overall stride is not performed here</span>
</span><span id="ResNetBlockSmall.__init__-85"><a href="#ResNetBlockSmall.__init__-85"><span class="linenos"> 85</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The 1st weight convolutional layer of the smaller building block. &quot;&quot;&quot;</span>
</span><span id="ResNetBlockSmall.__init__-86"><a href="#ResNetBlockSmall.__init__-86"><span class="linenos"> 86</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="ResNetBlockSmall.__init__-87"><a href="#ResNetBlockSmall.__init__-87"><span class="linenos"> 87</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">full_1st_layer_name</span>
</span><span id="ResNetBlockSmall.__init__-88"><a href="#ResNetBlockSmall.__init__-88"><span class="linenos"> 88</span></a>        <span class="p">)</span>  <span class="c1"># update the weighted layer names</span>
</span><span id="ResNetBlockSmall.__init__-89"><a href="#ResNetBlockSmall.__init__-89"><span class="linenos"> 89</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_normalisation</span><span class="p">:</span>
</span><span id="ResNetBlockSmall.__init__-90"><a href="#ResNetBlockSmall.__init__-90"><span class="linenos"> 90</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv_bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span>
</span><span id="ResNetBlockSmall.__init__-91"><a href="#ResNetBlockSmall.__init__-91"><span class="linenos"> 91</span></a>                <span class="n">num_features</span><span class="o">=</span><span class="n">layer_output_channels</span>
</span><span id="ResNetBlockSmall.__init__-92"><a href="#ResNetBlockSmall.__init__-92"><span class="linenos"> 92</span></a>            <span class="p">)</span>  <span class="c1"># construct the batch normalisation layer</span>
</span><span id="ResNetBlockSmall.__init__-93"><a href="#ResNetBlockSmall.__init__-93"><span class="linenos"> 93</span></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The batch normalisation (`nn.BatchNorm2d`) layer after the 1st weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="ResNetBlockSmall.__init__-94"><a href="#ResNetBlockSmall.__init__-94"><span class="linenos"> 94</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="ResNetBlockSmall.__init__-95"><a href="#ResNetBlockSmall.__init__-95"><span class="linenos"> 95</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation1</span> <span class="o">=</span> <span class="n">activation_layer</span><span class="p">()</span>  <span class="c1"># construct the activation layer</span>
</span><span id="ResNetBlockSmall.__init__-96"><a href="#ResNetBlockSmall.__init__-96"><span class="linenos"> 96</span></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The activation layer after the 1st weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="ResNetBlockSmall.__init__-97"><a href="#ResNetBlockSmall.__init__-97"><span class="linenos"> 97</span></a>
</span><span id="ResNetBlockSmall.__init__-98"><a href="#ResNetBlockSmall.__init__-98"><span class="linenos"> 98</span></a>        <span class="c1"># construct the 2nd weighted convolutional layer and attached layers (batchnorm, activation, etc)</span>
</span><span id="ResNetBlockSmall.__init__-99"><a href="#ResNetBlockSmall.__init__-99"><span class="linenos"> 99</span></a>        <span class="n">layer_input_channels</span> <span class="o">=</span> <span class="n">input_channels</span>  <span class="c1"># the input channels of the 2nd convolutional layer, which is `input_channels`, the same as the output channels of the 1st convolutional layer</span>
</span><span id="ResNetBlockSmall.__init__-100"><a href="#ResNetBlockSmall.__init__-100"><span class="linenos">100</span></a>        <span class="n">layer_output_channels</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="ResNetBlockSmall.__init__-101"><a href="#ResNetBlockSmall.__init__-101"><span class="linenos">101</span></a>            <span class="n">input_channels</span> <span class="o">*</span> <span class="mi">1</span>
</span><span id="ResNetBlockSmall.__init__-102"><a href="#ResNetBlockSmall.__init__-102"><span class="linenos">102</span></a>        <span class="p">)</span>  <span class="c1"># the output channels of the 2nd convolutional layer, which is the same as the input channels (without expansion)</span>
</span><span id="ResNetBlockSmall.__init__-103"><a href="#ResNetBlockSmall.__init__-103"><span class="linenos">103</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
</span><span id="ResNetBlockSmall.__init__-104"><a href="#ResNetBlockSmall.__init__-104"><span class="linenos">104</span></a>            <span class="n">in_channels</span><span class="o">=</span><span class="n">layer_input_channels</span><span class="p">,</span>
</span><span id="ResNetBlockSmall.__init__-105"><a href="#ResNetBlockSmall.__init__-105"><span class="linenos">105</span></a>            <span class="n">out_channels</span><span class="o">=</span><span class="n">layer_output_channels</span><span class="p">,</span>
</span><span id="ResNetBlockSmall.__init__-106"><a href="#ResNetBlockSmall.__init__-106"><span class="linenos">106</span></a>            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
</span><span id="ResNetBlockSmall.__init__-107"><a href="#ResNetBlockSmall.__init__-107"><span class="linenos">107</span></a>            <span class="n">stride</span><span class="o">=</span><span class="n">overall_stride</span><span class="p">,</span>
</span><span id="ResNetBlockSmall.__init__-108"><a href="#ResNetBlockSmall.__init__-108"><span class="linenos">108</span></a>            <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="ResNetBlockSmall.__init__-109"><a href="#ResNetBlockSmall.__init__-109"><span class="linenos">109</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="ResNetBlockSmall.__init__-110"><a href="#ResNetBlockSmall.__init__-110"><span class="linenos">110</span></a>        <span class="p">)</span>  <span class="c1"># construct the 2nd weight convolutional layer of the smaller building block. Overall stride is performed here</span>
</span><span id="ResNetBlockSmall.__init__-111"><a href="#ResNetBlockSmall.__init__-111"><span class="linenos">111</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The 2nd weight convolutional layer of the smaller building block. &quot;&quot;&quot;</span>
</span><span id="ResNetBlockSmall.__init__-112"><a href="#ResNetBlockSmall.__init__-112"><span class="linenos">112</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="ResNetBlockSmall.__init__-113"><a href="#ResNetBlockSmall.__init__-113"><span class="linenos">113</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">full_2nd_layer_name</span>
</span><span id="ResNetBlockSmall.__init__-114"><a href="#ResNetBlockSmall.__init__-114"><span class="linenos">114</span></a>        <span class="p">)</span>  <span class="c1"># update the weighted layer names</span>
</span><span id="ResNetBlockSmall.__init__-115"><a href="#ResNetBlockSmall.__init__-115"><span class="linenos">115</span></a>        <span class="k">if</span> <span class="n">batch_normalisation</span><span class="p">:</span>
</span><span id="ResNetBlockSmall.__init__-116"><a href="#ResNetBlockSmall.__init__-116"><span class="linenos">116</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv_bn2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span>
</span><span id="ResNetBlockSmall.__init__-117"><a href="#ResNetBlockSmall.__init__-117"><span class="linenos">117</span></a>                <span class="n">num_features</span><span class="o">=</span><span class="n">layer_output_channels</span>
</span><span id="ResNetBlockSmall.__init__-118"><a href="#ResNetBlockSmall.__init__-118"><span class="linenos">118</span></a>            <span class="p">)</span>  <span class="c1"># construct the batch normalisation layer</span>
</span><span id="ResNetBlockSmall.__init__-119"><a href="#ResNetBlockSmall.__init__-119"><span class="linenos">119</span></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The batch normalisation (`nn.BatchNorm2d`) layer after the 2nd weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="ResNetBlockSmall.__init__-120"><a href="#ResNetBlockSmall.__init__-120"><span class="linenos">120</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="ResNetBlockSmall.__init__-121"><a href="#ResNetBlockSmall.__init__-121"><span class="linenos">121</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation2</span> <span class="o">=</span> <span class="n">activation_layer</span><span class="p">()</span>  <span class="c1"># construct the activation layer</span>
</span><span id="ResNetBlockSmall.__init__-122"><a href="#ResNetBlockSmall.__init__-122"><span class="linenos">122</span></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The activation layer after the 2nd weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="ResNetBlockSmall.__init__-123"><a href="#ResNetBlockSmall.__init__-123"><span class="linenos">123</span></a>
</span><span id="ResNetBlockSmall.__init__-124"><a href="#ResNetBlockSmall.__init__-124"><span class="linenos">124</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">identity_downsample</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="ResNetBlockSmall.__init__-125"><a href="#ResNetBlockSmall.__init__-125"><span class="linenos">125</span></a>            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
</span><span id="ResNetBlockSmall.__init__-126"><a href="#ResNetBlockSmall.__init__-126"><span class="linenos">126</span></a>                <span class="n">in_channels</span><span class="o">=</span><span class="n">preceding_output_channels</span><span class="p">,</span>
</span><span id="ResNetBlockSmall.__init__-127"><a href="#ResNetBlockSmall.__init__-127"><span class="linenos">127</span></a>                <span class="n">out_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
</span><span id="ResNetBlockSmall.__init__-128"><a href="#ResNetBlockSmall.__init__-128"><span class="linenos">128</span></a>                <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="ResNetBlockSmall.__init__-129"><a href="#ResNetBlockSmall.__init__-129"><span class="linenos">129</span></a>                <span class="n">stride</span><span class="o">=</span><span class="n">overall_stride</span><span class="p">,</span>
</span><span id="ResNetBlockSmall.__init__-130"><a href="#ResNetBlockSmall.__init__-130"><span class="linenos">130</span></a>                <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="ResNetBlockSmall.__init__-131"><a href="#ResNetBlockSmall.__init__-131"><span class="linenos">131</span></a>            <span class="p">)</span>
</span><span id="ResNetBlockSmall.__init__-132"><a href="#ResNetBlockSmall.__init__-132"><span class="linenos">132</span></a>            <span class="k">if</span> <span class="n">preceding_output_channels</span> <span class="o">!=</span> <span class="n">input_channels</span> <span class="ow">or</span> <span class="n">overall_stride</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="ResNetBlockSmall.__init__-133"><a href="#ResNetBlockSmall.__init__-133"><span class="linenos">133</span></a>            <span class="k">else</span> <span class="kc">None</span>
</span><span id="ResNetBlockSmall.__init__-134"><a href="#ResNetBlockSmall.__init__-134"><span class="linenos">134</span></a>        <span class="p">)</span>  <span class="c1"># construct the identity downsample function</span>
</span><span id="ResNetBlockSmall.__init__-135"><a href="#ResNetBlockSmall.__init__-135"><span class="linenos">135</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The convolutional layer for downsampling identity in the shortcut connection if the dimension of identity from input doesn&#39;t match the output&#39;s. This case only happens when the number of input channels doesn&#39;t equal to the number of preceding output channels or a layer with stride &gt; 1 exists. &quot;&quot;&quot;</span>
</span></pre></div>


            <div class="docstring"><p>Construct and initialise the smaller building block.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>outer_layer_name</strong> (<code>str</code>): pass the name of the multi-building-block layer that contains this building block to construct the full name of each weighted convolutional layer.</li>
<li><strong>block_idx</strong> (<code>int</code>): the index of the building blocks in the multi-building-block layer to construct the full name of each weighted convolutional layer.</li>
<li><strong>preceding_output_channels</strong> (<code>int</code>): the number of channels of preceding output of this particular building block.</li>
<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block.</li>
<li><strong>overall_stride</strong> (<code>int</code>): the overall stride of this building block. This stride is performed at 2nd (last) convolutional layer where the 1st convolutional layer remain stride of 1.</li>
<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>
<li><strong>batch_normalisation</strong> (<code>bool</code>): whether to use batch normalisation after the weight convolutional layers. Default <code>True</code>, same as what the <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">original ResNet paper</a> does.</li>
<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>, because batch normalisation are doing the similar thing with bias.</li>
</ul>
</div>


                            </div>
                            <div id="ResNetBlockSmall.batch_normalisation" class="classattr">
                                <div class="attr variable">
            <span class="name">batch_normalisation</span><span class="annotation">: bool</span>

        
    </div>
    <a class="headerlink" href="#ResNetBlockSmall.batch_normalisation"></a>
    
            <div class="docstring"><p>Store whether to use batch normalisation after the fully-connected layers.</p>
</div>


                            </div>
                            <div id="ResNetBlockSmall.activation" class="classattr">
                                <div class="attr variable">
            <span class="name">activation</span><span class="annotation">: bool</span>

        
    </div>
    <a class="headerlink" href="#ResNetBlockSmall.activation"></a>
    
            <div class="docstring"><p>Store whether to use activation function after the fully-connected layers.</p>
</div>


                            </div>
                            <div id="ResNetBlockSmall.full_1st_layer_name" class="classattr">
                                <div class="attr variable">
            <span class="name">full_1st_layer_name</span>

        
    </div>
    <a class="headerlink" href="#ResNetBlockSmall.full_1st_layer_name"></a>
    
            <div class="docstring"><p>Format and store full name of the 1st weighted convolutional layer.</p>
</div>


                            </div>
                            <div id="ResNetBlockSmall.full_2nd_layer_name" class="classattr">
                                <div class="attr variable">
            <span class="name">full_2nd_layer_name</span>

        
    </div>
    <a class="headerlink" href="#ResNetBlockSmall.full_2nd_layer_name"></a>
    
            <div class="docstring"><p>Format and store full name of the 2nd weighted convolutional layer.</p>
</div>


                            </div>
                            <div id="ResNetBlockSmall.conv1" class="classattr">
                                <div class="attr variable">
            <span class="name">conv1</span>

        
    </div>
    <a class="headerlink" href="#ResNetBlockSmall.conv1"></a>
    
            <div class="docstring"><p>The 1st weight convolutional layer of the smaller building block.</p>
</div>


                            </div>
                            <div id="ResNetBlockSmall.conv2" class="classattr">
                                <div class="attr variable">
            <span class="name">conv2</span>

        
    </div>
    <a class="headerlink" href="#ResNetBlockSmall.conv2"></a>
    
            <div class="docstring"><p>The 2nd weight convolutional layer of the smaller building block.</p>
</div>


                            </div>
                            <div id="ResNetBlockSmall.identity_downsample" class="classattr">
                                <div class="attr variable">
            <span class="name">identity_downsample</span><span class="annotation">: torch.nn.modules.module.Module</span>

        
    </div>
    <a class="headerlink" href="#ResNetBlockSmall.identity_downsample"></a>
    
            <div class="docstring"><p>The convolutional layer for downsampling identity in the shortcut connection if the dimension of identity from input doesn't match the output's. This case only happens when the number of input channels doesn't equal to the number of preceding output channels or a layer with stride &gt; 1 exists.</p>
</div>


                            </div>
                            <div id="ResNetBlockSmall.forward" class="classattr">
                                        <input id="ResNetBlockSmall.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></span><span class="return-annotation">) -> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span>:</span></span>

                <label class="view-source-button" for="ResNetBlockSmall.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#ResNetBlockSmall.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ResNetBlockSmall.forward-137"><a href="#ResNetBlockSmall.forward-137"><span class="linenos">137</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
</span><span id="ResNetBlockSmall.forward-138"><a href="#ResNetBlockSmall.forward-138"><span class="linenos">138</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The forward pass for data.</span>
</span><span id="ResNetBlockSmall.forward-139"><a href="#ResNetBlockSmall.forward-139"><span class="linenos">139</span></a>
</span><span id="ResNetBlockSmall.forward-140"><a href="#ResNetBlockSmall.forward-140"><span class="linenos">140</span></a><span class="sd">        **Args:**</span>
</span><span id="ResNetBlockSmall.forward-141"><a href="#ResNetBlockSmall.forward-141"><span class="linenos">141</span></a><span class="sd">        - **input** (`Tensor`): the input feature maps.</span>
</span><span id="ResNetBlockSmall.forward-142"><a href="#ResNetBlockSmall.forward-142"><span class="linenos">142</span></a>
</span><span id="ResNetBlockSmall.forward-143"><a href="#ResNetBlockSmall.forward-143"><span class="linenos">143</span></a><span class="sd">        **Returns:**</span>
</span><span id="ResNetBlockSmall.forward-144"><a href="#ResNetBlockSmall.forward-144"><span class="linenos">144</span></a><span class="sd">        - **output_feature** (`Tensor`): the output feature maps.</span>
</span><span id="ResNetBlockSmall.forward-145"><a href="#ResNetBlockSmall.forward-145"><span class="linenos">145</span></a><span class="sd">        - **hidden_features** (`dict[str, Tensor]`): the hidden features (after activation) in each weighted layer. Key (`str`) is the weighted layer name, value (`Tensor`) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes.</span>
</span><span id="ResNetBlockSmall.forward-146"><a href="#ResNetBlockSmall.forward-146"><span class="linenos">146</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="ResNetBlockSmall.forward-147"><a href="#ResNetBlockSmall.forward-147"><span class="linenos">147</span></a>        <span class="n">hidden_features</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="ResNetBlockSmall.forward-148"><a href="#ResNetBlockSmall.forward-148"><span class="linenos">148</span></a>
</span><span id="ResNetBlockSmall.forward-149"><a href="#ResNetBlockSmall.forward-149"><span class="linenos">149</span></a>        <span class="n">identity</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="ResNetBlockSmall.forward-150"><a href="#ResNetBlockSmall.forward-150"><span class="linenos">150</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">identity_downsample</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span><span id="ResNetBlockSmall.forward-151"><a href="#ResNetBlockSmall.forward-151"><span class="linenos">151</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">identity_downsample</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span><span id="ResNetBlockSmall.forward-152"><a href="#ResNetBlockSmall.forward-152"><span class="linenos">152</span></a>            <span class="k">else</span> <span class="nb">input</span>
</span><span id="ResNetBlockSmall.forward-153"><a href="#ResNetBlockSmall.forward-153"><span class="linenos">153</span></a>        <span class="p">)</span>  <span class="c1"># remember the identity of input for the shortcut connection. Perform downsampling if its dimension doesn&#39;t match the output&#39;s</span>
</span><span id="ResNetBlockSmall.forward-154"><a href="#ResNetBlockSmall.forward-154"><span class="linenos">154</span></a>
</span><span id="ResNetBlockSmall.forward-155"><a href="#ResNetBlockSmall.forward-155"><span class="linenos">155</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="nb">input</span>
</span><span id="ResNetBlockSmall.forward-156"><a href="#ResNetBlockSmall.forward-156"><span class="linenos">156</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNetBlockSmall.forward-157"><a href="#ResNetBlockSmall.forward-157"><span class="linenos">157</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_normalisation</span><span class="p">:</span>
</span><span id="ResNetBlockSmall.forward-158"><a href="#ResNetBlockSmall.forward-158"><span class="linenos">158</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_bn1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNetBlockSmall.forward-159"><a href="#ResNetBlockSmall.forward-159"><span class="linenos">159</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="ResNetBlockSmall.forward-160"><a href="#ResNetBlockSmall.forward-160"><span class="linenos">160</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNetBlockSmall.forward-161"><a href="#ResNetBlockSmall.forward-161"><span class="linenos">161</span></a>        <span class="n">hidden_features</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_1st_layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>  <span class="c1"># store the hidden feature</span>
</span><span id="ResNetBlockSmall.forward-162"><a href="#ResNetBlockSmall.forward-162"><span class="linenos">162</span></a>
</span><span id="ResNetBlockSmall.forward-163"><a href="#ResNetBlockSmall.forward-163"><span class="linenos">163</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNetBlockSmall.forward-164"><a href="#ResNetBlockSmall.forward-164"><span class="linenos">164</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_normalisation</span><span class="p">:</span>
</span><span id="ResNetBlockSmall.forward-165"><a href="#ResNetBlockSmall.forward-165"><span class="linenos">165</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_bn2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNetBlockSmall.forward-166"><a href="#ResNetBlockSmall.forward-166"><span class="linenos">166</span></a>
</span><span id="ResNetBlockSmall.forward-167"><a href="#ResNetBlockSmall.forward-167"><span class="linenos">167</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">identity</span>
</span><span id="ResNetBlockSmall.forward-168"><a href="#ResNetBlockSmall.forward-168"><span class="linenos">168</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="ResNetBlockSmall.forward-169"><a href="#ResNetBlockSmall.forward-169"><span class="linenos">169</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># activation after the shortcut connection</span>
</span><span id="ResNetBlockSmall.forward-170"><a href="#ResNetBlockSmall.forward-170"><span class="linenos">170</span></a>        <span class="n">hidden_features</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_2nd_layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>  <span class="c1"># store the hidden feature</span>
</span><span id="ResNetBlockSmall.forward-171"><a href="#ResNetBlockSmall.forward-171"><span class="linenos">171</span></a>
</span><span id="ResNetBlockSmall.forward-172"><a href="#ResNetBlockSmall.forward-172"><span class="linenos">172</span></a>        <span class="n">output_feature</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="ResNetBlockSmall.forward-173"><a href="#ResNetBlockSmall.forward-173"><span class="linenos">173</span></a>
</span><span id="ResNetBlockSmall.forward-174"><a href="#ResNetBlockSmall.forward-174"><span class="linenos">174</span></a>        <span class="k">return</span> <span class="n">output_feature</span><span class="p">,</span> <span class="n">hidden_features</span>
</span></pre></div>


            <div class="docstring"><p>The forward pass for data.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>input</strong> (<code>Tensor</code>): the input feature maps.</li>
</ul>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>output_feature</strong> (<code>Tensor</code>): the output feature maps.</li>
<li><strong>hidden_features</strong> (<code>dict[str, Tensor]</code>): the hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name, value (<code>Tensor</code>) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes.</li>
</ul>
</div>


                            </div>
                </section>
                <section id="ResNetBlockLarge">
                            <input id="ResNetBlockLarge-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">ResNetBlockLarge</span><wbr>(<span class="base">clarena.backbones.base.CLBackbone</span>):

                <label class="view-source-button" for="ResNetBlockLarge-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#ResNetBlockLarge"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ResNetBlockLarge-177"><a href="#ResNetBlockLarge-177"><span class="linenos">177</span></a><span class="k">class</span><span class="w"> </span><span class="nc">ResNetBlockLarge</span><span class="p">(</span><span class="n">CLBackbone</span><span class="p">):</span>
</span><span id="ResNetBlockLarge-178"><a href="#ResNetBlockLarge-178"><span class="linenos">178</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The larger building block for ResNet-50/101/152. It is referred to &quot;bottleneck&quot; building block in the paper.</span>
</span><span id="ResNetBlockLarge-179"><a href="#ResNetBlockLarge-179"><span class="linenos">179</span></a>
</span><span id="ResNetBlockLarge-180"><a href="#ResNetBlockLarge-180"><span class="linenos">180</span></a><span class="sd">    It consists of 3 weight convolutional layers, each followed by an activation function. See Table 1 or Figure 5 (right) in the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html).</span>
</span><span id="ResNetBlockLarge-181"><a href="#ResNetBlockLarge-181"><span class="linenos">181</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="ResNetBlockLarge-182"><a href="#ResNetBlockLarge-182"><span class="linenos">182</span></a>
</span><span id="ResNetBlockLarge-183"><a href="#ResNetBlockLarge-183"><span class="linenos">183</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="ResNetBlockLarge-184"><a href="#ResNetBlockLarge-184"><span class="linenos">184</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="ResNetBlockLarge-185"><a href="#ResNetBlockLarge-185"><span class="linenos">185</span></a>        <span class="n">outer_layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="ResNetBlockLarge-186"><a href="#ResNetBlockLarge-186"><span class="linenos">186</span></a>        <span class="n">block_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNetBlockLarge-187"><a href="#ResNetBlockLarge-187"><span class="linenos">187</span></a>        <span class="n">preceding_output_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNetBlockLarge-188"><a href="#ResNetBlockLarge-188"><span class="linenos">188</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNetBlockLarge-189"><a href="#ResNetBlockLarge-189"><span class="linenos">189</span></a>        <span class="n">overall_stride</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNetBlockLarge-190"><a href="#ResNetBlockLarge-190"><span class="linenos">190</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="ResNetBlockLarge-191"><a href="#ResNetBlockLarge-191"><span class="linenos">191</span></a>        <span class="n">batch_normalisation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="ResNetBlockLarge-192"><a href="#ResNetBlockLarge-192"><span class="linenos">192</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="ResNetBlockLarge-193"><a href="#ResNetBlockLarge-193"><span class="linenos">193</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ResNetBlockLarge-194"><a href="#ResNetBlockLarge-194"><span class="linenos">194</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the larger building block.</span>
</span><span id="ResNetBlockLarge-195"><a href="#ResNetBlockLarge-195"><span class="linenos">195</span></a>
</span><span id="ResNetBlockLarge-196"><a href="#ResNetBlockLarge-196"><span class="linenos">196</span></a><span class="sd">        **Args:**</span>
</span><span id="ResNetBlockLarge-197"><a href="#ResNetBlockLarge-197"><span class="linenos">197</span></a><span class="sd">        - **outer_layer_name** (`str`): pass the name of the multi-building-block layer that contains this building block to construct the full name of each weighted convolutional layer.</span>
</span><span id="ResNetBlockLarge-198"><a href="#ResNetBlockLarge-198"><span class="linenos">198</span></a><span class="sd">        - **block_idx** (`int`): the index of the building blocks in the multi-building-block layer to construct the full name of each weighted convolutional layer.</span>
</span><span id="ResNetBlockLarge-199"><a href="#ResNetBlockLarge-199"><span class="linenos">199</span></a><span class="sd">        - **preceding_output_channels** (`int`): the number of channels of preceding output of this particular building block.</span>
</span><span id="ResNetBlockLarge-200"><a href="#ResNetBlockLarge-200"><span class="linenos">200</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input of this building block.</span>
</span><span id="ResNetBlockLarge-201"><a href="#ResNetBlockLarge-201"><span class="linenos">201</span></a><span class="sd">        - **overall_stride** (`int`): the overall stride of this building block. This stride is performed at 2nd (middle) convolutional layer where 1st and 3rd convolutional layers remain stride of 1.</span>
</span><span id="ResNetBlockLarge-202"><a href="#ResNetBlockLarge-202"><span class="linenos">202</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="ResNetBlockLarge-203"><a href="#ResNetBlockLarge-203"><span class="linenos">203</span></a><span class="sd">        - **batch_normalisation** (`bool`): whether to use batch normalisation after the weight convolutional layers. Default `True`, same as what the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) does.</span>
</span><span id="ResNetBlockLarge-204"><a href="#ResNetBlockLarge-204"><span class="linenos">204</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`, because batch normalisation are doing the similar thing with bias.</span>
</span><span id="ResNetBlockLarge-205"><a href="#ResNetBlockLarge-205"><span class="linenos">205</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="ResNetBlockLarge-206"><a href="#ResNetBlockLarge-206"><span class="linenos">206</span></a>        <span class="n">CLBackbone</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</span><span id="ResNetBlockLarge-207"><a href="#ResNetBlockLarge-207"><span class="linenos">207</span></a>
</span><span id="ResNetBlockLarge-208"><a href="#ResNetBlockLarge-208"><span class="linenos">208</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">batch_normalisation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">batch_normalisation</span>
</span><span id="ResNetBlockLarge-209"><a href="#ResNetBlockLarge-209"><span class="linenos">209</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store whether to use batch normalisation after the fully-connected layers.&quot;&quot;&quot;</span>
</span><span id="ResNetBlockLarge-210"><a href="#ResNetBlockLarge-210"><span class="linenos">210</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">activation_layer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span><span id="ResNetBlockLarge-211"><a href="#ResNetBlockLarge-211"><span class="linenos">211</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store whether to use activation function after the fully-connected layers.&quot;&quot;&quot;</span>
</span><span id="ResNetBlockLarge-212"><a href="#ResNetBlockLarge-212"><span class="linenos">212</span></a>
</span><span id="ResNetBlockLarge-213"><a href="#ResNetBlockLarge-213"><span class="linenos">213</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">full_1st_layer_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">outer_layer_name</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">block_idx</span><span class="si">}</span><span class="s2">/conv1&quot;</span>
</span><span id="ResNetBlockLarge-214"><a href="#ResNetBlockLarge-214"><span class="linenos">214</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Format and store full name of the 1st weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="ResNetBlockLarge-215"><a href="#ResNetBlockLarge-215"><span class="linenos">215</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">full_2nd_layer_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">outer_layer_name</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">block_idx</span><span class="si">}</span><span class="s2">_conv2&quot;</span>
</span><span id="ResNetBlockLarge-216"><a href="#ResNetBlockLarge-216"><span class="linenos">216</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Format and store full name of the 2nd weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="ResNetBlockLarge-217"><a href="#ResNetBlockLarge-217"><span class="linenos">217</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">full_3rd_layer_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">outer_layer_name</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">block_idx</span><span class="si">}</span><span class="s2">_conv3&quot;</span>
</span><span id="ResNetBlockLarge-218"><a href="#ResNetBlockLarge-218"><span class="linenos">218</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Format and store full name of the 3rd weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="ResNetBlockLarge-219"><a href="#ResNetBlockLarge-219"><span class="linenos">219</span></a>
</span><span id="ResNetBlockLarge-220"><a href="#ResNetBlockLarge-220"><span class="linenos">220</span></a>        <span class="c1"># construct the 1st weighted convolutional layer and attached layers (batchnorm, activation, etc)</span>
</span><span id="ResNetBlockLarge-221"><a href="#ResNetBlockLarge-221"><span class="linenos">221</span></a>        <span class="n">layer_input_channels</span> <span class="o">=</span> <span class="n">preceding_output_channels</span>  <span class="c1"># the input channels of the 1st convolutional layer, which receive the output channels of the preceding module</span>
</span><span id="ResNetBlockLarge-222"><a href="#ResNetBlockLarge-222"><span class="linenos">222</span></a>        <span class="n">layer_output_channels</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="ResNetBlockLarge-223"><a href="#ResNetBlockLarge-223"><span class="linenos">223</span></a>            <span class="n">input_channels</span>  <span class="c1"># the output channels of the 1st convolutional layer</span>
</span><span id="ResNetBlockLarge-224"><a href="#ResNetBlockLarge-224"><span class="linenos">224</span></a>        <span class="p">)</span>
</span><span id="ResNetBlockLarge-225"><a href="#ResNetBlockLarge-225"><span class="linenos">225</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
</span><span id="ResNetBlockLarge-226"><a href="#ResNetBlockLarge-226"><span class="linenos">226</span></a>            <span class="n">in_channels</span><span class="o">=</span><span class="n">layer_input_channels</span><span class="p">,</span>
</span><span id="ResNetBlockLarge-227"><a href="#ResNetBlockLarge-227"><span class="linenos">227</span></a>            <span class="n">out_channels</span><span class="o">=</span><span class="n">layer_output_channels</span><span class="p">,</span>
</span><span id="ResNetBlockLarge-228"><a href="#ResNetBlockLarge-228"><span class="linenos">228</span></a>            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="ResNetBlockLarge-229"><a href="#ResNetBlockLarge-229"><span class="linenos">229</span></a>            <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="ResNetBlockLarge-230"><a href="#ResNetBlockLarge-230"><span class="linenos">230</span></a>            <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
</span><span id="ResNetBlockLarge-231"><a href="#ResNetBlockLarge-231"><span class="linenos">231</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="ResNetBlockLarge-232"><a href="#ResNetBlockLarge-232"><span class="linenos">232</span></a>        <span class="p">)</span>  <span class="c1"># construct the 1st weight convolutional layer of the larger building block. Overall stride is not performed here</span>
</span><span id="ResNetBlockLarge-233"><a href="#ResNetBlockLarge-233"><span class="linenos">233</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The 1st weight convolutional layer of the larger building block. &quot;&quot;&quot;</span>
</span><span id="ResNetBlockLarge-234"><a href="#ResNetBlockLarge-234"><span class="linenos">234</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="ResNetBlockLarge-235"><a href="#ResNetBlockLarge-235"><span class="linenos">235</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">full_1st_layer_name</span>
</span><span id="ResNetBlockLarge-236"><a href="#ResNetBlockLarge-236"><span class="linenos">236</span></a>        <span class="p">)</span>  <span class="c1"># update the weighted layer names</span>
</span><span id="ResNetBlockLarge-237"><a href="#ResNetBlockLarge-237"><span class="linenos">237</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_normalisation</span><span class="p">:</span>
</span><span id="ResNetBlockLarge-238"><a href="#ResNetBlockLarge-238"><span class="linenos">238</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv_bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span>
</span><span id="ResNetBlockLarge-239"><a href="#ResNetBlockLarge-239"><span class="linenos">239</span></a>                <span class="n">num_features</span><span class="o">=</span><span class="n">layer_output_channels</span>
</span><span id="ResNetBlockLarge-240"><a href="#ResNetBlockLarge-240"><span class="linenos">240</span></a>            <span class="p">)</span>  <span class="c1"># construct the batch normalisation layer</span>
</span><span id="ResNetBlockLarge-241"><a href="#ResNetBlockLarge-241"><span class="linenos">241</span></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The batch normalisation (`nn.BatchNorm2d`) layer after the 1st weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="ResNetBlockLarge-242"><a href="#ResNetBlockLarge-242"><span class="linenos">242</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="ResNetBlockLarge-243"><a href="#ResNetBlockLarge-243"><span class="linenos">243</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation1</span> <span class="o">=</span> <span class="n">activation_layer</span><span class="p">()</span>  <span class="c1"># construct the activation layer</span>
</span><span id="ResNetBlockLarge-244"><a href="#ResNetBlockLarge-244"><span class="linenos">244</span></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The activation layer after the 1st weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="ResNetBlockLarge-245"><a href="#ResNetBlockLarge-245"><span class="linenos">245</span></a>
</span><span id="ResNetBlockLarge-246"><a href="#ResNetBlockLarge-246"><span class="linenos">246</span></a>        <span class="c1"># construct the 2nd weighted convolutional layer and attached layers (batchnorm, activation, etc)</span>
</span><span id="ResNetBlockLarge-247"><a href="#ResNetBlockLarge-247"><span class="linenos">247</span></a>        <span class="n">layer_input_channels</span> <span class="o">=</span> <span class="n">input_channels</span>  <span class="c1"># the input channels of the 2nd convolutional layer, which is `input_channels`, the same as the output channels of the 1st convolutional layer</span>
</span><span id="ResNetBlockLarge-248"><a href="#ResNetBlockLarge-248"><span class="linenos">248</span></a>        <span class="n">layer_output_channels</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="ResNetBlockLarge-249"><a href="#ResNetBlockLarge-249"><span class="linenos">249</span></a>            <span class="n">input_channels</span>
</span><span id="ResNetBlockLarge-250"><a href="#ResNetBlockLarge-250"><span class="linenos">250</span></a>            <span class="o">*</span> <span class="mi">1</span>  <span class="c1"># the output channels of the 2nd convolutional layer, which is the same as the input channels (without expansion)</span>
</span><span id="ResNetBlockLarge-251"><a href="#ResNetBlockLarge-251"><span class="linenos">251</span></a>        <span class="p">)</span>
</span><span id="ResNetBlockLarge-252"><a href="#ResNetBlockLarge-252"><span class="linenos">252</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
</span><span id="ResNetBlockLarge-253"><a href="#ResNetBlockLarge-253"><span class="linenos">253</span></a>            <span class="n">in_channels</span><span class="o">=</span><span class="n">layer_input_channels</span><span class="p">,</span>
</span><span id="ResNetBlockLarge-254"><a href="#ResNetBlockLarge-254"><span class="linenos">254</span></a>            <span class="n">out_channels</span><span class="o">=</span><span class="n">layer_output_channels</span><span class="p">,</span>
</span><span id="ResNetBlockLarge-255"><a href="#ResNetBlockLarge-255"><span class="linenos">255</span></a>            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
</span><span id="ResNetBlockLarge-256"><a href="#ResNetBlockLarge-256"><span class="linenos">256</span></a>            <span class="n">stride</span><span class="o">=</span><span class="n">overall_stride</span><span class="p">,</span>
</span><span id="ResNetBlockLarge-257"><a href="#ResNetBlockLarge-257"><span class="linenos">257</span></a>            <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="ResNetBlockLarge-258"><a href="#ResNetBlockLarge-258"><span class="linenos">258</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="ResNetBlockLarge-259"><a href="#ResNetBlockLarge-259"><span class="linenos">259</span></a>        <span class="p">)</span>  <span class="c1"># construct the 2nd weight convolutional layer of the larger building block. Overall stride is performed here</span>
</span><span id="ResNetBlockLarge-260"><a href="#ResNetBlockLarge-260"><span class="linenos">260</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The 2nd weight convolutional layer of the larger building block. &quot;&quot;&quot;</span>
</span><span id="ResNetBlockLarge-261"><a href="#ResNetBlockLarge-261"><span class="linenos">261</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="ResNetBlockLarge-262"><a href="#ResNetBlockLarge-262"><span class="linenos">262</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">full_2nd_layer_name</span>
</span><span id="ResNetBlockLarge-263"><a href="#ResNetBlockLarge-263"><span class="linenos">263</span></a>        <span class="p">)</span>  <span class="c1"># update the weighted layer names</span>
</span><span id="ResNetBlockLarge-264"><a href="#ResNetBlockLarge-264"><span class="linenos">264</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_normalisation</span><span class="p">:</span>
</span><span id="ResNetBlockLarge-265"><a href="#ResNetBlockLarge-265"><span class="linenos">265</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv_bn2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span>
</span><span id="ResNetBlockLarge-266"><a href="#ResNetBlockLarge-266"><span class="linenos">266</span></a>                <span class="n">num_features</span><span class="o">=</span><span class="n">layer_output_channels</span>
</span><span id="ResNetBlockLarge-267"><a href="#ResNetBlockLarge-267"><span class="linenos">267</span></a>            <span class="p">)</span>  <span class="c1"># construct the batch normalisation layer</span>
</span><span id="ResNetBlockLarge-268"><a href="#ResNetBlockLarge-268"><span class="linenos">268</span></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The batch normalisation (`nn.BatchNorm2d`) layer after the 2nd weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="ResNetBlockLarge-269"><a href="#ResNetBlockLarge-269"><span class="linenos">269</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="ResNetBlockLarge-270"><a href="#ResNetBlockLarge-270"><span class="linenos">270</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation2</span> <span class="o">=</span> <span class="n">activation_layer</span><span class="p">()</span>  <span class="c1"># construct the activation layer</span>
</span><span id="ResNetBlockLarge-271"><a href="#ResNetBlockLarge-271"><span class="linenos">271</span></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The activation layer after the 2nd weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="ResNetBlockLarge-272"><a href="#ResNetBlockLarge-272"><span class="linenos">272</span></a>
</span><span id="ResNetBlockLarge-273"><a href="#ResNetBlockLarge-273"><span class="linenos">273</span></a>        <span class="c1"># construct the 3rd weighted convolutional layer and attached layers (batchnorm, activation, etc)</span>
</span><span id="ResNetBlockLarge-274"><a href="#ResNetBlockLarge-274"><span class="linenos">274</span></a>        <span class="n">layer_input_channels</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="ResNetBlockLarge-275"><a href="#ResNetBlockLarge-275"><span class="linenos">275</span></a>            <span class="n">input_channels</span> <span class="o">*</span> <span class="mi">1</span>
</span><span id="ResNetBlockLarge-276"><a href="#ResNetBlockLarge-276"><span class="linenos">276</span></a>        <span class="p">)</span>  <span class="c1"># the input channels of the 2nd convolutional layer, which is `input_channels * 1`, the same as the output channels of the 1st convolutional layer</span>
</span><span id="ResNetBlockLarge-277"><a href="#ResNetBlockLarge-277"><span class="linenos">277</span></a>        <span class="n">layer_output_channels</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="ResNetBlockLarge-278"><a href="#ResNetBlockLarge-278"><span class="linenos">278</span></a>            <span class="n">input_channels</span>
</span><span id="ResNetBlockLarge-279"><a href="#ResNetBlockLarge-279"><span class="linenos">279</span></a>            <span class="o">*</span> <span class="mi">4</span>  <span class="c1"># the output channels of the 2nd convolutional layer, which is 4 times expanded as the input channels</span>
</span><span id="ResNetBlockLarge-280"><a href="#ResNetBlockLarge-280"><span class="linenos">280</span></a>        <span class="p">)</span>
</span><span id="ResNetBlockLarge-281"><a href="#ResNetBlockLarge-281"><span class="linenos">281</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
</span><span id="ResNetBlockLarge-282"><a href="#ResNetBlockLarge-282"><span class="linenos">282</span></a>            <span class="n">in_channels</span><span class="o">=</span><span class="n">layer_input_channels</span><span class="p">,</span>
</span><span id="ResNetBlockLarge-283"><a href="#ResNetBlockLarge-283"><span class="linenos">283</span></a>            <span class="n">out_channels</span><span class="o">=</span><span class="n">layer_output_channels</span><span class="p">,</span>
</span><span id="ResNetBlockLarge-284"><a href="#ResNetBlockLarge-284"><span class="linenos">284</span></a>            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="ResNetBlockLarge-285"><a href="#ResNetBlockLarge-285"><span class="linenos">285</span></a>            <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="ResNetBlockLarge-286"><a href="#ResNetBlockLarge-286"><span class="linenos">286</span></a>            <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
</span><span id="ResNetBlockLarge-287"><a href="#ResNetBlockLarge-287"><span class="linenos">287</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="ResNetBlockLarge-288"><a href="#ResNetBlockLarge-288"><span class="linenos">288</span></a>        <span class="p">)</span>  <span class="c1"># construct the 3rd weight convolutional layer of the larger building block. Overall stride is not performed here</span>
</span><span id="ResNetBlockLarge-289"><a href="#ResNetBlockLarge-289"><span class="linenos">289</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The 3rd weight convolutional layer of the larger building block. &quot;&quot;&quot;</span>
</span><span id="ResNetBlockLarge-290"><a href="#ResNetBlockLarge-290"><span class="linenos">290</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="ResNetBlockLarge-291"><a href="#ResNetBlockLarge-291"><span class="linenos">291</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">full_3rd_layer_name</span>
</span><span id="ResNetBlockLarge-292"><a href="#ResNetBlockLarge-292"><span class="linenos">292</span></a>        <span class="p">)</span>  <span class="c1"># update the weighted layer names</span>
</span><span id="ResNetBlockLarge-293"><a href="#ResNetBlockLarge-293"><span class="linenos">293</span></a>        <span class="k">if</span> <span class="n">batch_normalisation</span><span class="p">:</span>
</span><span id="ResNetBlockLarge-294"><a href="#ResNetBlockLarge-294"><span class="linenos">294</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv_bn3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span>
</span><span id="ResNetBlockLarge-295"><a href="#ResNetBlockLarge-295"><span class="linenos">295</span></a>                <span class="n">num_features</span><span class="o">=</span><span class="n">layer_output_channels</span>
</span><span id="ResNetBlockLarge-296"><a href="#ResNetBlockLarge-296"><span class="linenos">296</span></a>            <span class="p">)</span>  <span class="c1"># construct the batch normalisation layer</span>
</span><span id="ResNetBlockLarge-297"><a href="#ResNetBlockLarge-297"><span class="linenos">297</span></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The batch normalisation (`nn.BatchNorm2d`) layer after the 3rd weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="ResNetBlockLarge-298"><a href="#ResNetBlockLarge-298"><span class="linenos">298</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="ResNetBlockLarge-299"><a href="#ResNetBlockLarge-299"><span class="linenos">299</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation3</span> <span class="o">=</span> <span class="n">activation_layer</span><span class="p">()</span>  <span class="c1"># construct the activation layer</span>
</span><span id="ResNetBlockLarge-300"><a href="#ResNetBlockLarge-300"><span class="linenos">300</span></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The activation layer after the 3rd weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="ResNetBlockLarge-301"><a href="#ResNetBlockLarge-301"><span class="linenos">301</span></a>
</span><span id="ResNetBlockLarge-302"><a href="#ResNetBlockLarge-302"><span class="linenos">302</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">identity_downsample</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="ResNetBlockLarge-303"><a href="#ResNetBlockLarge-303"><span class="linenos">303</span></a>            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
</span><span id="ResNetBlockLarge-304"><a href="#ResNetBlockLarge-304"><span class="linenos">304</span></a>                <span class="n">in_channels</span><span class="o">=</span><span class="n">preceding_output_channels</span><span class="p">,</span>
</span><span id="ResNetBlockLarge-305"><a href="#ResNetBlockLarge-305"><span class="linenos">305</span></a>                <span class="n">out_channels</span><span class="o">=</span><span class="n">input_channels</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span>
</span><span id="ResNetBlockLarge-306"><a href="#ResNetBlockLarge-306"><span class="linenos">306</span></a>                <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="ResNetBlockLarge-307"><a href="#ResNetBlockLarge-307"><span class="linenos">307</span></a>                <span class="n">stride</span><span class="o">=</span><span class="n">overall_stride</span><span class="p">,</span>
</span><span id="ResNetBlockLarge-308"><a href="#ResNetBlockLarge-308"><span class="linenos">308</span></a>                <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="ResNetBlockLarge-309"><a href="#ResNetBlockLarge-309"><span class="linenos">309</span></a>            <span class="p">)</span>
</span><span id="ResNetBlockLarge-310"><a href="#ResNetBlockLarge-310"><span class="linenos">310</span></a>            <span class="k">if</span> <span class="n">preceding_output_channels</span> <span class="o">!=</span> <span class="n">input_channels</span> <span class="o">*</span> <span class="mi">4</span> <span class="ow">or</span> <span class="n">overall_stride</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="ResNetBlockLarge-311"><a href="#ResNetBlockLarge-311"><span class="linenos">311</span></a>            <span class="k">else</span> <span class="kc">None</span>
</span><span id="ResNetBlockLarge-312"><a href="#ResNetBlockLarge-312"><span class="linenos">312</span></a>        <span class="p">)</span>
</span><span id="ResNetBlockLarge-313"><a href="#ResNetBlockLarge-313"><span class="linenos">313</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The convolutional layer for downsampling identity in the shortcut connection if the dimension of identity from input doesn&#39;t match the output&#39;s. This case only happens when the number of input channels doesn&#39;t equal to the number of preceding output channels or a layer with stride &gt; 1 exists. &quot;&quot;&quot;</span>
</span><span id="ResNetBlockLarge-314"><a href="#ResNetBlockLarge-314"><span class="linenos">314</span></a>
</span><span id="ResNetBlockLarge-315"><a href="#ResNetBlockLarge-315"><span class="linenos">315</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
</span><span id="ResNetBlockLarge-316"><a href="#ResNetBlockLarge-316"><span class="linenos">316</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The forward pass for data.</span>
</span><span id="ResNetBlockLarge-317"><a href="#ResNetBlockLarge-317"><span class="linenos">317</span></a>
</span><span id="ResNetBlockLarge-318"><a href="#ResNetBlockLarge-318"><span class="linenos">318</span></a><span class="sd">        **Args:**</span>
</span><span id="ResNetBlockLarge-319"><a href="#ResNetBlockLarge-319"><span class="linenos">319</span></a><span class="sd">        - **input** (`Tensor`): the input feature maps.</span>
</span><span id="ResNetBlockLarge-320"><a href="#ResNetBlockLarge-320"><span class="linenos">320</span></a>
</span><span id="ResNetBlockLarge-321"><a href="#ResNetBlockLarge-321"><span class="linenos">321</span></a><span class="sd">        **Returns:**</span>
</span><span id="ResNetBlockLarge-322"><a href="#ResNetBlockLarge-322"><span class="linenos">322</span></a><span class="sd">        - **output_feature** (`Tensor`): the output feature maps.</span>
</span><span id="ResNetBlockLarge-323"><a href="#ResNetBlockLarge-323"><span class="linenos">323</span></a><span class="sd">        - **hidden_features** (`dict[str, Tensor]`): the hidden features (after activation) in each weighted layer. Key (`str`) is the weighted layer name, value (`Tensor`) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes.</span>
</span><span id="ResNetBlockLarge-324"><a href="#ResNetBlockLarge-324"><span class="linenos">324</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="ResNetBlockLarge-325"><a href="#ResNetBlockLarge-325"><span class="linenos">325</span></a>        <span class="n">hidden_features</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="ResNetBlockLarge-326"><a href="#ResNetBlockLarge-326"><span class="linenos">326</span></a>
</span><span id="ResNetBlockLarge-327"><a href="#ResNetBlockLarge-327"><span class="linenos">327</span></a>        <span class="n">identity</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="ResNetBlockLarge-328"><a href="#ResNetBlockLarge-328"><span class="linenos">328</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">identity_downsample</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span><span id="ResNetBlockLarge-329"><a href="#ResNetBlockLarge-329"><span class="linenos">329</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">identity_downsample</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span><span id="ResNetBlockLarge-330"><a href="#ResNetBlockLarge-330"><span class="linenos">330</span></a>            <span class="k">else</span> <span class="nb">input</span>
</span><span id="ResNetBlockLarge-331"><a href="#ResNetBlockLarge-331"><span class="linenos">331</span></a>        <span class="p">)</span>  <span class="c1"># remember the identity of input for the shortcut connection. Perform downsampling if its dimension doesn&#39;t match the output&#39;s</span>
</span><span id="ResNetBlockLarge-332"><a href="#ResNetBlockLarge-332"><span class="linenos">332</span></a>
</span><span id="ResNetBlockLarge-333"><a href="#ResNetBlockLarge-333"><span class="linenos">333</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="nb">input</span>
</span><span id="ResNetBlockLarge-334"><a href="#ResNetBlockLarge-334"><span class="linenos">334</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNetBlockLarge-335"><a href="#ResNetBlockLarge-335"><span class="linenos">335</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_normalisation</span><span class="p">:</span>
</span><span id="ResNetBlockLarge-336"><a href="#ResNetBlockLarge-336"><span class="linenos">336</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_bn1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNetBlockLarge-337"><a href="#ResNetBlockLarge-337"><span class="linenos">337</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="ResNetBlockLarge-338"><a href="#ResNetBlockLarge-338"><span class="linenos">338</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNetBlockLarge-339"><a href="#ResNetBlockLarge-339"><span class="linenos">339</span></a>        <span class="n">hidden_features</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_1st_layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>  <span class="c1"># store the hidden feature</span>
</span><span id="ResNetBlockLarge-340"><a href="#ResNetBlockLarge-340"><span class="linenos">340</span></a>
</span><span id="ResNetBlockLarge-341"><a href="#ResNetBlockLarge-341"><span class="linenos">341</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNetBlockLarge-342"><a href="#ResNetBlockLarge-342"><span class="linenos">342</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_normalisation</span><span class="p">:</span>
</span><span id="ResNetBlockLarge-343"><a href="#ResNetBlockLarge-343"><span class="linenos">343</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_bn2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNetBlockLarge-344"><a href="#ResNetBlockLarge-344"><span class="linenos">344</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="ResNetBlockLarge-345"><a href="#ResNetBlockLarge-345"><span class="linenos">345</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNetBlockLarge-346"><a href="#ResNetBlockLarge-346"><span class="linenos">346</span></a>        <span class="n">hidden_features</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_2nd_layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>  <span class="c1"># store the hidden feature</span>
</span><span id="ResNetBlockLarge-347"><a href="#ResNetBlockLarge-347"><span class="linenos">347</span></a>
</span><span id="ResNetBlockLarge-348"><a href="#ResNetBlockLarge-348"><span class="linenos">348</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNetBlockLarge-349"><a href="#ResNetBlockLarge-349"><span class="linenos">349</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_normalisation</span><span class="p">:</span>
</span><span id="ResNetBlockLarge-350"><a href="#ResNetBlockLarge-350"><span class="linenos">350</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_bn3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNetBlockLarge-351"><a href="#ResNetBlockLarge-351"><span class="linenos">351</span></a>
</span><span id="ResNetBlockLarge-352"><a href="#ResNetBlockLarge-352"><span class="linenos">352</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">identity</span>
</span><span id="ResNetBlockLarge-353"><a href="#ResNetBlockLarge-353"><span class="linenos">353</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="ResNetBlockLarge-354"><a href="#ResNetBlockLarge-354"><span class="linenos">354</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># activation after the shortcut connection</span>
</span><span id="ResNetBlockLarge-355"><a href="#ResNetBlockLarge-355"><span class="linenos">355</span></a>        <span class="n">hidden_features</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_3rd_layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>  <span class="c1"># store the hidden feature</span>
</span><span id="ResNetBlockLarge-356"><a href="#ResNetBlockLarge-356"><span class="linenos">356</span></a>
</span><span id="ResNetBlockLarge-357"><a href="#ResNetBlockLarge-357"><span class="linenos">357</span></a>        <span class="n">output_feature</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="ResNetBlockLarge-358"><a href="#ResNetBlockLarge-358"><span class="linenos">358</span></a>
</span><span id="ResNetBlockLarge-359"><a href="#ResNetBlockLarge-359"><span class="linenos">359</span></a>        <span class="k">return</span> <span class="n">output_feature</span><span class="p">,</span> <span class="n">hidden_features</span>
</span></pre></div>


            <div class="docstring"><p>The larger building block for ResNet-50/101/152. It is referred to "bottleneck" building block in the paper.</p>

<p>It consists of 3 weight convolutional layers, each followed by an activation function. See Table 1 or Figure 5 (right) in the <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">original ResNet paper</a>.</p>
</div>


                            <div id="ResNetBlockLarge.__init__" class="classattr">
                                        <input id="ResNetBlockLarge.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">ResNetBlockLarge</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">outer_layer_name</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">block_idx</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">preceding_output_channels</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">overall_stride</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	activation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class="param">	<span class="n">batch_normalisation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>,</span><span class="param">	<span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span></span>)</span>

                <label class="view-source-button" for="ResNetBlockLarge.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#ResNetBlockLarge.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ResNetBlockLarge.__init__-183"><a href="#ResNetBlockLarge.__init__-183"><span class="linenos">183</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="ResNetBlockLarge.__init__-184"><a href="#ResNetBlockLarge.__init__-184"><span class="linenos">184</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="ResNetBlockLarge.__init__-185"><a href="#ResNetBlockLarge.__init__-185"><span class="linenos">185</span></a>        <span class="n">outer_layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="ResNetBlockLarge.__init__-186"><a href="#ResNetBlockLarge.__init__-186"><span class="linenos">186</span></a>        <span class="n">block_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNetBlockLarge.__init__-187"><a href="#ResNetBlockLarge.__init__-187"><span class="linenos">187</span></a>        <span class="n">preceding_output_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNetBlockLarge.__init__-188"><a href="#ResNetBlockLarge.__init__-188"><span class="linenos">188</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNetBlockLarge.__init__-189"><a href="#ResNetBlockLarge.__init__-189"><span class="linenos">189</span></a>        <span class="n">overall_stride</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNetBlockLarge.__init__-190"><a href="#ResNetBlockLarge.__init__-190"><span class="linenos">190</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="ResNetBlockLarge.__init__-191"><a href="#ResNetBlockLarge.__init__-191"><span class="linenos">191</span></a>        <span class="n">batch_normalisation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="ResNetBlockLarge.__init__-192"><a href="#ResNetBlockLarge.__init__-192"><span class="linenos">192</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="ResNetBlockLarge.__init__-193"><a href="#ResNetBlockLarge.__init__-193"><span class="linenos">193</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ResNetBlockLarge.__init__-194"><a href="#ResNetBlockLarge.__init__-194"><span class="linenos">194</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the larger building block.</span>
</span><span id="ResNetBlockLarge.__init__-195"><a href="#ResNetBlockLarge.__init__-195"><span class="linenos">195</span></a>
</span><span id="ResNetBlockLarge.__init__-196"><a href="#ResNetBlockLarge.__init__-196"><span class="linenos">196</span></a><span class="sd">        **Args:**</span>
</span><span id="ResNetBlockLarge.__init__-197"><a href="#ResNetBlockLarge.__init__-197"><span class="linenos">197</span></a><span class="sd">        - **outer_layer_name** (`str`): pass the name of the multi-building-block layer that contains this building block to construct the full name of each weighted convolutional layer.</span>
</span><span id="ResNetBlockLarge.__init__-198"><a href="#ResNetBlockLarge.__init__-198"><span class="linenos">198</span></a><span class="sd">        - **block_idx** (`int`): the index of the building blocks in the multi-building-block layer to construct the full name of each weighted convolutional layer.</span>
</span><span id="ResNetBlockLarge.__init__-199"><a href="#ResNetBlockLarge.__init__-199"><span class="linenos">199</span></a><span class="sd">        - **preceding_output_channels** (`int`): the number of channels of preceding output of this particular building block.</span>
</span><span id="ResNetBlockLarge.__init__-200"><a href="#ResNetBlockLarge.__init__-200"><span class="linenos">200</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input of this building block.</span>
</span><span id="ResNetBlockLarge.__init__-201"><a href="#ResNetBlockLarge.__init__-201"><span class="linenos">201</span></a><span class="sd">        - **overall_stride** (`int`): the overall stride of this building block. This stride is performed at 2nd (middle) convolutional layer where 1st and 3rd convolutional layers remain stride of 1.</span>
</span><span id="ResNetBlockLarge.__init__-202"><a href="#ResNetBlockLarge.__init__-202"><span class="linenos">202</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="ResNetBlockLarge.__init__-203"><a href="#ResNetBlockLarge.__init__-203"><span class="linenos">203</span></a><span class="sd">        - **batch_normalisation** (`bool`): whether to use batch normalisation after the weight convolutional layers. Default `True`, same as what the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) does.</span>
</span><span id="ResNetBlockLarge.__init__-204"><a href="#ResNetBlockLarge.__init__-204"><span class="linenos">204</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`, because batch normalisation are doing the similar thing with bias.</span>
</span><span id="ResNetBlockLarge.__init__-205"><a href="#ResNetBlockLarge.__init__-205"><span class="linenos">205</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="ResNetBlockLarge.__init__-206"><a href="#ResNetBlockLarge.__init__-206"><span class="linenos">206</span></a>        <span class="n">CLBackbone</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</span><span id="ResNetBlockLarge.__init__-207"><a href="#ResNetBlockLarge.__init__-207"><span class="linenos">207</span></a>
</span><span id="ResNetBlockLarge.__init__-208"><a href="#ResNetBlockLarge.__init__-208"><span class="linenos">208</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">batch_normalisation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">batch_normalisation</span>
</span><span id="ResNetBlockLarge.__init__-209"><a href="#ResNetBlockLarge.__init__-209"><span class="linenos">209</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store whether to use batch normalisation after the fully-connected layers.&quot;&quot;&quot;</span>
</span><span id="ResNetBlockLarge.__init__-210"><a href="#ResNetBlockLarge.__init__-210"><span class="linenos">210</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">activation_layer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span><span id="ResNetBlockLarge.__init__-211"><a href="#ResNetBlockLarge.__init__-211"><span class="linenos">211</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store whether to use activation function after the fully-connected layers.&quot;&quot;&quot;</span>
</span><span id="ResNetBlockLarge.__init__-212"><a href="#ResNetBlockLarge.__init__-212"><span class="linenos">212</span></a>
</span><span id="ResNetBlockLarge.__init__-213"><a href="#ResNetBlockLarge.__init__-213"><span class="linenos">213</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">full_1st_layer_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">outer_layer_name</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">block_idx</span><span class="si">}</span><span class="s2">/conv1&quot;</span>
</span><span id="ResNetBlockLarge.__init__-214"><a href="#ResNetBlockLarge.__init__-214"><span class="linenos">214</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Format and store full name of the 1st weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="ResNetBlockLarge.__init__-215"><a href="#ResNetBlockLarge.__init__-215"><span class="linenos">215</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">full_2nd_layer_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">outer_layer_name</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">block_idx</span><span class="si">}</span><span class="s2">_conv2&quot;</span>
</span><span id="ResNetBlockLarge.__init__-216"><a href="#ResNetBlockLarge.__init__-216"><span class="linenos">216</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Format and store full name of the 2nd weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="ResNetBlockLarge.__init__-217"><a href="#ResNetBlockLarge.__init__-217"><span class="linenos">217</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">full_3rd_layer_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">outer_layer_name</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">block_idx</span><span class="si">}</span><span class="s2">_conv3&quot;</span>
</span><span id="ResNetBlockLarge.__init__-218"><a href="#ResNetBlockLarge.__init__-218"><span class="linenos">218</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Format and store full name of the 3rd weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="ResNetBlockLarge.__init__-219"><a href="#ResNetBlockLarge.__init__-219"><span class="linenos">219</span></a>
</span><span id="ResNetBlockLarge.__init__-220"><a href="#ResNetBlockLarge.__init__-220"><span class="linenos">220</span></a>        <span class="c1"># construct the 1st weighted convolutional layer and attached layers (batchnorm, activation, etc)</span>
</span><span id="ResNetBlockLarge.__init__-221"><a href="#ResNetBlockLarge.__init__-221"><span class="linenos">221</span></a>        <span class="n">layer_input_channels</span> <span class="o">=</span> <span class="n">preceding_output_channels</span>  <span class="c1"># the input channels of the 1st convolutional layer, which receive the output channels of the preceding module</span>
</span><span id="ResNetBlockLarge.__init__-222"><a href="#ResNetBlockLarge.__init__-222"><span class="linenos">222</span></a>        <span class="n">layer_output_channels</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="ResNetBlockLarge.__init__-223"><a href="#ResNetBlockLarge.__init__-223"><span class="linenos">223</span></a>            <span class="n">input_channels</span>  <span class="c1"># the output channels of the 1st convolutional layer</span>
</span><span id="ResNetBlockLarge.__init__-224"><a href="#ResNetBlockLarge.__init__-224"><span class="linenos">224</span></a>        <span class="p">)</span>
</span><span id="ResNetBlockLarge.__init__-225"><a href="#ResNetBlockLarge.__init__-225"><span class="linenos">225</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
</span><span id="ResNetBlockLarge.__init__-226"><a href="#ResNetBlockLarge.__init__-226"><span class="linenos">226</span></a>            <span class="n">in_channels</span><span class="o">=</span><span class="n">layer_input_channels</span><span class="p">,</span>
</span><span id="ResNetBlockLarge.__init__-227"><a href="#ResNetBlockLarge.__init__-227"><span class="linenos">227</span></a>            <span class="n">out_channels</span><span class="o">=</span><span class="n">layer_output_channels</span><span class="p">,</span>
</span><span id="ResNetBlockLarge.__init__-228"><a href="#ResNetBlockLarge.__init__-228"><span class="linenos">228</span></a>            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="ResNetBlockLarge.__init__-229"><a href="#ResNetBlockLarge.__init__-229"><span class="linenos">229</span></a>            <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="ResNetBlockLarge.__init__-230"><a href="#ResNetBlockLarge.__init__-230"><span class="linenos">230</span></a>            <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
</span><span id="ResNetBlockLarge.__init__-231"><a href="#ResNetBlockLarge.__init__-231"><span class="linenos">231</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="ResNetBlockLarge.__init__-232"><a href="#ResNetBlockLarge.__init__-232"><span class="linenos">232</span></a>        <span class="p">)</span>  <span class="c1"># construct the 1st weight convolutional layer of the larger building block. Overall stride is not performed here</span>
</span><span id="ResNetBlockLarge.__init__-233"><a href="#ResNetBlockLarge.__init__-233"><span class="linenos">233</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The 1st weight convolutional layer of the larger building block. &quot;&quot;&quot;</span>
</span><span id="ResNetBlockLarge.__init__-234"><a href="#ResNetBlockLarge.__init__-234"><span class="linenos">234</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="ResNetBlockLarge.__init__-235"><a href="#ResNetBlockLarge.__init__-235"><span class="linenos">235</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">full_1st_layer_name</span>
</span><span id="ResNetBlockLarge.__init__-236"><a href="#ResNetBlockLarge.__init__-236"><span class="linenos">236</span></a>        <span class="p">)</span>  <span class="c1"># update the weighted layer names</span>
</span><span id="ResNetBlockLarge.__init__-237"><a href="#ResNetBlockLarge.__init__-237"><span class="linenos">237</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_normalisation</span><span class="p">:</span>
</span><span id="ResNetBlockLarge.__init__-238"><a href="#ResNetBlockLarge.__init__-238"><span class="linenos">238</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv_bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span>
</span><span id="ResNetBlockLarge.__init__-239"><a href="#ResNetBlockLarge.__init__-239"><span class="linenos">239</span></a>                <span class="n">num_features</span><span class="o">=</span><span class="n">layer_output_channels</span>
</span><span id="ResNetBlockLarge.__init__-240"><a href="#ResNetBlockLarge.__init__-240"><span class="linenos">240</span></a>            <span class="p">)</span>  <span class="c1"># construct the batch normalisation layer</span>
</span><span id="ResNetBlockLarge.__init__-241"><a href="#ResNetBlockLarge.__init__-241"><span class="linenos">241</span></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The batch normalisation (`nn.BatchNorm2d`) layer after the 1st weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="ResNetBlockLarge.__init__-242"><a href="#ResNetBlockLarge.__init__-242"><span class="linenos">242</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="ResNetBlockLarge.__init__-243"><a href="#ResNetBlockLarge.__init__-243"><span class="linenos">243</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation1</span> <span class="o">=</span> <span class="n">activation_layer</span><span class="p">()</span>  <span class="c1"># construct the activation layer</span>
</span><span id="ResNetBlockLarge.__init__-244"><a href="#ResNetBlockLarge.__init__-244"><span class="linenos">244</span></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The activation layer after the 1st weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="ResNetBlockLarge.__init__-245"><a href="#ResNetBlockLarge.__init__-245"><span class="linenos">245</span></a>
</span><span id="ResNetBlockLarge.__init__-246"><a href="#ResNetBlockLarge.__init__-246"><span class="linenos">246</span></a>        <span class="c1"># construct the 2nd weighted convolutional layer and attached layers (batchnorm, activation, etc)</span>
</span><span id="ResNetBlockLarge.__init__-247"><a href="#ResNetBlockLarge.__init__-247"><span class="linenos">247</span></a>        <span class="n">layer_input_channels</span> <span class="o">=</span> <span class="n">input_channels</span>  <span class="c1"># the input channels of the 2nd convolutional layer, which is `input_channels`, the same as the output channels of the 1st convolutional layer</span>
</span><span id="ResNetBlockLarge.__init__-248"><a href="#ResNetBlockLarge.__init__-248"><span class="linenos">248</span></a>        <span class="n">layer_output_channels</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="ResNetBlockLarge.__init__-249"><a href="#ResNetBlockLarge.__init__-249"><span class="linenos">249</span></a>            <span class="n">input_channels</span>
</span><span id="ResNetBlockLarge.__init__-250"><a href="#ResNetBlockLarge.__init__-250"><span class="linenos">250</span></a>            <span class="o">*</span> <span class="mi">1</span>  <span class="c1"># the output channels of the 2nd convolutional layer, which is the same as the input channels (without expansion)</span>
</span><span id="ResNetBlockLarge.__init__-251"><a href="#ResNetBlockLarge.__init__-251"><span class="linenos">251</span></a>        <span class="p">)</span>
</span><span id="ResNetBlockLarge.__init__-252"><a href="#ResNetBlockLarge.__init__-252"><span class="linenos">252</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
</span><span id="ResNetBlockLarge.__init__-253"><a href="#ResNetBlockLarge.__init__-253"><span class="linenos">253</span></a>            <span class="n">in_channels</span><span class="o">=</span><span class="n">layer_input_channels</span><span class="p">,</span>
</span><span id="ResNetBlockLarge.__init__-254"><a href="#ResNetBlockLarge.__init__-254"><span class="linenos">254</span></a>            <span class="n">out_channels</span><span class="o">=</span><span class="n">layer_output_channels</span><span class="p">,</span>
</span><span id="ResNetBlockLarge.__init__-255"><a href="#ResNetBlockLarge.__init__-255"><span class="linenos">255</span></a>            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
</span><span id="ResNetBlockLarge.__init__-256"><a href="#ResNetBlockLarge.__init__-256"><span class="linenos">256</span></a>            <span class="n">stride</span><span class="o">=</span><span class="n">overall_stride</span><span class="p">,</span>
</span><span id="ResNetBlockLarge.__init__-257"><a href="#ResNetBlockLarge.__init__-257"><span class="linenos">257</span></a>            <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="ResNetBlockLarge.__init__-258"><a href="#ResNetBlockLarge.__init__-258"><span class="linenos">258</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="ResNetBlockLarge.__init__-259"><a href="#ResNetBlockLarge.__init__-259"><span class="linenos">259</span></a>        <span class="p">)</span>  <span class="c1"># construct the 2nd weight convolutional layer of the larger building block. Overall stride is performed here</span>
</span><span id="ResNetBlockLarge.__init__-260"><a href="#ResNetBlockLarge.__init__-260"><span class="linenos">260</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The 2nd weight convolutional layer of the larger building block. &quot;&quot;&quot;</span>
</span><span id="ResNetBlockLarge.__init__-261"><a href="#ResNetBlockLarge.__init__-261"><span class="linenos">261</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="ResNetBlockLarge.__init__-262"><a href="#ResNetBlockLarge.__init__-262"><span class="linenos">262</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">full_2nd_layer_name</span>
</span><span id="ResNetBlockLarge.__init__-263"><a href="#ResNetBlockLarge.__init__-263"><span class="linenos">263</span></a>        <span class="p">)</span>  <span class="c1"># update the weighted layer names</span>
</span><span id="ResNetBlockLarge.__init__-264"><a href="#ResNetBlockLarge.__init__-264"><span class="linenos">264</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_normalisation</span><span class="p">:</span>
</span><span id="ResNetBlockLarge.__init__-265"><a href="#ResNetBlockLarge.__init__-265"><span class="linenos">265</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv_bn2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span>
</span><span id="ResNetBlockLarge.__init__-266"><a href="#ResNetBlockLarge.__init__-266"><span class="linenos">266</span></a>                <span class="n">num_features</span><span class="o">=</span><span class="n">layer_output_channels</span>
</span><span id="ResNetBlockLarge.__init__-267"><a href="#ResNetBlockLarge.__init__-267"><span class="linenos">267</span></a>            <span class="p">)</span>  <span class="c1"># construct the batch normalisation layer</span>
</span><span id="ResNetBlockLarge.__init__-268"><a href="#ResNetBlockLarge.__init__-268"><span class="linenos">268</span></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The batch normalisation (`nn.BatchNorm2d`) layer after the 2nd weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="ResNetBlockLarge.__init__-269"><a href="#ResNetBlockLarge.__init__-269"><span class="linenos">269</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="ResNetBlockLarge.__init__-270"><a href="#ResNetBlockLarge.__init__-270"><span class="linenos">270</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation2</span> <span class="o">=</span> <span class="n">activation_layer</span><span class="p">()</span>  <span class="c1"># construct the activation layer</span>
</span><span id="ResNetBlockLarge.__init__-271"><a href="#ResNetBlockLarge.__init__-271"><span class="linenos">271</span></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The activation layer after the 2nd weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="ResNetBlockLarge.__init__-272"><a href="#ResNetBlockLarge.__init__-272"><span class="linenos">272</span></a>
</span><span id="ResNetBlockLarge.__init__-273"><a href="#ResNetBlockLarge.__init__-273"><span class="linenos">273</span></a>        <span class="c1"># construct the 3rd weighted convolutional layer and attached layers (batchnorm, activation, etc)</span>
</span><span id="ResNetBlockLarge.__init__-274"><a href="#ResNetBlockLarge.__init__-274"><span class="linenos">274</span></a>        <span class="n">layer_input_channels</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="ResNetBlockLarge.__init__-275"><a href="#ResNetBlockLarge.__init__-275"><span class="linenos">275</span></a>            <span class="n">input_channels</span> <span class="o">*</span> <span class="mi">1</span>
</span><span id="ResNetBlockLarge.__init__-276"><a href="#ResNetBlockLarge.__init__-276"><span class="linenos">276</span></a>        <span class="p">)</span>  <span class="c1"># the input channels of the 2nd convolutional layer, which is `input_channels * 1`, the same as the output channels of the 1st convolutional layer</span>
</span><span id="ResNetBlockLarge.__init__-277"><a href="#ResNetBlockLarge.__init__-277"><span class="linenos">277</span></a>        <span class="n">layer_output_channels</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="ResNetBlockLarge.__init__-278"><a href="#ResNetBlockLarge.__init__-278"><span class="linenos">278</span></a>            <span class="n">input_channels</span>
</span><span id="ResNetBlockLarge.__init__-279"><a href="#ResNetBlockLarge.__init__-279"><span class="linenos">279</span></a>            <span class="o">*</span> <span class="mi">4</span>  <span class="c1"># the output channels of the 2nd convolutional layer, which is 4 times expanded as the input channels</span>
</span><span id="ResNetBlockLarge.__init__-280"><a href="#ResNetBlockLarge.__init__-280"><span class="linenos">280</span></a>        <span class="p">)</span>
</span><span id="ResNetBlockLarge.__init__-281"><a href="#ResNetBlockLarge.__init__-281"><span class="linenos">281</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
</span><span id="ResNetBlockLarge.__init__-282"><a href="#ResNetBlockLarge.__init__-282"><span class="linenos">282</span></a>            <span class="n">in_channels</span><span class="o">=</span><span class="n">layer_input_channels</span><span class="p">,</span>
</span><span id="ResNetBlockLarge.__init__-283"><a href="#ResNetBlockLarge.__init__-283"><span class="linenos">283</span></a>            <span class="n">out_channels</span><span class="o">=</span><span class="n">layer_output_channels</span><span class="p">,</span>
</span><span id="ResNetBlockLarge.__init__-284"><a href="#ResNetBlockLarge.__init__-284"><span class="linenos">284</span></a>            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="ResNetBlockLarge.__init__-285"><a href="#ResNetBlockLarge.__init__-285"><span class="linenos">285</span></a>            <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="ResNetBlockLarge.__init__-286"><a href="#ResNetBlockLarge.__init__-286"><span class="linenos">286</span></a>            <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
</span><span id="ResNetBlockLarge.__init__-287"><a href="#ResNetBlockLarge.__init__-287"><span class="linenos">287</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="ResNetBlockLarge.__init__-288"><a href="#ResNetBlockLarge.__init__-288"><span class="linenos">288</span></a>        <span class="p">)</span>  <span class="c1"># construct the 3rd weight convolutional layer of the larger building block. Overall stride is not performed here</span>
</span><span id="ResNetBlockLarge.__init__-289"><a href="#ResNetBlockLarge.__init__-289"><span class="linenos">289</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The 3rd weight convolutional layer of the larger building block. &quot;&quot;&quot;</span>
</span><span id="ResNetBlockLarge.__init__-290"><a href="#ResNetBlockLarge.__init__-290"><span class="linenos">290</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="ResNetBlockLarge.__init__-291"><a href="#ResNetBlockLarge.__init__-291"><span class="linenos">291</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">full_3rd_layer_name</span>
</span><span id="ResNetBlockLarge.__init__-292"><a href="#ResNetBlockLarge.__init__-292"><span class="linenos">292</span></a>        <span class="p">)</span>  <span class="c1"># update the weighted layer names</span>
</span><span id="ResNetBlockLarge.__init__-293"><a href="#ResNetBlockLarge.__init__-293"><span class="linenos">293</span></a>        <span class="k">if</span> <span class="n">batch_normalisation</span><span class="p">:</span>
</span><span id="ResNetBlockLarge.__init__-294"><a href="#ResNetBlockLarge.__init__-294"><span class="linenos">294</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv_bn3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span>
</span><span id="ResNetBlockLarge.__init__-295"><a href="#ResNetBlockLarge.__init__-295"><span class="linenos">295</span></a>                <span class="n">num_features</span><span class="o">=</span><span class="n">layer_output_channels</span>
</span><span id="ResNetBlockLarge.__init__-296"><a href="#ResNetBlockLarge.__init__-296"><span class="linenos">296</span></a>            <span class="p">)</span>  <span class="c1"># construct the batch normalisation layer</span>
</span><span id="ResNetBlockLarge.__init__-297"><a href="#ResNetBlockLarge.__init__-297"><span class="linenos">297</span></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The batch normalisation (`nn.BatchNorm2d`) layer after the 3rd weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="ResNetBlockLarge.__init__-298"><a href="#ResNetBlockLarge.__init__-298"><span class="linenos">298</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="ResNetBlockLarge.__init__-299"><a href="#ResNetBlockLarge.__init__-299"><span class="linenos">299</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation3</span> <span class="o">=</span> <span class="n">activation_layer</span><span class="p">()</span>  <span class="c1"># construct the activation layer</span>
</span><span id="ResNetBlockLarge.__init__-300"><a href="#ResNetBlockLarge.__init__-300"><span class="linenos">300</span></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The activation layer after the 3rd weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="ResNetBlockLarge.__init__-301"><a href="#ResNetBlockLarge.__init__-301"><span class="linenos">301</span></a>
</span><span id="ResNetBlockLarge.__init__-302"><a href="#ResNetBlockLarge.__init__-302"><span class="linenos">302</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">identity_downsample</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="ResNetBlockLarge.__init__-303"><a href="#ResNetBlockLarge.__init__-303"><span class="linenos">303</span></a>            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
</span><span id="ResNetBlockLarge.__init__-304"><a href="#ResNetBlockLarge.__init__-304"><span class="linenos">304</span></a>                <span class="n">in_channels</span><span class="o">=</span><span class="n">preceding_output_channels</span><span class="p">,</span>
</span><span id="ResNetBlockLarge.__init__-305"><a href="#ResNetBlockLarge.__init__-305"><span class="linenos">305</span></a>                <span class="n">out_channels</span><span class="o">=</span><span class="n">input_channels</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span>
</span><span id="ResNetBlockLarge.__init__-306"><a href="#ResNetBlockLarge.__init__-306"><span class="linenos">306</span></a>                <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="ResNetBlockLarge.__init__-307"><a href="#ResNetBlockLarge.__init__-307"><span class="linenos">307</span></a>                <span class="n">stride</span><span class="o">=</span><span class="n">overall_stride</span><span class="p">,</span>
</span><span id="ResNetBlockLarge.__init__-308"><a href="#ResNetBlockLarge.__init__-308"><span class="linenos">308</span></a>                <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="ResNetBlockLarge.__init__-309"><a href="#ResNetBlockLarge.__init__-309"><span class="linenos">309</span></a>            <span class="p">)</span>
</span><span id="ResNetBlockLarge.__init__-310"><a href="#ResNetBlockLarge.__init__-310"><span class="linenos">310</span></a>            <span class="k">if</span> <span class="n">preceding_output_channels</span> <span class="o">!=</span> <span class="n">input_channels</span> <span class="o">*</span> <span class="mi">4</span> <span class="ow">or</span> <span class="n">overall_stride</span> <span class="o">!=</span> <span class="mi">1</span>
</span><span id="ResNetBlockLarge.__init__-311"><a href="#ResNetBlockLarge.__init__-311"><span class="linenos">311</span></a>            <span class="k">else</span> <span class="kc">None</span>
</span><span id="ResNetBlockLarge.__init__-312"><a href="#ResNetBlockLarge.__init__-312"><span class="linenos">312</span></a>        <span class="p">)</span>
</span><span id="ResNetBlockLarge.__init__-313"><a href="#ResNetBlockLarge.__init__-313"><span class="linenos">313</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The convolutional layer for downsampling identity in the shortcut connection if the dimension of identity from input doesn&#39;t match the output&#39;s. This case only happens when the number of input channels doesn&#39;t equal to the number of preceding output channels or a layer with stride &gt; 1 exists. &quot;&quot;&quot;</span>
</span></pre></div>


            <div class="docstring"><p>Construct and initialise the larger building block.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>outer_layer_name</strong> (<code>str</code>): pass the name of the multi-building-block layer that contains this building block to construct the full name of each weighted convolutional layer.</li>
<li><strong>block_idx</strong> (<code>int</code>): the index of the building blocks in the multi-building-block layer to construct the full name of each weighted convolutional layer.</li>
<li><strong>preceding_output_channels</strong> (<code>int</code>): the number of channels of preceding output of this particular building block.</li>
<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block.</li>
<li><strong>overall_stride</strong> (<code>int</code>): the overall stride of this building block. This stride is performed at 2nd (middle) convolutional layer where 1st and 3rd convolutional layers remain stride of 1.</li>
<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>
<li><strong>batch_normalisation</strong> (<code>bool</code>): whether to use batch normalisation after the weight convolutional layers. Default <code>True</code>, same as what the <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">original ResNet paper</a> does.</li>
<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>, because batch normalisation are doing the similar thing with bias.</li>
</ul>
</div>


                            </div>
                            <div id="ResNetBlockLarge.batch_normalisation" class="classattr">
                                <div class="attr variable">
            <span class="name">batch_normalisation</span><span class="annotation">: bool</span>

        
    </div>
    <a class="headerlink" href="#ResNetBlockLarge.batch_normalisation"></a>
    
            <div class="docstring"><p>Store whether to use batch normalisation after the fully-connected layers.</p>
</div>


                            </div>
                            <div id="ResNetBlockLarge.activation" class="classattr">
                                <div class="attr variable">
            <span class="name">activation</span><span class="annotation">: bool</span>

        
    </div>
    <a class="headerlink" href="#ResNetBlockLarge.activation"></a>
    
            <div class="docstring"><p>Store whether to use activation function after the fully-connected layers.</p>
</div>


                            </div>
                            <div id="ResNetBlockLarge.full_1st_layer_name" class="classattr">
                                <div class="attr variable">
            <span class="name">full_1st_layer_name</span>

        
    </div>
    <a class="headerlink" href="#ResNetBlockLarge.full_1st_layer_name"></a>
    
            <div class="docstring"><p>Format and store full name of the 1st weighted convolutional layer.</p>
</div>


                            </div>
                            <div id="ResNetBlockLarge.full_2nd_layer_name" class="classattr">
                                <div class="attr variable">
            <span class="name">full_2nd_layer_name</span>

        
    </div>
    <a class="headerlink" href="#ResNetBlockLarge.full_2nd_layer_name"></a>
    
            <div class="docstring"><p>Format and store full name of the 2nd weighted convolutional layer.</p>
</div>


                            </div>
                            <div id="ResNetBlockLarge.full_3rd_layer_name" class="classattr">
                                <div class="attr variable">
            <span class="name">full_3rd_layer_name</span>

        
    </div>
    <a class="headerlink" href="#ResNetBlockLarge.full_3rd_layer_name"></a>
    
            <div class="docstring"><p>Format and store full name of the 3rd weighted convolutional layer.</p>
</div>


                            </div>
                            <div id="ResNetBlockLarge.conv1" class="classattr">
                                <div class="attr variable">
            <span class="name">conv1</span>

        
    </div>
    <a class="headerlink" href="#ResNetBlockLarge.conv1"></a>
    
            <div class="docstring"><p>The 1st weight convolutional layer of the larger building block.</p>
</div>


                            </div>
                            <div id="ResNetBlockLarge.conv2" class="classattr">
                                <div class="attr variable">
            <span class="name">conv2</span>

        
    </div>
    <a class="headerlink" href="#ResNetBlockLarge.conv2"></a>
    
            <div class="docstring"><p>The 2nd weight convolutional layer of the larger building block.</p>
</div>


                            </div>
                            <div id="ResNetBlockLarge.conv3" class="classattr">
                                <div class="attr variable">
            <span class="name">conv3</span>

        
    </div>
    <a class="headerlink" href="#ResNetBlockLarge.conv3"></a>
    
            <div class="docstring"><p>The 3rd weight convolutional layer of the larger building block.</p>
</div>


                            </div>
                            <div id="ResNetBlockLarge.identity_downsample" class="classattr">
                                <div class="attr variable">
            <span class="name">identity_downsample</span><span class="annotation">: torch.nn.modules.module.Module</span>

        
    </div>
    <a class="headerlink" href="#ResNetBlockLarge.identity_downsample"></a>
    
            <div class="docstring"><p>The convolutional layer for downsampling identity in the shortcut connection if the dimension of identity from input doesn't match the output's. This case only happens when the number of input channels doesn't equal to the number of preceding output channels or a layer with stride &gt; 1 exists.</p>
</div>


                            </div>
                            <div id="ResNetBlockLarge.forward" class="classattr">
                                        <input id="ResNetBlockLarge.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></span><span class="return-annotation">) -> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span>:</span></span>

                <label class="view-source-button" for="ResNetBlockLarge.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#ResNetBlockLarge.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ResNetBlockLarge.forward-315"><a href="#ResNetBlockLarge.forward-315"><span class="linenos">315</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
</span><span id="ResNetBlockLarge.forward-316"><a href="#ResNetBlockLarge.forward-316"><span class="linenos">316</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The forward pass for data.</span>
</span><span id="ResNetBlockLarge.forward-317"><a href="#ResNetBlockLarge.forward-317"><span class="linenos">317</span></a>
</span><span id="ResNetBlockLarge.forward-318"><a href="#ResNetBlockLarge.forward-318"><span class="linenos">318</span></a><span class="sd">        **Args:**</span>
</span><span id="ResNetBlockLarge.forward-319"><a href="#ResNetBlockLarge.forward-319"><span class="linenos">319</span></a><span class="sd">        - **input** (`Tensor`): the input feature maps.</span>
</span><span id="ResNetBlockLarge.forward-320"><a href="#ResNetBlockLarge.forward-320"><span class="linenos">320</span></a>
</span><span id="ResNetBlockLarge.forward-321"><a href="#ResNetBlockLarge.forward-321"><span class="linenos">321</span></a><span class="sd">        **Returns:**</span>
</span><span id="ResNetBlockLarge.forward-322"><a href="#ResNetBlockLarge.forward-322"><span class="linenos">322</span></a><span class="sd">        - **output_feature** (`Tensor`): the output feature maps.</span>
</span><span id="ResNetBlockLarge.forward-323"><a href="#ResNetBlockLarge.forward-323"><span class="linenos">323</span></a><span class="sd">        - **hidden_features** (`dict[str, Tensor]`): the hidden features (after activation) in each weighted layer. Key (`str`) is the weighted layer name, value (`Tensor`) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes.</span>
</span><span id="ResNetBlockLarge.forward-324"><a href="#ResNetBlockLarge.forward-324"><span class="linenos">324</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="ResNetBlockLarge.forward-325"><a href="#ResNetBlockLarge.forward-325"><span class="linenos">325</span></a>        <span class="n">hidden_features</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="ResNetBlockLarge.forward-326"><a href="#ResNetBlockLarge.forward-326"><span class="linenos">326</span></a>
</span><span id="ResNetBlockLarge.forward-327"><a href="#ResNetBlockLarge.forward-327"><span class="linenos">327</span></a>        <span class="n">identity</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="ResNetBlockLarge.forward-328"><a href="#ResNetBlockLarge.forward-328"><span class="linenos">328</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">identity_downsample</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span><span id="ResNetBlockLarge.forward-329"><a href="#ResNetBlockLarge.forward-329"><span class="linenos">329</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">identity_downsample</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span><span id="ResNetBlockLarge.forward-330"><a href="#ResNetBlockLarge.forward-330"><span class="linenos">330</span></a>            <span class="k">else</span> <span class="nb">input</span>
</span><span id="ResNetBlockLarge.forward-331"><a href="#ResNetBlockLarge.forward-331"><span class="linenos">331</span></a>        <span class="p">)</span>  <span class="c1"># remember the identity of input for the shortcut connection. Perform downsampling if its dimension doesn&#39;t match the output&#39;s</span>
</span><span id="ResNetBlockLarge.forward-332"><a href="#ResNetBlockLarge.forward-332"><span class="linenos">332</span></a>
</span><span id="ResNetBlockLarge.forward-333"><a href="#ResNetBlockLarge.forward-333"><span class="linenos">333</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="nb">input</span>
</span><span id="ResNetBlockLarge.forward-334"><a href="#ResNetBlockLarge.forward-334"><span class="linenos">334</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNetBlockLarge.forward-335"><a href="#ResNetBlockLarge.forward-335"><span class="linenos">335</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_normalisation</span><span class="p">:</span>
</span><span id="ResNetBlockLarge.forward-336"><a href="#ResNetBlockLarge.forward-336"><span class="linenos">336</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_bn1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNetBlockLarge.forward-337"><a href="#ResNetBlockLarge.forward-337"><span class="linenos">337</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="ResNetBlockLarge.forward-338"><a href="#ResNetBlockLarge.forward-338"><span class="linenos">338</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNetBlockLarge.forward-339"><a href="#ResNetBlockLarge.forward-339"><span class="linenos">339</span></a>        <span class="n">hidden_features</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_1st_layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>  <span class="c1"># store the hidden feature</span>
</span><span id="ResNetBlockLarge.forward-340"><a href="#ResNetBlockLarge.forward-340"><span class="linenos">340</span></a>
</span><span id="ResNetBlockLarge.forward-341"><a href="#ResNetBlockLarge.forward-341"><span class="linenos">341</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNetBlockLarge.forward-342"><a href="#ResNetBlockLarge.forward-342"><span class="linenos">342</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_normalisation</span><span class="p">:</span>
</span><span id="ResNetBlockLarge.forward-343"><a href="#ResNetBlockLarge.forward-343"><span class="linenos">343</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_bn2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNetBlockLarge.forward-344"><a href="#ResNetBlockLarge.forward-344"><span class="linenos">344</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="ResNetBlockLarge.forward-345"><a href="#ResNetBlockLarge.forward-345"><span class="linenos">345</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNetBlockLarge.forward-346"><a href="#ResNetBlockLarge.forward-346"><span class="linenos">346</span></a>        <span class="n">hidden_features</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_2nd_layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>  <span class="c1"># store the hidden feature</span>
</span><span id="ResNetBlockLarge.forward-347"><a href="#ResNetBlockLarge.forward-347"><span class="linenos">347</span></a>
</span><span id="ResNetBlockLarge.forward-348"><a href="#ResNetBlockLarge.forward-348"><span class="linenos">348</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNetBlockLarge.forward-349"><a href="#ResNetBlockLarge.forward-349"><span class="linenos">349</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_normalisation</span><span class="p">:</span>
</span><span id="ResNetBlockLarge.forward-350"><a href="#ResNetBlockLarge.forward-350"><span class="linenos">350</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_bn3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNetBlockLarge.forward-351"><a href="#ResNetBlockLarge.forward-351"><span class="linenos">351</span></a>
</span><span id="ResNetBlockLarge.forward-352"><a href="#ResNetBlockLarge.forward-352"><span class="linenos">352</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">identity</span>
</span><span id="ResNetBlockLarge.forward-353"><a href="#ResNetBlockLarge.forward-353"><span class="linenos">353</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="ResNetBlockLarge.forward-354"><a href="#ResNetBlockLarge.forward-354"><span class="linenos">354</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># activation after the shortcut connection</span>
</span><span id="ResNetBlockLarge.forward-355"><a href="#ResNetBlockLarge.forward-355"><span class="linenos">355</span></a>        <span class="n">hidden_features</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_3rd_layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>  <span class="c1"># store the hidden feature</span>
</span><span id="ResNetBlockLarge.forward-356"><a href="#ResNetBlockLarge.forward-356"><span class="linenos">356</span></a>
</span><span id="ResNetBlockLarge.forward-357"><a href="#ResNetBlockLarge.forward-357"><span class="linenos">357</span></a>        <span class="n">output_feature</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="ResNetBlockLarge.forward-358"><a href="#ResNetBlockLarge.forward-358"><span class="linenos">358</span></a>
</span><span id="ResNetBlockLarge.forward-359"><a href="#ResNetBlockLarge.forward-359"><span class="linenos">359</span></a>        <span class="k">return</span> <span class="n">output_feature</span><span class="p">,</span> <span class="n">hidden_features</span>
</span></pre></div>


            <div class="docstring"><p>The forward pass for data.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>input</strong> (<code>Tensor</code>): the input feature maps.</li>
</ul>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>output_feature</strong> (<code>Tensor</code>): the output feature maps.</li>
<li><strong>hidden_features</strong> (<code>dict[str, Tensor]</code>): the hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name, value (<code>Tensor</code>) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes.</li>
</ul>
</div>


                            </div>
                </section>
                <section id="ResNetBase">
                            <input id="ResNetBase-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">ResNetBase</span><wbr>(<span class="base">clarena.backbones.base.CLBackbone</span>):

                <label class="view-source-button" for="ResNetBase-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#ResNetBase"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ResNetBase-362"><a href="#ResNetBase-362"><span class="linenos">362</span></a><span class="k">class</span><span class="w"> </span><span class="nc">ResNetBase</span><span class="p">(</span><span class="n">CLBackbone</span><span class="p">):</span>
</span><span id="ResNetBase-363"><a href="#ResNetBase-363"><span class="linenos">363</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The base class of [residual network (ResNet)](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html).</span>
</span><span id="ResNetBase-364"><a href="#ResNetBase-364"><span class="linenos">364</span></a>
</span><span id="ResNetBase-365"><a href="#ResNetBase-365"><span class="linenos">365</span></a><span class="sd">    ResNet is a convolutional network architecture, which has 1st convolutional parameter layer and a maxpooling layer, connecting to 4 convolutional layers which contains multiple convolutional parameter layer. Each layer of the 4 are constructed from basic building blocks which are either small (`ResNetBlockSmall`) or large (`ResNetBlockLarge`). Each building block contains several convolutional parameter layers. The building blocks are connected by a skip connection which is a direct connection from the input of the block to the output of the block, and this is why it&#39;s called residual (find &quot;shortcut connections&quot; in the paper for more details). After the 5th convolutional layer, there are average pooling layer and a fully connected layer which connects to the CL output heads.</span>
</span><span id="ResNetBase-366"><a href="#ResNetBase-366"><span class="linenos">366</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="ResNetBase-367"><a href="#ResNetBase-367"><span class="linenos">367</span></a>
</span><span id="ResNetBase-368"><a href="#ResNetBase-368"><span class="linenos">368</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="ResNetBase-369"><a href="#ResNetBase-369"><span class="linenos">369</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="ResNetBase-370"><a href="#ResNetBase-370"><span class="linenos">370</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNetBase-371"><a href="#ResNetBase-371"><span class="linenos">371</span></a>        <span class="n">building_block_type</span><span class="p">:</span> <span class="n">ResNetBlockSmall</span> <span class="o">|</span> <span class="n">ResNetBlockLarge</span><span class="p">,</span>
</span><span id="ResNetBase-372"><a href="#ResNetBase-372"><span class="linenos">372</span></a>        <span class="n">building_block_nums</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
</span><span id="ResNetBase-373"><a href="#ResNetBase-373"><span class="linenos">373</span></a>        <span class="n">building_block_preceding_output_channels</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
</span><span id="ResNetBase-374"><a href="#ResNetBase-374"><span class="linenos">374</span></a>        <span class="n">building_block_input_channels</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
</span><span id="ResNetBase-375"><a href="#ResNetBase-375"><span class="linenos">375</span></a>        <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNetBase-376"><a href="#ResNetBase-376"><span class="linenos">376</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="ResNetBase-377"><a href="#ResNetBase-377"><span class="linenos">377</span></a>        <span class="n">batch_normalisation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="ResNetBase-378"><a href="#ResNetBase-378"><span class="linenos">378</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="ResNetBase-379"><a href="#ResNetBase-379"><span class="linenos">379</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ResNetBase-380"><a href="#ResNetBase-380"><span class="linenos">380</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the ResNet backbone network.</span>
</span><span id="ResNetBase-381"><a href="#ResNetBase-381"><span class="linenos">381</span></a>
</span><span id="ResNetBase-382"><a href="#ResNetBase-382"><span class="linenos">382</span></a><span class="sd">        **Args:**</span>
</span><span id="ResNetBase-383"><a href="#ResNetBase-383"><span class="linenos">383</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input. Image data are kept channels when going in ResNet. Note that convolutional networks require number of input channels instead of dimension.</span>
</span><span id="ResNetBase-384"><a href="#ResNetBase-384"><span class="linenos">384</span></a><span class="sd">        - **building_block_type** (`ResNetBlockSmall` | `ResNetBlockLarge`): the type of building block used in the ResNet.</span>
</span><span id="ResNetBase-385"><a href="#ResNetBase-385"><span class="linenos">385</span></a><span class="sd">        - **building_block_nums** (`tuple[int, int, int, int]`): the number of building blocks in the 2-5 convolutional layer correspondingly.</span>
</span><span id="ResNetBase-386"><a href="#ResNetBase-386"><span class="linenos">386</span></a><span class="sd">        - **building_block_preceding_output_channels** (`tuple[int, int, int, int]`): the number of channels of preceding output of each building block in the 2-5 convolutional layer correspondingly.</span>
</span><span id="ResNetBase-387"><a href="#ResNetBase-387"><span class="linenos">387</span></a><span class="sd">        - **building_block_input_channels** (`tuple[int, int, int, int]`): the number of channels of input of each building block in the 2-5 convolutional layer correspondingly.</span>
</span><span id="ResNetBase-388"><a href="#ResNetBase-388"><span class="linenos">388</span></a><span class="sd">        - **output_dim** (`int`): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</span>
</span><span id="ResNetBase-389"><a href="#ResNetBase-389"><span class="linenos">389</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="ResNetBase-390"><a href="#ResNetBase-390"><span class="linenos">390</span></a><span class="sd">        - **batch_normalisation** (`bool`): whether to use batch normalisation after the weight convolutional layers. Default `True`, same as what the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) does.</span>
</span><span id="ResNetBase-391"><a href="#ResNetBase-391"><span class="linenos">391</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`, because batch normalisation are doing the similar thing with bias.</span>
</span><span id="ResNetBase-392"><a href="#ResNetBase-392"><span class="linenos">392</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="ResNetBase-393"><a href="#ResNetBase-393"><span class="linenos">393</span></a>        <span class="n">CLBackbone</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">)</span>
</span><span id="ResNetBase-394"><a href="#ResNetBase-394"><span class="linenos">394</span></a>
</span><span id="ResNetBase-395"><a href="#ResNetBase-395"><span class="linenos">395</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">batch_normalisation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">batch_normalisation</span>
</span><span id="ResNetBase-396"><a href="#ResNetBase-396"><span class="linenos">396</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store whether to use batch normalisation after the fully-connected layers.&quot;&quot;&quot;</span>
</span><span id="ResNetBase-397"><a href="#ResNetBase-397"><span class="linenos">397</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">activation_layer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span><span id="ResNetBase-398"><a href="#ResNetBase-398"><span class="linenos">398</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store whether to use activation function after the fully-connected layers.&quot;&quot;&quot;</span>
</span><span id="ResNetBase-399"><a href="#ResNetBase-399"><span class="linenos">399</span></a>
</span><span id="ResNetBase-400"><a href="#ResNetBase-400"><span class="linenos">400</span></a>        <span class="c1"># construct the 1st weighted convolutional layer and attached layers (batchnorm, activation, etc)</span>
</span><span id="ResNetBase-401"><a href="#ResNetBase-401"><span class="linenos">401</span></a>        <span class="n">layer_input_channels</span> <span class="o">=</span> <span class="n">input_channels</span>  <span class="c1"># the input channels of the 1st convolutional layer, which receive the input of the entire network</span>
</span><span id="ResNetBase-402"><a href="#ResNetBase-402"><span class="linenos">402</span></a>        <span class="n">layer_output_channels</span> <span class="o">=</span> <span class="mi">64</span>  <span class="c1"># the output channels of the 1st convolutional layer</span>
</span><span id="ResNetBase-403"><a href="#ResNetBase-403"><span class="linenos">403</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
</span><span id="ResNetBase-404"><a href="#ResNetBase-404"><span class="linenos">404</span></a>            <span class="n">in_channels</span><span class="o">=</span><span class="n">layer_input_channels</span><span class="p">,</span>
</span><span id="ResNetBase-405"><a href="#ResNetBase-405"><span class="linenos">405</span></a>            <span class="n">out_channels</span><span class="o">=</span><span class="n">layer_output_channels</span><span class="p">,</span>
</span><span id="ResNetBase-406"><a href="#ResNetBase-406"><span class="linenos">406</span></a>            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
</span><span id="ResNetBase-407"><a href="#ResNetBase-407"><span class="linenos">407</span></a>            <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
</span><span id="ResNetBase-408"><a href="#ResNetBase-408"><span class="linenos">408</span></a>            <span class="n">padding</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
</span><span id="ResNetBase-409"><a href="#ResNetBase-409"><span class="linenos">409</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="ResNetBase-410"><a href="#ResNetBase-410"><span class="linenos">410</span></a>        <span class="p">)</span>  <span class="c1"># construct the 1st weight convolutional layer of the entire ResNet</span>
</span><span id="ResNetBase-411"><a href="#ResNetBase-411"><span class="linenos">411</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The 1st weight convolutional layer of the entire ResNet. It  is always with fixed kernel size 7x7, stride 2, and padding 3. &quot;&quot;&quot;</span>
</span><span id="ResNetBase-412"><a href="#ResNetBase-412"><span class="linenos">412</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;conv1&quot;</span><span class="p">)</span>  <span class="c1"># collect the layer name to be masked</span>
</span><span id="ResNetBase-413"><a href="#ResNetBase-413"><span class="linenos">413</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_normalisation</span><span class="p">:</span>
</span><span id="ResNetBase-414"><a href="#ResNetBase-414"><span class="linenos">414</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv_bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span>
</span><span id="ResNetBase-415"><a href="#ResNetBase-415"><span class="linenos">415</span></a>                <span class="n">num_features</span><span class="o">=</span><span class="n">layer_output_channels</span>
</span><span id="ResNetBase-416"><a href="#ResNetBase-416"><span class="linenos">416</span></a>            <span class="p">)</span>  <span class="c1"># construct the batch normalisation layer</span>
</span><span id="ResNetBase-417"><a href="#ResNetBase-417"><span class="linenos">417</span></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The batch normalisation (`nn.BatchNorm2d`) layer after the 1st weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="ResNetBase-418"><a href="#ResNetBase-418"><span class="linenos">418</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="ResNetBase-419"><a href="#ResNetBase-419"><span class="linenos">419</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation1</span> <span class="o">=</span> <span class="n">activation_layer</span><span class="p">()</span>  <span class="c1"># construct the activation layer</span>
</span><span id="ResNetBase-420"><a href="#ResNetBase-420"><span class="linenos">420</span></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The activation layer after the 1st weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="ResNetBase-421"><a href="#ResNetBase-421"><span class="linenos">421</span></a>
</span><span id="ResNetBase-422"><a href="#ResNetBase-422"><span class="linenos">422</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1">#</span>
</span><span id="ResNetBase-423"><a href="#ResNetBase-423"><span class="linenos">423</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The max pooling layer which is laid in between 1st and 2nd convolutional layers with kernel size 3x3, stride 2. &quot;&quot;&quot;</span>
</span><span id="ResNetBase-424"><a href="#ResNetBase-424"><span class="linenos">424</span></a>
</span><span id="ResNetBase-425"><a href="#ResNetBase-425"><span class="linenos">425</span></a>        <span class="c1"># construct the 2nd convolutional layer with multiple blocks, and its attached layers (batchnorm, activation, etc)</span>
</span><span id="ResNetBase-426"><a href="#ResNetBase-426"><span class="linenos">426</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv2x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_multiple_blocks</span><span class="p">(</span>
</span><span id="ResNetBase-427"><a href="#ResNetBase-427"><span class="linenos">427</span></a>            <span class="n">layer_name</span><span class="o">=</span><span class="s2">&quot;conv2x&quot;</span><span class="p">,</span>
</span><span id="ResNetBase-428"><a href="#ResNetBase-428"><span class="linenos">428</span></a>            <span class="n">building_block_type</span><span class="o">=</span><span class="n">building_block_type</span><span class="p">,</span>
</span><span id="ResNetBase-429"><a href="#ResNetBase-429"><span class="linenos">429</span></a>            <span class="n">building_block_num</span><span class="o">=</span><span class="n">building_block_nums</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
</span><span id="ResNetBase-430"><a href="#ResNetBase-430"><span class="linenos">430</span></a>            <span class="n">preceding_output_channels</span><span class="o">=</span><span class="n">building_block_preceding_output_channels</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
</span><span id="ResNetBase-431"><a href="#ResNetBase-431"><span class="linenos">431</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">building_block_input_channels</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
</span><span id="ResNetBase-432"><a href="#ResNetBase-432"><span class="linenos">432</span></a>            <span class="n">overall_stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>  <span class="c1"># the overall stride of the 2nd convolutional layer should be 1, as the preceding maxpooling layer has stride 2, which already made 112x112 -&gt; 56x56. See Table 2 in the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) for details.</span>
</span><span id="ResNetBase-433"><a href="#ResNetBase-433"><span class="linenos">433</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="ResNetBase-434"><a href="#ResNetBase-434"><span class="linenos">434</span></a>            <span class="n">batch_normalisation</span><span class="o">=</span><span class="n">batch_normalisation</span><span class="p">,</span>
</span><span id="ResNetBase-435"><a href="#ResNetBase-435"><span class="linenos">435</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="ResNetBase-436"><a href="#ResNetBase-436"><span class="linenos">436</span></a>        <span class="p">)</span>
</span><span id="ResNetBase-437"><a href="#ResNetBase-437"><span class="linenos">437</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The 2nd convolutional layer of the ResNet, which contains multiple blocks. &quot;&quot;&quot;</span>
</span><span id="ResNetBase-438"><a href="#ResNetBase-438"><span class="linenos">438</span></a>
</span><span id="ResNetBase-439"><a href="#ResNetBase-439"><span class="linenos">439</span></a>        <span class="c1"># construct the 3rd convolutional layer with multiple blocks, and its attached layers (batchnorm, activation, etc)</span>
</span><span id="ResNetBase-440"><a href="#ResNetBase-440"><span class="linenos">440</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv3x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_multiple_blocks</span><span class="p">(</span>
</span><span id="ResNetBase-441"><a href="#ResNetBase-441"><span class="linenos">441</span></a>            <span class="n">layer_name</span><span class="o">=</span><span class="s2">&quot;conv3x&quot;</span><span class="p">,</span>
</span><span id="ResNetBase-442"><a href="#ResNetBase-442"><span class="linenos">442</span></a>            <span class="n">building_block_type</span><span class="o">=</span><span class="n">building_block_type</span><span class="p">,</span>
</span><span id="ResNetBase-443"><a href="#ResNetBase-443"><span class="linenos">443</span></a>            <span class="n">building_block_num</span><span class="o">=</span><span class="n">building_block_nums</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
</span><span id="ResNetBase-444"><a href="#ResNetBase-444"><span class="linenos">444</span></a>            <span class="n">preceding_output_channels</span><span class="o">=</span><span class="n">building_block_preceding_output_channels</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
</span><span id="ResNetBase-445"><a href="#ResNetBase-445"><span class="linenos">445</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">building_block_input_channels</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
</span><span id="ResNetBase-446"><a href="#ResNetBase-446"><span class="linenos">446</span></a>            <span class="n">overall_stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># the overall stride of the 3rd convolutional layer should be 2, making 56x56 -&gt; 28x28. See Table 2 in the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) for details.</span>
</span><span id="ResNetBase-447"><a href="#ResNetBase-447"><span class="linenos">447</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="ResNetBase-448"><a href="#ResNetBase-448"><span class="linenos">448</span></a>            <span class="n">batch_normalisation</span><span class="o">=</span><span class="n">batch_normalisation</span><span class="p">,</span>
</span><span id="ResNetBase-449"><a href="#ResNetBase-449"><span class="linenos">449</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="ResNetBase-450"><a href="#ResNetBase-450"><span class="linenos">450</span></a>        <span class="p">)</span>
</span><span id="ResNetBase-451"><a href="#ResNetBase-451"><span class="linenos">451</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The 3rd convolutional layer of the ResNet, which contains multiple blocks. &quot;&quot;&quot;</span>
</span><span id="ResNetBase-452"><a href="#ResNetBase-452"><span class="linenos">452</span></a>
</span><span id="ResNetBase-453"><a href="#ResNetBase-453"><span class="linenos">453</span></a>        <span class="c1"># construct the 4th convolutional layer with multiple blocks, and its attached layers (batchnorm, activation, etc)</span>
</span><span id="ResNetBase-454"><a href="#ResNetBase-454"><span class="linenos">454</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv4x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_multiple_blocks</span><span class="p">(</span>
</span><span id="ResNetBase-455"><a href="#ResNetBase-455"><span class="linenos">455</span></a>            <span class="n">layer_name</span><span class="o">=</span><span class="s2">&quot;conv4x&quot;</span><span class="p">,</span>
</span><span id="ResNetBase-456"><a href="#ResNetBase-456"><span class="linenos">456</span></a>            <span class="n">building_block_type</span><span class="o">=</span><span class="n">building_block_type</span><span class="p">,</span>
</span><span id="ResNetBase-457"><a href="#ResNetBase-457"><span class="linenos">457</span></a>            <span class="n">building_block_num</span><span class="o">=</span><span class="n">building_block_nums</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
</span><span id="ResNetBase-458"><a href="#ResNetBase-458"><span class="linenos">458</span></a>            <span class="n">preceding_output_channels</span><span class="o">=</span><span class="n">building_block_preceding_output_channels</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
</span><span id="ResNetBase-459"><a href="#ResNetBase-459"><span class="linenos">459</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">building_block_input_channels</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
</span><span id="ResNetBase-460"><a href="#ResNetBase-460"><span class="linenos">460</span></a>            <span class="n">overall_stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># the overall stride of the 4th convolutional layer should be 2, making 28x28 -&gt; 14x14. See Table 2 in the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) for details.</span>
</span><span id="ResNetBase-461"><a href="#ResNetBase-461"><span class="linenos">461</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="ResNetBase-462"><a href="#ResNetBase-462"><span class="linenos">462</span></a>            <span class="n">batch_normalisation</span><span class="o">=</span><span class="n">batch_normalisation</span><span class="p">,</span>
</span><span id="ResNetBase-463"><a href="#ResNetBase-463"><span class="linenos">463</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="ResNetBase-464"><a href="#ResNetBase-464"><span class="linenos">464</span></a>        <span class="p">)</span>
</span><span id="ResNetBase-465"><a href="#ResNetBase-465"><span class="linenos">465</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The 4th convolutional layer of the ResNet, which contains multiple blocks. &quot;&quot;&quot;</span>
</span><span id="ResNetBase-466"><a href="#ResNetBase-466"><span class="linenos">466</span></a>
</span><span id="ResNetBase-467"><a href="#ResNetBase-467"><span class="linenos">467</span></a>        <span class="c1"># construct the 5th convolutional layer with multiple blocks, and its attached layers (batchnorm, activation, etc)</span>
</span><span id="ResNetBase-468"><a href="#ResNetBase-468"><span class="linenos">468</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv5x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_multiple_blocks</span><span class="p">(</span>
</span><span id="ResNetBase-469"><a href="#ResNetBase-469"><span class="linenos">469</span></a>            <span class="n">layer_name</span><span class="o">=</span><span class="s2">&quot;conv5x&quot;</span><span class="p">,</span>
</span><span id="ResNetBase-470"><a href="#ResNetBase-470"><span class="linenos">470</span></a>            <span class="n">building_block_type</span><span class="o">=</span><span class="n">building_block_type</span><span class="p">,</span>
</span><span id="ResNetBase-471"><a href="#ResNetBase-471"><span class="linenos">471</span></a>            <span class="n">building_block_num</span><span class="o">=</span><span class="n">building_block_nums</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span>
</span><span id="ResNetBase-472"><a href="#ResNetBase-472"><span class="linenos">472</span></a>            <span class="n">preceding_output_channels</span><span class="o">=</span><span class="n">building_block_preceding_output_channels</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span>
</span><span id="ResNetBase-473"><a href="#ResNetBase-473"><span class="linenos">473</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">building_block_input_channels</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span>
</span><span id="ResNetBase-474"><a href="#ResNetBase-474"><span class="linenos">474</span></a>            <span class="n">overall_stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># the overall stride of the 2nd convolutional layer should be 2, making 14x14 -&gt; 7x7. See Table 2 in the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) for details.</span>
</span><span id="ResNetBase-475"><a href="#ResNetBase-475"><span class="linenos">475</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="ResNetBase-476"><a href="#ResNetBase-476"><span class="linenos">476</span></a>            <span class="n">batch_normalisation</span><span class="o">=</span><span class="n">batch_normalisation</span><span class="p">,</span>
</span><span id="ResNetBase-477"><a href="#ResNetBase-477"><span class="linenos">477</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="ResNetBase-478"><a href="#ResNetBase-478"><span class="linenos">478</span></a>        <span class="p">)</span>
</span><span id="ResNetBase-479"><a href="#ResNetBase-479"><span class="linenos">479</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The 5th convolutional layer of the ResNet, which contains multiple blocks. &quot;&quot;&quot;</span>
</span><span id="ResNetBase-480"><a href="#ResNetBase-480"><span class="linenos">480</span></a>
</span><span id="ResNetBase-481"><a href="#ResNetBase-481"><span class="linenos">481</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">avepool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">(</span><span class="n">output_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span><span id="ResNetBase-482"><a href="#ResNetBase-482"><span class="linenos">482</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The average pooling layer which is laid after the convolutional layers and before feature maps are flattened. &quot;&quot;&quot;</span>
</span><span id="ResNetBase-483"><a href="#ResNetBase-483"><span class="linenos">483</span></a>
</span><span id="ResNetBase-484"><a href="#ResNetBase-484"><span class="linenos">484</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_multiple_blocks</span><span class="p">(</span>
</span><span id="ResNetBase-485"><a href="#ResNetBase-485"><span class="linenos">485</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="ResNetBase-486"><a href="#ResNetBase-486"><span class="linenos">486</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="ResNetBase-487"><a href="#ResNetBase-487"><span class="linenos">487</span></a>        <span class="n">building_block_type</span><span class="p">:</span> <span class="n">ResNetBlockSmall</span> <span class="o">|</span> <span class="n">ResNetBlockLarge</span><span class="p">,</span>
</span><span id="ResNetBase-488"><a href="#ResNetBase-488"><span class="linenos">488</span></a>        <span class="n">building_block_num</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNetBase-489"><a href="#ResNetBase-489"><span class="linenos">489</span></a>        <span class="n">preceding_output_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNetBase-490"><a href="#ResNetBase-490"><span class="linenos">490</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNetBase-491"><a href="#ResNetBase-491"><span class="linenos">491</span></a>        <span class="n">overall_stride</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNetBase-492"><a href="#ResNetBase-492"><span class="linenos">492</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="ResNetBase-493"><a href="#ResNetBase-493"><span class="linenos">493</span></a>        <span class="n">batch_normalisation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="ResNetBase-494"><a href="#ResNetBase-494"><span class="linenos">494</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="ResNetBase-495"><a href="#ResNetBase-495"><span class="linenos">495</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">:</span>
</span><span id="ResNetBase-496"><a href="#ResNetBase-496"><span class="linenos">496</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct a layer consisting of multiple building blocks. It&#39;s used to construct the 2-5 convolutional layers of the ResNet.</span>
</span><span id="ResNetBase-497"><a href="#ResNetBase-497"><span class="linenos">497</span></a>
</span><span id="ResNetBase-498"><a href="#ResNetBase-498"><span class="linenos">498</span></a><span class="sd">        The &quot;shortcut connections&quot; are performed between the input and output of each building block:</span>
</span><span id="ResNetBase-499"><a href="#ResNetBase-499"><span class="linenos">499</span></a><span class="sd">        1. If the input and output of the building block have exactly the same dimensions (including number of channels and size), add the input to the output.</span>
</span><span id="ResNetBase-500"><a href="#ResNetBase-500"><span class="linenos">500</span></a><span class="sd">        2. If the input and output of the building block have different dimensions (including number of channels and size), add the input to the output after a convolutional layer to make the dimensions match.</span>
</span><span id="ResNetBase-501"><a href="#ResNetBase-501"><span class="linenos">501</span></a>
</span><span id="ResNetBase-502"><a href="#ResNetBase-502"><span class="linenos">502</span></a><span class="sd">        **Args:**</span>
</span><span id="ResNetBase-503"><a href="#ResNetBase-503"><span class="linenos">503</span></a><span class="sd">        - **layer_name** (`str`): pass the name of this multi-building-block layer to construct the full name of each weighted convolutional layer.</span>
</span><span id="ResNetBase-504"><a href="#ResNetBase-504"><span class="linenos">504</span></a><span class="sd">        - **building_block_type** (`ResNetBlockSmall` | `ResNetBlockLarge`): the type of the building block.</span>
</span><span id="ResNetBase-505"><a href="#ResNetBase-505"><span class="linenos">505</span></a><span class="sd">        - **building_block_num** (`int`): the number of building blocks in this multi-building-block layer.</span>
</span><span id="ResNetBase-506"><a href="#ResNetBase-506"><span class="linenos">506</span></a><span class="sd">        - **preceding_output_channels** (`int`): the number of channels of preceding output of this entire multi-building-block layer.</span>
</span><span id="ResNetBase-507"><a href="#ResNetBase-507"><span class="linenos">507</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input of this multi-building-block layer.</span>
</span><span id="ResNetBase-508"><a href="#ResNetBase-508"><span class="linenos">508</span></a><span class="sd">        - **overall_stride** (`int`): the overall stride of the building blocks. This stride is performed at the 1st building block where other building blocks remain their own overall stride of 1. Inside that building block, this stride is performed at certain convolutional layer in the building block where other convolutional layers remain stride of 1:</span>
</span><span id="ResNetBase-509"><a href="#ResNetBase-509"><span class="linenos">509</span></a><span class="sd">            - For `ResNetBlockSmall`, it performs at the 2nd (last) layer.</span>
</span><span id="ResNetBase-510"><a href="#ResNetBase-510"><span class="linenos">510</span></a><span class="sd">            - For `ResNetBlockLarge`, it performs at the 2nd (middle) layer.</span>
</span><span id="ResNetBase-511"><a href="#ResNetBase-511"><span class="linenos">511</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="ResNetBase-512"><a href="#ResNetBase-512"><span class="linenos">512</span></a><span class="sd">        - **batch_normalisation** (`bool`): whether to use batch normalisation after the weight convolutional layers. Default `True`, same as what the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) does.</span>
</span><span id="ResNetBase-513"><a href="#ResNetBase-513"><span class="linenos">513</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`, because batch normalisation are doing the similar thing with bias.</span>
</span><span id="ResNetBase-514"><a href="#ResNetBase-514"><span class="linenos">514</span></a>
</span><span id="ResNetBase-515"><a href="#ResNetBase-515"><span class="linenos">515</span></a><span class="sd">        **Returns:**</span>
</span><span id="ResNetBase-516"><a href="#ResNetBase-516"><span class="linenos">516</span></a><span class="sd">        - **layer** (`nn.Sequential`): the constructed layer consisting of multiple building blocks.</span>
</span><span id="ResNetBase-517"><a href="#ResNetBase-517"><span class="linenos">517</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="ResNetBase-518"><a href="#ResNetBase-518"><span class="linenos">518</span></a>
</span><span id="ResNetBase-519"><a href="#ResNetBase-519"><span class="linenos">519</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="ResNetBase-520"><a href="#ResNetBase-520"><span class="linenos">520</span></a>
</span><span id="ResNetBase-521"><a href="#ResNetBase-521"><span class="linenos">521</span></a>        <span class="k">for</span> <span class="n">block_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">building_block_num</span><span class="p">):</span>
</span><span id="ResNetBase-522"><a href="#ResNetBase-522"><span class="linenos">522</span></a>            <span class="n">layer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="ResNetBase-523"><a href="#ResNetBase-523"><span class="linenos">523</span></a>                <span class="n">building_block_type</span><span class="p">(</span>
</span><span id="ResNetBase-524"><a href="#ResNetBase-524"><span class="linenos">524</span></a>                    <span class="n">outer_layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="ResNetBase-525"><a href="#ResNetBase-525"><span class="linenos">525</span></a>                    <span class="n">block_idx</span><span class="o">=</span><span class="n">block_idx</span><span class="p">,</span>
</span><span id="ResNetBase-526"><a href="#ResNetBase-526"><span class="linenos">526</span></a>                    <span class="n">preceding_output_channels</span><span class="o">=</span><span class="p">(</span>
</span><span id="ResNetBase-527"><a href="#ResNetBase-527"><span class="linenos">527</span></a>                        <span class="n">preceding_output_channels</span>
</span><span id="ResNetBase-528"><a href="#ResNetBase-528"><span class="linenos">528</span></a>                        <span class="k">if</span> <span class="n">block_idx</span> <span class="o">==</span> <span class="mi">0</span>
</span><span id="ResNetBase-529"><a href="#ResNetBase-529"><span class="linenos">529</span></a>                        <span class="k">else</span> <span class="p">(</span>
</span><span id="ResNetBase-530"><a href="#ResNetBase-530"><span class="linenos">530</span></a>                            <span class="n">input_channels</span>
</span><span id="ResNetBase-531"><a href="#ResNetBase-531"><span class="linenos">531</span></a>                            <span class="k">if</span> <span class="n">building_block_type</span> <span class="o">==</span> <span class="n">ResNetBlockSmall</span>
</span><span id="ResNetBase-532"><a href="#ResNetBase-532"><span class="linenos">532</span></a>                            <span class="k">else</span> <span class="n">input_channels</span> <span class="o">*</span> <span class="mi">4</span>
</span><span id="ResNetBase-533"><a href="#ResNetBase-533"><span class="linenos">533</span></a>                        <span class="p">)</span>
</span><span id="ResNetBase-534"><a href="#ResNetBase-534"><span class="linenos">534</span></a>                    <span class="p">),</span>  <span class="c1"># if it&#39;s the 1st block in this multi-building-block layer, it should be the number of channels of the preceding output of this entire multi-building-block layer. Otherwise, it should be the number of channels from last building block where the number of channels is 4 times expanded as the input channels for `ResNetBlockLarge` than `ResNetBlockSmall`.</span>
</span><span id="ResNetBase-535"><a href="#ResNetBase-535"><span class="linenos">535</span></a>                    <span class="n">input_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
</span><span id="ResNetBase-536"><a href="#ResNetBase-536"><span class="linenos">536</span></a>                    <span class="n">overall_stride</span><span class="o">=</span><span class="p">(</span>
</span><span id="ResNetBase-537"><a href="#ResNetBase-537"><span class="linenos">537</span></a>                        <span class="n">overall_stride</span> <span class="k">if</span> <span class="n">block_idx</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">1</span>
</span><span id="ResNetBase-538"><a href="#ResNetBase-538"><span class="linenos">538</span></a>                    <span class="p">),</span>  <span class="c1"># only perform the overall stride at the 1st block in this multi-building-block layer</span>
</span><span id="ResNetBase-539"><a href="#ResNetBase-539"><span class="linenos">539</span></a>                    <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="ResNetBase-540"><a href="#ResNetBase-540"><span class="linenos">540</span></a>                    <span class="n">batch_normalisation</span><span class="o">=</span><span class="n">batch_normalisation</span><span class="p">,</span>
</span><span id="ResNetBase-541"><a href="#ResNetBase-541"><span class="linenos">541</span></a>                    <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="ResNetBase-542"><a href="#ResNetBase-542"><span class="linenos">542</span></a>                <span class="p">)</span>
</span><span id="ResNetBase-543"><a href="#ResNetBase-543"><span class="linenos">543</span></a>            <span class="p">)</span>
</span><span id="ResNetBase-544"><a href="#ResNetBase-544"><span class="linenos">544</span></a>
</span><span id="ResNetBase-545"><a href="#ResNetBase-545"><span class="linenos">545</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span> <span class="o">+=</span> <span class="n">layer</span><span class="p">[</span>
</span><span id="ResNetBase-546"><a href="#ResNetBase-546"><span class="linenos">546</span></a>                <span class="o">-</span><span class="mi">1</span>
</span><span id="ResNetBase-547"><a href="#ResNetBase-547"><span class="linenos">547</span></a>            <span class="p">]</span><span class="o">.</span><span class="n">weighted_layer_names</span>  <span class="c1"># collect the weighted layer names in the blocks and sync to the weighted layer names list in the outer network</span>
</span><span id="ResNetBase-548"><a href="#ResNetBase-548"><span class="linenos">548</span></a>
</span><span id="ResNetBase-549"><a href="#ResNetBase-549"><span class="linenos">549</span></a>        <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layer</span><span class="p">)</span>
</span><span id="ResNetBase-550"><a href="#ResNetBase-550"><span class="linenos">550</span></a>
</span><span id="ResNetBase-551"><a href="#ResNetBase-551"><span class="linenos">551</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="ResNetBase-552"><a href="#ResNetBase-552"><span class="linenos">552</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">stage</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">task_id</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="ResNetBase-553"><a href="#ResNetBase-553"><span class="linenos">553</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
</span><span id="ResNetBase-554"><a href="#ResNetBase-554"><span class="linenos">554</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The forward pass for data. It is the same for all tasks.</span>
</span><span id="ResNetBase-555"><a href="#ResNetBase-555"><span class="linenos">555</span></a>
</span><span id="ResNetBase-556"><a href="#ResNetBase-556"><span class="linenos">556</span></a><span class="sd">        **Args:**</span>
</span><span id="ResNetBase-557"><a href="#ResNetBase-557"><span class="linenos">557</span></a><span class="sd">        - **input** (`Tensor`): the input tensor from data.</span>
</span><span id="ResNetBase-558"><a href="#ResNetBase-558"><span class="linenos">558</span></a>
</span><span id="ResNetBase-559"><a href="#ResNetBase-559"><span class="linenos">559</span></a><span class="sd">        **Returns:**</span>
</span><span id="ResNetBase-560"><a href="#ResNetBase-560"><span class="linenos">560</span></a><span class="sd">        - **output_feature** (`Tensor`): the output feature tensor to be passed into heads. This is the main target of backpropagation.</span>
</span><span id="ResNetBase-561"><a href="#ResNetBase-561"><span class="linenos">561</span></a><span class="sd">        - **hidden_features** (`dict[str, Tensor]`): the hidden features (after activation) in each weighted layer. Key (`str`) is the weighted layer name, value (`Tensor`) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes.</span>
</span><span id="ResNetBase-562"><a href="#ResNetBase-562"><span class="linenos">562</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="ResNetBase-563"><a href="#ResNetBase-563"><span class="linenos">563</span></a>        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="ResNetBase-564"><a href="#ResNetBase-564"><span class="linenos">564</span></a>        <span class="n">hidden_features</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="ResNetBase-565"><a href="#ResNetBase-565"><span class="linenos">565</span></a>
</span><span id="ResNetBase-566"><a href="#ResNetBase-566"><span class="linenos">566</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="nb">input</span>
</span><span id="ResNetBase-567"><a href="#ResNetBase-567"><span class="linenos">567</span></a>
</span><span id="ResNetBase-568"><a href="#ResNetBase-568"><span class="linenos">568</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNetBase-569"><a href="#ResNetBase-569"><span class="linenos">569</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_normalisation</span><span class="p">:</span>
</span><span id="ResNetBase-570"><a href="#ResNetBase-570"><span class="linenos">570</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_bn1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNetBase-571"><a href="#ResNetBase-571"><span class="linenos">571</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="ResNetBase-572"><a href="#ResNetBase-572"><span class="linenos">572</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNetBase-573"><a href="#ResNetBase-573"><span class="linenos">573</span></a>        <span class="n">hidden_features</span><span class="p">[</span><span class="s2">&quot;conv1&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="ResNetBase-574"><a href="#ResNetBase-574"><span class="linenos">574</span></a>
</span><span id="ResNetBase-575"><a href="#ResNetBase-575"><span class="linenos">575</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNetBase-576"><a href="#ResNetBase-576"><span class="linenos">576</span></a>
</span><span id="ResNetBase-577"><a href="#ResNetBase-577"><span class="linenos">577</span></a>        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2x</span><span class="p">:</span>
</span><span id="ResNetBase-578"><a href="#ResNetBase-578"><span class="linenos">578</span></a>            <span class="n">x</span><span class="p">,</span> <span class="n">hidden_features_block</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNetBase-579"><a href="#ResNetBase-579"><span class="linenos">579</span></a>            <span class="n">hidden_features</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">hidden_features_block</span><span class="p">)</span>  <span class="c1"># store the hidden feature</span>
</span><span id="ResNetBase-580"><a href="#ResNetBase-580"><span class="linenos">580</span></a>        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv3x</span><span class="p">:</span>
</span><span id="ResNetBase-581"><a href="#ResNetBase-581"><span class="linenos">581</span></a>            <span class="n">x</span><span class="p">,</span> <span class="n">hidden_features_block</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNetBase-582"><a href="#ResNetBase-582"><span class="linenos">582</span></a>            <span class="n">hidden_features</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">hidden_features_block</span><span class="p">)</span>  <span class="c1"># store the hidden feature</span>
</span><span id="ResNetBase-583"><a href="#ResNetBase-583"><span class="linenos">583</span></a>        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv4x</span><span class="p">:</span>
</span><span id="ResNetBase-584"><a href="#ResNetBase-584"><span class="linenos">584</span></a>            <span class="n">x</span><span class="p">,</span> <span class="n">hidden_features_block</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNetBase-585"><a href="#ResNetBase-585"><span class="linenos">585</span></a>            <span class="n">hidden_features</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">hidden_features_block</span><span class="p">)</span>  <span class="c1"># store the hidden feature</span>
</span><span id="ResNetBase-586"><a href="#ResNetBase-586"><span class="linenos">586</span></a>        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv5x</span><span class="p">:</span>
</span><span id="ResNetBase-587"><a href="#ResNetBase-587"><span class="linenos">587</span></a>            <span class="n">x</span><span class="p">,</span> <span class="n">hidden_features_block</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNetBase-588"><a href="#ResNetBase-588"><span class="linenos">588</span></a>            <span class="n">hidden_features</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">hidden_features_block</span><span class="p">)</span>  <span class="c1"># store the hidden feature</span>
</span><span id="ResNetBase-589"><a href="#ResNetBase-589"><span class="linenos">589</span></a>
</span><span id="ResNetBase-590"><a href="#ResNetBase-590"><span class="linenos">590</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">avepool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNetBase-591"><a href="#ResNetBase-591"><span class="linenos">591</span></a>
</span><span id="ResNetBase-592"><a href="#ResNetBase-592"><span class="linenos">592</span></a>        <span class="n">output_feature</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># flatten before going through heads</span>
</span><span id="ResNetBase-593"><a href="#ResNetBase-593"><span class="linenos">593</span></a>
</span><span id="ResNetBase-594"><a href="#ResNetBase-594"><span class="linenos">594</span></a>        <span class="k">return</span> <span class="n">output_feature</span><span class="p">,</span> <span class="n">hidden_features</span>
</span></pre></div>


            <div class="docstring"><p>The base class of <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">residual network (ResNet)</a>.</p>

<p>ResNet is a convolutional network architecture, which has 1st convolutional parameter layer and a maxpooling layer, connecting to 4 convolutional layers which contains multiple convolutional parameter layer. Each layer of the 4 are constructed from basic building blocks which are either small (<code><a href="#ResNetBlockSmall">ResNetBlockSmall</a></code>) or large (<code><a href="#ResNetBlockLarge">ResNetBlockLarge</a></code>). Each building block contains several convolutional parameter layers. The building blocks are connected by a skip connection which is a direct connection from the input of the block to the output of the block, and this is why it's called residual (find "shortcut connections" in the paper for more details). After the 5th convolutional layer, there are average pooling layer and a fully connected layer which connects to the CL output heads.</p>
</div>


                            <div id="ResNetBase.__init__" class="classattr">
                                        <input id="ResNetBase.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">ResNetBase</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">building_block_type</span><span class="p">:</span> <span class="n"><a href="#ResNetBlockSmall">ResNetBlockSmall</a></span> <span class="o">|</span> <span class="n"><a href="#ResNetBlockLarge">ResNetBlockLarge</a></span>,</span><span class="param">	<span class="n">building_block_nums</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span>,</span><span class="param">	<span class="n">building_block_preceding_output_channels</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span>,</span><span class="param">	<span class="n">building_block_input_channels</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span>,</span><span class="param">	<span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	activation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class="param">	<span class="n">batch_normalisation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>,</span><span class="param">	<span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span></span>)</span>

                <label class="view-source-button" for="ResNetBase.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#ResNetBase.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ResNetBase.__init__-368"><a href="#ResNetBase.__init__-368"><span class="linenos">368</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="ResNetBase.__init__-369"><a href="#ResNetBase.__init__-369"><span class="linenos">369</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="ResNetBase.__init__-370"><a href="#ResNetBase.__init__-370"><span class="linenos">370</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNetBase.__init__-371"><a href="#ResNetBase.__init__-371"><span class="linenos">371</span></a>        <span class="n">building_block_type</span><span class="p">:</span> <span class="n">ResNetBlockSmall</span> <span class="o">|</span> <span class="n">ResNetBlockLarge</span><span class="p">,</span>
</span><span id="ResNetBase.__init__-372"><a href="#ResNetBase.__init__-372"><span class="linenos">372</span></a>        <span class="n">building_block_nums</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
</span><span id="ResNetBase.__init__-373"><a href="#ResNetBase.__init__-373"><span class="linenos">373</span></a>        <span class="n">building_block_preceding_output_channels</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
</span><span id="ResNetBase.__init__-374"><a href="#ResNetBase.__init__-374"><span class="linenos">374</span></a>        <span class="n">building_block_input_channels</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
</span><span id="ResNetBase.__init__-375"><a href="#ResNetBase.__init__-375"><span class="linenos">375</span></a>        <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNetBase.__init__-376"><a href="#ResNetBase.__init__-376"><span class="linenos">376</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="ResNetBase.__init__-377"><a href="#ResNetBase.__init__-377"><span class="linenos">377</span></a>        <span class="n">batch_normalisation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="ResNetBase.__init__-378"><a href="#ResNetBase.__init__-378"><span class="linenos">378</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="ResNetBase.__init__-379"><a href="#ResNetBase.__init__-379"><span class="linenos">379</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ResNetBase.__init__-380"><a href="#ResNetBase.__init__-380"><span class="linenos">380</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the ResNet backbone network.</span>
</span><span id="ResNetBase.__init__-381"><a href="#ResNetBase.__init__-381"><span class="linenos">381</span></a>
</span><span id="ResNetBase.__init__-382"><a href="#ResNetBase.__init__-382"><span class="linenos">382</span></a><span class="sd">        **Args:**</span>
</span><span id="ResNetBase.__init__-383"><a href="#ResNetBase.__init__-383"><span class="linenos">383</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input. Image data are kept channels when going in ResNet. Note that convolutional networks require number of input channels instead of dimension.</span>
</span><span id="ResNetBase.__init__-384"><a href="#ResNetBase.__init__-384"><span class="linenos">384</span></a><span class="sd">        - **building_block_type** (`ResNetBlockSmall` | `ResNetBlockLarge`): the type of building block used in the ResNet.</span>
</span><span id="ResNetBase.__init__-385"><a href="#ResNetBase.__init__-385"><span class="linenos">385</span></a><span class="sd">        - **building_block_nums** (`tuple[int, int, int, int]`): the number of building blocks in the 2-5 convolutional layer correspondingly.</span>
</span><span id="ResNetBase.__init__-386"><a href="#ResNetBase.__init__-386"><span class="linenos">386</span></a><span class="sd">        - **building_block_preceding_output_channels** (`tuple[int, int, int, int]`): the number of channels of preceding output of each building block in the 2-5 convolutional layer correspondingly.</span>
</span><span id="ResNetBase.__init__-387"><a href="#ResNetBase.__init__-387"><span class="linenos">387</span></a><span class="sd">        - **building_block_input_channels** (`tuple[int, int, int, int]`): the number of channels of input of each building block in the 2-5 convolutional layer correspondingly.</span>
</span><span id="ResNetBase.__init__-388"><a href="#ResNetBase.__init__-388"><span class="linenos">388</span></a><span class="sd">        - **output_dim** (`int`): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</span>
</span><span id="ResNetBase.__init__-389"><a href="#ResNetBase.__init__-389"><span class="linenos">389</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="ResNetBase.__init__-390"><a href="#ResNetBase.__init__-390"><span class="linenos">390</span></a><span class="sd">        - **batch_normalisation** (`bool`): whether to use batch normalisation after the weight convolutional layers. Default `True`, same as what the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) does.</span>
</span><span id="ResNetBase.__init__-391"><a href="#ResNetBase.__init__-391"><span class="linenos">391</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`, because batch normalisation are doing the similar thing with bias.</span>
</span><span id="ResNetBase.__init__-392"><a href="#ResNetBase.__init__-392"><span class="linenos">392</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="ResNetBase.__init__-393"><a href="#ResNetBase.__init__-393"><span class="linenos">393</span></a>        <span class="n">CLBackbone</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">)</span>
</span><span id="ResNetBase.__init__-394"><a href="#ResNetBase.__init__-394"><span class="linenos">394</span></a>
</span><span id="ResNetBase.__init__-395"><a href="#ResNetBase.__init__-395"><span class="linenos">395</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">batch_normalisation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">batch_normalisation</span>
</span><span id="ResNetBase.__init__-396"><a href="#ResNetBase.__init__-396"><span class="linenos">396</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store whether to use batch normalisation after the fully-connected layers.&quot;&quot;&quot;</span>
</span><span id="ResNetBase.__init__-397"><a href="#ResNetBase.__init__-397"><span class="linenos">397</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">activation_layer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span><span id="ResNetBase.__init__-398"><a href="#ResNetBase.__init__-398"><span class="linenos">398</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store whether to use activation function after the fully-connected layers.&quot;&quot;&quot;</span>
</span><span id="ResNetBase.__init__-399"><a href="#ResNetBase.__init__-399"><span class="linenos">399</span></a>
</span><span id="ResNetBase.__init__-400"><a href="#ResNetBase.__init__-400"><span class="linenos">400</span></a>        <span class="c1"># construct the 1st weighted convolutional layer and attached layers (batchnorm, activation, etc)</span>
</span><span id="ResNetBase.__init__-401"><a href="#ResNetBase.__init__-401"><span class="linenos">401</span></a>        <span class="n">layer_input_channels</span> <span class="o">=</span> <span class="n">input_channels</span>  <span class="c1"># the input channels of the 1st convolutional layer, which receive the input of the entire network</span>
</span><span id="ResNetBase.__init__-402"><a href="#ResNetBase.__init__-402"><span class="linenos">402</span></a>        <span class="n">layer_output_channels</span> <span class="o">=</span> <span class="mi">64</span>  <span class="c1"># the output channels of the 1st convolutional layer</span>
</span><span id="ResNetBase.__init__-403"><a href="#ResNetBase.__init__-403"><span class="linenos">403</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
</span><span id="ResNetBase.__init__-404"><a href="#ResNetBase.__init__-404"><span class="linenos">404</span></a>            <span class="n">in_channels</span><span class="o">=</span><span class="n">layer_input_channels</span><span class="p">,</span>
</span><span id="ResNetBase.__init__-405"><a href="#ResNetBase.__init__-405"><span class="linenos">405</span></a>            <span class="n">out_channels</span><span class="o">=</span><span class="n">layer_output_channels</span><span class="p">,</span>
</span><span id="ResNetBase.__init__-406"><a href="#ResNetBase.__init__-406"><span class="linenos">406</span></a>            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
</span><span id="ResNetBase.__init__-407"><a href="#ResNetBase.__init__-407"><span class="linenos">407</span></a>            <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
</span><span id="ResNetBase.__init__-408"><a href="#ResNetBase.__init__-408"><span class="linenos">408</span></a>            <span class="n">padding</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
</span><span id="ResNetBase.__init__-409"><a href="#ResNetBase.__init__-409"><span class="linenos">409</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="ResNetBase.__init__-410"><a href="#ResNetBase.__init__-410"><span class="linenos">410</span></a>        <span class="p">)</span>  <span class="c1"># construct the 1st weight convolutional layer of the entire ResNet</span>
</span><span id="ResNetBase.__init__-411"><a href="#ResNetBase.__init__-411"><span class="linenos">411</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The 1st weight convolutional layer of the entire ResNet. It  is always with fixed kernel size 7x7, stride 2, and padding 3. &quot;&quot;&quot;</span>
</span><span id="ResNetBase.__init__-412"><a href="#ResNetBase.__init__-412"><span class="linenos">412</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;conv1&quot;</span><span class="p">)</span>  <span class="c1"># collect the layer name to be masked</span>
</span><span id="ResNetBase.__init__-413"><a href="#ResNetBase.__init__-413"><span class="linenos">413</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_normalisation</span><span class="p">:</span>
</span><span id="ResNetBase.__init__-414"><a href="#ResNetBase.__init__-414"><span class="linenos">414</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv_bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span>
</span><span id="ResNetBase.__init__-415"><a href="#ResNetBase.__init__-415"><span class="linenos">415</span></a>                <span class="n">num_features</span><span class="o">=</span><span class="n">layer_output_channels</span>
</span><span id="ResNetBase.__init__-416"><a href="#ResNetBase.__init__-416"><span class="linenos">416</span></a>            <span class="p">)</span>  <span class="c1"># construct the batch normalisation layer</span>
</span><span id="ResNetBase.__init__-417"><a href="#ResNetBase.__init__-417"><span class="linenos">417</span></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The batch normalisation (`nn.BatchNorm2d`) layer after the 1st weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="ResNetBase.__init__-418"><a href="#ResNetBase.__init__-418"><span class="linenos">418</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="ResNetBase.__init__-419"><a href="#ResNetBase.__init__-419"><span class="linenos">419</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation1</span> <span class="o">=</span> <span class="n">activation_layer</span><span class="p">()</span>  <span class="c1"># construct the activation layer</span>
</span><span id="ResNetBase.__init__-420"><a href="#ResNetBase.__init__-420"><span class="linenos">420</span></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The activation layer after the 1st weighted convolutional layer. &quot;&quot;&quot;</span>
</span><span id="ResNetBase.__init__-421"><a href="#ResNetBase.__init__-421"><span class="linenos">421</span></a>
</span><span id="ResNetBase.__init__-422"><a href="#ResNetBase.__init__-422"><span class="linenos">422</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1">#</span>
</span><span id="ResNetBase.__init__-423"><a href="#ResNetBase.__init__-423"><span class="linenos">423</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The max pooling layer which is laid in between 1st and 2nd convolutional layers with kernel size 3x3, stride 2. &quot;&quot;&quot;</span>
</span><span id="ResNetBase.__init__-424"><a href="#ResNetBase.__init__-424"><span class="linenos">424</span></a>
</span><span id="ResNetBase.__init__-425"><a href="#ResNetBase.__init__-425"><span class="linenos">425</span></a>        <span class="c1"># construct the 2nd convolutional layer with multiple blocks, and its attached layers (batchnorm, activation, etc)</span>
</span><span id="ResNetBase.__init__-426"><a href="#ResNetBase.__init__-426"><span class="linenos">426</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv2x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_multiple_blocks</span><span class="p">(</span>
</span><span id="ResNetBase.__init__-427"><a href="#ResNetBase.__init__-427"><span class="linenos">427</span></a>            <span class="n">layer_name</span><span class="o">=</span><span class="s2">&quot;conv2x&quot;</span><span class="p">,</span>
</span><span id="ResNetBase.__init__-428"><a href="#ResNetBase.__init__-428"><span class="linenos">428</span></a>            <span class="n">building_block_type</span><span class="o">=</span><span class="n">building_block_type</span><span class="p">,</span>
</span><span id="ResNetBase.__init__-429"><a href="#ResNetBase.__init__-429"><span class="linenos">429</span></a>            <span class="n">building_block_num</span><span class="o">=</span><span class="n">building_block_nums</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
</span><span id="ResNetBase.__init__-430"><a href="#ResNetBase.__init__-430"><span class="linenos">430</span></a>            <span class="n">preceding_output_channels</span><span class="o">=</span><span class="n">building_block_preceding_output_channels</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
</span><span id="ResNetBase.__init__-431"><a href="#ResNetBase.__init__-431"><span class="linenos">431</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">building_block_input_channels</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
</span><span id="ResNetBase.__init__-432"><a href="#ResNetBase.__init__-432"><span class="linenos">432</span></a>            <span class="n">overall_stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>  <span class="c1"># the overall stride of the 2nd convolutional layer should be 1, as the preceding maxpooling layer has stride 2, which already made 112x112 -&gt; 56x56. See Table 2 in the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) for details.</span>
</span><span id="ResNetBase.__init__-433"><a href="#ResNetBase.__init__-433"><span class="linenos">433</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="ResNetBase.__init__-434"><a href="#ResNetBase.__init__-434"><span class="linenos">434</span></a>            <span class="n">batch_normalisation</span><span class="o">=</span><span class="n">batch_normalisation</span><span class="p">,</span>
</span><span id="ResNetBase.__init__-435"><a href="#ResNetBase.__init__-435"><span class="linenos">435</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="ResNetBase.__init__-436"><a href="#ResNetBase.__init__-436"><span class="linenos">436</span></a>        <span class="p">)</span>
</span><span id="ResNetBase.__init__-437"><a href="#ResNetBase.__init__-437"><span class="linenos">437</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The 2nd convolutional layer of the ResNet, which contains multiple blocks. &quot;&quot;&quot;</span>
</span><span id="ResNetBase.__init__-438"><a href="#ResNetBase.__init__-438"><span class="linenos">438</span></a>
</span><span id="ResNetBase.__init__-439"><a href="#ResNetBase.__init__-439"><span class="linenos">439</span></a>        <span class="c1"># construct the 3rd convolutional layer with multiple blocks, and its attached layers (batchnorm, activation, etc)</span>
</span><span id="ResNetBase.__init__-440"><a href="#ResNetBase.__init__-440"><span class="linenos">440</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv3x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_multiple_blocks</span><span class="p">(</span>
</span><span id="ResNetBase.__init__-441"><a href="#ResNetBase.__init__-441"><span class="linenos">441</span></a>            <span class="n">layer_name</span><span class="o">=</span><span class="s2">&quot;conv3x&quot;</span><span class="p">,</span>
</span><span id="ResNetBase.__init__-442"><a href="#ResNetBase.__init__-442"><span class="linenos">442</span></a>            <span class="n">building_block_type</span><span class="o">=</span><span class="n">building_block_type</span><span class="p">,</span>
</span><span id="ResNetBase.__init__-443"><a href="#ResNetBase.__init__-443"><span class="linenos">443</span></a>            <span class="n">building_block_num</span><span class="o">=</span><span class="n">building_block_nums</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
</span><span id="ResNetBase.__init__-444"><a href="#ResNetBase.__init__-444"><span class="linenos">444</span></a>            <span class="n">preceding_output_channels</span><span class="o">=</span><span class="n">building_block_preceding_output_channels</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
</span><span id="ResNetBase.__init__-445"><a href="#ResNetBase.__init__-445"><span class="linenos">445</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">building_block_input_channels</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
</span><span id="ResNetBase.__init__-446"><a href="#ResNetBase.__init__-446"><span class="linenos">446</span></a>            <span class="n">overall_stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># the overall stride of the 3rd convolutional layer should be 2, making 56x56 -&gt; 28x28. See Table 2 in the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) for details.</span>
</span><span id="ResNetBase.__init__-447"><a href="#ResNetBase.__init__-447"><span class="linenos">447</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="ResNetBase.__init__-448"><a href="#ResNetBase.__init__-448"><span class="linenos">448</span></a>            <span class="n">batch_normalisation</span><span class="o">=</span><span class="n">batch_normalisation</span><span class="p">,</span>
</span><span id="ResNetBase.__init__-449"><a href="#ResNetBase.__init__-449"><span class="linenos">449</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="ResNetBase.__init__-450"><a href="#ResNetBase.__init__-450"><span class="linenos">450</span></a>        <span class="p">)</span>
</span><span id="ResNetBase.__init__-451"><a href="#ResNetBase.__init__-451"><span class="linenos">451</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The 3rd convolutional layer of the ResNet, which contains multiple blocks. &quot;&quot;&quot;</span>
</span><span id="ResNetBase.__init__-452"><a href="#ResNetBase.__init__-452"><span class="linenos">452</span></a>
</span><span id="ResNetBase.__init__-453"><a href="#ResNetBase.__init__-453"><span class="linenos">453</span></a>        <span class="c1"># construct the 4th convolutional layer with multiple blocks, and its attached layers (batchnorm, activation, etc)</span>
</span><span id="ResNetBase.__init__-454"><a href="#ResNetBase.__init__-454"><span class="linenos">454</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv4x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_multiple_blocks</span><span class="p">(</span>
</span><span id="ResNetBase.__init__-455"><a href="#ResNetBase.__init__-455"><span class="linenos">455</span></a>            <span class="n">layer_name</span><span class="o">=</span><span class="s2">&quot;conv4x&quot;</span><span class="p">,</span>
</span><span id="ResNetBase.__init__-456"><a href="#ResNetBase.__init__-456"><span class="linenos">456</span></a>            <span class="n">building_block_type</span><span class="o">=</span><span class="n">building_block_type</span><span class="p">,</span>
</span><span id="ResNetBase.__init__-457"><a href="#ResNetBase.__init__-457"><span class="linenos">457</span></a>            <span class="n">building_block_num</span><span class="o">=</span><span class="n">building_block_nums</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
</span><span id="ResNetBase.__init__-458"><a href="#ResNetBase.__init__-458"><span class="linenos">458</span></a>            <span class="n">preceding_output_channels</span><span class="o">=</span><span class="n">building_block_preceding_output_channels</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
</span><span id="ResNetBase.__init__-459"><a href="#ResNetBase.__init__-459"><span class="linenos">459</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">building_block_input_channels</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
</span><span id="ResNetBase.__init__-460"><a href="#ResNetBase.__init__-460"><span class="linenos">460</span></a>            <span class="n">overall_stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># the overall stride of the 4th convolutional layer should be 2, making 28x28 -&gt; 14x14. See Table 2 in the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) for details.</span>
</span><span id="ResNetBase.__init__-461"><a href="#ResNetBase.__init__-461"><span class="linenos">461</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="ResNetBase.__init__-462"><a href="#ResNetBase.__init__-462"><span class="linenos">462</span></a>            <span class="n">batch_normalisation</span><span class="o">=</span><span class="n">batch_normalisation</span><span class="p">,</span>
</span><span id="ResNetBase.__init__-463"><a href="#ResNetBase.__init__-463"><span class="linenos">463</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="ResNetBase.__init__-464"><a href="#ResNetBase.__init__-464"><span class="linenos">464</span></a>        <span class="p">)</span>
</span><span id="ResNetBase.__init__-465"><a href="#ResNetBase.__init__-465"><span class="linenos">465</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The 4th convolutional layer of the ResNet, which contains multiple blocks. &quot;&quot;&quot;</span>
</span><span id="ResNetBase.__init__-466"><a href="#ResNetBase.__init__-466"><span class="linenos">466</span></a>
</span><span id="ResNetBase.__init__-467"><a href="#ResNetBase.__init__-467"><span class="linenos">467</span></a>        <span class="c1"># construct the 5th convolutional layer with multiple blocks, and its attached layers (batchnorm, activation, etc)</span>
</span><span id="ResNetBase.__init__-468"><a href="#ResNetBase.__init__-468"><span class="linenos">468</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv5x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_multiple_blocks</span><span class="p">(</span>
</span><span id="ResNetBase.__init__-469"><a href="#ResNetBase.__init__-469"><span class="linenos">469</span></a>            <span class="n">layer_name</span><span class="o">=</span><span class="s2">&quot;conv5x&quot;</span><span class="p">,</span>
</span><span id="ResNetBase.__init__-470"><a href="#ResNetBase.__init__-470"><span class="linenos">470</span></a>            <span class="n">building_block_type</span><span class="o">=</span><span class="n">building_block_type</span><span class="p">,</span>
</span><span id="ResNetBase.__init__-471"><a href="#ResNetBase.__init__-471"><span class="linenos">471</span></a>            <span class="n">building_block_num</span><span class="o">=</span><span class="n">building_block_nums</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span>
</span><span id="ResNetBase.__init__-472"><a href="#ResNetBase.__init__-472"><span class="linenos">472</span></a>            <span class="n">preceding_output_channels</span><span class="o">=</span><span class="n">building_block_preceding_output_channels</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span>
</span><span id="ResNetBase.__init__-473"><a href="#ResNetBase.__init__-473"><span class="linenos">473</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">building_block_input_channels</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span>
</span><span id="ResNetBase.__init__-474"><a href="#ResNetBase.__init__-474"><span class="linenos">474</span></a>            <span class="n">overall_stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># the overall stride of the 2nd convolutional layer should be 2, making 14x14 -&gt; 7x7. See Table 2 in the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) for details.</span>
</span><span id="ResNetBase.__init__-475"><a href="#ResNetBase.__init__-475"><span class="linenos">475</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="ResNetBase.__init__-476"><a href="#ResNetBase.__init__-476"><span class="linenos">476</span></a>            <span class="n">batch_normalisation</span><span class="o">=</span><span class="n">batch_normalisation</span><span class="p">,</span>
</span><span id="ResNetBase.__init__-477"><a href="#ResNetBase.__init__-477"><span class="linenos">477</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="ResNetBase.__init__-478"><a href="#ResNetBase.__init__-478"><span class="linenos">478</span></a>        <span class="p">)</span>
</span><span id="ResNetBase.__init__-479"><a href="#ResNetBase.__init__-479"><span class="linenos">479</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The 5th convolutional layer of the ResNet, which contains multiple blocks. &quot;&quot;&quot;</span>
</span><span id="ResNetBase.__init__-480"><a href="#ResNetBase.__init__-480"><span class="linenos">480</span></a>
</span><span id="ResNetBase.__init__-481"><a href="#ResNetBase.__init__-481"><span class="linenos">481</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">avepool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">(</span><span class="n">output_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span><span id="ResNetBase.__init__-482"><a href="#ResNetBase.__init__-482"><span class="linenos">482</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The average pooling layer which is laid after the convolutional layers and before feature maps are flattened. &quot;&quot;&quot;</span>
</span></pre></div>


            <div class="docstring"><p>Construct and initialise the ResNet backbone network.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input. Image data are kept channels when going in ResNet. Note that convolutional networks require number of input channels instead of dimension.</li>
<li><strong>building_block_type</strong> (<code><a href="#ResNetBlockSmall">ResNetBlockSmall</a></code> | <code><a href="#ResNetBlockLarge">ResNetBlockLarge</a></code>): the type of building block used in the ResNet.</li>
<li><strong>building_block_nums</strong> (<code>tuple[int, int, int, int]</code>): the number of building blocks in the 2-5 convolutional layer correspondingly.</li>
<li><strong>building_block_preceding_output_channels</strong> (<code>tuple[int, int, int, int]</code>): the number of channels of preceding output of each building block in the 2-5 convolutional layer correspondingly.</li>
<li><strong>building_block_input_channels</strong> (<code>tuple[int, int, int, int]</code>): the number of channels of input of each building block in the 2-5 convolutional layer correspondingly.</li>
<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>
<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>
<li><strong>batch_normalisation</strong> (<code>bool</code>): whether to use batch normalisation after the weight convolutional layers. Default <code>True</code>, same as what the <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">original ResNet paper</a> does.</li>
<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>, because batch normalisation are doing the similar thing with bias.</li>
</ul>
</div>


                            </div>
                            <div id="ResNetBase.batch_normalisation" class="classattr">
                                <div class="attr variable">
            <span class="name">batch_normalisation</span><span class="annotation">: bool</span>

        
    </div>
    <a class="headerlink" href="#ResNetBase.batch_normalisation"></a>
    
            <div class="docstring"><p>Store whether to use batch normalisation after the fully-connected layers.</p>
</div>


                            </div>
                            <div id="ResNetBase.activation" class="classattr">
                                <div class="attr variable">
            <span class="name">activation</span><span class="annotation">: bool</span>

        
    </div>
    <a class="headerlink" href="#ResNetBase.activation"></a>
    
            <div class="docstring"><p>Store whether to use activation function after the fully-connected layers.</p>
</div>


                            </div>
                            <div id="ResNetBase.conv1" class="classattr">
                                <div class="attr variable">
            <span class="name">conv1</span>

        
    </div>
    <a class="headerlink" href="#ResNetBase.conv1"></a>
    
            <div class="docstring"><p>The 1st weight convolutional layer of the entire ResNet. It  is always with fixed kernel size 7x7, stride 2, and padding 3.</p>
</div>


                            </div>
                            <div id="ResNetBase.maxpool" class="classattr">
                                <div class="attr variable">
            <span class="name">maxpool</span>

        
    </div>
    <a class="headerlink" href="#ResNetBase.maxpool"></a>
    
            <div class="docstring"><p>The max pooling layer which is laid in between 1st and 2nd convolutional layers with kernel size 3x3, stride 2.</p>
</div>


                            </div>
                            <div id="ResNetBase.conv2x" class="classattr">
                                <div class="attr variable">
            <span class="name">conv2x</span>

        
    </div>
    <a class="headerlink" href="#ResNetBase.conv2x"></a>
    
            <div class="docstring"><p>The 2nd convolutional layer of the ResNet, which contains multiple blocks.</p>
</div>


                            </div>
                            <div id="ResNetBase.conv3x" class="classattr">
                                <div class="attr variable">
            <span class="name">conv3x</span>

        
    </div>
    <a class="headerlink" href="#ResNetBase.conv3x"></a>
    
            <div class="docstring"><p>The 3rd convolutional layer of the ResNet, which contains multiple blocks.</p>
</div>


                            </div>
                            <div id="ResNetBase.conv4x" class="classattr">
                                <div class="attr variable">
            <span class="name">conv4x</span>

        
    </div>
    <a class="headerlink" href="#ResNetBase.conv4x"></a>
    
            <div class="docstring"><p>The 4th convolutional layer of the ResNet, which contains multiple blocks.</p>
</div>


                            </div>
                            <div id="ResNetBase.conv5x" class="classattr">
                                <div class="attr variable">
            <span class="name">conv5x</span>

        
    </div>
    <a class="headerlink" href="#ResNetBase.conv5x"></a>
    
            <div class="docstring"><p>The 5th convolutional layer of the ResNet, which contains multiple blocks.</p>
</div>


                            </div>
                            <div id="ResNetBase.avepool" class="classattr">
                                <div class="attr variable">
            <span class="name">avepool</span>

        
    </div>
    <a class="headerlink" href="#ResNetBase.avepool"></a>
    
            <div class="docstring"><p>The average pooling layer which is laid after the convolutional layers and before feature maps are flattened.</p>
</div>


                            </div>
                            <div id="ResNetBase.forward" class="classattr">
                                        <input id="ResNetBase.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">stage</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">task_id</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span></span><span class="return-annotation">) -> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span>:</span></span>

                <label class="view-source-button" for="ResNetBase.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#ResNetBase.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ResNetBase.forward-551"><a href="#ResNetBase.forward-551"><span class="linenos">551</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="ResNetBase.forward-552"><a href="#ResNetBase.forward-552"><span class="linenos">552</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">stage</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">task_id</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="ResNetBase.forward-553"><a href="#ResNetBase.forward-553"><span class="linenos">553</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
</span><span id="ResNetBase.forward-554"><a href="#ResNetBase.forward-554"><span class="linenos">554</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The forward pass for data. It is the same for all tasks.</span>
</span><span id="ResNetBase.forward-555"><a href="#ResNetBase.forward-555"><span class="linenos">555</span></a>
</span><span id="ResNetBase.forward-556"><a href="#ResNetBase.forward-556"><span class="linenos">556</span></a><span class="sd">        **Args:**</span>
</span><span id="ResNetBase.forward-557"><a href="#ResNetBase.forward-557"><span class="linenos">557</span></a><span class="sd">        - **input** (`Tensor`): the input tensor from data.</span>
</span><span id="ResNetBase.forward-558"><a href="#ResNetBase.forward-558"><span class="linenos">558</span></a>
</span><span id="ResNetBase.forward-559"><a href="#ResNetBase.forward-559"><span class="linenos">559</span></a><span class="sd">        **Returns:**</span>
</span><span id="ResNetBase.forward-560"><a href="#ResNetBase.forward-560"><span class="linenos">560</span></a><span class="sd">        - **output_feature** (`Tensor`): the output feature tensor to be passed into heads. This is the main target of backpropagation.</span>
</span><span id="ResNetBase.forward-561"><a href="#ResNetBase.forward-561"><span class="linenos">561</span></a><span class="sd">        - **hidden_features** (`dict[str, Tensor]`): the hidden features (after activation) in each weighted layer. Key (`str`) is the weighted layer name, value (`Tensor`) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes.</span>
</span><span id="ResNetBase.forward-562"><a href="#ResNetBase.forward-562"><span class="linenos">562</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="ResNetBase.forward-563"><a href="#ResNetBase.forward-563"><span class="linenos">563</span></a>        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="ResNetBase.forward-564"><a href="#ResNetBase.forward-564"><span class="linenos">564</span></a>        <span class="n">hidden_features</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="ResNetBase.forward-565"><a href="#ResNetBase.forward-565"><span class="linenos">565</span></a>
</span><span id="ResNetBase.forward-566"><a href="#ResNetBase.forward-566"><span class="linenos">566</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="nb">input</span>
</span><span id="ResNetBase.forward-567"><a href="#ResNetBase.forward-567"><span class="linenos">567</span></a>
</span><span id="ResNetBase.forward-568"><a href="#ResNetBase.forward-568"><span class="linenos">568</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNetBase.forward-569"><a href="#ResNetBase.forward-569"><span class="linenos">569</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_normalisation</span><span class="p">:</span>
</span><span id="ResNetBase.forward-570"><a href="#ResNetBase.forward-570"><span class="linenos">570</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_bn1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNetBase.forward-571"><a href="#ResNetBase.forward-571"><span class="linenos">571</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="ResNetBase.forward-572"><a href="#ResNetBase.forward-572"><span class="linenos">572</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNetBase.forward-573"><a href="#ResNetBase.forward-573"><span class="linenos">573</span></a>        <span class="n">hidden_features</span><span class="p">[</span><span class="s2">&quot;conv1&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="ResNetBase.forward-574"><a href="#ResNetBase.forward-574"><span class="linenos">574</span></a>
</span><span id="ResNetBase.forward-575"><a href="#ResNetBase.forward-575"><span class="linenos">575</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNetBase.forward-576"><a href="#ResNetBase.forward-576"><span class="linenos">576</span></a>
</span><span id="ResNetBase.forward-577"><a href="#ResNetBase.forward-577"><span class="linenos">577</span></a>        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2x</span><span class="p">:</span>
</span><span id="ResNetBase.forward-578"><a href="#ResNetBase.forward-578"><span class="linenos">578</span></a>            <span class="n">x</span><span class="p">,</span> <span class="n">hidden_features_block</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNetBase.forward-579"><a href="#ResNetBase.forward-579"><span class="linenos">579</span></a>            <span class="n">hidden_features</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">hidden_features_block</span><span class="p">)</span>  <span class="c1"># store the hidden feature</span>
</span><span id="ResNetBase.forward-580"><a href="#ResNetBase.forward-580"><span class="linenos">580</span></a>        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv3x</span><span class="p">:</span>
</span><span id="ResNetBase.forward-581"><a href="#ResNetBase.forward-581"><span class="linenos">581</span></a>            <span class="n">x</span><span class="p">,</span> <span class="n">hidden_features_block</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNetBase.forward-582"><a href="#ResNetBase.forward-582"><span class="linenos">582</span></a>            <span class="n">hidden_features</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">hidden_features_block</span><span class="p">)</span>  <span class="c1"># store the hidden feature</span>
</span><span id="ResNetBase.forward-583"><a href="#ResNetBase.forward-583"><span class="linenos">583</span></a>        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv4x</span><span class="p">:</span>
</span><span id="ResNetBase.forward-584"><a href="#ResNetBase.forward-584"><span class="linenos">584</span></a>            <span class="n">x</span><span class="p">,</span> <span class="n">hidden_features_block</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNetBase.forward-585"><a href="#ResNetBase.forward-585"><span class="linenos">585</span></a>            <span class="n">hidden_features</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">hidden_features_block</span><span class="p">)</span>  <span class="c1"># store the hidden feature</span>
</span><span id="ResNetBase.forward-586"><a href="#ResNetBase.forward-586"><span class="linenos">586</span></a>        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv5x</span><span class="p">:</span>
</span><span id="ResNetBase.forward-587"><a href="#ResNetBase.forward-587"><span class="linenos">587</span></a>            <span class="n">x</span><span class="p">,</span> <span class="n">hidden_features_block</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNetBase.forward-588"><a href="#ResNetBase.forward-588"><span class="linenos">588</span></a>            <span class="n">hidden_features</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">hidden_features_block</span><span class="p">)</span>  <span class="c1"># store the hidden feature</span>
</span><span id="ResNetBase.forward-589"><a href="#ResNetBase.forward-589"><span class="linenos">589</span></a>
</span><span id="ResNetBase.forward-590"><a href="#ResNetBase.forward-590"><span class="linenos">590</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">avepool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNetBase.forward-591"><a href="#ResNetBase.forward-591"><span class="linenos">591</span></a>
</span><span id="ResNetBase.forward-592"><a href="#ResNetBase.forward-592"><span class="linenos">592</span></a>        <span class="n">output_feature</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># flatten before going through heads</span>
</span><span id="ResNetBase.forward-593"><a href="#ResNetBase.forward-593"><span class="linenos">593</span></a>
</span><span id="ResNetBase.forward-594"><a href="#ResNetBase.forward-594"><span class="linenos">594</span></a>        <span class="k">return</span> <span class="n">output_feature</span><span class="p">,</span> <span class="n">hidden_features</span>
</span></pre></div>


            <div class="docstring"><p>The forward pass for data. It is the same for all tasks.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>input</strong> (<code>Tensor</code>): the input tensor from data.</li>
</ul>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>output_feature</strong> (<code>Tensor</code>): the output feature tensor to be passed into heads. This is the main target of backpropagation.</li>
<li><strong>hidden_features</strong> (<code>dict[str, Tensor]</code>): the hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name, value (<code>Tensor</code>) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes.</li>
</ul>
</div>


                            </div>
                </section>
                <section id="ResNet18">
                            <input id="ResNet18-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">ResNet18</span><wbr>(<span class="base"><a href="#ResNetBase">ResNetBase</a></span>):

                <label class="view-source-button" for="ResNet18-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#ResNet18"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ResNet18-597"><a href="#ResNet18-597"><span class="linenos">597</span></a><span class="k">class</span><span class="w"> </span><span class="nc">ResNet18</span><span class="p">(</span><span class="n">ResNetBase</span><span class="p">):</span>
</span><span id="ResNet18-598"><a href="#ResNet18-598"><span class="linenos">598</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;ResNet-18 backbone network.</span>
</span><span id="ResNet18-599"><a href="#ResNet18-599"><span class="linenos">599</span></a>
</span><span id="ResNet18-600"><a href="#ResNet18-600"><span class="linenos">600</span></a><span class="sd">    This is a smaller architecture proposed in the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html). It consists of 18 weight convolutional layers in total. See Table 1 in the paper for details.</span>
</span><span id="ResNet18-601"><a href="#ResNet18-601"><span class="linenos">601</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="ResNet18-602"><a href="#ResNet18-602"><span class="linenos">602</span></a>
</span><span id="ResNet18-603"><a href="#ResNet18-603"><span class="linenos">603</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="ResNet18-604"><a href="#ResNet18-604"><span class="linenos">604</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="ResNet18-605"><a href="#ResNet18-605"><span class="linenos">605</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNet18-606"><a href="#ResNet18-606"><span class="linenos">606</span></a>        <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNet18-607"><a href="#ResNet18-607"><span class="linenos">607</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="ResNet18-608"><a href="#ResNet18-608"><span class="linenos">608</span></a>        <span class="n">batch_normalisation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="ResNet18-609"><a href="#ResNet18-609"><span class="linenos">609</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="ResNet18-610"><a href="#ResNet18-610"><span class="linenos">610</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ResNet18-611"><a href="#ResNet18-611"><span class="linenos">611</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the ResNet-18 backbone network.</span>
</span><span id="ResNet18-612"><a href="#ResNet18-612"><span class="linenos">612</span></a>
</span><span id="ResNet18-613"><a href="#ResNet18-613"><span class="linenos">613</span></a><span class="sd">        **Args:**</span>
</span><span id="ResNet18-614"><a href="#ResNet18-614"><span class="linenos">614</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</span>
</span><span id="ResNet18-615"><a href="#ResNet18-615"><span class="linenos">615</span></a><span class="sd">        - **output_dim** (`int`): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</span>
</span><span id="ResNet18-616"><a href="#ResNet18-616"><span class="linenos">616</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="ResNet18-617"><a href="#ResNet18-617"><span class="linenos">617</span></a><span class="sd">        - **batch_normalisation** (`bool`): whether to use batch normalisation after the weight convolutional layers. Default `True`, same as what the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) does.</span>
</span><span id="ResNet18-618"><a href="#ResNet18-618"><span class="linenos">618</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`, because batch normalisation are doing the similar thing with bias.</span>
</span><span id="ResNet18-619"><a href="#ResNet18-619"><span class="linenos">619</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="ResNet18-620"><a href="#ResNet18-620"><span class="linenos">620</span></a>        <span class="n">ResNetBase</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="ResNet18-621"><a href="#ResNet18-621"><span class="linenos">621</span></a>            <span class="bp">self</span><span class="p">,</span>
</span><span id="ResNet18-622"><a href="#ResNet18-622"><span class="linenos">622</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
</span><span id="ResNet18-623"><a href="#ResNet18-623"><span class="linenos">623</span></a>            <span class="n">building_block_type</span><span class="o">=</span><span class="n">ResNetBlockSmall</span><span class="p">,</span>  <span class="c1"># use the smaller building block for ResNet-18</span>
</span><span id="ResNet18-624"><a href="#ResNet18-624"><span class="linenos">624</span></a>            <span class="n">building_block_nums</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
</span><span id="ResNet18-625"><a href="#ResNet18-625"><span class="linenos">625</span></a>            <span class="n">building_block_preceding_output_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>
</span><span id="ResNet18-626"><a href="#ResNet18-626"><span class="linenos">626</span></a>            <span class="n">building_block_input_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
</span><span id="ResNet18-627"><a href="#ResNet18-627"><span class="linenos">627</span></a>            <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span>
</span><span id="ResNet18-628"><a href="#ResNet18-628"><span class="linenos">628</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="ResNet18-629"><a href="#ResNet18-629"><span class="linenos">629</span></a>            <span class="n">batch_normalisation</span><span class="o">=</span><span class="n">batch_normalisation</span><span class="p">,</span>
</span><span id="ResNet18-630"><a href="#ResNet18-630"><span class="linenos">630</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="ResNet18-631"><a href="#ResNet18-631"><span class="linenos">631</span></a>        <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>ResNet-18 backbone network.</p>

<p>This is a smaller architecture proposed in the <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">original ResNet paper</a>. It consists of 18 weight convolutional layers in total. See Table 1 in the paper for details.</p>
</div>


                            <div id="ResNet18.__init__" class="classattr">
                                        <input id="ResNet18.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">ResNet18</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	activation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class="param">	<span class="n">batch_normalisation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>,</span><span class="param">	<span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span></span>)</span>

                <label class="view-source-button" for="ResNet18.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#ResNet18.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ResNet18.__init__-603"><a href="#ResNet18.__init__-603"><span class="linenos">603</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="ResNet18.__init__-604"><a href="#ResNet18.__init__-604"><span class="linenos">604</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="ResNet18.__init__-605"><a href="#ResNet18.__init__-605"><span class="linenos">605</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNet18.__init__-606"><a href="#ResNet18.__init__-606"><span class="linenos">606</span></a>        <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNet18.__init__-607"><a href="#ResNet18.__init__-607"><span class="linenos">607</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="ResNet18.__init__-608"><a href="#ResNet18.__init__-608"><span class="linenos">608</span></a>        <span class="n">batch_normalisation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="ResNet18.__init__-609"><a href="#ResNet18.__init__-609"><span class="linenos">609</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="ResNet18.__init__-610"><a href="#ResNet18.__init__-610"><span class="linenos">610</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ResNet18.__init__-611"><a href="#ResNet18.__init__-611"><span class="linenos">611</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the ResNet-18 backbone network.</span>
</span><span id="ResNet18.__init__-612"><a href="#ResNet18.__init__-612"><span class="linenos">612</span></a>
</span><span id="ResNet18.__init__-613"><a href="#ResNet18.__init__-613"><span class="linenos">613</span></a><span class="sd">        **Args:**</span>
</span><span id="ResNet18.__init__-614"><a href="#ResNet18.__init__-614"><span class="linenos">614</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</span>
</span><span id="ResNet18.__init__-615"><a href="#ResNet18.__init__-615"><span class="linenos">615</span></a><span class="sd">        - **output_dim** (`int`): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</span>
</span><span id="ResNet18.__init__-616"><a href="#ResNet18.__init__-616"><span class="linenos">616</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="ResNet18.__init__-617"><a href="#ResNet18.__init__-617"><span class="linenos">617</span></a><span class="sd">        - **batch_normalisation** (`bool`): whether to use batch normalisation after the weight convolutional layers. Default `True`, same as what the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) does.</span>
</span><span id="ResNet18.__init__-618"><a href="#ResNet18.__init__-618"><span class="linenos">618</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`, because batch normalisation are doing the similar thing with bias.</span>
</span><span id="ResNet18.__init__-619"><a href="#ResNet18.__init__-619"><span class="linenos">619</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="ResNet18.__init__-620"><a href="#ResNet18.__init__-620"><span class="linenos">620</span></a>        <span class="n">ResNetBase</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="ResNet18.__init__-621"><a href="#ResNet18.__init__-621"><span class="linenos">621</span></a>            <span class="bp">self</span><span class="p">,</span>
</span><span id="ResNet18.__init__-622"><a href="#ResNet18.__init__-622"><span class="linenos">622</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
</span><span id="ResNet18.__init__-623"><a href="#ResNet18.__init__-623"><span class="linenos">623</span></a>            <span class="n">building_block_type</span><span class="o">=</span><span class="n">ResNetBlockSmall</span><span class="p">,</span>  <span class="c1"># use the smaller building block for ResNet-18</span>
</span><span id="ResNet18.__init__-624"><a href="#ResNet18.__init__-624"><span class="linenos">624</span></a>            <span class="n">building_block_nums</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
</span><span id="ResNet18.__init__-625"><a href="#ResNet18.__init__-625"><span class="linenos">625</span></a>            <span class="n">building_block_preceding_output_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>
</span><span id="ResNet18.__init__-626"><a href="#ResNet18.__init__-626"><span class="linenos">626</span></a>            <span class="n">building_block_input_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
</span><span id="ResNet18.__init__-627"><a href="#ResNet18.__init__-627"><span class="linenos">627</span></a>            <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span>
</span><span id="ResNet18.__init__-628"><a href="#ResNet18.__init__-628"><span class="linenos">628</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="ResNet18.__init__-629"><a href="#ResNet18.__init__-629"><span class="linenos">629</span></a>            <span class="n">batch_normalisation</span><span class="o">=</span><span class="n">batch_normalisation</span><span class="p">,</span>
</span><span id="ResNet18.__init__-630"><a href="#ResNet18.__init__-630"><span class="linenos">630</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="ResNet18.__init__-631"><a href="#ResNet18.__init__-631"><span class="linenos">631</span></a>        <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Construct and initialise the ResNet-18 backbone network.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</li>
<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>
<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>
<li><strong>batch_normalisation</strong> (<code>bool</code>): whether to use batch normalisation after the weight convolutional layers. Default <code>True</code>, same as what the <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">original ResNet paper</a> does.</li>
<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>, because batch normalisation are doing the similar thing with bias.</li>
</ul>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt><a href="#ResNetBase">ResNetBase</a></dt>
                                <dd id="ResNet18.batch_normalisation" class="variable"><a href="#ResNetBase.batch_normalisation">batch_normalisation</a></dd>
                <dd id="ResNet18.activation" class="variable"><a href="#ResNetBase.activation">activation</a></dd>
                <dd id="ResNet18.conv1" class="variable"><a href="#ResNetBase.conv1">conv1</a></dd>
                <dd id="ResNet18.maxpool" class="variable"><a href="#ResNetBase.maxpool">maxpool</a></dd>
                <dd id="ResNet18.conv2x" class="variable"><a href="#ResNetBase.conv2x">conv2x</a></dd>
                <dd id="ResNet18.conv3x" class="variable"><a href="#ResNetBase.conv3x">conv3x</a></dd>
                <dd id="ResNet18.conv4x" class="variable"><a href="#ResNetBase.conv4x">conv4x</a></dd>
                <dd id="ResNet18.conv5x" class="variable"><a href="#ResNetBase.conv5x">conv5x</a></dd>
                <dd id="ResNet18.avepool" class="variable"><a href="#ResNetBase.avepool">avepool</a></dd>
                <dd id="ResNet18.forward" class="function"><a href="#ResNetBase.forward">forward</a></dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="ResNet34">
                            <input id="ResNet34-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">ResNet34</span><wbr>(<span class="base"><a href="#ResNetBase">ResNetBase</a></span>):

                <label class="view-source-button" for="ResNet34-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#ResNet34"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ResNet34-634"><a href="#ResNet34-634"><span class="linenos">634</span></a><span class="k">class</span><span class="w"> </span><span class="nc">ResNet34</span><span class="p">(</span><span class="n">ResNetBase</span><span class="p">):</span>
</span><span id="ResNet34-635"><a href="#ResNet34-635"><span class="linenos">635</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;ResNet-34 backbone network.</span>
</span><span id="ResNet34-636"><a href="#ResNet34-636"><span class="linenos">636</span></a>
</span><span id="ResNet34-637"><a href="#ResNet34-637"><span class="linenos">637</span></a><span class="sd">    This is a smaller architecture proposed in the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html). It consists of 34 weight convolutional layers in total. See Table 1 in the paper for details.</span>
</span><span id="ResNet34-638"><a href="#ResNet34-638"><span class="linenos">638</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="ResNet34-639"><a href="#ResNet34-639"><span class="linenos">639</span></a>
</span><span id="ResNet34-640"><a href="#ResNet34-640"><span class="linenos">640</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="ResNet34-641"><a href="#ResNet34-641"><span class="linenos">641</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="ResNet34-642"><a href="#ResNet34-642"><span class="linenos">642</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNet34-643"><a href="#ResNet34-643"><span class="linenos">643</span></a>        <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNet34-644"><a href="#ResNet34-644"><span class="linenos">644</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="ResNet34-645"><a href="#ResNet34-645"><span class="linenos">645</span></a>        <span class="n">batch_normalisation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="ResNet34-646"><a href="#ResNet34-646"><span class="linenos">646</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="ResNet34-647"><a href="#ResNet34-647"><span class="linenos">647</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ResNet34-648"><a href="#ResNet34-648"><span class="linenos">648</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the ResNet-34 backbone network.</span>
</span><span id="ResNet34-649"><a href="#ResNet34-649"><span class="linenos">649</span></a>
</span><span id="ResNet34-650"><a href="#ResNet34-650"><span class="linenos">650</span></a><span class="sd">        **Args:**</span>
</span><span id="ResNet34-651"><a href="#ResNet34-651"><span class="linenos">651</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</span>
</span><span id="ResNet34-652"><a href="#ResNet34-652"><span class="linenos">652</span></a><span class="sd">        - **output_dim** (`int`): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</span>
</span><span id="ResNet34-653"><a href="#ResNet34-653"><span class="linenos">653</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="ResNet34-654"><a href="#ResNet34-654"><span class="linenos">654</span></a><span class="sd">        - **batch_normalisation** (`bool`): whether to use batch normalisation after the weight convolutional layers. Default `True`, same as what the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) does.</span>
</span><span id="ResNet34-655"><a href="#ResNet34-655"><span class="linenos">655</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`, because batch normalisation are doing the similar thing with bias.</span>
</span><span id="ResNet34-656"><a href="#ResNet34-656"><span class="linenos">656</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="ResNet34-657"><a href="#ResNet34-657"><span class="linenos">657</span></a>        <span class="n">ResNetBase</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="ResNet34-658"><a href="#ResNet34-658"><span class="linenos">658</span></a>            <span class="bp">self</span><span class="p">,</span>
</span><span id="ResNet34-659"><a href="#ResNet34-659"><span class="linenos">659</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
</span><span id="ResNet34-660"><a href="#ResNet34-660"><span class="linenos">660</span></a>            <span class="n">building_block_type</span><span class="o">=</span><span class="n">ResNetBlockSmall</span><span class="p">,</span>  <span class="c1"># use the smaller building block for ResNet-34</span>
</span><span id="ResNet34-661"><a href="#ResNet34-661"><span class="linenos">661</span></a>            <span class="n">building_block_nums</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
</span><span id="ResNet34-662"><a href="#ResNet34-662"><span class="linenos">662</span></a>            <span class="n">building_block_preceding_output_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>
</span><span id="ResNet34-663"><a href="#ResNet34-663"><span class="linenos">663</span></a>            <span class="n">building_block_input_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
</span><span id="ResNet34-664"><a href="#ResNet34-664"><span class="linenos">664</span></a>            <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span>
</span><span id="ResNet34-665"><a href="#ResNet34-665"><span class="linenos">665</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="ResNet34-666"><a href="#ResNet34-666"><span class="linenos">666</span></a>            <span class="n">batch_normalisation</span><span class="o">=</span><span class="n">batch_normalisation</span><span class="p">,</span>
</span><span id="ResNet34-667"><a href="#ResNet34-667"><span class="linenos">667</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="ResNet34-668"><a href="#ResNet34-668"><span class="linenos">668</span></a>        <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>ResNet-34 backbone network.</p>

<p>This is a smaller architecture proposed in the <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">original ResNet paper</a>. It consists of 34 weight convolutional layers in total. See Table 1 in the paper for details.</p>
</div>


                            <div id="ResNet34.__init__" class="classattr">
                                        <input id="ResNet34.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">ResNet34</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	activation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class="param">	<span class="n">batch_normalisation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>,</span><span class="param">	<span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span></span>)</span>

                <label class="view-source-button" for="ResNet34.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#ResNet34.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ResNet34.__init__-640"><a href="#ResNet34.__init__-640"><span class="linenos">640</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="ResNet34.__init__-641"><a href="#ResNet34.__init__-641"><span class="linenos">641</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="ResNet34.__init__-642"><a href="#ResNet34.__init__-642"><span class="linenos">642</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNet34.__init__-643"><a href="#ResNet34.__init__-643"><span class="linenos">643</span></a>        <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNet34.__init__-644"><a href="#ResNet34.__init__-644"><span class="linenos">644</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="ResNet34.__init__-645"><a href="#ResNet34.__init__-645"><span class="linenos">645</span></a>        <span class="n">batch_normalisation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="ResNet34.__init__-646"><a href="#ResNet34.__init__-646"><span class="linenos">646</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="ResNet34.__init__-647"><a href="#ResNet34.__init__-647"><span class="linenos">647</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ResNet34.__init__-648"><a href="#ResNet34.__init__-648"><span class="linenos">648</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the ResNet-34 backbone network.</span>
</span><span id="ResNet34.__init__-649"><a href="#ResNet34.__init__-649"><span class="linenos">649</span></a>
</span><span id="ResNet34.__init__-650"><a href="#ResNet34.__init__-650"><span class="linenos">650</span></a><span class="sd">        **Args:**</span>
</span><span id="ResNet34.__init__-651"><a href="#ResNet34.__init__-651"><span class="linenos">651</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</span>
</span><span id="ResNet34.__init__-652"><a href="#ResNet34.__init__-652"><span class="linenos">652</span></a><span class="sd">        - **output_dim** (`int`): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</span>
</span><span id="ResNet34.__init__-653"><a href="#ResNet34.__init__-653"><span class="linenos">653</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="ResNet34.__init__-654"><a href="#ResNet34.__init__-654"><span class="linenos">654</span></a><span class="sd">        - **batch_normalisation** (`bool`): whether to use batch normalisation after the weight convolutional layers. Default `True`, same as what the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) does.</span>
</span><span id="ResNet34.__init__-655"><a href="#ResNet34.__init__-655"><span class="linenos">655</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`, because batch normalisation are doing the similar thing with bias.</span>
</span><span id="ResNet34.__init__-656"><a href="#ResNet34.__init__-656"><span class="linenos">656</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="ResNet34.__init__-657"><a href="#ResNet34.__init__-657"><span class="linenos">657</span></a>        <span class="n">ResNetBase</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="ResNet34.__init__-658"><a href="#ResNet34.__init__-658"><span class="linenos">658</span></a>            <span class="bp">self</span><span class="p">,</span>
</span><span id="ResNet34.__init__-659"><a href="#ResNet34.__init__-659"><span class="linenos">659</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
</span><span id="ResNet34.__init__-660"><a href="#ResNet34.__init__-660"><span class="linenos">660</span></a>            <span class="n">building_block_type</span><span class="o">=</span><span class="n">ResNetBlockSmall</span><span class="p">,</span>  <span class="c1"># use the smaller building block for ResNet-34</span>
</span><span id="ResNet34.__init__-661"><a href="#ResNet34.__init__-661"><span class="linenos">661</span></a>            <span class="n">building_block_nums</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
</span><span id="ResNet34.__init__-662"><a href="#ResNet34.__init__-662"><span class="linenos">662</span></a>            <span class="n">building_block_preceding_output_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>
</span><span id="ResNet34.__init__-663"><a href="#ResNet34.__init__-663"><span class="linenos">663</span></a>            <span class="n">building_block_input_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
</span><span id="ResNet34.__init__-664"><a href="#ResNet34.__init__-664"><span class="linenos">664</span></a>            <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span>
</span><span id="ResNet34.__init__-665"><a href="#ResNet34.__init__-665"><span class="linenos">665</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="ResNet34.__init__-666"><a href="#ResNet34.__init__-666"><span class="linenos">666</span></a>            <span class="n">batch_normalisation</span><span class="o">=</span><span class="n">batch_normalisation</span><span class="p">,</span>
</span><span id="ResNet34.__init__-667"><a href="#ResNet34.__init__-667"><span class="linenos">667</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="ResNet34.__init__-668"><a href="#ResNet34.__init__-668"><span class="linenos">668</span></a>        <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Construct and initialise the ResNet-34 backbone network.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</li>
<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>
<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>
<li><strong>batch_normalisation</strong> (<code>bool</code>): whether to use batch normalisation after the weight convolutional layers. Default <code>True</code>, same as what the <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">original ResNet paper</a> does.</li>
<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>, because batch normalisation are doing the similar thing with bias.</li>
</ul>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt><a href="#ResNetBase">ResNetBase</a></dt>
                                <dd id="ResNet34.batch_normalisation" class="variable"><a href="#ResNetBase.batch_normalisation">batch_normalisation</a></dd>
                <dd id="ResNet34.activation" class="variable"><a href="#ResNetBase.activation">activation</a></dd>
                <dd id="ResNet34.conv1" class="variable"><a href="#ResNetBase.conv1">conv1</a></dd>
                <dd id="ResNet34.maxpool" class="variable"><a href="#ResNetBase.maxpool">maxpool</a></dd>
                <dd id="ResNet34.conv2x" class="variable"><a href="#ResNetBase.conv2x">conv2x</a></dd>
                <dd id="ResNet34.conv3x" class="variable"><a href="#ResNetBase.conv3x">conv3x</a></dd>
                <dd id="ResNet34.conv4x" class="variable"><a href="#ResNetBase.conv4x">conv4x</a></dd>
                <dd id="ResNet34.conv5x" class="variable"><a href="#ResNetBase.conv5x">conv5x</a></dd>
                <dd id="ResNet34.avepool" class="variable"><a href="#ResNetBase.avepool">avepool</a></dd>
                <dd id="ResNet34.forward" class="function"><a href="#ResNetBase.forward">forward</a></dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="ResNet50">
                            <input id="ResNet50-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">ResNet50</span><wbr>(<span class="base"><a href="#ResNetBase">ResNetBase</a></span>):

                <label class="view-source-button" for="ResNet50-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#ResNet50"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ResNet50-671"><a href="#ResNet50-671"><span class="linenos">671</span></a><span class="k">class</span><span class="w"> </span><span class="nc">ResNet50</span><span class="p">(</span><span class="n">ResNetBase</span><span class="p">):</span>
</span><span id="ResNet50-672"><a href="#ResNet50-672"><span class="linenos">672</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;ResNet-50 backbone network.</span>
</span><span id="ResNet50-673"><a href="#ResNet50-673"><span class="linenos">673</span></a>
</span><span id="ResNet50-674"><a href="#ResNet50-674"><span class="linenos">674</span></a><span class="sd">    This is a larger architecture proposed in the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html). It consists of 50 weight convolutional layers in total. See Table 1 in the paper for details.</span>
</span><span id="ResNet50-675"><a href="#ResNet50-675"><span class="linenos">675</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="ResNet50-676"><a href="#ResNet50-676"><span class="linenos">676</span></a>
</span><span id="ResNet50-677"><a href="#ResNet50-677"><span class="linenos">677</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="ResNet50-678"><a href="#ResNet50-678"><span class="linenos">678</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="ResNet50-679"><a href="#ResNet50-679"><span class="linenos">679</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNet50-680"><a href="#ResNet50-680"><span class="linenos">680</span></a>        <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNet50-681"><a href="#ResNet50-681"><span class="linenos">681</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="ResNet50-682"><a href="#ResNet50-682"><span class="linenos">682</span></a>        <span class="n">batch_normalisation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="ResNet50-683"><a href="#ResNet50-683"><span class="linenos">683</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="ResNet50-684"><a href="#ResNet50-684"><span class="linenos">684</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ResNet50-685"><a href="#ResNet50-685"><span class="linenos">685</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the ResNet-50 backbone network.</span>
</span><span id="ResNet50-686"><a href="#ResNet50-686"><span class="linenos">686</span></a>
</span><span id="ResNet50-687"><a href="#ResNet50-687"><span class="linenos">687</span></a><span class="sd">        **Args:**</span>
</span><span id="ResNet50-688"><a href="#ResNet50-688"><span class="linenos">688</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</span>
</span><span id="ResNet50-689"><a href="#ResNet50-689"><span class="linenos">689</span></a><span class="sd">        - **output_dim** (`int`): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</span>
</span><span id="ResNet50-690"><a href="#ResNet50-690"><span class="linenos">690</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="ResNet50-691"><a href="#ResNet50-691"><span class="linenos">691</span></a><span class="sd">        - **batch_normalisation** (`bool`): whether to use batch normalisation after the weight convolutional layers. Default `True`, same as what the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) does.</span>
</span><span id="ResNet50-692"><a href="#ResNet50-692"><span class="linenos">692</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`, because batch normalisation are doing the similar thing with bias.</span>
</span><span id="ResNet50-693"><a href="#ResNet50-693"><span class="linenos">693</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="ResNet50-694"><a href="#ResNet50-694"><span class="linenos">694</span></a>        <span class="n">ResNetBase</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="ResNet50-695"><a href="#ResNet50-695"><span class="linenos">695</span></a>            <span class="bp">self</span><span class="p">,</span>
</span><span id="ResNet50-696"><a href="#ResNet50-696"><span class="linenos">696</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
</span><span id="ResNet50-697"><a href="#ResNet50-697"><span class="linenos">697</span></a>            <span class="n">building_block_type</span><span class="o">=</span><span class="n">ResNetBlockLarge</span><span class="p">,</span>  <span class="c1"># use the larger building block for ResNet-50</span>
</span><span id="ResNet50-698"><a href="#ResNet50-698"><span class="linenos">698</span></a>            <span class="n">building_block_nums</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
</span><span id="ResNet50-699"><a href="#ResNet50-699"><span class="linenos">699</span></a>            <span class="n">building_block_preceding_output_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span>
</span><span id="ResNet50-700"><a href="#ResNet50-700"><span class="linenos">700</span></a>            <span class="n">building_block_input_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
</span><span id="ResNet50-701"><a href="#ResNet50-701"><span class="linenos">701</span></a>            <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span>
</span><span id="ResNet50-702"><a href="#ResNet50-702"><span class="linenos">702</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="ResNet50-703"><a href="#ResNet50-703"><span class="linenos">703</span></a>            <span class="n">batch_normalisation</span><span class="o">=</span><span class="n">batch_normalisation</span><span class="p">,</span>
</span><span id="ResNet50-704"><a href="#ResNet50-704"><span class="linenos">704</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="ResNet50-705"><a href="#ResNet50-705"><span class="linenos">705</span></a>        <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>ResNet-50 backbone network.</p>

<p>This is a larger architecture proposed in the <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">original ResNet paper</a>. It consists of 50 weight convolutional layers in total. See Table 1 in the paper for details.</p>
</div>


                            <div id="ResNet50.__init__" class="classattr">
                                        <input id="ResNet50.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">ResNet50</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	activation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class="param">	<span class="n">batch_normalisation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>,</span><span class="param">	<span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span></span>)</span>

                <label class="view-source-button" for="ResNet50.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#ResNet50.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ResNet50.__init__-677"><a href="#ResNet50.__init__-677"><span class="linenos">677</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="ResNet50.__init__-678"><a href="#ResNet50.__init__-678"><span class="linenos">678</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="ResNet50.__init__-679"><a href="#ResNet50.__init__-679"><span class="linenos">679</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNet50.__init__-680"><a href="#ResNet50.__init__-680"><span class="linenos">680</span></a>        <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNet50.__init__-681"><a href="#ResNet50.__init__-681"><span class="linenos">681</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="ResNet50.__init__-682"><a href="#ResNet50.__init__-682"><span class="linenos">682</span></a>        <span class="n">batch_normalisation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="ResNet50.__init__-683"><a href="#ResNet50.__init__-683"><span class="linenos">683</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="ResNet50.__init__-684"><a href="#ResNet50.__init__-684"><span class="linenos">684</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ResNet50.__init__-685"><a href="#ResNet50.__init__-685"><span class="linenos">685</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the ResNet-50 backbone network.</span>
</span><span id="ResNet50.__init__-686"><a href="#ResNet50.__init__-686"><span class="linenos">686</span></a>
</span><span id="ResNet50.__init__-687"><a href="#ResNet50.__init__-687"><span class="linenos">687</span></a><span class="sd">        **Args:**</span>
</span><span id="ResNet50.__init__-688"><a href="#ResNet50.__init__-688"><span class="linenos">688</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</span>
</span><span id="ResNet50.__init__-689"><a href="#ResNet50.__init__-689"><span class="linenos">689</span></a><span class="sd">        - **output_dim** (`int`): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</span>
</span><span id="ResNet50.__init__-690"><a href="#ResNet50.__init__-690"><span class="linenos">690</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="ResNet50.__init__-691"><a href="#ResNet50.__init__-691"><span class="linenos">691</span></a><span class="sd">        - **batch_normalisation** (`bool`): whether to use batch normalisation after the weight convolutional layers. Default `True`, same as what the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) does.</span>
</span><span id="ResNet50.__init__-692"><a href="#ResNet50.__init__-692"><span class="linenos">692</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`, because batch normalisation are doing the similar thing with bias.</span>
</span><span id="ResNet50.__init__-693"><a href="#ResNet50.__init__-693"><span class="linenos">693</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="ResNet50.__init__-694"><a href="#ResNet50.__init__-694"><span class="linenos">694</span></a>        <span class="n">ResNetBase</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="ResNet50.__init__-695"><a href="#ResNet50.__init__-695"><span class="linenos">695</span></a>            <span class="bp">self</span><span class="p">,</span>
</span><span id="ResNet50.__init__-696"><a href="#ResNet50.__init__-696"><span class="linenos">696</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
</span><span id="ResNet50.__init__-697"><a href="#ResNet50.__init__-697"><span class="linenos">697</span></a>            <span class="n">building_block_type</span><span class="o">=</span><span class="n">ResNetBlockLarge</span><span class="p">,</span>  <span class="c1"># use the larger building block for ResNet-50</span>
</span><span id="ResNet50.__init__-698"><a href="#ResNet50.__init__-698"><span class="linenos">698</span></a>            <span class="n">building_block_nums</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
</span><span id="ResNet50.__init__-699"><a href="#ResNet50.__init__-699"><span class="linenos">699</span></a>            <span class="n">building_block_preceding_output_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span>
</span><span id="ResNet50.__init__-700"><a href="#ResNet50.__init__-700"><span class="linenos">700</span></a>            <span class="n">building_block_input_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
</span><span id="ResNet50.__init__-701"><a href="#ResNet50.__init__-701"><span class="linenos">701</span></a>            <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span>
</span><span id="ResNet50.__init__-702"><a href="#ResNet50.__init__-702"><span class="linenos">702</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="ResNet50.__init__-703"><a href="#ResNet50.__init__-703"><span class="linenos">703</span></a>            <span class="n">batch_normalisation</span><span class="o">=</span><span class="n">batch_normalisation</span><span class="p">,</span>
</span><span id="ResNet50.__init__-704"><a href="#ResNet50.__init__-704"><span class="linenos">704</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="ResNet50.__init__-705"><a href="#ResNet50.__init__-705"><span class="linenos">705</span></a>        <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Construct and initialise the ResNet-50 backbone network.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</li>
<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>
<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>
<li><strong>batch_normalisation</strong> (<code>bool</code>): whether to use batch normalisation after the weight convolutional layers. Default <code>True</code>, same as what the <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">original ResNet paper</a> does.</li>
<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>, because batch normalisation are doing the similar thing with bias.</li>
</ul>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt><a href="#ResNetBase">ResNetBase</a></dt>
                                <dd id="ResNet50.batch_normalisation" class="variable"><a href="#ResNetBase.batch_normalisation">batch_normalisation</a></dd>
                <dd id="ResNet50.activation" class="variable"><a href="#ResNetBase.activation">activation</a></dd>
                <dd id="ResNet50.conv1" class="variable"><a href="#ResNetBase.conv1">conv1</a></dd>
                <dd id="ResNet50.maxpool" class="variable"><a href="#ResNetBase.maxpool">maxpool</a></dd>
                <dd id="ResNet50.conv2x" class="variable"><a href="#ResNetBase.conv2x">conv2x</a></dd>
                <dd id="ResNet50.conv3x" class="variable"><a href="#ResNetBase.conv3x">conv3x</a></dd>
                <dd id="ResNet50.conv4x" class="variable"><a href="#ResNetBase.conv4x">conv4x</a></dd>
                <dd id="ResNet50.conv5x" class="variable"><a href="#ResNetBase.conv5x">conv5x</a></dd>
                <dd id="ResNet50.avepool" class="variable"><a href="#ResNetBase.avepool">avepool</a></dd>
                <dd id="ResNet50.forward" class="function"><a href="#ResNetBase.forward">forward</a></dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="ResNet101">
                            <input id="ResNet101-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">ResNet101</span><wbr>(<span class="base"><a href="#ResNetBase">ResNetBase</a></span>):

                <label class="view-source-button" for="ResNet101-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#ResNet101"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ResNet101-708"><a href="#ResNet101-708"><span class="linenos">708</span></a><span class="k">class</span><span class="w"> </span><span class="nc">ResNet101</span><span class="p">(</span><span class="n">ResNetBase</span><span class="p">):</span>
</span><span id="ResNet101-709"><a href="#ResNet101-709"><span class="linenos">709</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;ResNet-101 backbone network.</span>
</span><span id="ResNet101-710"><a href="#ResNet101-710"><span class="linenos">710</span></a>
</span><span id="ResNet101-711"><a href="#ResNet101-711"><span class="linenos">711</span></a><span class="sd">    This is a larger architecture proposed in the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html). It consists of 101 weight convolutional layers in total. See Table 1 in the paper for details.</span>
</span><span id="ResNet101-712"><a href="#ResNet101-712"><span class="linenos">712</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="ResNet101-713"><a href="#ResNet101-713"><span class="linenos">713</span></a>
</span><span id="ResNet101-714"><a href="#ResNet101-714"><span class="linenos">714</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="ResNet101-715"><a href="#ResNet101-715"><span class="linenos">715</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="ResNet101-716"><a href="#ResNet101-716"><span class="linenos">716</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNet101-717"><a href="#ResNet101-717"><span class="linenos">717</span></a>        <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNet101-718"><a href="#ResNet101-718"><span class="linenos">718</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="ResNet101-719"><a href="#ResNet101-719"><span class="linenos">719</span></a>        <span class="n">batch_normalisation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="ResNet101-720"><a href="#ResNet101-720"><span class="linenos">720</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="ResNet101-721"><a href="#ResNet101-721"><span class="linenos">721</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ResNet101-722"><a href="#ResNet101-722"><span class="linenos">722</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the ResNet-101 backbone network.</span>
</span><span id="ResNet101-723"><a href="#ResNet101-723"><span class="linenos">723</span></a>
</span><span id="ResNet101-724"><a href="#ResNet101-724"><span class="linenos">724</span></a><span class="sd">        **Args:**</span>
</span><span id="ResNet101-725"><a href="#ResNet101-725"><span class="linenos">725</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</span>
</span><span id="ResNet101-726"><a href="#ResNet101-726"><span class="linenos">726</span></a><span class="sd">        - **output_dim** (`int`): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</span>
</span><span id="ResNet101-727"><a href="#ResNet101-727"><span class="linenos">727</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="ResNet101-728"><a href="#ResNet101-728"><span class="linenos">728</span></a><span class="sd">        - **batch_normalisation** (`bool`): whether to use batch normalisation after the weight convolutional layers. Default `True`, same as what the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) does.</span>
</span><span id="ResNet101-729"><a href="#ResNet101-729"><span class="linenos">729</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`, because batch normalisation are doing the similar thing with bias.</span>
</span><span id="ResNet101-730"><a href="#ResNet101-730"><span class="linenos">730</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="ResNet101-731"><a href="#ResNet101-731"><span class="linenos">731</span></a>        <span class="n">ResNetBase</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="ResNet101-732"><a href="#ResNet101-732"><span class="linenos">732</span></a>            <span class="bp">self</span><span class="p">,</span>
</span><span id="ResNet101-733"><a href="#ResNet101-733"><span class="linenos">733</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
</span><span id="ResNet101-734"><a href="#ResNet101-734"><span class="linenos">734</span></a>            <span class="n">building_block_type</span><span class="o">=</span><span class="n">ResNetBlockLarge</span><span class="p">,</span>  <span class="c1"># use the larger building block for ResNet-101</span>
</span><span id="ResNet101-735"><a href="#ResNet101-735"><span class="linenos">735</span></a>            <span class="n">building_block_nums</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
</span><span id="ResNet101-736"><a href="#ResNet101-736"><span class="linenos">736</span></a>            <span class="n">building_block_preceding_output_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span>
</span><span id="ResNet101-737"><a href="#ResNet101-737"><span class="linenos">737</span></a>            <span class="n">building_block_input_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
</span><span id="ResNet101-738"><a href="#ResNet101-738"><span class="linenos">738</span></a>            <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span>
</span><span id="ResNet101-739"><a href="#ResNet101-739"><span class="linenos">739</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="ResNet101-740"><a href="#ResNet101-740"><span class="linenos">740</span></a>            <span class="n">batch_normalisation</span><span class="o">=</span><span class="n">batch_normalisation</span><span class="p">,</span>
</span><span id="ResNet101-741"><a href="#ResNet101-741"><span class="linenos">741</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="ResNet101-742"><a href="#ResNet101-742"><span class="linenos">742</span></a>        <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>ResNet-101 backbone network.</p>

<p>This is a larger architecture proposed in the <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">original ResNet paper</a>. It consists of 101 weight convolutional layers in total. See Table 1 in the paper for details.</p>
</div>


                            <div id="ResNet101.__init__" class="classattr">
                                        <input id="ResNet101.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">ResNet101</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	activation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class="param">	<span class="n">batch_normalisation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>,</span><span class="param">	<span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span></span>)</span>

                <label class="view-source-button" for="ResNet101.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#ResNet101.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ResNet101.__init__-714"><a href="#ResNet101.__init__-714"><span class="linenos">714</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="ResNet101.__init__-715"><a href="#ResNet101.__init__-715"><span class="linenos">715</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="ResNet101.__init__-716"><a href="#ResNet101.__init__-716"><span class="linenos">716</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNet101.__init__-717"><a href="#ResNet101.__init__-717"><span class="linenos">717</span></a>        <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNet101.__init__-718"><a href="#ResNet101.__init__-718"><span class="linenos">718</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="ResNet101.__init__-719"><a href="#ResNet101.__init__-719"><span class="linenos">719</span></a>        <span class="n">batch_normalisation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="ResNet101.__init__-720"><a href="#ResNet101.__init__-720"><span class="linenos">720</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="ResNet101.__init__-721"><a href="#ResNet101.__init__-721"><span class="linenos">721</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ResNet101.__init__-722"><a href="#ResNet101.__init__-722"><span class="linenos">722</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the ResNet-101 backbone network.</span>
</span><span id="ResNet101.__init__-723"><a href="#ResNet101.__init__-723"><span class="linenos">723</span></a>
</span><span id="ResNet101.__init__-724"><a href="#ResNet101.__init__-724"><span class="linenos">724</span></a><span class="sd">        **Args:**</span>
</span><span id="ResNet101.__init__-725"><a href="#ResNet101.__init__-725"><span class="linenos">725</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</span>
</span><span id="ResNet101.__init__-726"><a href="#ResNet101.__init__-726"><span class="linenos">726</span></a><span class="sd">        - **output_dim** (`int`): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</span>
</span><span id="ResNet101.__init__-727"><a href="#ResNet101.__init__-727"><span class="linenos">727</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="ResNet101.__init__-728"><a href="#ResNet101.__init__-728"><span class="linenos">728</span></a><span class="sd">        - **batch_normalisation** (`bool`): whether to use batch normalisation after the weight convolutional layers. Default `True`, same as what the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) does.</span>
</span><span id="ResNet101.__init__-729"><a href="#ResNet101.__init__-729"><span class="linenos">729</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`, because batch normalisation are doing the similar thing with bias.</span>
</span><span id="ResNet101.__init__-730"><a href="#ResNet101.__init__-730"><span class="linenos">730</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="ResNet101.__init__-731"><a href="#ResNet101.__init__-731"><span class="linenos">731</span></a>        <span class="n">ResNetBase</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="ResNet101.__init__-732"><a href="#ResNet101.__init__-732"><span class="linenos">732</span></a>            <span class="bp">self</span><span class="p">,</span>
</span><span id="ResNet101.__init__-733"><a href="#ResNet101.__init__-733"><span class="linenos">733</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
</span><span id="ResNet101.__init__-734"><a href="#ResNet101.__init__-734"><span class="linenos">734</span></a>            <span class="n">building_block_type</span><span class="o">=</span><span class="n">ResNetBlockLarge</span><span class="p">,</span>  <span class="c1"># use the larger building block for ResNet-101</span>
</span><span id="ResNet101.__init__-735"><a href="#ResNet101.__init__-735"><span class="linenos">735</span></a>            <span class="n">building_block_nums</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
</span><span id="ResNet101.__init__-736"><a href="#ResNet101.__init__-736"><span class="linenos">736</span></a>            <span class="n">building_block_preceding_output_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span>
</span><span id="ResNet101.__init__-737"><a href="#ResNet101.__init__-737"><span class="linenos">737</span></a>            <span class="n">building_block_input_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
</span><span id="ResNet101.__init__-738"><a href="#ResNet101.__init__-738"><span class="linenos">738</span></a>            <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span>
</span><span id="ResNet101.__init__-739"><a href="#ResNet101.__init__-739"><span class="linenos">739</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="ResNet101.__init__-740"><a href="#ResNet101.__init__-740"><span class="linenos">740</span></a>            <span class="n">batch_normalisation</span><span class="o">=</span><span class="n">batch_normalisation</span><span class="p">,</span>
</span><span id="ResNet101.__init__-741"><a href="#ResNet101.__init__-741"><span class="linenos">741</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="ResNet101.__init__-742"><a href="#ResNet101.__init__-742"><span class="linenos">742</span></a>        <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Construct and initialise the ResNet-101 backbone network.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</li>
<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>
<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>
<li><strong>batch_normalisation</strong> (<code>bool</code>): whether to use batch normalisation after the weight convolutional layers. Default <code>True</code>, same as what the <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">original ResNet paper</a> does.</li>
<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>, because batch normalisation are doing the similar thing with bias.</li>
</ul>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt><a href="#ResNetBase">ResNetBase</a></dt>
                                <dd id="ResNet101.batch_normalisation" class="variable"><a href="#ResNetBase.batch_normalisation">batch_normalisation</a></dd>
                <dd id="ResNet101.activation" class="variable"><a href="#ResNetBase.activation">activation</a></dd>
                <dd id="ResNet101.conv1" class="variable"><a href="#ResNetBase.conv1">conv1</a></dd>
                <dd id="ResNet101.maxpool" class="variable"><a href="#ResNetBase.maxpool">maxpool</a></dd>
                <dd id="ResNet101.conv2x" class="variable"><a href="#ResNetBase.conv2x">conv2x</a></dd>
                <dd id="ResNet101.conv3x" class="variable"><a href="#ResNetBase.conv3x">conv3x</a></dd>
                <dd id="ResNet101.conv4x" class="variable"><a href="#ResNetBase.conv4x">conv4x</a></dd>
                <dd id="ResNet101.conv5x" class="variable"><a href="#ResNetBase.conv5x">conv5x</a></dd>
                <dd id="ResNet101.avepool" class="variable"><a href="#ResNetBase.avepool">avepool</a></dd>
                <dd id="ResNet101.forward" class="function"><a href="#ResNetBase.forward">forward</a></dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="ResNet152">
                            <input id="ResNet152-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">ResNet152</span><wbr>(<span class="base"><a href="#ResNetBase">ResNetBase</a></span>):

                <label class="view-source-button" for="ResNet152-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#ResNet152"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ResNet152-745"><a href="#ResNet152-745"><span class="linenos">745</span></a><span class="k">class</span><span class="w"> </span><span class="nc">ResNet152</span><span class="p">(</span><span class="n">ResNetBase</span><span class="p">):</span>
</span><span id="ResNet152-746"><a href="#ResNet152-746"><span class="linenos">746</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;ResNet-152 backbone network.</span>
</span><span id="ResNet152-747"><a href="#ResNet152-747"><span class="linenos">747</span></a>
</span><span id="ResNet152-748"><a href="#ResNet152-748"><span class="linenos">748</span></a><span class="sd">    This is the largest architecture proposed in the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html). It consists of 152 weight convolutional layers in total. See Table 1 in the paper for details.</span>
</span><span id="ResNet152-749"><a href="#ResNet152-749"><span class="linenos">749</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="ResNet152-750"><a href="#ResNet152-750"><span class="linenos">750</span></a>
</span><span id="ResNet152-751"><a href="#ResNet152-751"><span class="linenos">751</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="ResNet152-752"><a href="#ResNet152-752"><span class="linenos">752</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="ResNet152-753"><a href="#ResNet152-753"><span class="linenos">753</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNet152-754"><a href="#ResNet152-754"><span class="linenos">754</span></a>        <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNet152-755"><a href="#ResNet152-755"><span class="linenos">755</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="ResNet152-756"><a href="#ResNet152-756"><span class="linenos">756</span></a>        <span class="n">batch_normalisation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="ResNet152-757"><a href="#ResNet152-757"><span class="linenos">757</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="ResNet152-758"><a href="#ResNet152-758"><span class="linenos">758</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ResNet152-759"><a href="#ResNet152-759"><span class="linenos">759</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the ResNet-50 backbone network.</span>
</span><span id="ResNet152-760"><a href="#ResNet152-760"><span class="linenos">760</span></a>
</span><span id="ResNet152-761"><a href="#ResNet152-761"><span class="linenos">761</span></a><span class="sd">        **Args:**</span>
</span><span id="ResNet152-762"><a href="#ResNet152-762"><span class="linenos">762</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</span>
</span><span id="ResNet152-763"><a href="#ResNet152-763"><span class="linenos">763</span></a><span class="sd">        - **output_dim** (`int`): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</span>
</span><span id="ResNet152-764"><a href="#ResNet152-764"><span class="linenos">764</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="ResNet152-765"><a href="#ResNet152-765"><span class="linenos">765</span></a><span class="sd">        - **batch_normalisation** (`bool`): whether to use batch normalisation after the weight convolutional layers. Default `True`, same as what the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) does.</span>
</span><span id="ResNet152-766"><a href="#ResNet152-766"><span class="linenos">766</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`, because batch normalisation are doing the similar thing with bias.</span>
</span><span id="ResNet152-767"><a href="#ResNet152-767"><span class="linenos">767</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="ResNet152-768"><a href="#ResNet152-768"><span class="linenos">768</span></a>        <span class="n">ResNetBase</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="ResNet152-769"><a href="#ResNet152-769"><span class="linenos">769</span></a>            <span class="bp">self</span><span class="p">,</span>
</span><span id="ResNet152-770"><a href="#ResNet152-770"><span class="linenos">770</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
</span><span id="ResNet152-771"><a href="#ResNet152-771"><span class="linenos">771</span></a>            <span class="n">building_block_type</span><span class="o">=</span><span class="n">ResNetBlockLarge</span><span class="p">,</span>  <span class="c1"># use the larger building block for ResNet-152</span>
</span><span id="ResNet152-772"><a href="#ResNet152-772"><span class="linenos">772</span></a>            <span class="n">building_block_nums</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">36</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
</span><span id="ResNet152-773"><a href="#ResNet152-773"><span class="linenos">773</span></a>            <span class="n">building_block_preceding_output_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span>
</span><span id="ResNet152-774"><a href="#ResNet152-774"><span class="linenos">774</span></a>            <span class="n">building_block_input_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
</span><span id="ResNet152-775"><a href="#ResNet152-775"><span class="linenos">775</span></a>            <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span>
</span><span id="ResNet152-776"><a href="#ResNet152-776"><span class="linenos">776</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="ResNet152-777"><a href="#ResNet152-777"><span class="linenos">777</span></a>            <span class="n">batch_normalisation</span><span class="o">=</span><span class="n">batch_normalisation</span><span class="p">,</span>
</span><span id="ResNet152-778"><a href="#ResNet152-778"><span class="linenos">778</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="ResNet152-779"><a href="#ResNet152-779"><span class="linenos">779</span></a>        <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>ResNet-152 backbone network.</p>

<p>This is the largest architecture proposed in the <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">original ResNet paper</a>. It consists of 152 weight convolutional layers in total. See Table 1 in the paper for details.</p>
</div>


                            <div id="ResNet152.__init__" class="classattr">
                                        <input id="ResNet152.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">ResNet152</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	activation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class="param">	<span class="n">batch_normalisation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>,</span><span class="param">	<span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span></span>)</span>

                <label class="view-source-button" for="ResNet152.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#ResNet152.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ResNet152.__init__-751"><a href="#ResNet152.__init__-751"><span class="linenos">751</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="ResNet152.__init__-752"><a href="#ResNet152.__init__-752"><span class="linenos">752</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="ResNet152.__init__-753"><a href="#ResNet152.__init__-753"><span class="linenos">753</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNet152.__init__-754"><a href="#ResNet152.__init__-754"><span class="linenos">754</span></a>        <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNet152.__init__-755"><a href="#ResNet152.__init__-755"><span class="linenos">755</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="ResNet152.__init__-756"><a href="#ResNet152.__init__-756"><span class="linenos">756</span></a>        <span class="n">batch_normalisation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="ResNet152.__init__-757"><a href="#ResNet152.__init__-757"><span class="linenos">757</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="ResNet152.__init__-758"><a href="#ResNet152.__init__-758"><span class="linenos">758</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ResNet152.__init__-759"><a href="#ResNet152.__init__-759"><span class="linenos">759</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the ResNet-50 backbone network.</span>
</span><span id="ResNet152.__init__-760"><a href="#ResNet152.__init__-760"><span class="linenos">760</span></a>
</span><span id="ResNet152.__init__-761"><a href="#ResNet152.__init__-761"><span class="linenos">761</span></a><span class="sd">        **Args:**</span>
</span><span id="ResNet152.__init__-762"><a href="#ResNet152.__init__-762"><span class="linenos">762</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</span>
</span><span id="ResNet152.__init__-763"><a href="#ResNet152.__init__-763"><span class="linenos">763</span></a><span class="sd">        - **output_dim** (`int`): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</span>
</span><span id="ResNet152.__init__-764"><a href="#ResNet152.__init__-764"><span class="linenos">764</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="ResNet152.__init__-765"><a href="#ResNet152.__init__-765"><span class="linenos">765</span></a><span class="sd">        - **batch_normalisation** (`bool`): whether to use batch normalisation after the weight convolutional layers. Default `True`, same as what the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) does.</span>
</span><span id="ResNet152.__init__-766"><a href="#ResNet152.__init__-766"><span class="linenos">766</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`, because batch normalisation are doing the similar thing with bias.</span>
</span><span id="ResNet152.__init__-767"><a href="#ResNet152.__init__-767"><span class="linenos">767</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="ResNet152.__init__-768"><a href="#ResNet152.__init__-768"><span class="linenos">768</span></a>        <span class="n">ResNetBase</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="ResNet152.__init__-769"><a href="#ResNet152.__init__-769"><span class="linenos">769</span></a>            <span class="bp">self</span><span class="p">,</span>
</span><span id="ResNet152.__init__-770"><a href="#ResNet152.__init__-770"><span class="linenos">770</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
</span><span id="ResNet152.__init__-771"><a href="#ResNet152.__init__-771"><span class="linenos">771</span></a>            <span class="n">building_block_type</span><span class="o">=</span><span class="n">ResNetBlockLarge</span><span class="p">,</span>  <span class="c1"># use the larger building block for ResNet-152</span>
</span><span id="ResNet152.__init__-772"><a href="#ResNet152.__init__-772"><span class="linenos">772</span></a>            <span class="n">building_block_nums</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">36</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
</span><span id="ResNet152.__init__-773"><a href="#ResNet152.__init__-773"><span class="linenos">773</span></a>            <span class="n">building_block_preceding_output_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span>
</span><span id="ResNet152.__init__-774"><a href="#ResNet152.__init__-774"><span class="linenos">774</span></a>            <span class="n">building_block_input_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
</span><span id="ResNet152.__init__-775"><a href="#ResNet152.__init__-775"><span class="linenos">775</span></a>            <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span>
</span><span id="ResNet152.__init__-776"><a href="#ResNet152.__init__-776"><span class="linenos">776</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="ResNet152.__init__-777"><a href="#ResNet152.__init__-777"><span class="linenos">777</span></a>            <span class="n">batch_normalisation</span><span class="o">=</span><span class="n">batch_normalisation</span><span class="p">,</span>
</span><span id="ResNet152.__init__-778"><a href="#ResNet152.__init__-778"><span class="linenos">778</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="ResNet152.__init__-779"><a href="#ResNet152.__init__-779"><span class="linenos">779</span></a>        <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Construct and initialise the ResNet-50 backbone network.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</li>
<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>
<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>
<li><strong>batch_normalisation</strong> (<code>bool</code>): whether to use batch normalisation after the weight convolutional layers. Default <code>True</code>, same as what the <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">original ResNet paper</a> does.</li>
<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>, because batch normalisation are doing the similar thing with bias.</li>
</ul>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt><a href="#ResNetBase">ResNetBase</a></dt>
                                <dd id="ResNet152.batch_normalisation" class="variable"><a href="#ResNetBase.batch_normalisation">batch_normalisation</a></dd>
                <dd id="ResNet152.activation" class="variable"><a href="#ResNetBase.activation">activation</a></dd>
                <dd id="ResNet152.conv1" class="variable"><a href="#ResNetBase.conv1">conv1</a></dd>
                <dd id="ResNet152.maxpool" class="variable"><a href="#ResNetBase.maxpool">maxpool</a></dd>
                <dd id="ResNet152.conv2x" class="variable"><a href="#ResNetBase.conv2x">conv2x</a></dd>
                <dd id="ResNet152.conv3x" class="variable"><a href="#ResNetBase.conv3x">conv3x</a></dd>
                <dd id="ResNet152.conv4x" class="variable"><a href="#ResNetBase.conv4x">conv4x</a></dd>
                <dd id="ResNet152.conv5x" class="variable"><a href="#ResNetBase.conv5x">conv5x</a></dd>
                <dd id="ResNet152.avepool" class="variable"><a href="#ResNetBase.avepool">avepool</a></dd>
                <dd id="ResNet152.forward" class="function"><a href="#ResNetBase.forward">forward</a></dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="HATMaskResNetBlockSmall">
                            <input id="HATMaskResNetBlockSmall-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">HATMaskResNetBlockSmall</span><wbr>(<span class="base">clarena.backbones.base.HATMaskBackbone</span>, <span class="base"><a href="#ResNetBlockSmall">ResNetBlockSmall</a></span>):

                <label class="view-source-button" for="HATMaskResNetBlockSmall-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HATMaskResNetBlockSmall"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HATMaskResNetBlockSmall-782"><a href="#HATMaskResNetBlockSmall-782"><span class="linenos">782</span></a><span class="k">class</span><span class="w"> </span><span class="nc">HATMaskResNetBlockSmall</span><span class="p">(</span><span class="n">HATMaskBackbone</span><span class="p">,</span> <span class="n">ResNetBlockSmall</span><span class="p">):</span>
</span><span id="HATMaskResNetBlockSmall-783"><a href="#HATMaskResNetBlockSmall-783"><span class="linenos">783</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The smaller building block for HAT masked ResNet-18/34.</span>
</span><span id="HATMaskResNetBlockSmall-784"><a href="#HATMaskResNetBlockSmall-784"><span class="linenos">784</span></a>
</span><span id="HATMaskResNetBlockSmall-785"><a href="#HATMaskResNetBlockSmall-785"><span class="linenos">785</span></a><span class="sd">    It consists of 2 weight convolutional layers, each followed by an activation function. See Table 1 or Figure 5 (left) in the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html).</span>
</span><span id="HATMaskResNetBlockSmall-786"><a href="#HATMaskResNetBlockSmall-786"><span class="linenos">786</span></a>
</span><span id="HATMaskResNetBlockSmall-787"><a href="#HATMaskResNetBlockSmall-787"><span class="linenos">787</span></a><span class="sd">    Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the unit-wise task embedding and gate function.</span>
</span><span id="HATMaskResNetBlockSmall-788"><a href="#HATMaskResNetBlockSmall-788"><span class="linenos">788</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="HATMaskResNetBlockSmall-789"><a href="#HATMaskResNetBlockSmall-789"><span class="linenos">789</span></a>
</span><span id="HATMaskResNetBlockSmall-790"><a href="#HATMaskResNetBlockSmall-790"><span class="linenos">790</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="HATMaskResNetBlockSmall-791"><a href="#HATMaskResNetBlockSmall-791"><span class="linenos">791</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall-792"><a href="#HATMaskResNetBlockSmall-792"><span class="linenos">792</span></a>        <span class="n">outer_layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall-793"><a href="#HATMaskResNetBlockSmall-793"><span class="linenos">793</span></a>        <span class="n">block_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall-794"><a href="#HATMaskResNetBlockSmall-794"><span class="linenos">794</span></a>        <span class="n">preceding_output_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall-795"><a href="#HATMaskResNetBlockSmall-795"><span class="linenos">795</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall-796"><a href="#HATMaskResNetBlockSmall-796"><span class="linenos">796</span></a>        <span class="n">overall_stride</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall-797"><a href="#HATMaskResNetBlockSmall-797"><span class="linenos">797</span></a>        <span class="n">gate</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall-798"><a href="#HATMaskResNetBlockSmall-798"><span class="linenos">798</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall-799"><a href="#HATMaskResNetBlockSmall-799"><span class="linenos">799</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall-800"><a href="#HATMaskResNetBlockSmall-800"><span class="linenos">800</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HATMaskResNetBlockSmall-801"><a href="#HATMaskResNetBlockSmall-801"><span class="linenos">801</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the smaller building block with task embedding.</span>
</span><span id="HATMaskResNetBlockSmall-802"><a href="#HATMaskResNetBlockSmall-802"><span class="linenos">802</span></a>
</span><span id="HATMaskResNetBlockSmall-803"><a href="#HATMaskResNetBlockSmall-803"><span class="linenos">803</span></a><span class="sd">        **Args:**</span>
</span><span id="HATMaskResNetBlockSmall-804"><a href="#HATMaskResNetBlockSmall-804"><span class="linenos">804</span></a><span class="sd">        - **outer_layer_name** (`str`): pass the name of the multi-building-block layer that contains this building block to construct the full name of each weighted convolutional layer.</span>
</span><span id="HATMaskResNetBlockSmall-805"><a href="#HATMaskResNetBlockSmall-805"><span class="linenos">805</span></a><span class="sd">        - **block_idx** (`int`): the index of the building blocks in the multi-building-block layer to construct the full name of each weighted convolutional layer.</span>
</span><span id="HATMaskResNetBlockSmall-806"><a href="#HATMaskResNetBlockSmall-806"><span class="linenos">806</span></a><span class="sd">        - **preceding_output_channels** (`int`): the number of channels of preceding output of this particular building block.</span>
</span><span id="HATMaskResNetBlockSmall-807"><a href="#HATMaskResNetBlockSmall-807"><span class="linenos">807</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input of this building block.</span>
</span><span id="HATMaskResNetBlockSmall-808"><a href="#HATMaskResNetBlockSmall-808"><span class="linenos">808</span></a><span class="sd">        - **overall_stride** (`int`): the overall stride of this building block. This stride is performed at 2nd (last) convolutional layer where the 1st convolutional layer remain stride of 1.</span>
</span><span id="HATMaskResNetBlockSmall-809"><a href="#HATMaskResNetBlockSmall-809"><span class="linenos">809</span></a><span class="sd">        - **gate** (`str`): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:</span>
</span><span id="HATMaskResNetBlockSmall-810"><a href="#HATMaskResNetBlockSmall-810"><span class="linenos">810</span></a><span class="sd">            - `sigmoid`: the sigmoid function.</span>
</span><span id="HATMaskResNetBlockSmall-811"><a href="#HATMaskResNetBlockSmall-811"><span class="linenos">811</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="HATMaskResNetBlockSmall-812"><a href="#HATMaskResNetBlockSmall-812"><span class="linenos">812</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`.</span>
</span><span id="HATMaskResNetBlockSmall-813"><a href="#HATMaskResNetBlockSmall-813"><span class="linenos">813</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HATMaskResNetBlockSmall-814"><a href="#HATMaskResNetBlockSmall-814"><span class="linenos">814</span></a>        <span class="n">HATMaskBackbone</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">gate</span><span class="o">=</span><span class="n">gate</span><span class="p">)</span>
</span><span id="HATMaskResNetBlockSmall-815"><a href="#HATMaskResNetBlockSmall-815"><span class="linenos">815</span></a>        <span class="n">ResNetBlockSmall</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="HATMaskResNetBlockSmall-816"><a href="#HATMaskResNetBlockSmall-816"><span class="linenos">816</span></a>            <span class="bp">self</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall-817"><a href="#HATMaskResNetBlockSmall-817"><span class="linenos">817</span></a>            <span class="n">outer_layer_name</span><span class="o">=</span><span class="n">outer_layer_name</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall-818"><a href="#HATMaskResNetBlockSmall-818"><span class="linenos">818</span></a>            <span class="n">block_idx</span><span class="o">=</span><span class="n">block_idx</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall-819"><a href="#HATMaskResNetBlockSmall-819"><span class="linenos">819</span></a>            <span class="n">preceding_output_channels</span><span class="o">=</span><span class="n">preceding_output_channels</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall-820"><a href="#HATMaskResNetBlockSmall-820"><span class="linenos">820</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall-821"><a href="#HATMaskResNetBlockSmall-821"><span class="linenos">821</span></a>            <span class="n">overall_stride</span><span class="o">=</span><span class="n">overall_stride</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall-822"><a href="#HATMaskResNetBlockSmall-822"><span class="linenos">822</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall-823"><a href="#HATMaskResNetBlockSmall-823"><span class="linenos">823</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall-824"><a href="#HATMaskResNetBlockSmall-824"><span class="linenos">824</span></a>        <span class="p">)</span>
</span><span id="HATMaskResNetBlockSmall-825"><a href="#HATMaskResNetBlockSmall-825"><span class="linenos">825</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">register_hat_mask_module_explicitly</span><span class="p">(</span><span class="n">gate</span><span class="o">=</span><span class="n">gate</span><span class="p">)</span>
</span><span id="HATMaskResNetBlockSmall-826"><a href="#HATMaskResNetBlockSmall-826"><span class="linenos">826</span></a>
</span><span id="HATMaskResNetBlockSmall-827"><a href="#HATMaskResNetBlockSmall-827"><span class="linenos">827</span></a>        <span class="c1"># construct the task embedding over the 1st weighted convolutional layer. It is channel-wise</span>
</span><span id="HATMaskResNetBlockSmall-828"><a href="#HATMaskResNetBlockSmall-828"><span class="linenos">828</span></a>        <span class="n">layer_output_channels</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="HATMaskResNetBlockSmall-829"><a href="#HATMaskResNetBlockSmall-829"><span class="linenos">829</span></a>            <span class="n">input_channels</span>  <span class="c1"># the output channels of the 1st convolutional layer</span>
</span><span id="HATMaskResNetBlockSmall-830"><a href="#HATMaskResNetBlockSmall-830"><span class="linenos">830</span></a>        <span class="p">)</span>
</span><span id="HATMaskResNetBlockSmall-831"><a href="#HATMaskResNetBlockSmall-831"><span class="linenos">831</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_1st_layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
</span><span id="HATMaskResNetBlockSmall-832"><a href="#HATMaskResNetBlockSmall-832"><span class="linenos">832</span></a>            <span class="n">num_embeddings</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="n">layer_output_channels</span>
</span><span id="HATMaskResNetBlockSmall-833"><a href="#HATMaskResNetBlockSmall-833"><span class="linenos">833</span></a>        <span class="p">)</span>
</span><span id="HATMaskResNetBlockSmall-834"><a href="#HATMaskResNetBlockSmall-834"><span class="linenos">834</span></a>
</span><span id="HATMaskResNetBlockSmall-835"><a href="#HATMaskResNetBlockSmall-835"><span class="linenos">835</span></a>        <span class="c1"># construct the task embedding over the 2nd weighted convolutional layer. It is channel-wise</span>
</span><span id="HATMaskResNetBlockSmall-836"><a href="#HATMaskResNetBlockSmall-836"><span class="linenos">836</span></a>        <span class="n">layer_output_channels</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="HATMaskResNetBlockSmall-837"><a href="#HATMaskResNetBlockSmall-837"><span class="linenos">837</span></a>            <span class="n">input_channels</span> <span class="o">*</span> <span class="mi">1</span>
</span><span id="HATMaskResNetBlockSmall-838"><a href="#HATMaskResNetBlockSmall-838"><span class="linenos">838</span></a>        <span class="p">)</span>  <span class="c1"># the output channels of the 2nd convolutional layer, which is the same as the input channels (without expansion)</span>
</span><span id="HATMaskResNetBlockSmall-839"><a href="#HATMaskResNetBlockSmall-839"><span class="linenos">839</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_2nd_layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
</span><span id="HATMaskResNetBlockSmall-840"><a href="#HATMaskResNetBlockSmall-840"><span class="linenos">840</span></a>            <span class="n">num_embeddings</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="n">layer_output_channels</span>
</span><span id="HATMaskResNetBlockSmall-841"><a href="#HATMaskResNetBlockSmall-841"><span class="linenos">841</span></a>        <span class="p">)</span>
</span><span id="HATMaskResNetBlockSmall-842"><a href="#HATMaskResNetBlockSmall-842"><span class="linenos">842</span></a>
</span><span id="HATMaskResNetBlockSmall-843"><a href="#HATMaskResNetBlockSmall-843"><span class="linenos">843</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="HATMaskResNetBlockSmall-844"><a href="#HATMaskResNetBlockSmall-844"><span class="linenos">844</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall-845"><a href="#HATMaskResNetBlockSmall-845"><span class="linenos">845</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall-846"><a href="#HATMaskResNetBlockSmall-846"><span class="linenos">846</span></a>        <span class="n">stage</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall-847"><a href="#HATMaskResNetBlockSmall-847"><span class="linenos">847</span></a>        <span class="n">s_max</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall-848"><a href="#HATMaskResNetBlockSmall-848"><span class="linenos">848</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall-849"><a href="#HATMaskResNetBlockSmall-849"><span class="linenos">849</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall-850"><a href="#HATMaskResNetBlockSmall-850"><span class="linenos">850</span></a>        <span class="n">test_mask</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall-851"><a href="#HATMaskResNetBlockSmall-851"><span class="linenos">851</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
</span><span id="HATMaskResNetBlockSmall-852"><a href="#HATMaskResNetBlockSmall-852"><span class="linenos">852</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The forward pass for data from task `task_id`. Task-specific mask for `task_id` are applied to the units which are channels in each weighted convolutional layer.</span>
</span><span id="HATMaskResNetBlockSmall-853"><a href="#HATMaskResNetBlockSmall-853"><span class="linenos">853</span></a>
</span><span id="HATMaskResNetBlockSmall-854"><a href="#HATMaskResNetBlockSmall-854"><span class="linenos">854</span></a><span class="sd">        **Args:**</span>
</span><span id="HATMaskResNetBlockSmall-855"><a href="#HATMaskResNetBlockSmall-855"><span class="linenos">855</span></a><span class="sd">        - **input** (`Tensor`): The input tensor from data.</span>
</span><span id="HATMaskResNetBlockSmall-856"><a href="#HATMaskResNetBlockSmall-856"><span class="linenos">856</span></a><span class="sd">        - **stage** (`str`): the stage of the forward pass, should be one of the following:</span>
</span><span id="HATMaskResNetBlockSmall-857"><a href="#HATMaskResNetBlockSmall-857"><span class="linenos">857</span></a><span class="sd">            1. &#39;train&#39;: training stage.</span>
</span><span id="HATMaskResNetBlockSmall-858"><a href="#HATMaskResNetBlockSmall-858"><span class="linenos">858</span></a><span class="sd">            2. &#39;validation&#39;: validation stage.</span>
</span><span id="HATMaskResNetBlockSmall-859"><a href="#HATMaskResNetBlockSmall-859"><span class="linenos">859</span></a><span class="sd">            3. &#39;test&#39;: testing stage.</span>
</span><span id="HATMaskResNetBlockSmall-860"><a href="#HATMaskResNetBlockSmall-860"><span class="linenos">860</span></a><span class="sd">        - **s_max** (`float` | `None`): the maximum scaling factor in the gate function. Doesn&#39;t apply to testing stage. See chapter 2.4 &quot;Hard Attention Training&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="HATMaskResNetBlockSmall-861"><a href="#HATMaskResNetBlockSmall-861"><span class="linenos">861</span></a><span class="sd">        - **batch_idx** (`int` | `None`): the current batch index. Applies only to training stage. For other stages, it is default `None`.</span>
</span><span id="HATMaskResNetBlockSmall-862"><a href="#HATMaskResNetBlockSmall-862"><span class="linenos">862</span></a><span class="sd">        - **num_batches** (`int` | `None`): the total number of batches. Applies only to training stage. For other stages, it is default `None`.</span>
</span><span id="HATMaskResNetBlockSmall-863"><a href="#HATMaskResNetBlockSmall-863"><span class="linenos">863</span></a><span class="sd">        - **test_mask** (`dict[str, Tensor]` | `None`): the binary mask used for test. Applies only to testing stage. For other stages, it is default `None`.</span>
</span><span id="HATMaskResNetBlockSmall-864"><a href="#HATMaskResNetBlockSmall-864"><span class="linenos">864</span></a>
</span><span id="HATMaskResNetBlockSmall-865"><a href="#HATMaskResNetBlockSmall-865"><span class="linenos">865</span></a><span class="sd">        **Returns:**</span>
</span><span id="HATMaskResNetBlockSmall-866"><a href="#HATMaskResNetBlockSmall-866"><span class="linenos">866</span></a><span class="sd">        - **output_feature** (`Tensor`): the output feature maps.</span>
</span><span id="HATMaskResNetBlockSmall-867"><a href="#HATMaskResNetBlockSmall-867"><span class="linenos">867</span></a><span class="sd">        - **mask** (`dict[str, Tensor]`): the mask for the current task. Key (`str`) is layer name, value (`Tensor`) is the mask tensor. The mask tensor has size (number of units).</span>
</span><span id="HATMaskResNetBlockSmall-868"><a href="#HATMaskResNetBlockSmall-868"><span class="linenos">868</span></a><span class="sd">        - **hidden_features** (`dict[str, Tensor]`): the hidden features (after activation) in each weighted layer. Key (`str`) is the weighted layer name, value (`Tensor`) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes. Although HAT algorithm does not need this, it is still provided for API consistence for other HAT-based algorithms inherited this `forward()` method of `HAT` class.</span>
</span><span id="HATMaskResNetBlockSmall-869"><a href="#HATMaskResNetBlockSmall-869"><span class="linenos">869</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HATMaskResNetBlockSmall-870"><a href="#HATMaskResNetBlockSmall-870"><span class="linenos">870</span></a>        <span class="n">hidden_features</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="HATMaskResNetBlockSmall-871"><a href="#HATMaskResNetBlockSmall-871"><span class="linenos">871</span></a>
</span><span id="HATMaskResNetBlockSmall-872"><a href="#HATMaskResNetBlockSmall-872"><span class="linenos">872</span></a>        <span class="c1"># get the mask for the current task from the task embedding in this stage</span>
</span><span id="HATMaskResNetBlockSmall-873"><a href="#HATMaskResNetBlockSmall-873"><span class="linenos">873</span></a>        <span class="n">mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_mask</span><span class="p">(</span>
</span><span id="HATMaskResNetBlockSmall-874"><a href="#HATMaskResNetBlockSmall-874"><span class="linenos">874</span></a>            <span class="n">stage</span><span class="o">=</span><span class="n">stage</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall-875"><a href="#HATMaskResNetBlockSmall-875"><span class="linenos">875</span></a>            <span class="n">s_max</span><span class="o">=</span><span class="n">s_max</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall-876"><a href="#HATMaskResNetBlockSmall-876"><span class="linenos">876</span></a>            <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall-877"><a href="#HATMaskResNetBlockSmall-877"><span class="linenos">877</span></a>            <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall-878"><a href="#HATMaskResNetBlockSmall-878"><span class="linenos">878</span></a>            <span class="n">test_mask</span><span class="o">=</span><span class="n">test_mask</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall-879"><a href="#HATMaskResNetBlockSmall-879"><span class="linenos">879</span></a>        <span class="p">)</span>
</span><span id="HATMaskResNetBlockSmall-880"><a href="#HATMaskResNetBlockSmall-880"><span class="linenos">880</span></a>
</span><span id="HATMaskResNetBlockSmall-881"><a href="#HATMaskResNetBlockSmall-881"><span class="linenos">881</span></a>        <span class="n">identity</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="HATMaskResNetBlockSmall-882"><a href="#HATMaskResNetBlockSmall-882"><span class="linenos">882</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">identity_downsample</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span><span id="HATMaskResNetBlockSmall-883"><a href="#HATMaskResNetBlockSmall-883"><span class="linenos">883</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">identity_downsample</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span><span id="HATMaskResNetBlockSmall-884"><a href="#HATMaskResNetBlockSmall-884"><span class="linenos">884</span></a>            <span class="k">else</span> <span class="nb">input</span>
</span><span id="HATMaskResNetBlockSmall-885"><a href="#HATMaskResNetBlockSmall-885"><span class="linenos">885</span></a>        <span class="p">)</span>  <span class="c1"># remember the identity of input for the shortcut connection. Perform downsampling if its dimension doesn&#39;t match the output&#39;s</span>
</span><span id="HATMaskResNetBlockSmall-886"><a href="#HATMaskResNetBlockSmall-886"><span class="linenos">886</span></a>
</span><span id="HATMaskResNetBlockSmall-887"><a href="#HATMaskResNetBlockSmall-887"><span class="linenos">887</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="nb">input</span>
</span><span id="HATMaskResNetBlockSmall-888"><a href="#HATMaskResNetBlockSmall-888"><span class="linenos">888</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># weighted convolutional layer first</span>
</span><span id="HATMaskResNetBlockSmall-889"><a href="#HATMaskResNetBlockSmall-889"><span class="linenos">889</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span>
</span><span id="HATMaskResNetBlockSmall-890"><a href="#HATMaskResNetBlockSmall-890"><span class="linenos">890</span></a>            <span class="n">mask</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_1st_layer_name</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="HATMaskResNetBlockSmall-891"><a href="#HATMaskResNetBlockSmall-891"><span class="linenos">891</span></a>        <span class="p">)</span>  <span class="c1"># apply the mask to the 1st convolutional layer. Broadcast the dimension of mask to match the input</span>
</span><span id="HATMaskResNetBlockSmall-892"><a href="#HATMaskResNetBlockSmall-892"><span class="linenos">892</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="HATMaskResNetBlockSmall-893"><a href="#HATMaskResNetBlockSmall-893"><span class="linenos">893</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># activation function third</span>
</span><span id="HATMaskResNetBlockSmall-894"><a href="#HATMaskResNetBlockSmall-894"><span class="linenos">894</span></a>        <span class="n">hidden_features</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_1st_layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>  <span class="c1"># store the hidden feature</span>
</span><span id="HATMaskResNetBlockSmall-895"><a href="#HATMaskResNetBlockSmall-895"><span class="linenos">895</span></a>
</span><span id="HATMaskResNetBlockSmall-896"><a href="#HATMaskResNetBlockSmall-896"><span class="linenos">896</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># weighted convolutional layer first</span>
</span><span id="HATMaskResNetBlockSmall-897"><a href="#HATMaskResNetBlockSmall-897"><span class="linenos">897</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">identity</span>
</span><span id="HATMaskResNetBlockSmall-898"><a href="#HATMaskResNetBlockSmall-898"><span class="linenos">898</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span>
</span><span id="HATMaskResNetBlockSmall-899"><a href="#HATMaskResNetBlockSmall-899"><span class="linenos">899</span></a>            <span class="n">mask</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_2nd_layer_name</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="HATMaskResNetBlockSmall-900"><a href="#HATMaskResNetBlockSmall-900"><span class="linenos">900</span></a>        <span class="p">)</span>  <span class="c1"># apply the mask to the 2nd convolutional layer after the shortcut connection. Broadcast the dimension of mask to match the input</span>
</span><span id="HATMaskResNetBlockSmall-901"><a href="#HATMaskResNetBlockSmall-901"><span class="linenos">901</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="HATMaskResNetBlockSmall-902"><a href="#HATMaskResNetBlockSmall-902"><span class="linenos">902</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># activation after the shortcut connection</span>
</span><span id="HATMaskResNetBlockSmall-903"><a href="#HATMaskResNetBlockSmall-903"><span class="linenos">903</span></a>        <span class="n">hidden_features</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_2nd_layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>  <span class="c1"># store the hidden feature</span>
</span><span id="HATMaskResNetBlockSmall-904"><a href="#HATMaskResNetBlockSmall-904"><span class="linenos">904</span></a>
</span><span id="HATMaskResNetBlockSmall-905"><a href="#HATMaskResNetBlockSmall-905"><span class="linenos">905</span></a>        <span class="n">output_feature</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="HATMaskResNetBlockSmall-906"><a href="#HATMaskResNetBlockSmall-906"><span class="linenos">906</span></a>
</span><span id="HATMaskResNetBlockSmall-907"><a href="#HATMaskResNetBlockSmall-907"><span class="linenos">907</span></a>        <span class="k">return</span> <span class="n">output_feature</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">hidden_features</span>
</span></pre></div>


            <div class="docstring"><p>The smaller building block for HAT masked ResNet-18/34.</p>

<p>It consists of 2 weight convolutional layers, each followed by an activation function. See Table 1 or Figure 5 (left) in the <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">original ResNet paper</a>.</p>

<p>Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the unit-wise task embedding and gate function.</p>
</div>


                            <div id="HATMaskResNetBlockSmall.__init__" class="classattr">
                                        <input id="HATMaskResNetBlockSmall.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">HATMaskResNetBlockSmall</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">outer_layer_name</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">block_idx</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">preceding_output_channels</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">overall_stride</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">gate</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	activation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class="param">	<span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span></span>)</span>

                <label class="view-source-button" for="HATMaskResNetBlockSmall.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HATMaskResNetBlockSmall.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HATMaskResNetBlockSmall.__init__-790"><a href="#HATMaskResNetBlockSmall.__init__-790"><span class="linenos">790</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="HATMaskResNetBlockSmall.__init__-791"><a href="#HATMaskResNetBlockSmall.__init__-791"><span class="linenos">791</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall.__init__-792"><a href="#HATMaskResNetBlockSmall.__init__-792"><span class="linenos">792</span></a>        <span class="n">outer_layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall.__init__-793"><a href="#HATMaskResNetBlockSmall.__init__-793"><span class="linenos">793</span></a>        <span class="n">block_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall.__init__-794"><a href="#HATMaskResNetBlockSmall.__init__-794"><span class="linenos">794</span></a>        <span class="n">preceding_output_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall.__init__-795"><a href="#HATMaskResNetBlockSmall.__init__-795"><span class="linenos">795</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall.__init__-796"><a href="#HATMaskResNetBlockSmall.__init__-796"><span class="linenos">796</span></a>        <span class="n">overall_stride</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall.__init__-797"><a href="#HATMaskResNetBlockSmall.__init__-797"><span class="linenos">797</span></a>        <span class="n">gate</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall.__init__-798"><a href="#HATMaskResNetBlockSmall.__init__-798"><span class="linenos">798</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall.__init__-799"><a href="#HATMaskResNetBlockSmall.__init__-799"><span class="linenos">799</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall.__init__-800"><a href="#HATMaskResNetBlockSmall.__init__-800"><span class="linenos">800</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HATMaskResNetBlockSmall.__init__-801"><a href="#HATMaskResNetBlockSmall.__init__-801"><span class="linenos">801</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the smaller building block with task embedding.</span>
</span><span id="HATMaskResNetBlockSmall.__init__-802"><a href="#HATMaskResNetBlockSmall.__init__-802"><span class="linenos">802</span></a>
</span><span id="HATMaskResNetBlockSmall.__init__-803"><a href="#HATMaskResNetBlockSmall.__init__-803"><span class="linenos">803</span></a><span class="sd">        **Args:**</span>
</span><span id="HATMaskResNetBlockSmall.__init__-804"><a href="#HATMaskResNetBlockSmall.__init__-804"><span class="linenos">804</span></a><span class="sd">        - **outer_layer_name** (`str`): pass the name of the multi-building-block layer that contains this building block to construct the full name of each weighted convolutional layer.</span>
</span><span id="HATMaskResNetBlockSmall.__init__-805"><a href="#HATMaskResNetBlockSmall.__init__-805"><span class="linenos">805</span></a><span class="sd">        - **block_idx** (`int`): the index of the building blocks in the multi-building-block layer to construct the full name of each weighted convolutional layer.</span>
</span><span id="HATMaskResNetBlockSmall.__init__-806"><a href="#HATMaskResNetBlockSmall.__init__-806"><span class="linenos">806</span></a><span class="sd">        - **preceding_output_channels** (`int`): the number of channels of preceding output of this particular building block.</span>
</span><span id="HATMaskResNetBlockSmall.__init__-807"><a href="#HATMaskResNetBlockSmall.__init__-807"><span class="linenos">807</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input of this building block.</span>
</span><span id="HATMaskResNetBlockSmall.__init__-808"><a href="#HATMaskResNetBlockSmall.__init__-808"><span class="linenos">808</span></a><span class="sd">        - **overall_stride** (`int`): the overall stride of this building block. This stride is performed at 2nd (last) convolutional layer where the 1st convolutional layer remain stride of 1.</span>
</span><span id="HATMaskResNetBlockSmall.__init__-809"><a href="#HATMaskResNetBlockSmall.__init__-809"><span class="linenos">809</span></a><span class="sd">        - **gate** (`str`): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:</span>
</span><span id="HATMaskResNetBlockSmall.__init__-810"><a href="#HATMaskResNetBlockSmall.__init__-810"><span class="linenos">810</span></a><span class="sd">            - `sigmoid`: the sigmoid function.</span>
</span><span id="HATMaskResNetBlockSmall.__init__-811"><a href="#HATMaskResNetBlockSmall.__init__-811"><span class="linenos">811</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="HATMaskResNetBlockSmall.__init__-812"><a href="#HATMaskResNetBlockSmall.__init__-812"><span class="linenos">812</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`.</span>
</span><span id="HATMaskResNetBlockSmall.__init__-813"><a href="#HATMaskResNetBlockSmall.__init__-813"><span class="linenos">813</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HATMaskResNetBlockSmall.__init__-814"><a href="#HATMaskResNetBlockSmall.__init__-814"><span class="linenos">814</span></a>        <span class="n">HATMaskBackbone</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">gate</span><span class="o">=</span><span class="n">gate</span><span class="p">)</span>
</span><span id="HATMaskResNetBlockSmall.__init__-815"><a href="#HATMaskResNetBlockSmall.__init__-815"><span class="linenos">815</span></a>        <span class="n">ResNetBlockSmall</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="HATMaskResNetBlockSmall.__init__-816"><a href="#HATMaskResNetBlockSmall.__init__-816"><span class="linenos">816</span></a>            <span class="bp">self</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall.__init__-817"><a href="#HATMaskResNetBlockSmall.__init__-817"><span class="linenos">817</span></a>            <span class="n">outer_layer_name</span><span class="o">=</span><span class="n">outer_layer_name</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall.__init__-818"><a href="#HATMaskResNetBlockSmall.__init__-818"><span class="linenos">818</span></a>            <span class="n">block_idx</span><span class="o">=</span><span class="n">block_idx</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall.__init__-819"><a href="#HATMaskResNetBlockSmall.__init__-819"><span class="linenos">819</span></a>            <span class="n">preceding_output_channels</span><span class="o">=</span><span class="n">preceding_output_channels</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall.__init__-820"><a href="#HATMaskResNetBlockSmall.__init__-820"><span class="linenos">820</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall.__init__-821"><a href="#HATMaskResNetBlockSmall.__init__-821"><span class="linenos">821</span></a>            <span class="n">overall_stride</span><span class="o">=</span><span class="n">overall_stride</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall.__init__-822"><a href="#HATMaskResNetBlockSmall.__init__-822"><span class="linenos">822</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall.__init__-823"><a href="#HATMaskResNetBlockSmall.__init__-823"><span class="linenos">823</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall.__init__-824"><a href="#HATMaskResNetBlockSmall.__init__-824"><span class="linenos">824</span></a>        <span class="p">)</span>
</span><span id="HATMaskResNetBlockSmall.__init__-825"><a href="#HATMaskResNetBlockSmall.__init__-825"><span class="linenos">825</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">register_hat_mask_module_explicitly</span><span class="p">(</span><span class="n">gate</span><span class="o">=</span><span class="n">gate</span><span class="p">)</span>
</span><span id="HATMaskResNetBlockSmall.__init__-826"><a href="#HATMaskResNetBlockSmall.__init__-826"><span class="linenos">826</span></a>
</span><span id="HATMaskResNetBlockSmall.__init__-827"><a href="#HATMaskResNetBlockSmall.__init__-827"><span class="linenos">827</span></a>        <span class="c1"># construct the task embedding over the 1st weighted convolutional layer. It is channel-wise</span>
</span><span id="HATMaskResNetBlockSmall.__init__-828"><a href="#HATMaskResNetBlockSmall.__init__-828"><span class="linenos">828</span></a>        <span class="n">layer_output_channels</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="HATMaskResNetBlockSmall.__init__-829"><a href="#HATMaskResNetBlockSmall.__init__-829"><span class="linenos">829</span></a>            <span class="n">input_channels</span>  <span class="c1"># the output channels of the 1st convolutional layer</span>
</span><span id="HATMaskResNetBlockSmall.__init__-830"><a href="#HATMaskResNetBlockSmall.__init__-830"><span class="linenos">830</span></a>        <span class="p">)</span>
</span><span id="HATMaskResNetBlockSmall.__init__-831"><a href="#HATMaskResNetBlockSmall.__init__-831"><span class="linenos">831</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_1st_layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
</span><span id="HATMaskResNetBlockSmall.__init__-832"><a href="#HATMaskResNetBlockSmall.__init__-832"><span class="linenos">832</span></a>            <span class="n">num_embeddings</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="n">layer_output_channels</span>
</span><span id="HATMaskResNetBlockSmall.__init__-833"><a href="#HATMaskResNetBlockSmall.__init__-833"><span class="linenos">833</span></a>        <span class="p">)</span>
</span><span id="HATMaskResNetBlockSmall.__init__-834"><a href="#HATMaskResNetBlockSmall.__init__-834"><span class="linenos">834</span></a>
</span><span id="HATMaskResNetBlockSmall.__init__-835"><a href="#HATMaskResNetBlockSmall.__init__-835"><span class="linenos">835</span></a>        <span class="c1"># construct the task embedding over the 2nd weighted convolutional layer. It is channel-wise</span>
</span><span id="HATMaskResNetBlockSmall.__init__-836"><a href="#HATMaskResNetBlockSmall.__init__-836"><span class="linenos">836</span></a>        <span class="n">layer_output_channels</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="HATMaskResNetBlockSmall.__init__-837"><a href="#HATMaskResNetBlockSmall.__init__-837"><span class="linenos">837</span></a>            <span class="n">input_channels</span> <span class="o">*</span> <span class="mi">1</span>
</span><span id="HATMaskResNetBlockSmall.__init__-838"><a href="#HATMaskResNetBlockSmall.__init__-838"><span class="linenos">838</span></a>        <span class="p">)</span>  <span class="c1"># the output channels of the 2nd convolutional layer, which is the same as the input channels (without expansion)</span>
</span><span id="HATMaskResNetBlockSmall.__init__-839"><a href="#HATMaskResNetBlockSmall.__init__-839"><span class="linenos">839</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_2nd_layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
</span><span id="HATMaskResNetBlockSmall.__init__-840"><a href="#HATMaskResNetBlockSmall.__init__-840"><span class="linenos">840</span></a>            <span class="n">num_embeddings</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="n">layer_output_channels</span>
</span><span id="HATMaskResNetBlockSmall.__init__-841"><a href="#HATMaskResNetBlockSmall.__init__-841"><span class="linenos">841</span></a>        <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Construct and initialise the smaller building block with task embedding.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>outer_layer_name</strong> (<code>str</code>): pass the name of the multi-building-block layer that contains this building block to construct the full name of each weighted convolutional layer.</li>
<li><strong>block_idx</strong> (<code>int</code>): the index of the building blocks in the multi-building-block layer to construct the full name of each weighted convolutional layer.</li>
<li><strong>preceding_output_channels</strong> (<code>int</code>): the number of channels of preceding output of this particular building block.</li>
<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block.</li>
<li><strong>overall_stride</strong> (<code>int</code>): the overall stride of this building block. This stride is performed at 2nd (last) convolutional layer where the 1st convolutional layer remain stride of 1.</li>
<li><strong>gate</strong> (<code>str</code>): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:
<ul>
<li><code>sigmoid</code>: the sigmoid function.</li>
</ul></li>
<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>
<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>.</li>
</ul>
</div>


                            </div>
                            <div id="HATMaskResNetBlockSmall.forward" class="classattr">
                                        <input id="HATMaskResNetBlockSmall.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">stage</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">s_max</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">test_mask</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span></span><span class="return-annotation">) -> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span>:</span></span>

                <label class="view-source-button" for="HATMaskResNetBlockSmall.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HATMaskResNetBlockSmall.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HATMaskResNetBlockSmall.forward-843"><a href="#HATMaskResNetBlockSmall.forward-843"><span class="linenos">843</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="HATMaskResNetBlockSmall.forward-844"><a href="#HATMaskResNetBlockSmall.forward-844"><span class="linenos">844</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall.forward-845"><a href="#HATMaskResNetBlockSmall.forward-845"><span class="linenos">845</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall.forward-846"><a href="#HATMaskResNetBlockSmall.forward-846"><span class="linenos">846</span></a>        <span class="n">stage</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall.forward-847"><a href="#HATMaskResNetBlockSmall.forward-847"><span class="linenos">847</span></a>        <span class="n">s_max</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall.forward-848"><a href="#HATMaskResNetBlockSmall.forward-848"><span class="linenos">848</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall.forward-849"><a href="#HATMaskResNetBlockSmall.forward-849"><span class="linenos">849</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall.forward-850"><a href="#HATMaskResNetBlockSmall.forward-850"><span class="linenos">850</span></a>        <span class="n">test_mask</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall.forward-851"><a href="#HATMaskResNetBlockSmall.forward-851"><span class="linenos">851</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
</span><span id="HATMaskResNetBlockSmall.forward-852"><a href="#HATMaskResNetBlockSmall.forward-852"><span class="linenos">852</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The forward pass for data from task `task_id`. Task-specific mask for `task_id` are applied to the units which are channels in each weighted convolutional layer.</span>
</span><span id="HATMaskResNetBlockSmall.forward-853"><a href="#HATMaskResNetBlockSmall.forward-853"><span class="linenos">853</span></a>
</span><span id="HATMaskResNetBlockSmall.forward-854"><a href="#HATMaskResNetBlockSmall.forward-854"><span class="linenos">854</span></a><span class="sd">        **Args:**</span>
</span><span id="HATMaskResNetBlockSmall.forward-855"><a href="#HATMaskResNetBlockSmall.forward-855"><span class="linenos">855</span></a><span class="sd">        - **input** (`Tensor`): The input tensor from data.</span>
</span><span id="HATMaskResNetBlockSmall.forward-856"><a href="#HATMaskResNetBlockSmall.forward-856"><span class="linenos">856</span></a><span class="sd">        - **stage** (`str`): the stage of the forward pass, should be one of the following:</span>
</span><span id="HATMaskResNetBlockSmall.forward-857"><a href="#HATMaskResNetBlockSmall.forward-857"><span class="linenos">857</span></a><span class="sd">            1. &#39;train&#39;: training stage.</span>
</span><span id="HATMaskResNetBlockSmall.forward-858"><a href="#HATMaskResNetBlockSmall.forward-858"><span class="linenos">858</span></a><span class="sd">            2. &#39;validation&#39;: validation stage.</span>
</span><span id="HATMaskResNetBlockSmall.forward-859"><a href="#HATMaskResNetBlockSmall.forward-859"><span class="linenos">859</span></a><span class="sd">            3. &#39;test&#39;: testing stage.</span>
</span><span id="HATMaskResNetBlockSmall.forward-860"><a href="#HATMaskResNetBlockSmall.forward-860"><span class="linenos">860</span></a><span class="sd">        - **s_max** (`float` | `None`): the maximum scaling factor in the gate function. Doesn&#39;t apply to testing stage. See chapter 2.4 &quot;Hard Attention Training&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="HATMaskResNetBlockSmall.forward-861"><a href="#HATMaskResNetBlockSmall.forward-861"><span class="linenos">861</span></a><span class="sd">        - **batch_idx** (`int` | `None`): the current batch index. Applies only to training stage. For other stages, it is default `None`.</span>
</span><span id="HATMaskResNetBlockSmall.forward-862"><a href="#HATMaskResNetBlockSmall.forward-862"><span class="linenos">862</span></a><span class="sd">        - **num_batches** (`int` | `None`): the total number of batches. Applies only to training stage. For other stages, it is default `None`.</span>
</span><span id="HATMaskResNetBlockSmall.forward-863"><a href="#HATMaskResNetBlockSmall.forward-863"><span class="linenos">863</span></a><span class="sd">        - **test_mask** (`dict[str, Tensor]` | `None`): the binary mask used for test. Applies only to testing stage. For other stages, it is default `None`.</span>
</span><span id="HATMaskResNetBlockSmall.forward-864"><a href="#HATMaskResNetBlockSmall.forward-864"><span class="linenos">864</span></a>
</span><span id="HATMaskResNetBlockSmall.forward-865"><a href="#HATMaskResNetBlockSmall.forward-865"><span class="linenos">865</span></a><span class="sd">        **Returns:**</span>
</span><span id="HATMaskResNetBlockSmall.forward-866"><a href="#HATMaskResNetBlockSmall.forward-866"><span class="linenos">866</span></a><span class="sd">        - **output_feature** (`Tensor`): the output feature maps.</span>
</span><span id="HATMaskResNetBlockSmall.forward-867"><a href="#HATMaskResNetBlockSmall.forward-867"><span class="linenos">867</span></a><span class="sd">        - **mask** (`dict[str, Tensor]`): the mask for the current task. Key (`str`) is layer name, value (`Tensor`) is the mask tensor. The mask tensor has size (number of units).</span>
</span><span id="HATMaskResNetBlockSmall.forward-868"><a href="#HATMaskResNetBlockSmall.forward-868"><span class="linenos">868</span></a><span class="sd">        - **hidden_features** (`dict[str, Tensor]`): the hidden features (after activation) in each weighted layer. Key (`str`) is the weighted layer name, value (`Tensor`) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes. Although HAT algorithm does not need this, it is still provided for API consistence for other HAT-based algorithms inherited this `forward()` method of `HAT` class.</span>
</span><span id="HATMaskResNetBlockSmall.forward-869"><a href="#HATMaskResNetBlockSmall.forward-869"><span class="linenos">869</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HATMaskResNetBlockSmall.forward-870"><a href="#HATMaskResNetBlockSmall.forward-870"><span class="linenos">870</span></a>        <span class="n">hidden_features</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="HATMaskResNetBlockSmall.forward-871"><a href="#HATMaskResNetBlockSmall.forward-871"><span class="linenos">871</span></a>
</span><span id="HATMaskResNetBlockSmall.forward-872"><a href="#HATMaskResNetBlockSmall.forward-872"><span class="linenos">872</span></a>        <span class="c1"># get the mask for the current task from the task embedding in this stage</span>
</span><span id="HATMaskResNetBlockSmall.forward-873"><a href="#HATMaskResNetBlockSmall.forward-873"><span class="linenos">873</span></a>        <span class="n">mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_mask</span><span class="p">(</span>
</span><span id="HATMaskResNetBlockSmall.forward-874"><a href="#HATMaskResNetBlockSmall.forward-874"><span class="linenos">874</span></a>            <span class="n">stage</span><span class="o">=</span><span class="n">stage</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall.forward-875"><a href="#HATMaskResNetBlockSmall.forward-875"><span class="linenos">875</span></a>            <span class="n">s_max</span><span class="o">=</span><span class="n">s_max</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall.forward-876"><a href="#HATMaskResNetBlockSmall.forward-876"><span class="linenos">876</span></a>            <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall.forward-877"><a href="#HATMaskResNetBlockSmall.forward-877"><span class="linenos">877</span></a>            <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall.forward-878"><a href="#HATMaskResNetBlockSmall.forward-878"><span class="linenos">878</span></a>            <span class="n">test_mask</span><span class="o">=</span><span class="n">test_mask</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockSmall.forward-879"><a href="#HATMaskResNetBlockSmall.forward-879"><span class="linenos">879</span></a>        <span class="p">)</span>
</span><span id="HATMaskResNetBlockSmall.forward-880"><a href="#HATMaskResNetBlockSmall.forward-880"><span class="linenos">880</span></a>
</span><span id="HATMaskResNetBlockSmall.forward-881"><a href="#HATMaskResNetBlockSmall.forward-881"><span class="linenos">881</span></a>        <span class="n">identity</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="HATMaskResNetBlockSmall.forward-882"><a href="#HATMaskResNetBlockSmall.forward-882"><span class="linenos">882</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">identity_downsample</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span><span id="HATMaskResNetBlockSmall.forward-883"><a href="#HATMaskResNetBlockSmall.forward-883"><span class="linenos">883</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">identity_downsample</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span><span id="HATMaskResNetBlockSmall.forward-884"><a href="#HATMaskResNetBlockSmall.forward-884"><span class="linenos">884</span></a>            <span class="k">else</span> <span class="nb">input</span>
</span><span id="HATMaskResNetBlockSmall.forward-885"><a href="#HATMaskResNetBlockSmall.forward-885"><span class="linenos">885</span></a>        <span class="p">)</span>  <span class="c1"># remember the identity of input for the shortcut connection. Perform downsampling if its dimension doesn&#39;t match the output&#39;s</span>
</span><span id="HATMaskResNetBlockSmall.forward-886"><a href="#HATMaskResNetBlockSmall.forward-886"><span class="linenos">886</span></a>
</span><span id="HATMaskResNetBlockSmall.forward-887"><a href="#HATMaskResNetBlockSmall.forward-887"><span class="linenos">887</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="nb">input</span>
</span><span id="HATMaskResNetBlockSmall.forward-888"><a href="#HATMaskResNetBlockSmall.forward-888"><span class="linenos">888</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># weighted convolutional layer first</span>
</span><span id="HATMaskResNetBlockSmall.forward-889"><a href="#HATMaskResNetBlockSmall.forward-889"><span class="linenos">889</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span>
</span><span id="HATMaskResNetBlockSmall.forward-890"><a href="#HATMaskResNetBlockSmall.forward-890"><span class="linenos">890</span></a>            <span class="n">mask</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_1st_layer_name</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="HATMaskResNetBlockSmall.forward-891"><a href="#HATMaskResNetBlockSmall.forward-891"><span class="linenos">891</span></a>        <span class="p">)</span>  <span class="c1"># apply the mask to the 1st convolutional layer. Broadcast the dimension of mask to match the input</span>
</span><span id="HATMaskResNetBlockSmall.forward-892"><a href="#HATMaskResNetBlockSmall.forward-892"><span class="linenos">892</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="HATMaskResNetBlockSmall.forward-893"><a href="#HATMaskResNetBlockSmall.forward-893"><span class="linenos">893</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># activation function third</span>
</span><span id="HATMaskResNetBlockSmall.forward-894"><a href="#HATMaskResNetBlockSmall.forward-894"><span class="linenos">894</span></a>        <span class="n">hidden_features</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_1st_layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>  <span class="c1"># store the hidden feature</span>
</span><span id="HATMaskResNetBlockSmall.forward-895"><a href="#HATMaskResNetBlockSmall.forward-895"><span class="linenos">895</span></a>
</span><span id="HATMaskResNetBlockSmall.forward-896"><a href="#HATMaskResNetBlockSmall.forward-896"><span class="linenos">896</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># weighted convolutional layer first</span>
</span><span id="HATMaskResNetBlockSmall.forward-897"><a href="#HATMaskResNetBlockSmall.forward-897"><span class="linenos">897</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">identity</span>
</span><span id="HATMaskResNetBlockSmall.forward-898"><a href="#HATMaskResNetBlockSmall.forward-898"><span class="linenos">898</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span>
</span><span id="HATMaskResNetBlockSmall.forward-899"><a href="#HATMaskResNetBlockSmall.forward-899"><span class="linenos">899</span></a>            <span class="n">mask</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_2nd_layer_name</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="HATMaskResNetBlockSmall.forward-900"><a href="#HATMaskResNetBlockSmall.forward-900"><span class="linenos">900</span></a>        <span class="p">)</span>  <span class="c1"># apply the mask to the 2nd convolutional layer after the shortcut connection. Broadcast the dimension of mask to match the input</span>
</span><span id="HATMaskResNetBlockSmall.forward-901"><a href="#HATMaskResNetBlockSmall.forward-901"><span class="linenos">901</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="HATMaskResNetBlockSmall.forward-902"><a href="#HATMaskResNetBlockSmall.forward-902"><span class="linenos">902</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># activation after the shortcut connection</span>
</span><span id="HATMaskResNetBlockSmall.forward-903"><a href="#HATMaskResNetBlockSmall.forward-903"><span class="linenos">903</span></a>        <span class="n">hidden_features</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_2nd_layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>  <span class="c1"># store the hidden feature</span>
</span><span id="HATMaskResNetBlockSmall.forward-904"><a href="#HATMaskResNetBlockSmall.forward-904"><span class="linenos">904</span></a>
</span><span id="HATMaskResNetBlockSmall.forward-905"><a href="#HATMaskResNetBlockSmall.forward-905"><span class="linenos">905</span></a>        <span class="n">output_feature</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="HATMaskResNetBlockSmall.forward-906"><a href="#HATMaskResNetBlockSmall.forward-906"><span class="linenos">906</span></a>
</span><span id="HATMaskResNetBlockSmall.forward-907"><a href="#HATMaskResNetBlockSmall.forward-907"><span class="linenos">907</span></a>        <span class="k">return</span> <span class="n">output_feature</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">hidden_features</span>
</span></pre></div>


            <div class="docstring"><p>The forward pass for data from task <code><a href="#HATMaskResNetBlockSmall.task_id">task_id</a></code>. Task-specific mask for <code><a href="#HATMaskResNetBlockSmall.task_id">task_id</a></code> are applied to the units which are channels in each weighted convolutional layer.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>input</strong> (<code>Tensor</code>): The input tensor from data.</li>
<li><strong>stage</strong> (<code>str</code>): the stage of the forward pass, should be one of the following:
<ol>
<li>'train': training stage.</li>
<li>'validation': validation stage.</li>
<li>'test': testing stage.</li>
</ol></li>
<li><strong>s_max</strong> (<code><a href="#HATMaskResNetBlockSmall.float">float</a></code> | <code>None</code>): the maximum scaling factor in the gate function. Doesn't apply to testing stage. See chapter 2.4 "Hard Attention Training" in <a href="http://proceedings.mlr.press/v80/serra18a">HAT paper</a>.</li>
<li><strong>batch_idx</strong> (<code>int</code> | <code>None</code>): the current batch index. Applies only to training stage. For other stages, it is default <code>None</code>.</li>
<li><strong>num_batches</strong> (<code>int</code> | <code>None</code>): the total number of batches. Applies only to training stage. For other stages, it is default <code>None</code>.</li>
<li><strong>test_mask</strong> (<code>dict[str, Tensor]</code> | <code>None</code>): the binary mask used for test. Applies only to testing stage. For other stages, it is default <code>None</code>.</li>
</ul>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>output_feature</strong> (<code>Tensor</code>): the output feature maps.</li>
<li><strong>mask</strong> (<code>dict[str, Tensor]</code>): the mask for the current task. Key (<code>str</code>) is layer name, value (<code>Tensor</code>) is the mask tensor. The mask tensor has size (number of units).</li>
<li><strong>hidden_features</strong> (<code>dict[str, Tensor]</code>): the hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name, value (<code>Tensor</code>) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes. Although HAT algorithm does not need this, it is still provided for API consistence for other HAT-based algorithms inherited this <code><a href="#HATMaskResNetBlockSmall.forward">forward()</a></code> method of <code>HAT</code> class.</li>
</ul>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt><a href="#ResNetBlockSmall">ResNetBlockSmall</a></dt>
                                <dd id="HATMaskResNetBlockSmall.batch_normalisation" class="variable"><a href="#ResNetBlockSmall.batch_normalisation">batch_normalisation</a></dd>
                <dd id="HATMaskResNetBlockSmall.activation" class="variable"><a href="#ResNetBlockSmall.activation">activation</a></dd>
                <dd id="HATMaskResNetBlockSmall.full_1st_layer_name" class="variable"><a href="#ResNetBlockSmall.full_1st_layer_name">full_1st_layer_name</a></dd>
                <dd id="HATMaskResNetBlockSmall.full_2nd_layer_name" class="variable"><a href="#ResNetBlockSmall.full_2nd_layer_name">full_2nd_layer_name</a></dd>
                <dd id="HATMaskResNetBlockSmall.conv1" class="variable"><a href="#ResNetBlockSmall.conv1">conv1</a></dd>
                <dd id="HATMaskResNetBlockSmall.conv2" class="variable"><a href="#ResNetBlockSmall.conv2">conv2</a></dd>
                <dd id="HATMaskResNetBlockSmall.identity_downsample" class="variable"><a href="#ResNetBlockSmall.identity_downsample">identity_downsample</a></dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="HATMaskResNetBlockLarge">
                            <input id="HATMaskResNetBlockLarge-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">HATMaskResNetBlockLarge</span><wbr>(<span class="base">clarena.backbones.base.HATMaskBackbone</span>, <span class="base"><a href="#ResNetBlockLarge">ResNetBlockLarge</a></span>):

                <label class="view-source-button" for="HATMaskResNetBlockLarge-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HATMaskResNetBlockLarge"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HATMaskResNetBlockLarge-910"><a href="#HATMaskResNetBlockLarge-910"><span class="linenos"> 910</span></a><span class="k">class</span><span class="w"> </span><span class="nc">HATMaskResNetBlockLarge</span><span class="p">(</span><span class="n">HATMaskBackbone</span><span class="p">,</span> <span class="n">ResNetBlockLarge</span><span class="p">):</span>
</span><span id="HATMaskResNetBlockLarge-911"><a href="#HATMaskResNetBlockLarge-911"><span class="linenos"> 911</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The larger building block for ResNet-50/101/152. It is referred to &quot;bottleneck&quot; building block in the ResNet paper.</span>
</span><span id="HATMaskResNetBlockLarge-912"><a href="#HATMaskResNetBlockLarge-912"><span class="linenos"> 912</span></a>
</span><span id="HATMaskResNetBlockLarge-913"><a href="#HATMaskResNetBlockLarge-913"><span class="linenos"> 913</span></a><span class="sd">    It consists of 3 weight convolutional layers, each followed by an activation function. See Table 1 or Figure 5 (right) in the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html).</span>
</span><span id="HATMaskResNetBlockLarge-914"><a href="#HATMaskResNetBlockLarge-914"><span class="linenos"> 914</span></a>
</span><span id="HATMaskResNetBlockLarge-915"><a href="#HATMaskResNetBlockLarge-915"><span class="linenos"> 915</span></a><span class="sd">    Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the unit-wise task embedding and gate function.</span>
</span><span id="HATMaskResNetBlockLarge-916"><a href="#HATMaskResNetBlockLarge-916"><span class="linenos"> 916</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="HATMaskResNetBlockLarge-917"><a href="#HATMaskResNetBlockLarge-917"><span class="linenos"> 917</span></a>
</span><span id="HATMaskResNetBlockLarge-918"><a href="#HATMaskResNetBlockLarge-918"><span class="linenos"> 918</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="HATMaskResNetBlockLarge-919"><a href="#HATMaskResNetBlockLarge-919"><span class="linenos"> 919</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge-920"><a href="#HATMaskResNetBlockLarge-920"><span class="linenos"> 920</span></a>        <span class="n">outer_layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge-921"><a href="#HATMaskResNetBlockLarge-921"><span class="linenos"> 921</span></a>        <span class="n">block_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge-922"><a href="#HATMaskResNetBlockLarge-922"><span class="linenos"> 922</span></a>        <span class="n">preceding_output_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge-923"><a href="#HATMaskResNetBlockLarge-923"><span class="linenos"> 923</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge-924"><a href="#HATMaskResNetBlockLarge-924"><span class="linenos"> 924</span></a>        <span class="n">overall_stride</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge-925"><a href="#HATMaskResNetBlockLarge-925"><span class="linenos"> 925</span></a>        <span class="n">gate</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge-926"><a href="#HATMaskResNetBlockLarge-926"><span class="linenos"> 926</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge-927"><a href="#HATMaskResNetBlockLarge-927"><span class="linenos"> 927</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge-928"><a href="#HATMaskResNetBlockLarge-928"><span class="linenos"> 928</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HATMaskResNetBlockLarge-929"><a href="#HATMaskResNetBlockLarge-929"><span class="linenos"> 929</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the larger building block with task embedding.</span>
</span><span id="HATMaskResNetBlockLarge-930"><a href="#HATMaskResNetBlockLarge-930"><span class="linenos"> 930</span></a>
</span><span id="HATMaskResNetBlockLarge-931"><a href="#HATMaskResNetBlockLarge-931"><span class="linenos"> 931</span></a><span class="sd">        **Args:**</span>
</span><span id="HATMaskResNetBlockLarge-932"><a href="#HATMaskResNetBlockLarge-932"><span class="linenos"> 932</span></a><span class="sd">        - **outer_layer_name** (`str`): pass the name of the multi-building-block layer that contains this building block to construct the full name of each weighted convolutional layer.</span>
</span><span id="HATMaskResNetBlockLarge-933"><a href="#HATMaskResNetBlockLarge-933"><span class="linenos"> 933</span></a><span class="sd">        - **block_idx** (`int`): the index of the building blocks in the multi-building-block layer to construct the full name of each weighted convolutional layer.</span>
</span><span id="HATMaskResNetBlockLarge-934"><a href="#HATMaskResNetBlockLarge-934"><span class="linenos"> 934</span></a><span class="sd">        - **preceding_output_channels** (`int`): the number of channels of preceding output of this particular building block.</span>
</span><span id="HATMaskResNetBlockLarge-935"><a href="#HATMaskResNetBlockLarge-935"><span class="linenos"> 935</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input of this building block.</span>
</span><span id="HATMaskResNetBlockLarge-936"><a href="#HATMaskResNetBlockLarge-936"><span class="linenos"> 936</span></a><span class="sd">        - **overall_stride** (`int`): the overall stride of this building block. This stride is performed at 2nd (middle) convolutional layer where 1st and 3rd convolutional layers remain stride of 1.</span>
</span><span id="HATMaskResNetBlockLarge-937"><a href="#HATMaskResNetBlockLarge-937"><span class="linenos"> 937</span></a><span class="sd">        - **gate** (`str`): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:</span>
</span><span id="HATMaskResNetBlockLarge-938"><a href="#HATMaskResNetBlockLarge-938"><span class="linenos"> 938</span></a><span class="sd">            - `sigmoid`: the sigmoid function.</span>
</span><span id="HATMaskResNetBlockLarge-939"><a href="#HATMaskResNetBlockLarge-939"><span class="linenos"> 939</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="HATMaskResNetBlockLarge-940"><a href="#HATMaskResNetBlockLarge-940"><span class="linenos"> 940</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`.</span>
</span><span id="HATMaskResNetBlockLarge-941"><a href="#HATMaskResNetBlockLarge-941"><span class="linenos"> 941</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HATMaskResNetBlockLarge-942"><a href="#HATMaskResNetBlockLarge-942"><span class="linenos"> 942</span></a>        <span class="n">HATMaskBackbone</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">gate</span><span class="o">=</span><span class="n">gate</span><span class="p">)</span>
</span><span id="HATMaskResNetBlockLarge-943"><a href="#HATMaskResNetBlockLarge-943"><span class="linenos"> 943</span></a>        <span class="n">ResNetBlockLarge</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="HATMaskResNetBlockLarge-944"><a href="#HATMaskResNetBlockLarge-944"><span class="linenos"> 944</span></a>            <span class="bp">self</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge-945"><a href="#HATMaskResNetBlockLarge-945"><span class="linenos"> 945</span></a>            <span class="n">outer_layer_name</span><span class="o">=</span><span class="n">outer_layer_name</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge-946"><a href="#HATMaskResNetBlockLarge-946"><span class="linenos"> 946</span></a>            <span class="n">block_idx</span><span class="o">=</span><span class="n">block_idx</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge-947"><a href="#HATMaskResNetBlockLarge-947"><span class="linenos"> 947</span></a>            <span class="n">preceding_output_channels</span><span class="o">=</span><span class="n">preceding_output_channels</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge-948"><a href="#HATMaskResNetBlockLarge-948"><span class="linenos"> 948</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge-949"><a href="#HATMaskResNetBlockLarge-949"><span class="linenos"> 949</span></a>            <span class="n">overall_stride</span><span class="o">=</span><span class="n">overall_stride</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge-950"><a href="#HATMaskResNetBlockLarge-950"><span class="linenos"> 950</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge-951"><a href="#HATMaskResNetBlockLarge-951"><span class="linenos"> 951</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge-952"><a href="#HATMaskResNetBlockLarge-952"><span class="linenos"> 952</span></a>        <span class="p">)</span>
</span><span id="HATMaskResNetBlockLarge-953"><a href="#HATMaskResNetBlockLarge-953"><span class="linenos"> 953</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">register_hat_mask_module_explicitly</span><span class="p">(</span><span class="n">gate</span><span class="o">=</span><span class="n">gate</span><span class="p">)</span>
</span><span id="HATMaskResNetBlockLarge-954"><a href="#HATMaskResNetBlockLarge-954"><span class="linenos"> 954</span></a>
</span><span id="HATMaskResNetBlockLarge-955"><a href="#HATMaskResNetBlockLarge-955"><span class="linenos"> 955</span></a>        <span class="c1"># construct the task embedding over the 1st weighted convolutional layer. It is channel-wise</span>
</span><span id="HATMaskResNetBlockLarge-956"><a href="#HATMaskResNetBlockLarge-956"><span class="linenos"> 956</span></a>        <span class="n">layer_output_channels</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="HATMaskResNetBlockLarge-957"><a href="#HATMaskResNetBlockLarge-957"><span class="linenos"> 957</span></a>            <span class="n">input_channels</span>  <span class="c1"># the output channels of the 1st convolutional layer</span>
</span><span id="HATMaskResNetBlockLarge-958"><a href="#HATMaskResNetBlockLarge-958"><span class="linenos"> 958</span></a>        <span class="p">)</span>
</span><span id="HATMaskResNetBlockLarge-959"><a href="#HATMaskResNetBlockLarge-959"><span class="linenos"> 959</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_1st_layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
</span><span id="HATMaskResNetBlockLarge-960"><a href="#HATMaskResNetBlockLarge-960"><span class="linenos"> 960</span></a>            <span class="n">num_embeddings</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="n">layer_output_channels</span>
</span><span id="HATMaskResNetBlockLarge-961"><a href="#HATMaskResNetBlockLarge-961"><span class="linenos"> 961</span></a>        <span class="p">)</span>
</span><span id="HATMaskResNetBlockLarge-962"><a href="#HATMaskResNetBlockLarge-962"><span class="linenos"> 962</span></a>
</span><span id="HATMaskResNetBlockLarge-963"><a href="#HATMaskResNetBlockLarge-963"><span class="linenos"> 963</span></a>        <span class="c1"># construct the task embedding over the 2nd weighted convolutional layer. It is channel-wise</span>
</span><span id="HATMaskResNetBlockLarge-964"><a href="#HATMaskResNetBlockLarge-964"><span class="linenos"> 964</span></a>        <span class="n">layer_output_channels</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="HATMaskResNetBlockLarge-965"><a href="#HATMaskResNetBlockLarge-965"><span class="linenos"> 965</span></a>            <span class="n">input_channels</span> <span class="o">*</span> <span class="mi">1</span>
</span><span id="HATMaskResNetBlockLarge-966"><a href="#HATMaskResNetBlockLarge-966"><span class="linenos"> 966</span></a>        <span class="p">)</span>  <span class="c1"># the output channels of the 2nd convolutional layer, which is the same as the input channels (without expansion)</span>
</span><span id="HATMaskResNetBlockLarge-967"><a href="#HATMaskResNetBlockLarge-967"><span class="linenos"> 967</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_2nd_layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
</span><span id="HATMaskResNetBlockLarge-968"><a href="#HATMaskResNetBlockLarge-968"><span class="linenos"> 968</span></a>            <span class="n">num_embeddings</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="n">layer_output_channels</span>
</span><span id="HATMaskResNetBlockLarge-969"><a href="#HATMaskResNetBlockLarge-969"><span class="linenos"> 969</span></a>        <span class="p">)</span>
</span><span id="HATMaskResNetBlockLarge-970"><a href="#HATMaskResNetBlockLarge-970"><span class="linenos"> 970</span></a>
</span><span id="HATMaskResNetBlockLarge-971"><a href="#HATMaskResNetBlockLarge-971"><span class="linenos"> 971</span></a>        <span class="c1"># construct the task embedding over the 3rd weighted convolutional layer. It is channel-wise</span>
</span><span id="HATMaskResNetBlockLarge-972"><a href="#HATMaskResNetBlockLarge-972"><span class="linenos"> 972</span></a>        <span class="n">layer_output_channels</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="HATMaskResNetBlockLarge-973"><a href="#HATMaskResNetBlockLarge-973"><span class="linenos"> 973</span></a>            <span class="n">input_channels</span>
</span><span id="HATMaskResNetBlockLarge-974"><a href="#HATMaskResNetBlockLarge-974"><span class="linenos"> 974</span></a>            <span class="o">*</span> <span class="mi">4</span>  <span class="c1"># the output channels of the 2nd convolutional layer, which is 4 times expanded as the input channels</span>
</span><span id="HATMaskResNetBlockLarge-975"><a href="#HATMaskResNetBlockLarge-975"><span class="linenos"> 975</span></a>        <span class="p">)</span>
</span><span id="HATMaskResNetBlockLarge-976"><a href="#HATMaskResNetBlockLarge-976"><span class="linenos"> 976</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_3rd_layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
</span><span id="HATMaskResNetBlockLarge-977"><a href="#HATMaskResNetBlockLarge-977"><span class="linenos"> 977</span></a>            <span class="n">num_embeddings</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="n">layer_output_channels</span>
</span><span id="HATMaskResNetBlockLarge-978"><a href="#HATMaskResNetBlockLarge-978"><span class="linenos"> 978</span></a>        <span class="p">)</span>
</span><span id="HATMaskResNetBlockLarge-979"><a href="#HATMaskResNetBlockLarge-979"><span class="linenos"> 979</span></a>
</span><span id="HATMaskResNetBlockLarge-980"><a href="#HATMaskResNetBlockLarge-980"><span class="linenos"> 980</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="HATMaskResNetBlockLarge-981"><a href="#HATMaskResNetBlockLarge-981"><span class="linenos"> 981</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge-982"><a href="#HATMaskResNetBlockLarge-982"><span class="linenos"> 982</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge-983"><a href="#HATMaskResNetBlockLarge-983"><span class="linenos"> 983</span></a>        <span class="n">stage</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge-984"><a href="#HATMaskResNetBlockLarge-984"><span class="linenos"> 984</span></a>        <span class="n">s_max</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge-985"><a href="#HATMaskResNetBlockLarge-985"><span class="linenos"> 985</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge-986"><a href="#HATMaskResNetBlockLarge-986"><span class="linenos"> 986</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge-987"><a href="#HATMaskResNetBlockLarge-987"><span class="linenos"> 987</span></a>        <span class="n">test_mask</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge-988"><a href="#HATMaskResNetBlockLarge-988"><span class="linenos"> 988</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
</span><span id="HATMaskResNetBlockLarge-989"><a href="#HATMaskResNetBlockLarge-989"><span class="linenos"> 989</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The forward pass for data from task `task_id`. Task-specific mask for `task_id` are applied to the units which are channels in each weighted convolutional layer.</span>
</span><span id="HATMaskResNetBlockLarge-990"><a href="#HATMaskResNetBlockLarge-990"><span class="linenos"> 990</span></a>
</span><span id="HATMaskResNetBlockLarge-991"><a href="#HATMaskResNetBlockLarge-991"><span class="linenos"> 991</span></a><span class="sd">        **Args:**</span>
</span><span id="HATMaskResNetBlockLarge-992"><a href="#HATMaskResNetBlockLarge-992"><span class="linenos"> 992</span></a><span class="sd">        - **input** (`Tensor`): The input tensor from data.</span>
</span><span id="HATMaskResNetBlockLarge-993"><a href="#HATMaskResNetBlockLarge-993"><span class="linenos"> 993</span></a><span class="sd">        - **stage** (`str`): the stage of the forward pass, should be one of the following:</span>
</span><span id="HATMaskResNetBlockLarge-994"><a href="#HATMaskResNetBlockLarge-994"><span class="linenos"> 994</span></a><span class="sd">            1. &#39;train&#39;: training stage.</span>
</span><span id="HATMaskResNetBlockLarge-995"><a href="#HATMaskResNetBlockLarge-995"><span class="linenos"> 995</span></a><span class="sd">            2. &#39;validation&#39;: validation stage.</span>
</span><span id="HATMaskResNetBlockLarge-996"><a href="#HATMaskResNetBlockLarge-996"><span class="linenos"> 996</span></a><span class="sd">            3. &#39;test&#39;: testing stage.</span>
</span><span id="HATMaskResNetBlockLarge-997"><a href="#HATMaskResNetBlockLarge-997"><span class="linenos"> 997</span></a><span class="sd">        - **s_max** (`float` | `None`): the maximum scaling factor in the gate function. Doesn&#39;t apply to testing stage. See chapter 2.4 &quot;Hard Attention Training&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="HATMaskResNetBlockLarge-998"><a href="#HATMaskResNetBlockLarge-998"><span class="linenos"> 998</span></a><span class="sd">        - **batch_idx** (`int` | `None`): the current batch index. Applies only to training stage. For other stages, it is default `None`.</span>
</span><span id="HATMaskResNetBlockLarge-999"><a href="#HATMaskResNetBlockLarge-999"><span class="linenos"> 999</span></a><span class="sd">        - **num_batches** (`int` | `None`): the total number of batches. Applies only to training stage. For other stages, it is default `None`.</span>
</span><span id="HATMaskResNetBlockLarge-1000"><a href="#HATMaskResNetBlockLarge-1000"><span class="linenos">1000</span></a><span class="sd">        - **test_mask** (`dict[str, Tensor]` | `None`): the binary mask used for test. Applies only to testing stage. For other stages, it is default `None`.</span>
</span><span id="HATMaskResNetBlockLarge-1001"><a href="#HATMaskResNetBlockLarge-1001"><span class="linenos">1001</span></a>
</span><span id="HATMaskResNetBlockLarge-1002"><a href="#HATMaskResNetBlockLarge-1002"><span class="linenos">1002</span></a><span class="sd">        **Returns:**</span>
</span><span id="HATMaskResNetBlockLarge-1003"><a href="#HATMaskResNetBlockLarge-1003"><span class="linenos">1003</span></a><span class="sd">        - **output_feature** (`Tensor`): the output feature maps.</span>
</span><span id="HATMaskResNetBlockLarge-1004"><a href="#HATMaskResNetBlockLarge-1004"><span class="linenos">1004</span></a><span class="sd">        - **mask** (`dict[str, Tensor]`): the mask for the current task. Key (`str`) is layer name, value (`Tensor`) is the mask tensor. The mask tensor has size (number of units).</span>
</span><span id="HATMaskResNetBlockLarge-1005"><a href="#HATMaskResNetBlockLarge-1005"><span class="linenos">1005</span></a><span class="sd">        - **hidden_features** (`dict[str, Tensor]`): the hidden features (after activation) in each weighted layer. Key (`str`) is the weighted layer name, value (`Tensor`) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes. Although HAT algorithm does not need this, it is still provided for API consistence for other HAT-based algorithms inherited this `forward()` method of `HAT` class.</span>
</span><span id="HATMaskResNetBlockLarge-1006"><a href="#HATMaskResNetBlockLarge-1006"><span class="linenos">1006</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HATMaskResNetBlockLarge-1007"><a href="#HATMaskResNetBlockLarge-1007"><span class="linenos">1007</span></a>        <span class="n">hidden_features</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="HATMaskResNetBlockLarge-1008"><a href="#HATMaskResNetBlockLarge-1008"><span class="linenos">1008</span></a>
</span><span id="HATMaskResNetBlockLarge-1009"><a href="#HATMaskResNetBlockLarge-1009"><span class="linenos">1009</span></a>        <span class="c1"># get the mask for the current task from the task embedding in this stage</span>
</span><span id="HATMaskResNetBlockLarge-1010"><a href="#HATMaskResNetBlockLarge-1010"><span class="linenos">1010</span></a>        <span class="n">mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_mask</span><span class="p">(</span>
</span><span id="HATMaskResNetBlockLarge-1011"><a href="#HATMaskResNetBlockLarge-1011"><span class="linenos">1011</span></a>            <span class="n">stage</span><span class="o">=</span><span class="n">stage</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge-1012"><a href="#HATMaskResNetBlockLarge-1012"><span class="linenos">1012</span></a>            <span class="n">s_max</span><span class="o">=</span><span class="n">s_max</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge-1013"><a href="#HATMaskResNetBlockLarge-1013"><span class="linenos">1013</span></a>            <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge-1014"><a href="#HATMaskResNetBlockLarge-1014"><span class="linenos">1014</span></a>            <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge-1015"><a href="#HATMaskResNetBlockLarge-1015"><span class="linenos">1015</span></a>            <span class="n">test_mask</span><span class="o">=</span><span class="n">test_mask</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge-1016"><a href="#HATMaskResNetBlockLarge-1016"><span class="linenos">1016</span></a>        <span class="p">)</span>
</span><span id="HATMaskResNetBlockLarge-1017"><a href="#HATMaskResNetBlockLarge-1017"><span class="linenos">1017</span></a>
</span><span id="HATMaskResNetBlockLarge-1018"><a href="#HATMaskResNetBlockLarge-1018"><span class="linenos">1018</span></a>        <span class="n">identity</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="HATMaskResNetBlockLarge-1019"><a href="#HATMaskResNetBlockLarge-1019"><span class="linenos">1019</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">identity_downsample</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span><span id="HATMaskResNetBlockLarge-1020"><a href="#HATMaskResNetBlockLarge-1020"><span class="linenos">1020</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">identity_downsample</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span><span id="HATMaskResNetBlockLarge-1021"><a href="#HATMaskResNetBlockLarge-1021"><span class="linenos">1021</span></a>            <span class="k">else</span> <span class="nb">input</span>
</span><span id="HATMaskResNetBlockLarge-1022"><a href="#HATMaskResNetBlockLarge-1022"><span class="linenos">1022</span></a>        <span class="p">)</span>  <span class="c1"># remember the identity of input for the shortcut connection. Perform downsampling if its dimension doesn&#39;t match the output&#39;s</span>
</span><span id="HATMaskResNetBlockLarge-1023"><a href="#HATMaskResNetBlockLarge-1023"><span class="linenos">1023</span></a>
</span><span id="HATMaskResNetBlockLarge-1024"><a href="#HATMaskResNetBlockLarge-1024"><span class="linenos">1024</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="nb">input</span>
</span><span id="HATMaskResNetBlockLarge-1025"><a href="#HATMaskResNetBlockLarge-1025"><span class="linenos">1025</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># weighted convolutional layer first</span>
</span><span id="HATMaskResNetBlockLarge-1026"><a href="#HATMaskResNetBlockLarge-1026"><span class="linenos">1026</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span>
</span><span id="HATMaskResNetBlockLarge-1027"><a href="#HATMaskResNetBlockLarge-1027"><span class="linenos">1027</span></a>            <span class="n">mask</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_1st_layer_name</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="HATMaskResNetBlockLarge-1028"><a href="#HATMaskResNetBlockLarge-1028"><span class="linenos">1028</span></a>        <span class="p">)</span>  <span class="c1"># apply the mask to the 1st convolutional layer. Broadcast the dimension of mask to match the input</span>
</span><span id="HATMaskResNetBlockLarge-1029"><a href="#HATMaskResNetBlockLarge-1029"><span class="linenos">1029</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="HATMaskResNetBlockLarge-1030"><a href="#HATMaskResNetBlockLarge-1030"><span class="linenos">1030</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># activation function third</span>
</span><span id="HATMaskResNetBlockLarge-1031"><a href="#HATMaskResNetBlockLarge-1031"><span class="linenos">1031</span></a>        <span class="n">hidden_features</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_1st_layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>  <span class="c1"># store the hidden feature</span>
</span><span id="HATMaskResNetBlockLarge-1032"><a href="#HATMaskResNetBlockLarge-1032"><span class="linenos">1032</span></a>
</span><span id="HATMaskResNetBlockLarge-1033"><a href="#HATMaskResNetBlockLarge-1033"><span class="linenos">1033</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># weighted convolutional layer first</span>
</span><span id="HATMaskResNetBlockLarge-1034"><a href="#HATMaskResNetBlockLarge-1034"><span class="linenos">1034</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span>
</span><span id="HATMaskResNetBlockLarge-1035"><a href="#HATMaskResNetBlockLarge-1035"><span class="linenos">1035</span></a>            <span class="n">mask</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_2nd_layer_name</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="HATMaskResNetBlockLarge-1036"><a href="#HATMaskResNetBlockLarge-1036"><span class="linenos">1036</span></a>        <span class="p">)</span>  <span class="c1"># apply the mask to the 2nd convolutional layer. Broadcast the dimension of mask to match the input</span>
</span><span id="HATMaskResNetBlockLarge-1037"><a href="#HATMaskResNetBlockLarge-1037"><span class="linenos">1037</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="HATMaskResNetBlockLarge-1038"><a href="#HATMaskResNetBlockLarge-1038"><span class="linenos">1038</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># activation function third</span>
</span><span id="HATMaskResNetBlockLarge-1039"><a href="#HATMaskResNetBlockLarge-1039"><span class="linenos">1039</span></a>        <span class="n">hidden_features</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_2nd_layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>  <span class="c1"># store the hidden feature</span>
</span><span id="HATMaskResNetBlockLarge-1040"><a href="#HATMaskResNetBlockLarge-1040"><span class="linenos">1040</span></a>
</span><span id="HATMaskResNetBlockLarge-1041"><a href="#HATMaskResNetBlockLarge-1041"><span class="linenos">1041</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># weighted convolutional layer first</span>
</span><span id="HATMaskResNetBlockLarge-1042"><a href="#HATMaskResNetBlockLarge-1042"><span class="linenos">1042</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">identity</span>
</span><span id="HATMaskResNetBlockLarge-1043"><a href="#HATMaskResNetBlockLarge-1043"><span class="linenos">1043</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span>
</span><span id="HATMaskResNetBlockLarge-1044"><a href="#HATMaskResNetBlockLarge-1044"><span class="linenos">1044</span></a>            <span class="n">mask</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_3rd_layer_name</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="HATMaskResNetBlockLarge-1045"><a href="#HATMaskResNetBlockLarge-1045"><span class="linenos">1045</span></a>        <span class="p">)</span>  <span class="c1"># apply the mask to the 3rd convolutional layer after the shortcut connection. Broadcast the dimension of mask to match the input</span>
</span><span id="HATMaskResNetBlockLarge-1046"><a href="#HATMaskResNetBlockLarge-1046"><span class="linenos">1046</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="HATMaskResNetBlockLarge-1047"><a href="#HATMaskResNetBlockLarge-1047"><span class="linenos">1047</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># activation after the shortcut connection</span>
</span><span id="HATMaskResNetBlockLarge-1048"><a href="#HATMaskResNetBlockLarge-1048"><span class="linenos">1048</span></a>        <span class="n">hidden_features</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_3rd_layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>  <span class="c1"># store the hidden feature</span>
</span><span id="HATMaskResNetBlockLarge-1049"><a href="#HATMaskResNetBlockLarge-1049"><span class="linenos">1049</span></a>
</span><span id="HATMaskResNetBlockLarge-1050"><a href="#HATMaskResNetBlockLarge-1050"><span class="linenos">1050</span></a>        <span class="n">output_feature</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="HATMaskResNetBlockLarge-1051"><a href="#HATMaskResNetBlockLarge-1051"><span class="linenos">1051</span></a>
</span><span id="HATMaskResNetBlockLarge-1052"><a href="#HATMaskResNetBlockLarge-1052"><span class="linenos">1052</span></a>        <span class="k">return</span> <span class="n">output_feature</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">hidden_features</span>
</span></pre></div>


            <div class="docstring"><p>The larger building block for ResNet-50/101/152. It is referred to "bottleneck" building block in the ResNet paper.</p>

<p>It consists of 3 weight convolutional layers, each followed by an activation function. See Table 1 or Figure 5 (right) in the <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">original ResNet paper</a>.</p>

<p>Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the unit-wise task embedding and gate function.</p>
</div>


                            <div id="HATMaskResNetBlockLarge.__init__" class="classattr">
                                        <input id="HATMaskResNetBlockLarge.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">HATMaskResNetBlockLarge</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">outer_layer_name</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">block_idx</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">preceding_output_channels</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">overall_stride</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">gate</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	activation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class="param">	<span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span></span>)</span>

                <label class="view-source-button" for="HATMaskResNetBlockLarge.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HATMaskResNetBlockLarge.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HATMaskResNetBlockLarge.__init__-918"><a href="#HATMaskResNetBlockLarge.__init__-918"><span class="linenos">918</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="HATMaskResNetBlockLarge.__init__-919"><a href="#HATMaskResNetBlockLarge.__init__-919"><span class="linenos">919</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge.__init__-920"><a href="#HATMaskResNetBlockLarge.__init__-920"><span class="linenos">920</span></a>        <span class="n">outer_layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge.__init__-921"><a href="#HATMaskResNetBlockLarge.__init__-921"><span class="linenos">921</span></a>        <span class="n">block_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge.__init__-922"><a href="#HATMaskResNetBlockLarge.__init__-922"><span class="linenos">922</span></a>        <span class="n">preceding_output_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge.__init__-923"><a href="#HATMaskResNetBlockLarge.__init__-923"><span class="linenos">923</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge.__init__-924"><a href="#HATMaskResNetBlockLarge.__init__-924"><span class="linenos">924</span></a>        <span class="n">overall_stride</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge.__init__-925"><a href="#HATMaskResNetBlockLarge.__init__-925"><span class="linenos">925</span></a>        <span class="n">gate</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge.__init__-926"><a href="#HATMaskResNetBlockLarge.__init__-926"><span class="linenos">926</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge.__init__-927"><a href="#HATMaskResNetBlockLarge.__init__-927"><span class="linenos">927</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge.__init__-928"><a href="#HATMaskResNetBlockLarge.__init__-928"><span class="linenos">928</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HATMaskResNetBlockLarge.__init__-929"><a href="#HATMaskResNetBlockLarge.__init__-929"><span class="linenos">929</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the larger building block with task embedding.</span>
</span><span id="HATMaskResNetBlockLarge.__init__-930"><a href="#HATMaskResNetBlockLarge.__init__-930"><span class="linenos">930</span></a>
</span><span id="HATMaskResNetBlockLarge.__init__-931"><a href="#HATMaskResNetBlockLarge.__init__-931"><span class="linenos">931</span></a><span class="sd">        **Args:**</span>
</span><span id="HATMaskResNetBlockLarge.__init__-932"><a href="#HATMaskResNetBlockLarge.__init__-932"><span class="linenos">932</span></a><span class="sd">        - **outer_layer_name** (`str`): pass the name of the multi-building-block layer that contains this building block to construct the full name of each weighted convolutional layer.</span>
</span><span id="HATMaskResNetBlockLarge.__init__-933"><a href="#HATMaskResNetBlockLarge.__init__-933"><span class="linenos">933</span></a><span class="sd">        - **block_idx** (`int`): the index of the building blocks in the multi-building-block layer to construct the full name of each weighted convolutional layer.</span>
</span><span id="HATMaskResNetBlockLarge.__init__-934"><a href="#HATMaskResNetBlockLarge.__init__-934"><span class="linenos">934</span></a><span class="sd">        - **preceding_output_channels** (`int`): the number of channels of preceding output of this particular building block.</span>
</span><span id="HATMaskResNetBlockLarge.__init__-935"><a href="#HATMaskResNetBlockLarge.__init__-935"><span class="linenos">935</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input of this building block.</span>
</span><span id="HATMaskResNetBlockLarge.__init__-936"><a href="#HATMaskResNetBlockLarge.__init__-936"><span class="linenos">936</span></a><span class="sd">        - **overall_stride** (`int`): the overall stride of this building block. This stride is performed at 2nd (middle) convolutional layer where 1st and 3rd convolutional layers remain stride of 1.</span>
</span><span id="HATMaskResNetBlockLarge.__init__-937"><a href="#HATMaskResNetBlockLarge.__init__-937"><span class="linenos">937</span></a><span class="sd">        - **gate** (`str`): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:</span>
</span><span id="HATMaskResNetBlockLarge.__init__-938"><a href="#HATMaskResNetBlockLarge.__init__-938"><span class="linenos">938</span></a><span class="sd">            - `sigmoid`: the sigmoid function.</span>
</span><span id="HATMaskResNetBlockLarge.__init__-939"><a href="#HATMaskResNetBlockLarge.__init__-939"><span class="linenos">939</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="HATMaskResNetBlockLarge.__init__-940"><a href="#HATMaskResNetBlockLarge.__init__-940"><span class="linenos">940</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`.</span>
</span><span id="HATMaskResNetBlockLarge.__init__-941"><a href="#HATMaskResNetBlockLarge.__init__-941"><span class="linenos">941</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HATMaskResNetBlockLarge.__init__-942"><a href="#HATMaskResNetBlockLarge.__init__-942"><span class="linenos">942</span></a>        <span class="n">HATMaskBackbone</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">gate</span><span class="o">=</span><span class="n">gate</span><span class="p">)</span>
</span><span id="HATMaskResNetBlockLarge.__init__-943"><a href="#HATMaskResNetBlockLarge.__init__-943"><span class="linenos">943</span></a>        <span class="n">ResNetBlockLarge</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="HATMaskResNetBlockLarge.__init__-944"><a href="#HATMaskResNetBlockLarge.__init__-944"><span class="linenos">944</span></a>            <span class="bp">self</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge.__init__-945"><a href="#HATMaskResNetBlockLarge.__init__-945"><span class="linenos">945</span></a>            <span class="n">outer_layer_name</span><span class="o">=</span><span class="n">outer_layer_name</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge.__init__-946"><a href="#HATMaskResNetBlockLarge.__init__-946"><span class="linenos">946</span></a>            <span class="n">block_idx</span><span class="o">=</span><span class="n">block_idx</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge.__init__-947"><a href="#HATMaskResNetBlockLarge.__init__-947"><span class="linenos">947</span></a>            <span class="n">preceding_output_channels</span><span class="o">=</span><span class="n">preceding_output_channels</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge.__init__-948"><a href="#HATMaskResNetBlockLarge.__init__-948"><span class="linenos">948</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge.__init__-949"><a href="#HATMaskResNetBlockLarge.__init__-949"><span class="linenos">949</span></a>            <span class="n">overall_stride</span><span class="o">=</span><span class="n">overall_stride</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge.__init__-950"><a href="#HATMaskResNetBlockLarge.__init__-950"><span class="linenos">950</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge.__init__-951"><a href="#HATMaskResNetBlockLarge.__init__-951"><span class="linenos">951</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge.__init__-952"><a href="#HATMaskResNetBlockLarge.__init__-952"><span class="linenos">952</span></a>        <span class="p">)</span>
</span><span id="HATMaskResNetBlockLarge.__init__-953"><a href="#HATMaskResNetBlockLarge.__init__-953"><span class="linenos">953</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">register_hat_mask_module_explicitly</span><span class="p">(</span><span class="n">gate</span><span class="o">=</span><span class="n">gate</span><span class="p">)</span>
</span><span id="HATMaskResNetBlockLarge.__init__-954"><a href="#HATMaskResNetBlockLarge.__init__-954"><span class="linenos">954</span></a>
</span><span id="HATMaskResNetBlockLarge.__init__-955"><a href="#HATMaskResNetBlockLarge.__init__-955"><span class="linenos">955</span></a>        <span class="c1"># construct the task embedding over the 1st weighted convolutional layer. It is channel-wise</span>
</span><span id="HATMaskResNetBlockLarge.__init__-956"><a href="#HATMaskResNetBlockLarge.__init__-956"><span class="linenos">956</span></a>        <span class="n">layer_output_channels</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="HATMaskResNetBlockLarge.__init__-957"><a href="#HATMaskResNetBlockLarge.__init__-957"><span class="linenos">957</span></a>            <span class="n">input_channels</span>  <span class="c1"># the output channels of the 1st convolutional layer</span>
</span><span id="HATMaskResNetBlockLarge.__init__-958"><a href="#HATMaskResNetBlockLarge.__init__-958"><span class="linenos">958</span></a>        <span class="p">)</span>
</span><span id="HATMaskResNetBlockLarge.__init__-959"><a href="#HATMaskResNetBlockLarge.__init__-959"><span class="linenos">959</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_1st_layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
</span><span id="HATMaskResNetBlockLarge.__init__-960"><a href="#HATMaskResNetBlockLarge.__init__-960"><span class="linenos">960</span></a>            <span class="n">num_embeddings</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="n">layer_output_channels</span>
</span><span id="HATMaskResNetBlockLarge.__init__-961"><a href="#HATMaskResNetBlockLarge.__init__-961"><span class="linenos">961</span></a>        <span class="p">)</span>
</span><span id="HATMaskResNetBlockLarge.__init__-962"><a href="#HATMaskResNetBlockLarge.__init__-962"><span class="linenos">962</span></a>
</span><span id="HATMaskResNetBlockLarge.__init__-963"><a href="#HATMaskResNetBlockLarge.__init__-963"><span class="linenos">963</span></a>        <span class="c1"># construct the task embedding over the 2nd weighted convolutional layer. It is channel-wise</span>
</span><span id="HATMaskResNetBlockLarge.__init__-964"><a href="#HATMaskResNetBlockLarge.__init__-964"><span class="linenos">964</span></a>        <span class="n">layer_output_channels</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="HATMaskResNetBlockLarge.__init__-965"><a href="#HATMaskResNetBlockLarge.__init__-965"><span class="linenos">965</span></a>            <span class="n">input_channels</span> <span class="o">*</span> <span class="mi">1</span>
</span><span id="HATMaskResNetBlockLarge.__init__-966"><a href="#HATMaskResNetBlockLarge.__init__-966"><span class="linenos">966</span></a>        <span class="p">)</span>  <span class="c1"># the output channels of the 2nd convolutional layer, which is the same as the input channels (without expansion)</span>
</span><span id="HATMaskResNetBlockLarge.__init__-967"><a href="#HATMaskResNetBlockLarge.__init__-967"><span class="linenos">967</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_2nd_layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
</span><span id="HATMaskResNetBlockLarge.__init__-968"><a href="#HATMaskResNetBlockLarge.__init__-968"><span class="linenos">968</span></a>            <span class="n">num_embeddings</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="n">layer_output_channels</span>
</span><span id="HATMaskResNetBlockLarge.__init__-969"><a href="#HATMaskResNetBlockLarge.__init__-969"><span class="linenos">969</span></a>        <span class="p">)</span>
</span><span id="HATMaskResNetBlockLarge.__init__-970"><a href="#HATMaskResNetBlockLarge.__init__-970"><span class="linenos">970</span></a>
</span><span id="HATMaskResNetBlockLarge.__init__-971"><a href="#HATMaskResNetBlockLarge.__init__-971"><span class="linenos">971</span></a>        <span class="c1"># construct the task embedding over the 3rd weighted convolutional layer. It is channel-wise</span>
</span><span id="HATMaskResNetBlockLarge.__init__-972"><a href="#HATMaskResNetBlockLarge.__init__-972"><span class="linenos">972</span></a>        <span class="n">layer_output_channels</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="HATMaskResNetBlockLarge.__init__-973"><a href="#HATMaskResNetBlockLarge.__init__-973"><span class="linenos">973</span></a>            <span class="n">input_channels</span>
</span><span id="HATMaskResNetBlockLarge.__init__-974"><a href="#HATMaskResNetBlockLarge.__init__-974"><span class="linenos">974</span></a>            <span class="o">*</span> <span class="mi">4</span>  <span class="c1"># the output channels of the 2nd convolutional layer, which is 4 times expanded as the input channels</span>
</span><span id="HATMaskResNetBlockLarge.__init__-975"><a href="#HATMaskResNetBlockLarge.__init__-975"><span class="linenos">975</span></a>        <span class="p">)</span>
</span><span id="HATMaskResNetBlockLarge.__init__-976"><a href="#HATMaskResNetBlockLarge.__init__-976"><span class="linenos">976</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_3rd_layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
</span><span id="HATMaskResNetBlockLarge.__init__-977"><a href="#HATMaskResNetBlockLarge.__init__-977"><span class="linenos">977</span></a>            <span class="n">num_embeddings</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="n">layer_output_channels</span>
</span><span id="HATMaskResNetBlockLarge.__init__-978"><a href="#HATMaskResNetBlockLarge.__init__-978"><span class="linenos">978</span></a>        <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Construct and initialise the larger building block with task embedding.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>outer_layer_name</strong> (<code>str</code>): pass the name of the multi-building-block layer that contains this building block to construct the full name of each weighted convolutional layer.</li>
<li><strong>block_idx</strong> (<code>int</code>): the index of the building blocks in the multi-building-block layer to construct the full name of each weighted convolutional layer.</li>
<li><strong>preceding_output_channels</strong> (<code>int</code>): the number of channels of preceding output of this particular building block.</li>
<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block.</li>
<li><strong>overall_stride</strong> (<code>int</code>): the overall stride of this building block. This stride is performed at 2nd (middle) convolutional layer where 1st and 3rd convolutional layers remain stride of 1.</li>
<li><strong>gate</strong> (<code>str</code>): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:
<ul>
<li><code>sigmoid</code>: the sigmoid function.</li>
</ul></li>
<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>
<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>.</li>
</ul>
</div>


                            </div>
                            <div id="HATMaskResNetBlockLarge.forward" class="classattr">
                                        <input id="HATMaskResNetBlockLarge.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">stage</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">s_max</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">test_mask</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span></span><span class="return-annotation">) -> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span>:</span></span>

                <label class="view-source-button" for="HATMaskResNetBlockLarge.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HATMaskResNetBlockLarge.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HATMaskResNetBlockLarge.forward-980"><a href="#HATMaskResNetBlockLarge.forward-980"><span class="linenos"> 980</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="HATMaskResNetBlockLarge.forward-981"><a href="#HATMaskResNetBlockLarge.forward-981"><span class="linenos"> 981</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge.forward-982"><a href="#HATMaskResNetBlockLarge.forward-982"><span class="linenos"> 982</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge.forward-983"><a href="#HATMaskResNetBlockLarge.forward-983"><span class="linenos"> 983</span></a>        <span class="n">stage</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge.forward-984"><a href="#HATMaskResNetBlockLarge.forward-984"><span class="linenos"> 984</span></a>        <span class="n">s_max</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge.forward-985"><a href="#HATMaskResNetBlockLarge.forward-985"><span class="linenos"> 985</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge.forward-986"><a href="#HATMaskResNetBlockLarge.forward-986"><span class="linenos"> 986</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge.forward-987"><a href="#HATMaskResNetBlockLarge.forward-987"><span class="linenos"> 987</span></a>        <span class="n">test_mask</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge.forward-988"><a href="#HATMaskResNetBlockLarge.forward-988"><span class="linenos"> 988</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
</span><span id="HATMaskResNetBlockLarge.forward-989"><a href="#HATMaskResNetBlockLarge.forward-989"><span class="linenos"> 989</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The forward pass for data from task `task_id`. Task-specific mask for `task_id` are applied to the units which are channels in each weighted convolutional layer.</span>
</span><span id="HATMaskResNetBlockLarge.forward-990"><a href="#HATMaskResNetBlockLarge.forward-990"><span class="linenos"> 990</span></a>
</span><span id="HATMaskResNetBlockLarge.forward-991"><a href="#HATMaskResNetBlockLarge.forward-991"><span class="linenos"> 991</span></a><span class="sd">        **Args:**</span>
</span><span id="HATMaskResNetBlockLarge.forward-992"><a href="#HATMaskResNetBlockLarge.forward-992"><span class="linenos"> 992</span></a><span class="sd">        - **input** (`Tensor`): The input tensor from data.</span>
</span><span id="HATMaskResNetBlockLarge.forward-993"><a href="#HATMaskResNetBlockLarge.forward-993"><span class="linenos"> 993</span></a><span class="sd">        - **stage** (`str`): the stage of the forward pass, should be one of the following:</span>
</span><span id="HATMaskResNetBlockLarge.forward-994"><a href="#HATMaskResNetBlockLarge.forward-994"><span class="linenos"> 994</span></a><span class="sd">            1. &#39;train&#39;: training stage.</span>
</span><span id="HATMaskResNetBlockLarge.forward-995"><a href="#HATMaskResNetBlockLarge.forward-995"><span class="linenos"> 995</span></a><span class="sd">            2. &#39;validation&#39;: validation stage.</span>
</span><span id="HATMaskResNetBlockLarge.forward-996"><a href="#HATMaskResNetBlockLarge.forward-996"><span class="linenos"> 996</span></a><span class="sd">            3. &#39;test&#39;: testing stage.</span>
</span><span id="HATMaskResNetBlockLarge.forward-997"><a href="#HATMaskResNetBlockLarge.forward-997"><span class="linenos"> 997</span></a><span class="sd">        - **s_max** (`float` | `None`): the maximum scaling factor in the gate function. Doesn&#39;t apply to testing stage. See chapter 2.4 &quot;Hard Attention Training&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="HATMaskResNetBlockLarge.forward-998"><a href="#HATMaskResNetBlockLarge.forward-998"><span class="linenos"> 998</span></a><span class="sd">        - **batch_idx** (`int` | `None`): the current batch index. Applies only to training stage. For other stages, it is default `None`.</span>
</span><span id="HATMaskResNetBlockLarge.forward-999"><a href="#HATMaskResNetBlockLarge.forward-999"><span class="linenos"> 999</span></a><span class="sd">        - **num_batches** (`int` | `None`): the total number of batches. Applies only to training stage. For other stages, it is default `None`.</span>
</span><span id="HATMaskResNetBlockLarge.forward-1000"><a href="#HATMaskResNetBlockLarge.forward-1000"><span class="linenos">1000</span></a><span class="sd">        - **test_mask** (`dict[str, Tensor]` | `None`): the binary mask used for test. Applies only to testing stage. For other stages, it is default `None`.</span>
</span><span id="HATMaskResNetBlockLarge.forward-1001"><a href="#HATMaskResNetBlockLarge.forward-1001"><span class="linenos">1001</span></a>
</span><span id="HATMaskResNetBlockLarge.forward-1002"><a href="#HATMaskResNetBlockLarge.forward-1002"><span class="linenos">1002</span></a><span class="sd">        **Returns:**</span>
</span><span id="HATMaskResNetBlockLarge.forward-1003"><a href="#HATMaskResNetBlockLarge.forward-1003"><span class="linenos">1003</span></a><span class="sd">        - **output_feature** (`Tensor`): the output feature maps.</span>
</span><span id="HATMaskResNetBlockLarge.forward-1004"><a href="#HATMaskResNetBlockLarge.forward-1004"><span class="linenos">1004</span></a><span class="sd">        - **mask** (`dict[str, Tensor]`): the mask for the current task. Key (`str`) is layer name, value (`Tensor`) is the mask tensor. The mask tensor has size (number of units).</span>
</span><span id="HATMaskResNetBlockLarge.forward-1005"><a href="#HATMaskResNetBlockLarge.forward-1005"><span class="linenos">1005</span></a><span class="sd">        - **hidden_features** (`dict[str, Tensor]`): the hidden features (after activation) in each weighted layer. Key (`str`) is the weighted layer name, value (`Tensor`) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes. Although HAT algorithm does not need this, it is still provided for API consistence for other HAT-based algorithms inherited this `forward()` method of `HAT` class.</span>
</span><span id="HATMaskResNetBlockLarge.forward-1006"><a href="#HATMaskResNetBlockLarge.forward-1006"><span class="linenos">1006</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HATMaskResNetBlockLarge.forward-1007"><a href="#HATMaskResNetBlockLarge.forward-1007"><span class="linenos">1007</span></a>        <span class="n">hidden_features</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="HATMaskResNetBlockLarge.forward-1008"><a href="#HATMaskResNetBlockLarge.forward-1008"><span class="linenos">1008</span></a>
</span><span id="HATMaskResNetBlockLarge.forward-1009"><a href="#HATMaskResNetBlockLarge.forward-1009"><span class="linenos">1009</span></a>        <span class="c1"># get the mask for the current task from the task embedding in this stage</span>
</span><span id="HATMaskResNetBlockLarge.forward-1010"><a href="#HATMaskResNetBlockLarge.forward-1010"><span class="linenos">1010</span></a>        <span class="n">mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_mask</span><span class="p">(</span>
</span><span id="HATMaskResNetBlockLarge.forward-1011"><a href="#HATMaskResNetBlockLarge.forward-1011"><span class="linenos">1011</span></a>            <span class="n">stage</span><span class="o">=</span><span class="n">stage</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge.forward-1012"><a href="#HATMaskResNetBlockLarge.forward-1012"><span class="linenos">1012</span></a>            <span class="n">s_max</span><span class="o">=</span><span class="n">s_max</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge.forward-1013"><a href="#HATMaskResNetBlockLarge.forward-1013"><span class="linenos">1013</span></a>            <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge.forward-1014"><a href="#HATMaskResNetBlockLarge.forward-1014"><span class="linenos">1014</span></a>            <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge.forward-1015"><a href="#HATMaskResNetBlockLarge.forward-1015"><span class="linenos">1015</span></a>            <span class="n">test_mask</span><span class="o">=</span><span class="n">test_mask</span><span class="p">,</span>
</span><span id="HATMaskResNetBlockLarge.forward-1016"><a href="#HATMaskResNetBlockLarge.forward-1016"><span class="linenos">1016</span></a>        <span class="p">)</span>
</span><span id="HATMaskResNetBlockLarge.forward-1017"><a href="#HATMaskResNetBlockLarge.forward-1017"><span class="linenos">1017</span></a>
</span><span id="HATMaskResNetBlockLarge.forward-1018"><a href="#HATMaskResNetBlockLarge.forward-1018"><span class="linenos">1018</span></a>        <span class="n">identity</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="HATMaskResNetBlockLarge.forward-1019"><a href="#HATMaskResNetBlockLarge.forward-1019"><span class="linenos">1019</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">identity_downsample</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span><span id="HATMaskResNetBlockLarge.forward-1020"><a href="#HATMaskResNetBlockLarge.forward-1020"><span class="linenos">1020</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">identity_downsample</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span><span id="HATMaskResNetBlockLarge.forward-1021"><a href="#HATMaskResNetBlockLarge.forward-1021"><span class="linenos">1021</span></a>            <span class="k">else</span> <span class="nb">input</span>
</span><span id="HATMaskResNetBlockLarge.forward-1022"><a href="#HATMaskResNetBlockLarge.forward-1022"><span class="linenos">1022</span></a>        <span class="p">)</span>  <span class="c1"># remember the identity of input for the shortcut connection. Perform downsampling if its dimension doesn&#39;t match the output&#39;s</span>
</span><span id="HATMaskResNetBlockLarge.forward-1023"><a href="#HATMaskResNetBlockLarge.forward-1023"><span class="linenos">1023</span></a>
</span><span id="HATMaskResNetBlockLarge.forward-1024"><a href="#HATMaskResNetBlockLarge.forward-1024"><span class="linenos">1024</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="nb">input</span>
</span><span id="HATMaskResNetBlockLarge.forward-1025"><a href="#HATMaskResNetBlockLarge.forward-1025"><span class="linenos">1025</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># weighted convolutional layer first</span>
</span><span id="HATMaskResNetBlockLarge.forward-1026"><a href="#HATMaskResNetBlockLarge.forward-1026"><span class="linenos">1026</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span>
</span><span id="HATMaskResNetBlockLarge.forward-1027"><a href="#HATMaskResNetBlockLarge.forward-1027"><span class="linenos">1027</span></a>            <span class="n">mask</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_1st_layer_name</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="HATMaskResNetBlockLarge.forward-1028"><a href="#HATMaskResNetBlockLarge.forward-1028"><span class="linenos">1028</span></a>        <span class="p">)</span>  <span class="c1"># apply the mask to the 1st convolutional layer. Broadcast the dimension of mask to match the input</span>
</span><span id="HATMaskResNetBlockLarge.forward-1029"><a href="#HATMaskResNetBlockLarge.forward-1029"><span class="linenos">1029</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="HATMaskResNetBlockLarge.forward-1030"><a href="#HATMaskResNetBlockLarge.forward-1030"><span class="linenos">1030</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># activation function third</span>
</span><span id="HATMaskResNetBlockLarge.forward-1031"><a href="#HATMaskResNetBlockLarge.forward-1031"><span class="linenos">1031</span></a>        <span class="n">hidden_features</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_1st_layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>  <span class="c1"># store the hidden feature</span>
</span><span id="HATMaskResNetBlockLarge.forward-1032"><a href="#HATMaskResNetBlockLarge.forward-1032"><span class="linenos">1032</span></a>
</span><span id="HATMaskResNetBlockLarge.forward-1033"><a href="#HATMaskResNetBlockLarge.forward-1033"><span class="linenos">1033</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># weighted convolutional layer first</span>
</span><span id="HATMaskResNetBlockLarge.forward-1034"><a href="#HATMaskResNetBlockLarge.forward-1034"><span class="linenos">1034</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span>
</span><span id="HATMaskResNetBlockLarge.forward-1035"><a href="#HATMaskResNetBlockLarge.forward-1035"><span class="linenos">1035</span></a>            <span class="n">mask</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_2nd_layer_name</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="HATMaskResNetBlockLarge.forward-1036"><a href="#HATMaskResNetBlockLarge.forward-1036"><span class="linenos">1036</span></a>        <span class="p">)</span>  <span class="c1"># apply the mask to the 2nd convolutional layer. Broadcast the dimension of mask to match the input</span>
</span><span id="HATMaskResNetBlockLarge.forward-1037"><a href="#HATMaskResNetBlockLarge.forward-1037"><span class="linenos">1037</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="HATMaskResNetBlockLarge.forward-1038"><a href="#HATMaskResNetBlockLarge.forward-1038"><span class="linenos">1038</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># activation function third</span>
</span><span id="HATMaskResNetBlockLarge.forward-1039"><a href="#HATMaskResNetBlockLarge.forward-1039"><span class="linenos">1039</span></a>        <span class="n">hidden_features</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_2nd_layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>  <span class="c1"># store the hidden feature</span>
</span><span id="HATMaskResNetBlockLarge.forward-1040"><a href="#HATMaskResNetBlockLarge.forward-1040"><span class="linenos">1040</span></a>
</span><span id="HATMaskResNetBlockLarge.forward-1041"><a href="#HATMaskResNetBlockLarge.forward-1041"><span class="linenos">1041</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># weighted convolutional layer first</span>
</span><span id="HATMaskResNetBlockLarge.forward-1042"><a href="#HATMaskResNetBlockLarge.forward-1042"><span class="linenos">1042</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">identity</span>
</span><span id="HATMaskResNetBlockLarge.forward-1043"><a href="#HATMaskResNetBlockLarge.forward-1043"><span class="linenos">1043</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span>
</span><span id="HATMaskResNetBlockLarge.forward-1044"><a href="#HATMaskResNetBlockLarge.forward-1044"><span class="linenos">1044</span></a>            <span class="n">mask</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_3rd_layer_name</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="HATMaskResNetBlockLarge.forward-1045"><a href="#HATMaskResNetBlockLarge.forward-1045"><span class="linenos">1045</span></a>        <span class="p">)</span>  <span class="c1"># apply the mask to the 3rd convolutional layer after the shortcut connection. Broadcast the dimension of mask to match the input</span>
</span><span id="HATMaskResNetBlockLarge.forward-1046"><a href="#HATMaskResNetBlockLarge.forward-1046"><span class="linenos">1046</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="HATMaskResNetBlockLarge.forward-1047"><a href="#HATMaskResNetBlockLarge.forward-1047"><span class="linenos">1047</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># activation after the shortcut connection</span>
</span><span id="HATMaskResNetBlockLarge.forward-1048"><a href="#HATMaskResNetBlockLarge.forward-1048"><span class="linenos">1048</span></a>        <span class="n">hidden_features</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">full_3rd_layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>  <span class="c1"># store the hidden feature</span>
</span><span id="HATMaskResNetBlockLarge.forward-1049"><a href="#HATMaskResNetBlockLarge.forward-1049"><span class="linenos">1049</span></a>
</span><span id="HATMaskResNetBlockLarge.forward-1050"><a href="#HATMaskResNetBlockLarge.forward-1050"><span class="linenos">1050</span></a>        <span class="n">output_feature</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="HATMaskResNetBlockLarge.forward-1051"><a href="#HATMaskResNetBlockLarge.forward-1051"><span class="linenos">1051</span></a>
</span><span id="HATMaskResNetBlockLarge.forward-1052"><a href="#HATMaskResNetBlockLarge.forward-1052"><span class="linenos">1052</span></a>        <span class="k">return</span> <span class="n">output_feature</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">hidden_features</span>
</span></pre></div>


            <div class="docstring"><p>The forward pass for data from task <code><a href="#HATMaskResNetBlockLarge.task_id">task_id</a></code>. Task-specific mask for <code><a href="#HATMaskResNetBlockLarge.task_id">task_id</a></code> are applied to the units which are channels in each weighted convolutional layer.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>input</strong> (<code>Tensor</code>): The input tensor from data.</li>
<li><strong>stage</strong> (<code>str</code>): the stage of the forward pass, should be one of the following:
<ol>
<li>'train': training stage.</li>
<li>'validation': validation stage.</li>
<li>'test': testing stage.</li>
</ol></li>
<li><strong>s_max</strong> (<code><a href="#HATMaskResNetBlockLarge.float">float</a></code> | <code>None</code>): the maximum scaling factor in the gate function. Doesn't apply to testing stage. See chapter 2.4 "Hard Attention Training" in <a href="http://proceedings.mlr.press/v80/serra18a">HAT paper</a>.</li>
<li><strong>batch_idx</strong> (<code>int</code> | <code>None</code>): the current batch index. Applies only to training stage. For other stages, it is default <code>None</code>.</li>
<li><strong>num_batches</strong> (<code>int</code> | <code>None</code>): the total number of batches. Applies only to training stage. For other stages, it is default <code>None</code>.</li>
<li><strong>test_mask</strong> (<code>dict[str, Tensor]</code> | <code>None</code>): the binary mask used for test. Applies only to testing stage. For other stages, it is default <code>None</code>.</li>
</ul>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>output_feature</strong> (<code>Tensor</code>): the output feature maps.</li>
<li><strong>mask</strong> (<code>dict[str, Tensor]</code>): the mask for the current task. Key (<code>str</code>) is layer name, value (<code>Tensor</code>) is the mask tensor. The mask tensor has size (number of units).</li>
<li><strong>hidden_features</strong> (<code>dict[str, Tensor]</code>): the hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name, value (<code>Tensor</code>) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes. Although HAT algorithm does not need this, it is still provided for API consistence for other HAT-based algorithms inherited this <code><a href="#HATMaskResNetBlockLarge.forward">forward()</a></code> method of <code>HAT</code> class.</li>
</ul>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt><a href="#ResNetBlockLarge">ResNetBlockLarge</a></dt>
                                <dd id="HATMaskResNetBlockLarge.batch_normalisation" class="variable"><a href="#ResNetBlockLarge.batch_normalisation">batch_normalisation</a></dd>
                <dd id="HATMaskResNetBlockLarge.activation" class="variable"><a href="#ResNetBlockLarge.activation">activation</a></dd>
                <dd id="HATMaskResNetBlockLarge.full_1st_layer_name" class="variable"><a href="#ResNetBlockLarge.full_1st_layer_name">full_1st_layer_name</a></dd>
                <dd id="HATMaskResNetBlockLarge.full_2nd_layer_name" class="variable"><a href="#ResNetBlockLarge.full_2nd_layer_name">full_2nd_layer_name</a></dd>
                <dd id="HATMaskResNetBlockLarge.full_3rd_layer_name" class="variable"><a href="#ResNetBlockLarge.full_3rd_layer_name">full_3rd_layer_name</a></dd>
                <dd id="HATMaskResNetBlockLarge.conv1" class="variable"><a href="#ResNetBlockLarge.conv1">conv1</a></dd>
                <dd id="HATMaskResNetBlockLarge.conv2" class="variable"><a href="#ResNetBlockLarge.conv2">conv2</a></dd>
                <dd id="HATMaskResNetBlockLarge.conv3" class="variable"><a href="#ResNetBlockLarge.conv3">conv3</a></dd>
                <dd id="HATMaskResNetBlockLarge.identity_downsample" class="variable"><a href="#ResNetBlockLarge.identity_downsample">identity_downsample</a></dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="HATMaskResNetBase">
                            <input id="HATMaskResNetBase-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">HATMaskResNetBase</span><wbr>(<span class="base"><a href="#ResNetBase">ResNetBase</a></span>, <span class="base">clarena.backbones.base.HATMaskBackbone</span>):

                <label class="view-source-button" for="HATMaskResNetBase-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HATMaskResNetBase"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HATMaskResNetBase-1055"><a href="#HATMaskResNetBase-1055"><span class="linenos">1055</span></a><span class="k">class</span><span class="w"> </span><span class="nc">HATMaskResNetBase</span><span class="p">(</span><span class="n">ResNetBase</span><span class="p">,</span> <span class="n">HATMaskBackbone</span><span class="p">):</span>
</span><span id="HATMaskResNetBase-1056"><a href="#HATMaskResNetBase-1056"><span class="linenos">1056</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The base class of HAT masked [residual network (ResNet)](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html).</span>
</span><span id="HATMaskResNetBase-1057"><a href="#HATMaskResNetBase-1057"><span class="linenos">1057</span></a>
</span><span id="HATMaskResNetBase-1058"><a href="#HATMaskResNetBase-1058"><span class="linenos">1058</span></a><span class="sd">    [HAT (Hard Attention to the Task, 2018)](http://proceedings.mlr.press/v80/serra18a) is an architecture-based continual learning approach that uses learnable hard attention masks to select the task-specific parameters.</span>
</span><span id="HATMaskResNetBase-1059"><a href="#HATMaskResNetBase-1059"><span class="linenos">1059</span></a>
</span><span id="HATMaskResNetBase-1060"><a href="#HATMaskResNetBase-1060"><span class="linenos">1060</span></a><span class="sd">    ResNet is a convolutional network architecture, which has 1st convolutional parameter layer and a maxpooling layer, connecting to 4 convolutional layers which contains multiple convolutional parameter layer. Each layer of the 4 are constructed from basic building blocks which are either small (`ResNetBlockSmall`) or large (`ResNetBlockLarge`). Each building block contains several convolutional parameter layers. The building blocks are connected by a skip connection which is a direct connection from the input of the block to the output of the block, and this is why it&#39;s called residual (find &quot;shortcut connections&quot; in the paper for more details). After the 5th convolutional layer, there are average pooling layer and a fully connected layer which connects to the CL output heads.</span>
</span><span id="HATMaskResNetBase-1061"><a href="#HATMaskResNetBase-1061"><span class="linenos">1061</span></a>
</span><span id="HATMaskResNetBase-1062"><a href="#HATMaskResNetBase-1062"><span class="linenos">1062</span></a><span class="sd">    Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the unit-wise task embedding and gate function.</span>
</span><span id="HATMaskResNetBase-1063"><a href="#HATMaskResNetBase-1063"><span class="linenos">1063</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="HATMaskResNetBase-1064"><a href="#HATMaskResNetBase-1064"><span class="linenos">1064</span></a>
</span><span id="HATMaskResNetBase-1065"><a href="#HATMaskResNetBase-1065"><span class="linenos">1065</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="HATMaskResNetBase-1066"><a href="#HATMaskResNetBase-1066"><span class="linenos">1066</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1067"><a href="#HATMaskResNetBase-1067"><span class="linenos">1067</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1068"><a href="#HATMaskResNetBase-1068"><span class="linenos">1068</span></a>        <span class="n">building_block_type</span><span class="p">:</span> <span class="n">HATMaskResNetBlockSmall</span> <span class="o">|</span> <span class="n">HATMaskResNetBlockLarge</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1069"><a href="#HATMaskResNetBase-1069"><span class="linenos">1069</span></a>        <span class="n">building_block_nums</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
</span><span id="HATMaskResNetBase-1070"><a href="#HATMaskResNetBase-1070"><span class="linenos">1070</span></a>        <span class="n">building_block_preceding_output_channels</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
</span><span id="HATMaskResNetBase-1071"><a href="#HATMaskResNetBase-1071"><span class="linenos">1071</span></a>        <span class="n">building_block_input_channels</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
</span><span id="HATMaskResNetBase-1072"><a href="#HATMaskResNetBase-1072"><span class="linenos">1072</span></a>        <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1073"><a href="#HATMaskResNetBase-1073"><span class="linenos">1073</span></a>        <span class="n">gate</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1074"><a href="#HATMaskResNetBase-1074"><span class="linenos">1074</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1075"><a href="#HATMaskResNetBase-1075"><span class="linenos">1075</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1076"><a href="#HATMaskResNetBase-1076"><span class="linenos">1076</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HATMaskResNetBase-1077"><a href="#HATMaskResNetBase-1077"><span class="linenos">1077</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the HAT masked ResNet backbone network with task embedding. Note that batch normalisation is incompatible with HAT mechanism.</span>
</span><span id="HATMaskResNetBase-1078"><a href="#HATMaskResNetBase-1078"><span class="linenos">1078</span></a>
</span><span id="HATMaskResNetBase-1079"><a href="#HATMaskResNetBase-1079"><span class="linenos">1079</span></a><span class="sd">        **Args:**</span>
</span><span id="HATMaskResNetBase-1080"><a href="#HATMaskResNetBase-1080"><span class="linenos">1080</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input. Image data are kept channels when going in ResNet. Note that convolutional networks require number of input channels instead of dimension.</span>
</span><span id="HATMaskResNetBase-1081"><a href="#HATMaskResNetBase-1081"><span class="linenos">1081</span></a><span class="sd">        - **building_block_type** (`HATMaskResNetBlockSmall` | `HATMaskResNetBlockLarge`): the type of building block used in the ResNet.</span>
</span><span id="HATMaskResNetBase-1082"><a href="#HATMaskResNetBase-1082"><span class="linenos">1082</span></a><span class="sd">        - **building_block_nums** (`tuple[int, int, int, int]`): the number of building blocks in the 2-5 convolutional layer correspondingly.</span>
</span><span id="HATMaskResNetBase-1083"><a href="#HATMaskResNetBase-1083"><span class="linenos">1083</span></a><span class="sd">        - **building_block_preceding_output_channels** (`tuple[int, int, int, int]`): the number of channels of preceding output of each building block in the 2-5 convolutional layer correspondingly.</span>
</span><span id="HATMaskResNetBase-1084"><a href="#HATMaskResNetBase-1084"><span class="linenos">1084</span></a><span class="sd">        - **building_block_input_channels** (`tuple[int, int, int, int]`): the number of channels of input of each building block in the 2-5 convolutional layer correspondingly.</span>
</span><span id="HATMaskResNetBase-1085"><a href="#HATMaskResNetBase-1085"><span class="linenos">1085</span></a><span class="sd">        - **output_dim** (`int`): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</span>
</span><span id="HATMaskResNetBase-1086"><a href="#HATMaskResNetBase-1086"><span class="linenos">1086</span></a><span class="sd">        - **gate** (`str`): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:</span>
</span><span id="HATMaskResNetBase-1087"><a href="#HATMaskResNetBase-1087"><span class="linenos">1087</span></a><span class="sd">            - `sigmoid`: the sigmoid function.</span>
</span><span id="HATMaskResNetBase-1088"><a href="#HATMaskResNetBase-1088"><span class="linenos">1088</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="HATMaskResNetBase-1089"><a href="#HATMaskResNetBase-1089"><span class="linenos">1089</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`, because batch normalisation are doing the similar thing with bias.</span>
</span><span id="HATMaskResNetBase-1090"><a href="#HATMaskResNetBase-1090"><span class="linenos">1090</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HATMaskResNetBase-1091"><a href="#HATMaskResNetBase-1091"><span class="linenos">1091</span></a>        <span class="c1"># init from both inherited classes</span>
</span><span id="HATMaskResNetBase-1092"><a href="#HATMaskResNetBase-1092"><span class="linenos">1092</span></a>        <span class="n">HATMaskBackbone</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span> <span class="n">gate</span><span class="o">=</span><span class="n">gate</span><span class="p">)</span>
</span><span id="HATMaskResNetBase-1093"><a href="#HATMaskResNetBase-1093"><span class="linenos">1093</span></a>        <span class="n">ResNetBase</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="HATMaskResNetBase-1094"><a href="#HATMaskResNetBase-1094"><span class="linenos">1094</span></a>            <span class="bp">self</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1095"><a href="#HATMaskResNetBase-1095"><span class="linenos">1095</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1096"><a href="#HATMaskResNetBase-1096"><span class="linenos">1096</span></a>            <span class="n">building_block_type</span><span class="o">=</span><span class="n">building_block_type</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1097"><a href="#HATMaskResNetBase-1097"><span class="linenos">1097</span></a>            <span class="n">building_block_nums</span><span class="o">=</span><span class="n">building_block_nums</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1098"><a href="#HATMaskResNetBase-1098"><span class="linenos">1098</span></a>            <span class="n">building_block_preceding_output_channels</span><span class="o">=</span><span class="n">building_block_preceding_output_channels</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1099"><a href="#HATMaskResNetBase-1099"><span class="linenos">1099</span></a>            <span class="n">building_block_input_channels</span><span class="o">=</span><span class="n">building_block_input_channels</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1100"><a href="#HATMaskResNetBase-1100"><span class="linenos">1100</span></a>            <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1101"><a href="#HATMaskResNetBase-1101"><span class="linenos">1101</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1102"><a href="#HATMaskResNetBase-1102"><span class="linenos">1102</span></a>            <span class="n">batch_normalisation</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>  <span class="c1"># batch normalisation is incompatible with HAT mechanism</span>
</span><span id="HATMaskResNetBase-1103"><a href="#HATMaskResNetBase-1103"><span class="linenos">1103</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1104"><a href="#HATMaskResNetBase-1104"><span class="linenos">1104</span></a>        <span class="p">)</span>
</span><span id="HATMaskResNetBase-1105"><a href="#HATMaskResNetBase-1105"><span class="linenos">1105</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">register_hat_mask_module_explicitly</span><span class="p">(</span>
</span><span id="HATMaskResNetBase-1106"><a href="#HATMaskResNetBase-1106"><span class="linenos">1106</span></a>            <span class="n">gate</span><span class="o">=</span><span class="n">gate</span>
</span><span id="HATMaskResNetBase-1107"><a href="#HATMaskResNetBase-1107"><span class="linenos">1107</span></a>        <span class="p">)</span>  <span class="c1"># register all `nn.Module`s for HATMaskBackbone explicitly because the second `__init__()` wipes out them inited by the first `__init__()`</span>
</span><span id="HATMaskResNetBase-1108"><a href="#HATMaskResNetBase-1108"><span class="linenos">1108</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">update_multiple_blocks_task_embedding</span><span class="p">()</span>
</span><span id="HATMaskResNetBase-1109"><a href="#HATMaskResNetBase-1109"><span class="linenos">1109</span></a>
</span><span id="HATMaskResNetBase-1110"><a href="#HATMaskResNetBase-1110"><span class="linenos">1110</span></a>        <span class="c1"># construct the task embedding over the 1st weighted convolutional layers. It is channel-wise</span>
</span><span id="HATMaskResNetBase-1111"><a href="#HATMaskResNetBase-1111"><span class="linenos">1111</span></a>        <span class="n">layer_output_channels</span> <span class="o">=</span> <span class="mi">64</span>  <span class="c1"># the output channels of the 1st convolutional layer</span>
</span><span id="HATMaskResNetBase-1112"><a href="#HATMaskResNetBase-1112"><span class="linenos">1112</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="p">[</span><span class="s2">&quot;conv1&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
</span><span id="HATMaskResNetBase-1113"><a href="#HATMaskResNetBase-1113"><span class="linenos">1113</span></a>            <span class="n">num_embeddings</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="n">layer_output_channels</span>
</span><span id="HATMaskResNetBase-1114"><a href="#HATMaskResNetBase-1114"><span class="linenos">1114</span></a>        <span class="p">)</span>
</span><span id="HATMaskResNetBase-1115"><a href="#HATMaskResNetBase-1115"><span class="linenos">1115</span></a>
</span><span id="HATMaskResNetBase-1116"><a href="#HATMaskResNetBase-1116"><span class="linenos">1116</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_multiple_blocks</span><span class="p">(</span>
</span><span id="HATMaskResNetBase-1117"><a href="#HATMaskResNetBase-1117"><span class="linenos">1117</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1118"><a href="#HATMaskResNetBase-1118"><span class="linenos">1118</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1119"><a href="#HATMaskResNetBase-1119"><span class="linenos">1119</span></a>        <span class="n">building_block_type</span><span class="p">:</span> <span class="n">HATMaskResNetBlockSmall</span> <span class="o">|</span> <span class="n">HATMaskResNetBlockLarge</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1120"><a href="#HATMaskResNetBase-1120"><span class="linenos">1120</span></a>        <span class="n">building_block_num</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1121"><a href="#HATMaskResNetBase-1121"><span class="linenos">1121</span></a>        <span class="n">preceding_output_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1122"><a href="#HATMaskResNetBase-1122"><span class="linenos">1122</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1123"><a href="#HATMaskResNetBase-1123"><span class="linenos">1123</span></a>        <span class="n">overall_stride</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1124"><a href="#HATMaskResNetBase-1124"><span class="linenos">1124</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1125"><a href="#HATMaskResNetBase-1125"><span class="linenos">1125</span></a>        <span class="n">batch_normalisation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1126"><a href="#HATMaskResNetBase-1126"><span class="linenos">1126</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1127"><a href="#HATMaskResNetBase-1127"><span class="linenos">1127</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HATMaskResNetBase-1128"><a href="#HATMaskResNetBase-1128"><span class="linenos">1128</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct a layer consisting of multiple building blocks with task embedding. It&#39;s used to construct the 2-5 convolutional layers of the HAT masked ResNet.</span>
</span><span id="HATMaskResNetBase-1129"><a href="#HATMaskResNetBase-1129"><span class="linenos">1129</span></a>
</span><span id="HATMaskResNetBase-1130"><a href="#HATMaskResNetBase-1130"><span class="linenos">1130</span></a><span class="sd">        The &quot;shortcut connections&quot; are performed between the input and output of each building block:</span>
</span><span id="HATMaskResNetBase-1131"><a href="#HATMaskResNetBase-1131"><span class="linenos">1131</span></a><span class="sd">        1. If the input and output of the building block have exactly the same dimensions (including number of channels and size), add the input to the output.</span>
</span><span id="HATMaskResNetBase-1132"><a href="#HATMaskResNetBase-1132"><span class="linenos">1132</span></a><span class="sd">        2. If the input and output of the building block have different dimensions (including number of channels and size), add the input to the output after a convolutional layer to make the dimensions match.</span>
</span><span id="HATMaskResNetBase-1133"><a href="#HATMaskResNetBase-1133"><span class="linenos">1133</span></a>
</span><span id="HATMaskResNetBase-1134"><a href="#HATMaskResNetBase-1134"><span class="linenos">1134</span></a><span class="sd">        **Args:**</span>
</span><span id="HATMaskResNetBase-1135"><a href="#HATMaskResNetBase-1135"><span class="linenos">1135</span></a><span class="sd">        - **layer_name** (`str`): pass the name of this multi-building-block layer to construct the full name of each weighted convolutional layer.</span>
</span><span id="HATMaskResNetBase-1136"><a href="#HATMaskResNetBase-1136"><span class="linenos">1136</span></a><span class="sd">        - **building_block_type** (`HATMaskResNetBlockSmall` | `HATMaskResNetBlockLarge`): the type of the building block.</span>
</span><span id="HATMaskResNetBase-1137"><a href="#HATMaskResNetBase-1137"><span class="linenos">1137</span></a><span class="sd">        - **building_block_num** (`int`): the number of building blocks in this multi-building-block layer.</span>
</span><span id="HATMaskResNetBase-1138"><a href="#HATMaskResNetBase-1138"><span class="linenos">1138</span></a><span class="sd">        - **preceding_output_channels** (`int`): the number of channels of preceding output of this entire multi-building-block layer.</span>
</span><span id="HATMaskResNetBase-1139"><a href="#HATMaskResNetBase-1139"><span class="linenos">1139</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input of this multi-building-block layer.</span>
</span><span id="HATMaskResNetBase-1140"><a href="#HATMaskResNetBase-1140"><span class="linenos">1140</span></a><span class="sd">        - **overall_stride** (`int`): the overall stride of the building blocks. This stride is performed at the 1st building block where other building blocks remain their own overall stride of 1. Inside that building block, this stride is performed at certain convolutional layer in the building block where other convolutional layers remain stride of 1:</span>
</span><span id="HATMaskResNetBase-1141"><a href="#HATMaskResNetBase-1141"><span class="linenos">1141</span></a><span class="sd">            - For `ResNetBlockSmall`, it performs at the 2nd (last) layer.</span>
</span><span id="HATMaskResNetBase-1142"><a href="#HATMaskResNetBase-1142"><span class="linenos">1142</span></a><span class="sd">            - For `ResNetBlockLarge`, it performs at the 2nd (middle) layer.</span>
</span><span id="HATMaskResNetBase-1143"><a href="#HATMaskResNetBase-1143"><span class="linenos">1143</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="HATMaskResNetBase-1144"><a href="#HATMaskResNetBase-1144"><span class="linenos">1144</span></a><span class="sd">        - **batch_normalisation** (`bool`): whether to use batch normalisation after the weight convolutional layers. In HATMaskResNet, batch normalisation is incompatible with HAT mechanism and shoule be always set `False`. We include this argument for compatibility with the original ResNet API.</span>
</span><span id="HATMaskResNetBase-1145"><a href="#HATMaskResNetBase-1145"><span class="linenos">1145</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`.</span>
</span><span id="HATMaskResNetBase-1146"><a href="#HATMaskResNetBase-1146"><span class="linenos">1146</span></a>
</span><span id="HATMaskResNetBase-1147"><a href="#HATMaskResNetBase-1147"><span class="linenos">1147</span></a><span class="sd">        **Returns:**</span>
</span><span id="HATMaskResNetBase-1148"><a href="#HATMaskResNetBase-1148"><span class="linenos">1148</span></a><span class="sd">        - **layer** (`nn.Sequential`): the constructed layer consisting of multiple building blocks.</span>
</span><span id="HATMaskResNetBase-1149"><a href="#HATMaskResNetBase-1149"><span class="linenos">1149</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HATMaskResNetBase-1150"><a href="#HATMaskResNetBase-1150"><span class="linenos">1150</span></a>
</span><span id="HATMaskResNetBase-1151"><a href="#HATMaskResNetBase-1151"><span class="linenos">1151</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="HATMaskResNetBase-1152"><a href="#HATMaskResNetBase-1152"><span class="linenos">1152</span></a>
</span><span id="HATMaskResNetBase-1153"><a href="#HATMaskResNetBase-1153"><span class="linenos">1153</span></a>        <span class="k">for</span> <span class="n">block_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">building_block_num</span><span class="p">):</span>
</span><span id="HATMaskResNetBase-1154"><a href="#HATMaskResNetBase-1154"><span class="linenos">1154</span></a>            <span class="n">layer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="HATMaskResNetBase-1155"><a href="#HATMaskResNetBase-1155"><span class="linenos">1155</span></a>                <span class="n">building_block_type</span><span class="p">(</span>
</span><span id="HATMaskResNetBase-1156"><a href="#HATMaskResNetBase-1156"><span class="linenos">1156</span></a>                    <span class="n">outer_layer_name</span><span class="o">=</span><span class="n">layer_name</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1157"><a href="#HATMaskResNetBase-1157"><span class="linenos">1157</span></a>                    <span class="n">block_idx</span><span class="o">=</span><span class="n">block_idx</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1158"><a href="#HATMaskResNetBase-1158"><span class="linenos">1158</span></a>                    <span class="n">preceding_output_channels</span><span class="o">=</span><span class="p">(</span>
</span><span id="HATMaskResNetBase-1159"><a href="#HATMaskResNetBase-1159"><span class="linenos">1159</span></a>                        <span class="n">preceding_output_channels</span>
</span><span id="HATMaskResNetBase-1160"><a href="#HATMaskResNetBase-1160"><span class="linenos">1160</span></a>                        <span class="k">if</span> <span class="n">block_idx</span> <span class="o">==</span> <span class="mi">0</span>
</span><span id="HATMaskResNetBase-1161"><a href="#HATMaskResNetBase-1161"><span class="linenos">1161</span></a>                        <span class="k">else</span> <span class="p">(</span>
</span><span id="HATMaskResNetBase-1162"><a href="#HATMaskResNetBase-1162"><span class="linenos">1162</span></a>                            <span class="n">input_channels</span>
</span><span id="HATMaskResNetBase-1163"><a href="#HATMaskResNetBase-1163"><span class="linenos">1163</span></a>                            <span class="k">if</span> <span class="n">building_block_type</span> <span class="o">==</span> <span class="n">HATMaskResNetBlockSmall</span>
</span><span id="HATMaskResNetBase-1164"><a href="#HATMaskResNetBase-1164"><span class="linenos">1164</span></a>                            <span class="k">else</span> <span class="n">input_channels</span> <span class="o">*</span> <span class="mi">4</span>
</span><span id="HATMaskResNetBase-1165"><a href="#HATMaskResNetBase-1165"><span class="linenos">1165</span></a>                        <span class="p">)</span>
</span><span id="HATMaskResNetBase-1166"><a href="#HATMaskResNetBase-1166"><span class="linenos">1166</span></a>                    <span class="p">),</span>  <span class="c1"># if it&#39;s the 1st block in this multi-building-block layer, it should be the number of channels of the preceding output of this entire multi-building-block layer. Otherwise, it should be the number of channels from last building block where the number of channels is 4 times of the input channels for `ResNetBlockLarge` than `ResNetBlockSmall`.</span>
</span><span id="HATMaskResNetBase-1167"><a href="#HATMaskResNetBase-1167"><span class="linenos">1167</span></a>                    <span class="n">input_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1168"><a href="#HATMaskResNetBase-1168"><span class="linenos">1168</span></a>                    <span class="n">overall_stride</span><span class="o">=</span><span class="p">(</span>
</span><span id="HATMaskResNetBase-1169"><a href="#HATMaskResNetBase-1169"><span class="linenos">1169</span></a>                        <span class="n">overall_stride</span> <span class="k">if</span> <span class="n">block_idx</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">1</span>
</span><span id="HATMaskResNetBase-1170"><a href="#HATMaskResNetBase-1170"><span class="linenos">1170</span></a>                    <span class="p">),</span>  <span class="c1"># only perform the overall stride at the 1st block in this multi-building-block layer</span>
</span><span id="HATMaskResNetBase-1171"><a href="#HATMaskResNetBase-1171"><span class="linenos">1171</span></a>                    <span class="n">gate</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">gate</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1172"><a href="#HATMaskResNetBase-1172"><span class="linenos">1172</span></a>                    <span class="c1"># no batch normalisation in HAT masked blocks</span>
</span><span id="HATMaskResNetBase-1173"><a href="#HATMaskResNetBase-1173"><span class="linenos">1173</span></a>                    <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1174"><a href="#HATMaskResNetBase-1174"><span class="linenos">1174</span></a>                    <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1175"><a href="#HATMaskResNetBase-1175"><span class="linenos">1175</span></a>                <span class="p">)</span>
</span><span id="HATMaskResNetBase-1176"><a href="#HATMaskResNetBase-1176"><span class="linenos">1176</span></a>            <span class="p">)</span>
</span><span id="HATMaskResNetBase-1177"><a href="#HATMaskResNetBase-1177"><span class="linenos">1177</span></a>
</span><span id="HATMaskResNetBase-1178"><a href="#HATMaskResNetBase-1178"><span class="linenos">1178</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span> <span class="o">+=</span> <span class="n">layer</span><span class="p">[</span>
</span><span id="HATMaskResNetBase-1179"><a href="#HATMaskResNetBase-1179"><span class="linenos">1179</span></a>                <span class="o">-</span><span class="mi">1</span>
</span><span id="HATMaskResNetBase-1180"><a href="#HATMaskResNetBase-1180"><span class="linenos">1180</span></a>            <span class="p">]</span><span class="o">.</span><span class="n">weighted_layer_names</span>  <span class="c1"># collect the weighted layer names in the blocks and sync to the weighted layer names list in the outer network</span>
</span><span id="HATMaskResNetBase-1181"><a href="#HATMaskResNetBase-1181"><span class="linenos">1181</span></a>
</span><span id="HATMaskResNetBase-1182"><a href="#HATMaskResNetBase-1182"><span class="linenos">1182</span></a>        <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layer</span><span class="p">)</span>
</span><span id="HATMaskResNetBase-1183"><a href="#HATMaskResNetBase-1183"><span class="linenos">1183</span></a>
</span><span id="HATMaskResNetBase-1184"><a href="#HATMaskResNetBase-1184"><span class="linenos">1184</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">update_multiple_blocks_task_embedding</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HATMaskResNetBase-1185"><a href="#HATMaskResNetBase-1185"><span class="linenos">1185</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Collect the task embeddings in the multiple building blocks (2-5 convolutional layers) and sync to the weighted layer names list in the outer network.</span>
</span><span id="HATMaskResNetBase-1186"><a href="#HATMaskResNetBase-1186"><span class="linenos">1186</span></a>
</span><span id="HATMaskResNetBase-1187"><a href="#HATMaskResNetBase-1187"><span class="linenos">1187</span></a><span class="sd">        This should only be called explicitly after the `__init__()` method, just because task embedding as `nn.Module` instance was wiped out at the beginning of it.</span>
</span><span id="HATMaskResNetBase-1188"><a href="#HATMaskResNetBase-1188"><span class="linenos">1188</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HATMaskResNetBase-1189"><a href="#HATMaskResNetBase-1189"><span class="linenos">1189</span></a>        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2x</span><span class="p">:</span>
</span><span id="HATMaskResNetBase-1190"><a href="#HATMaskResNetBase-1190"><span class="linenos">1190</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">block</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="p">)</span>
</span><span id="HATMaskResNetBase-1191"><a href="#HATMaskResNetBase-1191"><span class="linenos">1191</span></a>        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv3x</span><span class="p">:</span>
</span><span id="HATMaskResNetBase-1192"><a href="#HATMaskResNetBase-1192"><span class="linenos">1192</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">block</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="p">)</span>
</span><span id="HATMaskResNetBase-1193"><a href="#HATMaskResNetBase-1193"><span class="linenos">1193</span></a>        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv4x</span><span class="p">:</span>
</span><span id="HATMaskResNetBase-1194"><a href="#HATMaskResNetBase-1194"><span class="linenos">1194</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">block</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="p">)</span>
</span><span id="HATMaskResNetBase-1195"><a href="#HATMaskResNetBase-1195"><span class="linenos">1195</span></a>        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv5x</span><span class="p">:</span>
</span><span id="HATMaskResNetBase-1196"><a href="#HATMaskResNetBase-1196"><span class="linenos">1196</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">block</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="p">)</span>
</span><span id="HATMaskResNetBase-1197"><a href="#HATMaskResNetBase-1197"><span class="linenos">1197</span></a>
</span><span id="HATMaskResNetBase-1198"><a href="#HATMaskResNetBase-1198"><span class="linenos">1198</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="HATMaskResNetBase-1199"><a href="#HATMaskResNetBase-1199"><span class="linenos">1199</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1200"><a href="#HATMaskResNetBase-1200"><span class="linenos">1200</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1201"><a href="#HATMaskResNetBase-1201"><span class="linenos">1201</span></a>        <span class="n">stage</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1202"><a href="#HATMaskResNetBase-1202"><span class="linenos">1202</span></a>        <span class="n">s_max</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1203"><a href="#HATMaskResNetBase-1203"><span class="linenos">1203</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1204"><a href="#HATMaskResNetBase-1204"><span class="linenos">1204</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1205"><a href="#HATMaskResNetBase-1205"><span class="linenos">1205</span></a>        <span class="n">test_mask</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1206"><a href="#HATMaskResNetBase-1206"><span class="linenos">1206</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
</span><span id="HATMaskResNetBase-1207"><a href="#HATMaskResNetBase-1207"><span class="linenos">1207</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The forward pass for data from task `task_id`. Task-specific mask for `task_id` are applied to the units which are channels in each weighted convolutional layer.</span>
</span><span id="HATMaskResNetBase-1208"><a href="#HATMaskResNetBase-1208"><span class="linenos">1208</span></a>
</span><span id="HATMaskResNetBase-1209"><a href="#HATMaskResNetBase-1209"><span class="linenos">1209</span></a><span class="sd">        **Args:**</span>
</span><span id="HATMaskResNetBase-1210"><a href="#HATMaskResNetBase-1210"><span class="linenos">1210</span></a><span class="sd">        - **input** (`Tensor`): the input tensor from data.</span>
</span><span id="HATMaskResNetBase-1211"><a href="#HATMaskResNetBase-1211"><span class="linenos">1211</span></a><span class="sd">        - **stage** (`str`): the stage of the forward pass, should be one of the following:</span>
</span><span id="HATMaskResNetBase-1212"><a href="#HATMaskResNetBase-1212"><span class="linenos">1212</span></a><span class="sd">            1. &#39;train&#39;: training stage.</span>
</span><span id="HATMaskResNetBase-1213"><a href="#HATMaskResNetBase-1213"><span class="linenos">1213</span></a><span class="sd">            2. &#39;validation&#39;: validation stage.</span>
</span><span id="HATMaskResNetBase-1214"><a href="#HATMaskResNetBase-1214"><span class="linenos">1214</span></a><span class="sd">            3. &#39;test&#39;: testing stage.</span>
</span><span id="HATMaskResNetBase-1215"><a href="#HATMaskResNetBase-1215"><span class="linenos">1215</span></a><span class="sd">        - **s_max** (`float` | `None`): the maximum scaling factor in the gate function. Doesn&#39;t apply to testing stage. See chapter 2.4 &quot;Hard Attention Training&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="HATMaskResNetBase-1216"><a href="#HATMaskResNetBase-1216"><span class="linenos">1216</span></a><span class="sd">        - **batch_idx** (`int` | `None`): the current batch index. Applies only to training stage. For other stages, it is default `None`.</span>
</span><span id="HATMaskResNetBase-1217"><a href="#HATMaskResNetBase-1217"><span class="linenos">1217</span></a><span class="sd">        - **num_batches** (`int` | `None`): the total number of batches. Applies only to training stage. For other stages, it is default `None`.</span>
</span><span id="HATMaskResNetBase-1218"><a href="#HATMaskResNetBase-1218"><span class="linenos">1218</span></a><span class="sd">        - **test_mask** (`dict[str, Tensor]` | `None`): the binary mask used for test. Applies only to testing stage. For other stages, it is default `None`.</span>
</span><span id="HATMaskResNetBase-1219"><a href="#HATMaskResNetBase-1219"><span class="linenos">1219</span></a>
</span><span id="HATMaskResNetBase-1220"><a href="#HATMaskResNetBase-1220"><span class="linenos">1220</span></a><span class="sd">        **Returns:**</span>
</span><span id="HATMaskResNetBase-1221"><a href="#HATMaskResNetBase-1221"><span class="linenos">1221</span></a><span class="sd">        - **output_feature** (`Tensor`): the output feature tensor to be passed to the heads.</span>
</span><span id="HATMaskResNetBase-1222"><a href="#HATMaskResNetBase-1222"><span class="linenos">1222</span></a><span class="sd">        - **mask** (`dict[str, Tensor]`): the mask for the current task. Key (`str`) is layer name, value (`Tensor`) is the mask tensor. The mask tensor has size (number of units).</span>
</span><span id="HATMaskResNetBase-1223"><a href="#HATMaskResNetBase-1223"><span class="linenos">1223</span></a><span class="sd">        - **hidden_features** (`dict[str, Tensor]`): the hidden features (after activation) in each weighted layer. Key (`str`) is the weighted layer name, value (`Tensor`) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes. Although HAT algorithm does not need this, it is still provided for API consistence for other HAT-based algorithms inherited this `forward()` method of `HAT` class.</span>
</span><span id="HATMaskResNetBase-1224"><a href="#HATMaskResNetBase-1224"><span class="linenos">1224</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HATMaskResNetBase-1225"><a href="#HATMaskResNetBase-1225"><span class="linenos">1225</span></a>        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="HATMaskResNetBase-1226"><a href="#HATMaskResNetBase-1226"><span class="linenos">1226</span></a>        <span class="n">hidden_features</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="HATMaskResNetBase-1227"><a href="#HATMaskResNetBase-1227"><span class="linenos">1227</span></a>
</span><span id="HATMaskResNetBase-1228"><a href="#HATMaskResNetBase-1228"><span class="linenos">1228</span></a>        <span class="c1"># get the mask for the current task from the task embedding in this stage</span>
</span><span id="HATMaskResNetBase-1229"><a href="#HATMaskResNetBase-1229"><span class="linenos">1229</span></a>        <span class="n">mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_mask</span><span class="p">(</span>
</span><span id="HATMaskResNetBase-1230"><a href="#HATMaskResNetBase-1230"><span class="linenos">1230</span></a>            <span class="n">stage</span><span class="o">=</span><span class="n">stage</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1231"><a href="#HATMaskResNetBase-1231"><span class="linenos">1231</span></a>            <span class="n">s_max</span><span class="o">=</span><span class="n">s_max</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1232"><a href="#HATMaskResNetBase-1232"><span class="linenos">1232</span></a>            <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1233"><a href="#HATMaskResNetBase-1233"><span class="linenos">1233</span></a>            <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1234"><a href="#HATMaskResNetBase-1234"><span class="linenos">1234</span></a>            <span class="n">test_mask</span><span class="o">=</span><span class="n">test_mask</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1235"><a href="#HATMaskResNetBase-1235"><span class="linenos">1235</span></a>        <span class="p">)</span>
</span><span id="HATMaskResNetBase-1236"><a href="#HATMaskResNetBase-1236"><span class="linenos">1236</span></a>
</span><span id="HATMaskResNetBase-1237"><a href="#HATMaskResNetBase-1237"><span class="linenos">1237</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="nb">input</span>
</span><span id="HATMaskResNetBase-1238"><a href="#HATMaskResNetBase-1238"><span class="linenos">1238</span></a>
</span><span id="HATMaskResNetBase-1239"><a href="#HATMaskResNetBase-1239"><span class="linenos">1239</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="HATMaskResNetBase-1240"><a href="#HATMaskResNetBase-1240"><span class="linenos">1240</span></a>
</span><span id="HATMaskResNetBase-1241"><a href="#HATMaskResNetBase-1241"><span class="linenos">1241</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span>
</span><span id="HATMaskResNetBase-1242"><a href="#HATMaskResNetBase-1242"><span class="linenos">1242</span></a>            <span class="n">mask</span><span class="p">[</span><span class="s2">&quot;conv1&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="HATMaskResNetBase-1243"><a href="#HATMaskResNetBase-1243"><span class="linenos">1243</span></a>        <span class="p">)</span>  <span class="c1"># apply the mask to the 1st convolutional layer. Broadcast the dimension of mask to match the input</span>
</span><span id="HATMaskResNetBase-1244"><a href="#HATMaskResNetBase-1244"><span class="linenos">1244</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="HATMaskResNetBase-1245"><a href="#HATMaskResNetBase-1245"><span class="linenos">1245</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="HATMaskResNetBase-1246"><a href="#HATMaskResNetBase-1246"><span class="linenos">1246</span></a>        <span class="n">hidden_features</span><span class="p">[</span><span class="s2">&quot;conv1&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="HATMaskResNetBase-1247"><a href="#HATMaskResNetBase-1247"><span class="linenos">1247</span></a>
</span><span id="HATMaskResNetBase-1248"><a href="#HATMaskResNetBase-1248"><span class="linenos">1248</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="HATMaskResNetBase-1249"><a href="#HATMaskResNetBase-1249"><span class="linenos">1249</span></a>
</span><span id="HATMaskResNetBase-1250"><a href="#HATMaskResNetBase-1250"><span class="linenos">1250</span></a>        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2x</span><span class="p">:</span>
</span><span id="HATMaskResNetBase-1251"><a href="#HATMaskResNetBase-1251"><span class="linenos">1251</span></a>            <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">hidden_features_block</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span>
</span><span id="HATMaskResNetBase-1252"><a href="#HATMaskResNetBase-1252"><span class="linenos">1252</span></a>                <span class="n">x</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1253"><a href="#HATMaskResNetBase-1253"><span class="linenos">1253</span></a>                <span class="n">stage</span><span class="o">=</span><span class="n">stage</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1254"><a href="#HATMaskResNetBase-1254"><span class="linenos">1254</span></a>                <span class="n">s_max</span><span class="o">=</span><span class="n">s_max</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1255"><a href="#HATMaskResNetBase-1255"><span class="linenos">1255</span></a>                <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1256"><a href="#HATMaskResNetBase-1256"><span class="linenos">1256</span></a>                <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1257"><a href="#HATMaskResNetBase-1257"><span class="linenos">1257</span></a>                <span class="n">test_mask</span><span class="o">=</span><span class="n">test_mask</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1258"><a href="#HATMaskResNetBase-1258"><span class="linenos">1258</span></a>            <span class="p">)</span>
</span><span id="HATMaskResNetBase-1259"><a href="#HATMaskResNetBase-1259"><span class="linenos">1259</span></a>            <span class="n">hidden_features</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">hidden_features_block</span><span class="p">)</span>  <span class="c1"># store the hidden feature</span>
</span><span id="HATMaskResNetBase-1260"><a href="#HATMaskResNetBase-1260"><span class="linenos">1260</span></a>        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv3x</span><span class="p">:</span>
</span><span id="HATMaskResNetBase-1261"><a href="#HATMaskResNetBase-1261"><span class="linenos">1261</span></a>            <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">hidden_features_block</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span>
</span><span id="HATMaskResNetBase-1262"><a href="#HATMaskResNetBase-1262"><span class="linenos">1262</span></a>                <span class="n">x</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1263"><a href="#HATMaskResNetBase-1263"><span class="linenos">1263</span></a>                <span class="n">stage</span><span class="o">=</span><span class="n">stage</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1264"><a href="#HATMaskResNetBase-1264"><span class="linenos">1264</span></a>                <span class="n">s_max</span><span class="o">=</span><span class="n">s_max</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1265"><a href="#HATMaskResNetBase-1265"><span class="linenos">1265</span></a>                <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1266"><a href="#HATMaskResNetBase-1266"><span class="linenos">1266</span></a>                <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1267"><a href="#HATMaskResNetBase-1267"><span class="linenos">1267</span></a>                <span class="n">test_mask</span><span class="o">=</span><span class="n">test_mask</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1268"><a href="#HATMaskResNetBase-1268"><span class="linenos">1268</span></a>            <span class="p">)</span>
</span><span id="HATMaskResNetBase-1269"><a href="#HATMaskResNetBase-1269"><span class="linenos">1269</span></a>            <span class="n">hidden_features</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">hidden_features_block</span><span class="p">)</span>  <span class="c1"># store the hidden feature</span>
</span><span id="HATMaskResNetBase-1270"><a href="#HATMaskResNetBase-1270"><span class="linenos">1270</span></a>        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv4x</span><span class="p">:</span>
</span><span id="HATMaskResNetBase-1271"><a href="#HATMaskResNetBase-1271"><span class="linenos">1271</span></a>            <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">hidden_features_block</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span>
</span><span id="HATMaskResNetBase-1272"><a href="#HATMaskResNetBase-1272"><span class="linenos">1272</span></a>                <span class="n">x</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1273"><a href="#HATMaskResNetBase-1273"><span class="linenos">1273</span></a>                <span class="n">stage</span><span class="o">=</span><span class="n">stage</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1274"><a href="#HATMaskResNetBase-1274"><span class="linenos">1274</span></a>                <span class="n">s_max</span><span class="o">=</span><span class="n">s_max</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1275"><a href="#HATMaskResNetBase-1275"><span class="linenos">1275</span></a>                <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1276"><a href="#HATMaskResNetBase-1276"><span class="linenos">1276</span></a>                <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1277"><a href="#HATMaskResNetBase-1277"><span class="linenos">1277</span></a>                <span class="n">test_mask</span><span class="o">=</span><span class="n">test_mask</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1278"><a href="#HATMaskResNetBase-1278"><span class="linenos">1278</span></a>            <span class="p">)</span>
</span><span id="HATMaskResNetBase-1279"><a href="#HATMaskResNetBase-1279"><span class="linenos">1279</span></a>            <span class="n">hidden_features</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">hidden_features_block</span><span class="p">)</span>  <span class="c1"># store the hidden feature</span>
</span><span id="HATMaskResNetBase-1280"><a href="#HATMaskResNetBase-1280"><span class="linenos">1280</span></a>        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv5x</span><span class="p">:</span>
</span><span id="HATMaskResNetBase-1281"><a href="#HATMaskResNetBase-1281"><span class="linenos">1281</span></a>            <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">hidden_features_block</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span>
</span><span id="HATMaskResNetBase-1282"><a href="#HATMaskResNetBase-1282"><span class="linenos">1282</span></a>                <span class="n">x</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1283"><a href="#HATMaskResNetBase-1283"><span class="linenos">1283</span></a>                <span class="n">stage</span><span class="o">=</span><span class="n">stage</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1284"><a href="#HATMaskResNetBase-1284"><span class="linenos">1284</span></a>                <span class="n">s_max</span><span class="o">=</span><span class="n">s_max</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1285"><a href="#HATMaskResNetBase-1285"><span class="linenos">1285</span></a>                <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1286"><a href="#HATMaskResNetBase-1286"><span class="linenos">1286</span></a>                <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1287"><a href="#HATMaskResNetBase-1287"><span class="linenos">1287</span></a>                <span class="n">test_mask</span><span class="o">=</span><span class="n">test_mask</span><span class="p">,</span>
</span><span id="HATMaskResNetBase-1288"><a href="#HATMaskResNetBase-1288"><span class="linenos">1288</span></a>            <span class="p">)</span>
</span><span id="HATMaskResNetBase-1289"><a href="#HATMaskResNetBase-1289"><span class="linenos">1289</span></a>            <span class="n">hidden_features</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">hidden_features_block</span><span class="p">)</span>  <span class="c1"># store the hidden feature</span>
</span><span id="HATMaskResNetBase-1290"><a href="#HATMaskResNetBase-1290"><span class="linenos">1290</span></a>
</span><span id="HATMaskResNetBase-1291"><a href="#HATMaskResNetBase-1291"><span class="linenos">1291</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">avepool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="HATMaskResNetBase-1292"><a href="#HATMaskResNetBase-1292"><span class="linenos">1292</span></a>
</span><span id="HATMaskResNetBase-1293"><a href="#HATMaskResNetBase-1293"><span class="linenos">1293</span></a>        <span class="n">output_feature</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># flatten before going through heads</span>
</span><span id="HATMaskResNetBase-1294"><a href="#HATMaskResNetBase-1294"><span class="linenos">1294</span></a>
</span><span id="HATMaskResNetBase-1295"><a href="#HATMaskResNetBase-1295"><span class="linenos">1295</span></a>        <span class="k">return</span> <span class="n">output_feature</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">hidden_features</span>
</span></pre></div>


            <div class="docstring"><p>The base class of HAT masked <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">residual network (ResNet)</a>.</p>

<p><a href="http://proceedings.mlr.press/v80/serra18a">HAT (Hard Attention to the Task, 2018)</a> is an architecture-based continual learning approach that uses learnable hard attention masks to select the task-specific parameters.</p>

<p>ResNet is a convolutional network architecture, which has 1st convolutional parameter layer and a maxpooling layer, connecting to 4 convolutional layers which contains multiple convolutional parameter layer. Each layer of the 4 are constructed from basic building blocks which are either small (<code><a href="#ResNetBlockSmall">ResNetBlockSmall</a></code>) or large (<code><a href="#ResNetBlockLarge">ResNetBlockLarge</a></code>). Each building block contains several convolutional parameter layers. The building blocks are connected by a skip connection which is a direct connection from the input of the block to the output of the block, and this is why it's called residual (find "shortcut connections" in the paper for more details). After the 5th convolutional layer, there are average pooling layer and a fully connected layer which connects to the CL output heads.</p>

<p>Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the unit-wise task embedding and gate function.</p>
</div>


                            <div id="HATMaskResNetBase.__init__" class="classattr">
                                        <input id="HATMaskResNetBase.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">HATMaskResNetBase</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">building_block_type</span><span class="p">:</span> <span class="n"><a href="#HATMaskResNetBlockSmall">HATMaskResNetBlockSmall</a></span> <span class="o">|</span> <span class="n"><a href="#HATMaskResNetBlockLarge">HATMaskResNetBlockLarge</a></span>,</span><span class="param">	<span class="n">building_block_nums</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span>,</span><span class="param">	<span class="n">building_block_preceding_output_channels</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span>,</span><span class="param">	<span class="n">building_block_input_channels</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span>,</span><span class="param">	<span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">gate</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	activation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class="param">	<span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span></span>)</span>

                <label class="view-source-button" for="HATMaskResNetBase.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HATMaskResNetBase.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HATMaskResNetBase.__init__-1065"><a href="#HATMaskResNetBase.__init__-1065"><span class="linenos">1065</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="HATMaskResNetBase.__init__-1066"><a href="#HATMaskResNetBase.__init__-1066"><span class="linenos">1066</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.__init__-1067"><a href="#HATMaskResNetBase.__init__-1067"><span class="linenos">1067</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.__init__-1068"><a href="#HATMaskResNetBase.__init__-1068"><span class="linenos">1068</span></a>        <span class="n">building_block_type</span><span class="p">:</span> <span class="n">HATMaskResNetBlockSmall</span> <span class="o">|</span> <span class="n">HATMaskResNetBlockLarge</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.__init__-1069"><a href="#HATMaskResNetBase.__init__-1069"><span class="linenos">1069</span></a>        <span class="n">building_block_nums</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
</span><span id="HATMaskResNetBase.__init__-1070"><a href="#HATMaskResNetBase.__init__-1070"><span class="linenos">1070</span></a>        <span class="n">building_block_preceding_output_channels</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
</span><span id="HATMaskResNetBase.__init__-1071"><a href="#HATMaskResNetBase.__init__-1071"><span class="linenos">1071</span></a>        <span class="n">building_block_input_channels</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
</span><span id="HATMaskResNetBase.__init__-1072"><a href="#HATMaskResNetBase.__init__-1072"><span class="linenos">1072</span></a>        <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.__init__-1073"><a href="#HATMaskResNetBase.__init__-1073"><span class="linenos">1073</span></a>        <span class="n">gate</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.__init__-1074"><a href="#HATMaskResNetBase.__init__-1074"><span class="linenos">1074</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.__init__-1075"><a href="#HATMaskResNetBase.__init__-1075"><span class="linenos">1075</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.__init__-1076"><a href="#HATMaskResNetBase.__init__-1076"><span class="linenos">1076</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HATMaskResNetBase.__init__-1077"><a href="#HATMaskResNetBase.__init__-1077"><span class="linenos">1077</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the HAT masked ResNet backbone network with task embedding. Note that batch normalisation is incompatible with HAT mechanism.</span>
</span><span id="HATMaskResNetBase.__init__-1078"><a href="#HATMaskResNetBase.__init__-1078"><span class="linenos">1078</span></a>
</span><span id="HATMaskResNetBase.__init__-1079"><a href="#HATMaskResNetBase.__init__-1079"><span class="linenos">1079</span></a><span class="sd">        **Args:**</span>
</span><span id="HATMaskResNetBase.__init__-1080"><a href="#HATMaskResNetBase.__init__-1080"><span class="linenos">1080</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input. Image data are kept channels when going in ResNet. Note that convolutional networks require number of input channels instead of dimension.</span>
</span><span id="HATMaskResNetBase.__init__-1081"><a href="#HATMaskResNetBase.__init__-1081"><span class="linenos">1081</span></a><span class="sd">        - **building_block_type** (`HATMaskResNetBlockSmall` | `HATMaskResNetBlockLarge`): the type of building block used in the ResNet.</span>
</span><span id="HATMaskResNetBase.__init__-1082"><a href="#HATMaskResNetBase.__init__-1082"><span class="linenos">1082</span></a><span class="sd">        - **building_block_nums** (`tuple[int, int, int, int]`): the number of building blocks in the 2-5 convolutional layer correspondingly.</span>
</span><span id="HATMaskResNetBase.__init__-1083"><a href="#HATMaskResNetBase.__init__-1083"><span class="linenos">1083</span></a><span class="sd">        - **building_block_preceding_output_channels** (`tuple[int, int, int, int]`): the number of channels of preceding output of each building block in the 2-5 convolutional layer correspondingly.</span>
</span><span id="HATMaskResNetBase.__init__-1084"><a href="#HATMaskResNetBase.__init__-1084"><span class="linenos">1084</span></a><span class="sd">        - **building_block_input_channels** (`tuple[int, int, int, int]`): the number of channels of input of each building block in the 2-5 convolutional layer correspondingly.</span>
</span><span id="HATMaskResNetBase.__init__-1085"><a href="#HATMaskResNetBase.__init__-1085"><span class="linenos">1085</span></a><span class="sd">        - **output_dim** (`int`): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</span>
</span><span id="HATMaskResNetBase.__init__-1086"><a href="#HATMaskResNetBase.__init__-1086"><span class="linenos">1086</span></a><span class="sd">        - **gate** (`str`): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:</span>
</span><span id="HATMaskResNetBase.__init__-1087"><a href="#HATMaskResNetBase.__init__-1087"><span class="linenos">1087</span></a><span class="sd">            - `sigmoid`: the sigmoid function.</span>
</span><span id="HATMaskResNetBase.__init__-1088"><a href="#HATMaskResNetBase.__init__-1088"><span class="linenos">1088</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="HATMaskResNetBase.__init__-1089"><a href="#HATMaskResNetBase.__init__-1089"><span class="linenos">1089</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`, because batch normalisation are doing the similar thing with bias.</span>
</span><span id="HATMaskResNetBase.__init__-1090"><a href="#HATMaskResNetBase.__init__-1090"><span class="linenos">1090</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HATMaskResNetBase.__init__-1091"><a href="#HATMaskResNetBase.__init__-1091"><span class="linenos">1091</span></a>        <span class="c1"># init from both inherited classes</span>
</span><span id="HATMaskResNetBase.__init__-1092"><a href="#HATMaskResNetBase.__init__-1092"><span class="linenos">1092</span></a>        <span class="n">HATMaskBackbone</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span> <span class="n">gate</span><span class="o">=</span><span class="n">gate</span><span class="p">)</span>
</span><span id="HATMaskResNetBase.__init__-1093"><a href="#HATMaskResNetBase.__init__-1093"><span class="linenos">1093</span></a>        <span class="n">ResNetBase</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="HATMaskResNetBase.__init__-1094"><a href="#HATMaskResNetBase.__init__-1094"><span class="linenos">1094</span></a>            <span class="bp">self</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.__init__-1095"><a href="#HATMaskResNetBase.__init__-1095"><span class="linenos">1095</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.__init__-1096"><a href="#HATMaskResNetBase.__init__-1096"><span class="linenos">1096</span></a>            <span class="n">building_block_type</span><span class="o">=</span><span class="n">building_block_type</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.__init__-1097"><a href="#HATMaskResNetBase.__init__-1097"><span class="linenos">1097</span></a>            <span class="n">building_block_nums</span><span class="o">=</span><span class="n">building_block_nums</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.__init__-1098"><a href="#HATMaskResNetBase.__init__-1098"><span class="linenos">1098</span></a>            <span class="n">building_block_preceding_output_channels</span><span class="o">=</span><span class="n">building_block_preceding_output_channels</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.__init__-1099"><a href="#HATMaskResNetBase.__init__-1099"><span class="linenos">1099</span></a>            <span class="n">building_block_input_channels</span><span class="o">=</span><span class="n">building_block_input_channels</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.__init__-1100"><a href="#HATMaskResNetBase.__init__-1100"><span class="linenos">1100</span></a>            <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.__init__-1101"><a href="#HATMaskResNetBase.__init__-1101"><span class="linenos">1101</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.__init__-1102"><a href="#HATMaskResNetBase.__init__-1102"><span class="linenos">1102</span></a>            <span class="n">batch_normalisation</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>  <span class="c1"># batch normalisation is incompatible with HAT mechanism</span>
</span><span id="HATMaskResNetBase.__init__-1103"><a href="#HATMaskResNetBase.__init__-1103"><span class="linenos">1103</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.__init__-1104"><a href="#HATMaskResNetBase.__init__-1104"><span class="linenos">1104</span></a>        <span class="p">)</span>
</span><span id="HATMaskResNetBase.__init__-1105"><a href="#HATMaskResNetBase.__init__-1105"><span class="linenos">1105</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">register_hat_mask_module_explicitly</span><span class="p">(</span>
</span><span id="HATMaskResNetBase.__init__-1106"><a href="#HATMaskResNetBase.__init__-1106"><span class="linenos">1106</span></a>            <span class="n">gate</span><span class="o">=</span><span class="n">gate</span>
</span><span id="HATMaskResNetBase.__init__-1107"><a href="#HATMaskResNetBase.__init__-1107"><span class="linenos">1107</span></a>        <span class="p">)</span>  <span class="c1"># register all `nn.Module`s for HATMaskBackbone explicitly because the second `__init__()` wipes out them inited by the first `__init__()`</span>
</span><span id="HATMaskResNetBase.__init__-1108"><a href="#HATMaskResNetBase.__init__-1108"><span class="linenos">1108</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">update_multiple_blocks_task_embedding</span><span class="p">()</span>
</span><span id="HATMaskResNetBase.__init__-1109"><a href="#HATMaskResNetBase.__init__-1109"><span class="linenos">1109</span></a>
</span><span id="HATMaskResNetBase.__init__-1110"><a href="#HATMaskResNetBase.__init__-1110"><span class="linenos">1110</span></a>        <span class="c1"># construct the task embedding over the 1st weighted convolutional layers. It is channel-wise</span>
</span><span id="HATMaskResNetBase.__init__-1111"><a href="#HATMaskResNetBase.__init__-1111"><span class="linenos">1111</span></a>        <span class="n">layer_output_channels</span> <span class="o">=</span> <span class="mi">64</span>  <span class="c1"># the output channels of the 1st convolutional layer</span>
</span><span id="HATMaskResNetBase.__init__-1112"><a href="#HATMaskResNetBase.__init__-1112"><span class="linenos">1112</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="p">[</span><span class="s2">&quot;conv1&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
</span><span id="HATMaskResNetBase.__init__-1113"><a href="#HATMaskResNetBase.__init__-1113"><span class="linenos">1113</span></a>            <span class="n">num_embeddings</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="n">layer_output_channels</span>
</span><span id="HATMaskResNetBase.__init__-1114"><a href="#HATMaskResNetBase.__init__-1114"><span class="linenos">1114</span></a>        <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Construct and initialise the HAT masked ResNet backbone network with task embedding. Note that batch normalisation is incompatible with HAT mechanism.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input. Image data are kept channels when going in ResNet. Note that convolutional networks require number of input channels instead of dimension.</li>
<li><strong>building_block_type</strong> (<code><a href="#HATMaskResNetBlockSmall">HATMaskResNetBlockSmall</a></code> | <code><a href="#HATMaskResNetBlockLarge">HATMaskResNetBlockLarge</a></code>): the type of building block used in the ResNet.</li>
<li><strong>building_block_nums</strong> (<code>tuple[int, int, int, int]</code>): the number of building blocks in the 2-5 convolutional layer correspondingly.</li>
<li><strong>building_block_preceding_output_channels</strong> (<code>tuple[int, int, int, int]</code>): the number of channels of preceding output of each building block in the 2-5 convolutional layer correspondingly.</li>
<li><strong>building_block_input_channels</strong> (<code>tuple[int, int, int, int]</code>): the number of channels of input of each building block in the 2-5 convolutional layer correspondingly.</li>
<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>
<li><strong>gate</strong> (<code>str</code>): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:
<ul>
<li><code>sigmoid</code>: the sigmoid function.</li>
</ul></li>
<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>
<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>, because batch normalisation are doing the similar thing with bias.</li>
</ul>
</div>


                            </div>
                            <div id="HATMaskResNetBase.update_multiple_blocks_task_embedding" class="classattr">
                                        <input id="HATMaskResNetBase.update_multiple_blocks_task_embedding-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">update_multiple_blocks_task_embedding</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span></span><span class="return-annotation">) -> <span class="kc">None</span>:</span></span>

                <label class="view-source-button" for="HATMaskResNetBase.update_multiple_blocks_task_embedding-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HATMaskResNetBase.update_multiple_blocks_task_embedding"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HATMaskResNetBase.update_multiple_blocks_task_embedding-1184"><a href="#HATMaskResNetBase.update_multiple_blocks_task_embedding-1184"><span class="linenos">1184</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">update_multiple_blocks_task_embedding</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HATMaskResNetBase.update_multiple_blocks_task_embedding-1185"><a href="#HATMaskResNetBase.update_multiple_blocks_task_embedding-1185"><span class="linenos">1185</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Collect the task embeddings in the multiple building blocks (2-5 convolutional layers) and sync to the weighted layer names list in the outer network.</span>
</span><span id="HATMaskResNetBase.update_multiple_blocks_task_embedding-1186"><a href="#HATMaskResNetBase.update_multiple_blocks_task_embedding-1186"><span class="linenos">1186</span></a>
</span><span id="HATMaskResNetBase.update_multiple_blocks_task_embedding-1187"><a href="#HATMaskResNetBase.update_multiple_blocks_task_embedding-1187"><span class="linenos">1187</span></a><span class="sd">        This should only be called explicitly after the `__init__()` method, just because task embedding as `nn.Module` instance was wiped out at the beginning of it.</span>
</span><span id="HATMaskResNetBase.update_multiple_blocks_task_embedding-1188"><a href="#HATMaskResNetBase.update_multiple_blocks_task_embedding-1188"><span class="linenos">1188</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HATMaskResNetBase.update_multiple_blocks_task_embedding-1189"><a href="#HATMaskResNetBase.update_multiple_blocks_task_embedding-1189"><span class="linenos">1189</span></a>        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2x</span><span class="p">:</span>
</span><span id="HATMaskResNetBase.update_multiple_blocks_task_embedding-1190"><a href="#HATMaskResNetBase.update_multiple_blocks_task_embedding-1190"><span class="linenos">1190</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">block</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="p">)</span>
</span><span id="HATMaskResNetBase.update_multiple_blocks_task_embedding-1191"><a href="#HATMaskResNetBase.update_multiple_blocks_task_embedding-1191"><span class="linenos">1191</span></a>        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv3x</span><span class="p">:</span>
</span><span id="HATMaskResNetBase.update_multiple_blocks_task_embedding-1192"><a href="#HATMaskResNetBase.update_multiple_blocks_task_embedding-1192"><span class="linenos">1192</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">block</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="p">)</span>
</span><span id="HATMaskResNetBase.update_multiple_blocks_task_embedding-1193"><a href="#HATMaskResNetBase.update_multiple_blocks_task_embedding-1193"><span class="linenos">1193</span></a>        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv4x</span><span class="p">:</span>
</span><span id="HATMaskResNetBase.update_multiple_blocks_task_embedding-1194"><a href="#HATMaskResNetBase.update_multiple_blocks_task_embedding-1194"><span class="linenos">1194</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">block</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="p">)</span>
</span><span id="HATMaskResNetBase.update_multiple_blocks_task_embedding-1195"><a href="#HATMaskResNetBase.update_multiple_blocks_task_embedding-1195"><span class="linenos">1195</span></a>        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv5x</span><span class="p">:</span>
</span><span id="HATMaskResNetBase.update_multiple_blocks_task_embedding-1196"><a href="#HATMaskResNetBase.update_multiple_blocks_task_embedding-1196"><span class="linenos">1196</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">block</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Collect the task embeddings in the multiple building blocks (2-5 convolutional layers) and sync to the weighted layer names list in the outer network.</p>

<p>This should only be called explicitly after the <code><a href="#HATMaskResNetBase.__init__">__init__()</a></code> method, just because task embedding as <code>nn.Module</code> instance was wiped out at the beginning of it.</p>
</div>


                            </div>
                            <div id="HATMaskResNetBase.forward" class="classattr">
                                        <input id="HATMaskResNetBase.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">stage</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">s_max</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">test_mask</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span></span><span class="return-annotation">) -> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span>:</span></span>

                <label class="view-source-button" for="HATMaskResNetBase.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HATMaskResNetBase.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HATMaskResNetBase.forward-1198"><a href="#HATMaskResNetBase.forward-1198"><span class="linenos">1198</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="HATMaskResNetBase.forward-1199"><a href="#HATMaskResNetBase.forward-1199"><span class="linenos">1199</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.forward-1200"><a href="#HATMaskResNetBase.forward-1200"><span class="linenos">1200</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.forward-1201"><a href="#HATMaskResNetBase.forward-1201"><span class="linenos">1201</span></a>        <span class="n">stage</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.forward-1202"><a href="#HATMaskResNetBase.forward-1202"><span class="linenos">1202</span></a>        <span class="n">s_max</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.forward-1203"><a href="#HATMaskResNetBase.forward-1203"><span class="linenos">1203</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.forward-1204"><a href="#HATMaskResNetBase.forward-1204"><span class="linenos">1204</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.forward-1205"><a href="#HATMaskResNetBase.forward-1205"><span class="linenos">1205</span></a>        <span class="n">test_mask</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.forward-1206"><a href="#HATMaskResNetBase.forward-1206"><span class="linenos">1206</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
</span><span id="HATMaskResNetBase.forward-1207"><a href="#HATMaskResNetBase.forward-1207"><span class="linenos">1207</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The forward pass for data from task `task_id`. Task-specific mask for `task_id` are applied to the units which are channels in each weighted convolutional layer.</span>
</span><span id="HATMaskResNetBase.forward-1208"><a href="#HATMaskResNetBase.forward-1208"><span class="linenos">1208</span></a>
</span><span id="HATMaskResNetBase.forward-1209"><a href="#HATMaskResNetBase.forward-1209"><span class="linenos">1209</span></a><span class="sd">        **Args:**</span>
</span><span id="HATMaskResNetBase.forward-1210"><a href="#HATMaskResNetBase.forward-1210"><span class="linenos">1210</span></a><span class="sd">        - **input** (`Tensor`): the input tensor from data.</span>
</span><span id="HATMaskResNetBase.forward-1211"><a href="#HATMaskResNetBase.forward-1211"><span class="linenos">1211</span></a><span class="sd">        - **stage** (`str`): the stage of the forward pass, should be one of the following:</span>
</span><span id="HATMaskResNetBase.forward-1212"><a href="#HATMaskResNetBase.forward-1212"><span class="linenos">1212</span></a><span class="sd">            1. &#39;train&#39;: training stage.</span>
</span><span id="HATMaskResNetBase.forward-1213"><a href="#HATMaskResNetBase.forward-1213"><span class="linenos">1213</span></a><span class="sd">            2. &#39;validation&#39;: validation stage.</span>
</span><span id="HATMaskResNetBase.forward-1214"><a href="#HATMaskResNetBase.forward-1214"><span class="linenos">1214</span></a><span class="sd">            3. &#39;test&#39;: testing stage.</span>
</span><span id="HATMaskResNetBase.forward-1215"><a href="#HATMaskResNetBase.forward-1215"><span class="linenos">1215</span></a><span class="sd">        - **s_max** (`float` | `None`): the maximum scaling factor in the gate function. Doesn&#39;t apply to testing stage. See chapter 2.4 &quot;Hard Attention Training&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="HATMaskResNetBase.forward-1216"><a href="#HATMaskResNetBase.forward-1216"><span class="linenos">1216</span></a><span class="sd">        - **batch_idx** (`int` | `None`): the current batch index. Applies only to training stage. For other stages, it is default `None`.</span>
</span><span id="HATMaskResNetBase.forward-1217"><a href="#HATMaskResNetBase.forward-1217"><span class="linenos">1217</span></a><span class="sd">        - **num_batches** (`int` | `None`): the total number of batches. Applies only to training stage. For other stages, it is default `None`.</span>
</span><span id="HATMaskResNetBase.forward-1218"><a href="#HATMaskResNetBase.forward-1218"><span class="linenos">1218</span></a><span class="sd">        - **test_mask** (`dict[str, Tensor]` | `None`): the binary mask used for test. Applies only to testing stage. For other stages, it is default `None`.</span>
</span><span id="HATMaskResNetBase.forward-1219"><a href="#HATMaskResNetBase.forward-1219"><span class="linenos">1219</span></a>
</span><span id="HATMaskResNetBase.forward-1220"><a href="#HATMaskResNetBase.forward-1220"><span class="linenos">1220</span></a><span class="sd">        **Returns:**</span>
</span><span id="HATMaskResNetBase.forward-1221"><a href="#HATMaskResNetBase.forward-1221"><span class="linenos">1221</span></a><span class="sd">        - **output_feature** (`Tensor`): the output feature tensor to be passed to the heads.</span>
</span><span id="HATMaskResNetBase.forward-1222"><a href="#HATMaskResNetBase.forward-1222"><span class="linenos">1222</span></a><span class="sd">        - **mask** (`dict[str, Tensor]`): the mask for the current task. Key (`str`) is layer name, value (`Tensor`) is the mask tensor. The mask tensor has size (number of units).</span>
</span><span id="HATMaskResNetBase.forward-1223"><a href="#HATMaskResNetBase.forward-1223"><span class="linenos">1223</span></a><span class="sd">        - **hidden_features** (`dict[str, Tensor]`): the hidden features (after activation) in each weighted layer. Key (`str`) is the weighted layer name, value (`Tensor`) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes. Although HAT algorithm does not need this, it is still provided for API consistence for other HAT-based algorithms inherited this `forward()` method of `HAT` class.</span>
</span><span id="HATMaskResNetBase.forward-1224"><a href="#HATMaskResNetBase.forward-1224"><span class="linenos">1224</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HATMaskResNetBase.forward-1225"><a href="#HATMaskResNetBase.forward-1225"><span class="linenos">1225</span></a>        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="HATMaskResNetBase.forward-1226"><a href="#HATMaskResNetBase.forward-1226"><span class="linenos">1226</span></a>        <span class="n">hidden_features</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="HATMaskResNetBase.forward-1227"><a href="#HATMaskResNetBase.forward-1227"><span class="linenos">1227</span></a>
</span><span id="HATMaskResNetBase.forward-1228"><a href="#HATMaskResNetBase.forward-1228"><span class="linenos">1228</span></a>        <span class="c1"># get the mask for the current task from the task embedding in this stage</span>
</span><span id="HATMaskResNetBase.forward-1229"><a href="#HATMaskResNetBase.forward-1229"><span class="linenos">1229</span></a>        <span class="n">mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_mask</span><span class="p">(</span>
</span><span id="HATMaskResNetBase.forward-1230"><a href="#HATMaskResNetBase.forward-1230"><span class="linenos">1230</span></a>            <span class="n">stage</span><span class="o">=</span><span class="n">stage</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.forward-1231"><a href="#HATMaskResNetBase.forward-1231"><span class="linenos">1231</span></a>            <span class="n">s_max</span><span class="o">=</span><span class="n">s_max</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.forward-1232"><a href="#HATMaskResNetBase.forward-1232"><span class="linenos">1232</span></a>            <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.forward-1233"><a href="#HATMaskResNetBase.forward-1233"><span class="linenos">1233</span></a>            <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.forward-1234"><a href="#HATMaskResNetBase.forward-1234"><span class="linenos">1234</span></a>            <span class="n">test_mask</span><span class="o">=</span><span class="n">test_mask</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.forward-1235"><a href="#HATMaskResNetBase.forward-1235"><span class="linenos">1235</span></a>        <span class="p">)</span>
</span><span id="HATMaskResNetBase.forward-1236"><a href="#HATMaskResNetBase.forward-1236"><span class="linenos">1236</span></a>
</span><span id="HATMaskResNetBase.forward-1237"><a href="#HATMaskResNetBase.forward-1237"><span class="linenos">1237</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="nb">input</span>
</span><span id="HATMaskResNetBase.forward-1238"><a href="#HATMaskResNetBase.forward-1238"><span class="linenos">1238</span></a>
</span><span id="HATMaskResNetBase.forward-1239"><a href="#HATMaskResNetBase.forward-1239"><span class="linenos">1239</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="HATMaskResNetBase.forward-1240"><a href="#HATMaskResNetBase.forward-1240"><span class="linenos">1240</span></a>
</span><span id="HATMaskResNetBase.forward-1241"><a href="#HATMaskResNetBase.forward-1241"><span class="linenos">1241</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span>
</span><span id="HATMaskResNetBase.forward-1242"><a href="#HATMaskResNetBase.forward-1242"><span class="linenos">1242</span></a>            <span class="n">mask</span><span class="p">[</span><span class="s2">&quot;conv1&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="HATMaskResNetBase.forward-1243"><a href="#HATMaskResNetBase.forward-1243"><span class="linenos">1243</span></a>        <span class="p">)</span>  <span class="c1"># apply the mask to the 1st convolutional layer. Broadcast the dimension of mask to match the input</span>
</span><span id="HATMaskResNetBase.forward-1244"><a href="#HATMaskResNetBase.forward-1244"><span class="linenos">1244</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span>
</span><span id="HATMaskResNetBase.forward-1245"><a href="#HATMaskResNetBase.forward-1245"><span class="linenos">1245</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_activation1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="HATMaskResNetBase.forward-1246"><a href="#HATMaskResNetBase.forward-1246"><span class="linenos">1246</span></a>        <span class="n">hidden_features</span><span class="p">[</span><span class="s2">&quot;conv1&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="HATMaskResNetBase.forward-1247"><a href="#HATMaskResNetBase.forward-1247"><span class="linenos">1247</span></a>
</span><span id="HATMaskResNetBase.forward-1248"><a href="#HATMaskResNetBase.forward-1248"><span class="linenos">1248</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="HATMaskResNetBase.forward-1249"><a href="#HATMaskResNetBase.forward-1249"><span class="linenos">1249</span></a>
</span><span id="HATMaskResNetBase.forward-1250"><a href="#HATMaskResNetBase.forward-1250"><span class="linenos">1250</span></a>        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2x</span><span class="p">:</span>
</span><span id="HATMaskResNetBase.forward-1251"><a href="#HATMaskResNetBase.forward-1251"><span class="linenos">1251</span></a>            <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">hidden_features_block</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span>
</span><span id="HATMaskResNetBase.forward-1252"><a href="#HATMaskResNetBase.forward-1252"><span class="linenos">1252</span></a>                <span class="n">x</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.forward-1253"><a href="#HATMaskResNetBase.forward-1253"><span class="linenos">1253</span></a>                <span class="n">stage</span><span class="o">=</span><span class="n">stage</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.forward-1254"><a href="#HATMaskResNetBase.forward-1254"><span class="linenos">1254</span></a>                <span class="n">s_max</span><span class="o">=</span><span class="n">s_max</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.forward-1255"><a href="#HATMaskResNetBase.forward-1255"><span class="linenos">1255</span></a>                <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.forward-1256"><a href="#HATMaskResNetBase.forward-1256"><span class="linenos">1256</span></a>                <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.forward-1257"><a href="#HATMaskResNetBase.forward-1257"><span class="linenos">1257</span></a>                <span class="n">test_mask</span><span class="o">=</span><span class="n">test_mask</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.forward-1258"><a href="#HATMaskResNetBase.forward-1258"><span class="linenos">1258</span></a>            <span class="p">)</span>
</span><span id="HATMaskResNetBase.forward-1259"><a href="#HATMaskResNetBase.forward-1259"><span class="linenos">1259</span></a>            <span class="n">hidden_features</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">hidden_features_block</span><span class="p">)</span>  <span class="c1"># store the hidden feature</span>
</span><span id="HATMaskResNetBase.forward-1260"><a href="#HATMaskResNetBase.forward-1260"><span class="linenos">1260</span></a>        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv3x</span><span class="p">:</span>
</span><span id="HATMaskResNetBase.forward-1261"><a href="#HATMaskResNetBase.forward-1261"><span class="linenos">1261</span></a>            <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">hidden_features_block</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span>
</span><span id="HATMaskResNetBase.forward-1262"><a href="#HATMaskResNetBase.forward-1262"><span class="linenos">1262</span></a>                <span class="n">x</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.forward-1263"><a href="#HATMaskResNetBase.forward-1263"><span class="linenos">1263</span></a>                <span class="n">stage</span><span class="o">=</span><span class="n">stage</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.forward-1264"><a href="#HATMaskResNetBase.forward-1264"><span class="linenos">1264</span></a>                <span class="n">s_max</span><span class="o">=</span><span class="n">s_max</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.forward-1265"><a href="#HATMaskResNetBase.forward-1265"><span class="linenos">1265</span></a>                <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.forward-1266"><a href="#HATMaskResNetBase.forward-1266"><span class="linenos">1266</span></a>                <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.forward-1267"><a href="#HATMaskResNetBase.forward-1267"><span class="linenos">1267</span></a>                <span class="n">test_mask</span><span class="o">=</span><span class="n">test_mask</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.forward-1268"><a href="#HATMaskResNetBase.forward-1268"><span class="linenos">1268</span></a>            <span class="p">)</span>
</span><span id="HATMaskResNetBase.forward-1269"><a href="#HATMaskResNetBase.forward-1269"><span class="linenos">1269</span></a>            <span class="n">hidden_features</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">hidden_features_block</span><span class="p">)</span>  <span class="c1"># store the hidden feature</span>
</span><span id="HATMaskResNetBase.forward-1270"><a href="#HATMaskResNetBase.forward-1270"><span class="linenos">1270</span></a>        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv4x</span><span class="p">:</span>
</span><span id="HATMaskResNetBase.forward-1271"><a href="#HATMaskResNetBase.forward-1271"><span class="linenos">1271</span></a>            <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">hidden_features_block</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span>
</span><span id="HATMaskResNetBase.forward-1272"><a href="#HATMaskResNetBase.forward-1272"><span class="linenos">1272</span></a>                <span class="n">x</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.forward-1273"><a href="#HATMaskResNetBase.forward-1273"><span class="linenos">1273</span></a>                <span class="n">stage</span><span class="o">=</span><span class="n">stage</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.forward-1274"><a href="#HATMaskResNetBase.forward-1274"><span class="linenos">1274</span></a>                <span class="n">s_max</span><span class="o">=</span><span class="n">s_max</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.forward-1275"><a href="#HATMaskResNetBase.forward-1275"><span class="linenos">1275</span></a>                <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.forward-1276"><a href="#HATMaskResNetBase.forward-1276"><span class="linenos">1276</span></a>                <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.forward-1277"><a href="#HATMaskResNetBase.forward-1277"><span class="linenos">1277</span></a>                <span class="n">test_mask</span><span class="o">=</span><span class="n">test_mask</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.forward-1278"><a href="#HATMaskResNetBase.forward-1278"><span class="linenos">1278</span></a>            <span class="p">)</span>
</span><span id="HATMaskResNetBase.forward-1279"><a href="#HATMaskResNetBase.forward-1279"><span class="linenos">1279</span></a>            <span class="n">hidden_features</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">hidden_features_block</span><span class="p">)</span>  <span class="c1"># store the hidden feature</span>
</span><span id="HATMaskResNetBase.forward-1280"><a href="#HATMaskResNetBase.forward-1280"><span class="linenos">1280</span></a>        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv5x</span><span class="p">:</span>
</span><span id="HATMaskResNetBase.forward-1281"><a href="#HATMaskResNetBase.forward-1281"><span class="linenos">1281</span></a>            <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">hidden_features_block</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span>
</span><span id="HATMaskResNetBase.forward-1282"><a href="#HATMaskResNetBase.forward-1282"><span class="linenos">1282</span></a>                <span class="n">x</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.forward-1283"><a href="#HATMaskResNetBase.forward-1283"><span class="linenos">1283</span></a>                <span class="n">stage</span><span class="o">=</span><span class="n">stage</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.forward-1284"><a href="#HATMaskResNetBase.forward-1284"><span class="linenos">1284</span></a>                <span class="n">s_max</span><span class="o">=</span><span class="n">s_max</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.forward-1285"><a href="#HATMaskResNetBase.forward-1285"><span class="linenos">1285</span></a>                <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.forward-1286"><a href="#HATMaskResNetBase.forward-1286"><span class="linenos">1286</span></a>                <span class="n">num_batches</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.forward-1287"><a href="#HATMaskResNetBase.forward-1287"><span class="linenos">1287</span></a>                <span class="n">test_mask</span><span class="o">=</span><span class="n">test_mask</span><span class="p">,</span>
</span><span id="HATMaskResNetBase.forward-1288"><a href="#HATMaskResNetBase.forward-1288"><span class="linenos">1288</span></a>            <span class="p">)</span>
</span><span id="HATMaskResNetBase.forward-1289"><a href="#HATMaskResNetBase.forward-1289"><span class="linenos">1289</span></a>            <span class="n">hidden_features</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">hidden_features_block</span><span class="p">)</span>  <span class="c1"># store the hidden feature</span>
</span><span id="HATMaskResNetBase.forward-1290"><a href="#HATMaskResNetBase.forward-1290"><span class="linenos">1290</span></a>
</span><span id="HATMaskResNetBase.forward-1291"><a href="#HATMaskResNetBase.forward-1291"><span class="linenos">1291</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">avepool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="HATMaskResNetBase.forward-1292"><a href="#HATMaskResNetBase.forward-1292"><span class="linenos">1292</span></a>
</span><span id="HATMaskResNetBase.forward-1293"><a href="#HATMaskResNetBase.forward-1293"><span class="linenos">1293</span></a>        <span class="n">output_feature</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># flatten before going through heads</span>
</span><span id="HATMaskResNetBase.forward-1294"><a href="#HATMaskResNetBase.forward-1294"><span class="linenos">1294</span></a>
</span><span id="HATMaskResNetBase.forward-1295"><a href="#HATMaskResNetBase.forward-1295"><span class="linenos">1295</span></a>        <span class="k">return</span> <span class="n">output_feature</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">hidden_features</span>
</span></pre></div>


            <div class="docstring"><p>The forward pass for data from task <code><a href="#HATMaskResNetBase.task_id">task_id</a></code>. Task-specific mask for <code><a href="#HATMaskResNetBase.task_id">task_id</a></code> are applied to the units which are channels in each weighted convolutional layer.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>input</strong> (<code>Tensor</code>): the input tensor from data.</li>
<li><strong>stage</strong> (<code>str</code>): the stage of the forward pass, should be one of the following:
<ol>
<li>'train': training stage.</li>
<li>'validation': validation stage.</li>
<li>'test': testing stage.</li>
</ol></li>
<li><strong>s_max</strong> (<code><a href="#HATMaskResNetBase.float">float</a></code> | <code>None</code>): the maximum scaling factor in the gate function. Doesn't apply to testing stage. See chapter 2.4 "Hard Attention Training" in <a href="http://proceedings.mlr.press/v80/serra18a">HAT paper</a>.</li>
<li><strong>batch_idx</strong> (<code>int</code> | <code>None</code>): the current batch index. Applies only to training stage. For other stages, it is default <code>None</code>.</li>
<li><strong>num_batches</strong> (<code>int</code> | <code>None</code>): the total number of batches. Applies only to training stage. For other stages, it is default <code>None</code>.</li>
<li><strong>test_mask</strong> (<code>dict[str, Tensor]</code> | <code>None</code>): the binary mask used for test. Applies only to testing stage. For other stages, it is default <code>None</code>.</li>
</ul>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>output_feature</strong> (<code>Tensor</code>): the output feature tensor to be passed to the heads.</li>
<li><strong>mask</strong> (<code>dict[str, Tensor]</code>): the mask for the current task. Key (<code>str</code>) is layer name, value (<code>Tensor</code>) is the mask tensor. The mask tensor has size (number of units).</li>
<li><strong>hidden_features</strong> (<code>dict[str, Tensor]</code>): the hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name, value (<code>Tensor</code>) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes. Although HAT algorithm does not need this, it is still provided for API consistence for other HAT-based algorithms inherited this <code><a href="#HATMaskResNetBase.forward">forward()</a></code> method of <code>HAT</code> class.</li>
</ul>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt><a href="#ResNetBase">ResNetBase</a></dt>
                                <dd id="HATMaskResNetBase.batch_normalisation" class="variable"><a href="#ResNetBase.batch_normalisation">batch_normalisation</a></dd>
                <dd id="HATMaskResNetBase.activation" class="variable"><a href="#ResNetBase.activation">activation</a></dd>
                <dd id="HATMaskResNetBase.conv1" class="variable"><a href="#ResNetBase.conv1">conv1</a></dd>
                <dd id="HATMaskResNetBase.maxpool" class="variable"><a href="#ResNetBase.maxpool">maxpool</a></dd>
                <dd id="HATMaskResNetBase.conv2x" class="variable"><a href="#ResNetBase.conv2x">conv2x</a></dd>
                <dd id="HATMaskResNetBase.conv3x" class="variable"><a href="#ResNetBase.conv3x">conv3x</a></dd>
                <dd id="HATMaskResNetBase.conv4x" class="variable"><a href="#ResNetBase.conv4x">conv4x</a></dd>
                <dd id="HATMaskResNetBase.conv5x" class="variable"><a href="#ResNetBase.conv5x">conv5x</a></dd>
                <dd id="HATMaskResNetBase.avepool" class="variable"><a href="#ResNetBase.avepool">avepool</a></dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="HATMaskResNet18">
                            <input id="HATMaskResNet18-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">HATMaskResNet18</span><wbr>(<span class="base"><a href="#HATMaskResNetBase">HATMaskResNetBase</a></span>):

                <label class="view-source-button" for="HATMaskResNet18-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HATMaskResNet18"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HATMaskResNet18-1298"><a href="#HATMaskResNet18-1298"><span class="linenos">1298</span></a><span class="k">class</span><span class="w"> </span><span class="nc">HATMaskResNet18</span><span class="p">(</span><span class="n">HATMaskResNetBase</span><span class="p">):</span>
</span><span id="HATMaskResNet18-1299"><a href="#HATMaskResNet18-1299"><span class="linenos">1299</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;HAT masked ResNet-18 backbone network.</span>
</span><span id="HATMaskResNet18-1300"><a href="#HATMaskResNet18-1300"><span class="linenos">1300</span></a>
</span><span id="HATMaskResNet18-1301"><a href="#HATMaskResNet18-1301"><span class="linenos">1301</span></a><span class="sd">    [HAT (Hard Attention to the Task, 2018)](http://proceedings.mlr.press/v80/serra18a) is an architecture-based continual learning approach that uses learnable hard attention masks to select the task-specific parameters.</span>
</span><span id="HATMaskResNet18-1302"><a href="#HATMaskResNet18-1302"><span class="linenos">1302</span></a>
</span><span id="HATMaskResNet18-1303"><a href="#HATMaskResNet18-1303"><span class="linenos">1303</span></a><span class="sd">    ResNet-18 is a smaller architecture proposed in the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html). It consists of 18 weight convolutional layers in total. See Table 1 in the paper for details.</span>
</span><span id="HATMaskResNet18-1304"><a href="#HATMaskResNet18-1304"><span class="linenos">1304</span></a>
</span><span id="HATMaskResNet18-1305"><a href="#HATMaskResNet18-1305"><span class="linenos">1305</span></a><span class="sd">    Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the unit-wise task embedding and gate function.</span>
</span><span id="HATMaskResNet18-1306"><a href="#HATMaskResNet18-1306"><span class="linenos">1306</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="HATMaskResNet18-1307"><a href="#HATMaskResNet18-1307"><span class="linenos">1307</span></a>
</span><span id="HATMaskResNet18-1308"><a href="#HATMaskResNet18-1308"><span class="linenos">1308</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="HATMaskResNet18-1309"><a href="#HATMaskResNet18-1309"><span class="linenos">1309</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="HATMaskResNet18-1310"><a href="#HATMaskResNet18-1310"><span class="linenos">1310</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HATMaskResNet18-1311"><a href="#HATMaskResNet18-1311"><span class="linenos">1311</span></a>        <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HATMaskResNet18-1312"><a href="#HATMaskResNet18-1312"><span class="linenos">1312</span></a>        <span class="n">gate</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="HATMaskResNet18-1313"><a href="#HATMaskResNet18-1313"><span class="linenos">1313</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="HATMaskResNet18-1314"><a href="#HATMaskResNet18-1314"><span class="linenos">1314</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="HATMaskResNet18-1315"><a href="#HATMaskResNet18-1315"><span class="linenos">1315</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HATMaskResNet18-1316"><a href="#HATMaskResNet18-1316"><span class="linenos">1316</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the ResNet-18 backbone network with task embedding. Note that batch normalisation is incompatible with HAT mechanism.</span>
</span><span id="HATMaskResNet18-1317"><a href="#HATMaskResNet18-1317"><span class="linenos">1317</span></a>
</span><span id="HATMaskResNet18-1318"><a href="#HATMaskResNet18-1318"><span class="linenos">1318</span></a><span class="sd">        **Args:**</span>
</span><span id="HATMaskResNet18-1319"><a href="#HATMaskResNet18-1319"><span class="linenos">1319</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</span>
</span><span id="HATMaskResNet18-1320"><a href="#HATMaskResNet18-1320"><span class="linenos">1320</span></a><span class="sd">        - **output_dim** (`int`): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</span>
</span><span id="HATMaskResNet18-1321"><a href="#HATMaskResNet18-1321"><span class="linenos">1321</span></a><span class="sd">        - **gate** (`str`): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:</span>
</span><span id="HATMaskResNet18-1322"><a href="#HATMaskResNet18-1322"><span class="linenos">1322</span></a><span class="sd">            - `sigmoid`: the sigmoid function.</span>
</span><span id="HATMaskResNet18-1323"><a href="#HATMaskResNet18-1323"><span class="linenos">1323</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="HATMaskResNet18-1324"><a href="#HATMaskResNet18-1324"><span class="linenos">1324</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`.</span>
</span><span id="HATMaskResNet18-1325"><a href="#HATMaskResNet18-1325"><span class="linenos">1325</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HATMaskResNet18-1326"><a href="#HATMaskResNet18-1326"><span class="linenos">1326</span></a>        <span class="n">HATMaskResNetBase</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="HATMaskResNet18-1327"><a href="#HATMaskResNet18-1327"><span class="linenos">1327</span></a>            <span class="bp">self</span><span class="p">,</span>
</span><span id="HATMaskResNet18-1328"><a href="#HATMaskResNet18-1328"><span class="linenos">1328</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
</span><span id="HATMaskResNet18-1329"><a href="#HATMaskResNet18-1329"><span class="linenos">1329</span></a>            <span class="n">building_block_type</span><span class="o">=</span><span class="n">HATMaskResNetBlockSmall</span><span class="p">,</span>  <span class="c1"># use the smaller building block for ResNet-18</span>
</span><span id="HATMaskResNet18-1330"><a href="#HATMaskResNet18-1330"><span class="linenos">1330</span></a>            <span class="n">building_block_nums</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
</span><span id="HATMaskResNet18-1331"><a href="#HATMaskResNet18-1331"><span class="linenos">1331</span></a>            <span class="n">building_block_preceding_output_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>
</span><span id="HATMaskResNet18-1332"><a href="#HATMaskResNet18-1332"><span class="linenos">1332</span></a>            <span class="n">building_block_input_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
</span><span id="HATMaskResNet18-1333"><a href="#HATMaskResNet18-1333"><span class="linenos">1333</span></a>            <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span>
</span><span id="HATMaskResNet18-1334"><a href="#HATMaskResNet18-1334"><span class="linenos">1334</span></a>            <span class="n">gate</span><span class="o">=</span><span class="n">gate</span><span class="p">,</span>
</span><span id="HATMaskResNet18-1335"><a href="#HATMaskResNet18-1335"><span class="linenos">1335</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="HATMaskResNet18-1336"><a href="#HATMaskResNet18-1336"><span class="linenos">1336</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="HATMaskResNet18-1337"><a href="#HATMaskResNet18-1337"><span class="linenos">1337</span></a>        <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>HAT masked ResNet-18 backbone network.</p>

<p><a href="http://proceedings.mlr.press/v80/serra18a">HAT (Hard Attention to the Task, 2018)</a> is an architecture-based continual learning approach that uses learnable hard attention masks to select the task-specific parameters.</p>

<p>ResNet-18 is a smaller architecture proposed in the <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">original ResNet paper</a>. It consists of 18 weight convolutional layers in total. See Table 1 in the paper for details.</p>

<p>Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the unit-wise task embedding and gate function.</p>
</div>


                            <div id="HATMaskResNet18.__init__" class="classattr">
                                        <input id="HATMaskResNet18.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">HATMaskResNet18</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">gate</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	activation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class="param">	<span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span></span>)</span>

                <label class="view-source-button" for="HATMaskResNet18.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HATMaskResNet18.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HATMaskResNet18.__init__-1308"><a href="#HATMaskResNet18.__init__-1308"><span class="linenos">1308</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="HATMaskResNet18.__init__-1309"><a href="#HATMaskResNet18.__init__-1309"><span class="linenos">1309</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="HATMaskResNet18.__init__-1310"><a href="#HATMaskResNet18.__init__-1310"><span class="linenos">1310</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HATMaskResNet18.__init__-1311"><a href="#HATMaskResNet18.__init__-1311"><span class="linenos">1311</span></a>        <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HATMaskResNet18.__init__-1312"><a href="#HATMaskResNet18.__init__-1312"><span class="linenos">1312</span></a>        <span class="n">gate</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="HATMaskResNet18.__init__-1313"><a href="#HATMaskResNet18.__init__-1313"><span class="linenos">1313</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="HATMaskResNet18.__init__-1314"><a href="#HATMaskResNet18.__init__-1314"><span class="linenos">1314</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="HATMaskResNet18.__init__-1315"><a href="#HATMaskResNet18.__init__-1315"><span class="linenos">1315</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HATMaskResNet18.__init__-1316"><a href="#HATMaskResNet18.__init__-1316"><span class="linenos">1316</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the ResNet-18 backbone network with task embedding. Note that batch normalisation is incompatible with HAT mechanism.</span>
</span><span id="HATMaskResNet18.__init__-1317"><a href="#HATMaskResNet18.__init__-1317"><span class="linenos">1317</span></a>
</span><span id="HATMaskResNet18.__init__-1318"><a href="#HATMaskResNet18.__init__-1318"><span class="linenos">1318</span></a><span class="sd">        **Args:**</span>
</span><span id="HATMaskResNet18.__init__-1319"><a href="#HATMaskResNet18.__init__-1319"><span class="linenos">1319</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</span>
</span><span id="HATMaskResNet18.__init__-1320"><a href="#HATMaskResNet18.__init__-1320"><span class="linenos">1320</span></a><span class="sd">        - **output_dim** (`int`): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</span>
</span><span id="HATMaskResNet18.__init__-1321"><a href="#HATMaskResNet18.__init__-1321"><span class="linenos">1321</span></a><span class="sd">        - **gate** (`str`): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:</span>
</span><span id="HATMaskResNet18.__init__-1322"><a href="#HATMaskResNet18.__init__-1322"><span class="linenos">1322</span></a><span class="sd">            - `sigmoid`: the sigmoid function.</span>
</span><span id="HATMaskResNet18.__init__-1323"><a href="#HATMaskResNet18.__init__-1323"><span class="linenos">1323</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="HATMaskResNet18.__init__-1324"><a href="#HATMaskResNet18.__init__-1324"><span class="linenos">1324</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`.</span>
</span><span id="HATMaskResNet18.__init__-1325"><a href="#HATMaskResNet18.__init__-1325"><span class="linenos">1325</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HATMaskResNet18.__init__-1326"><a href="#HATMaskResNet18.__init__-1326"><span class="linenos">1326</span></a>        <span class="n">HATMaskResNetBase</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="HATMaskResNet18.__init__-1327"><a href="#HATMaskResNet18.__init__-1327"><span class="linenos">1327</span></a>            <span class="bp">self</span><span class="p">,</span>
</span><span id="HATMaskResNet18.__init__-1328"><a href="#HATMaskResNet18.__init__-1328"><span class="linenos">1328</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
</span><span id="HATMaskResNet18.__init__-1329"><a href="#HATMaskResNet18.__init__-1329"><span class="linenos">1329</span></a>            <span class="n">building_block_type</span><span class="o">=</span><span class="n">HATMaskResNetBlockSmall</span><span class="p">,</span>  <span class="c1"># use the smaller building block for ResNet-18</span>
</span><span id="HATMaskResNet18.__init__-1330"><a href="#HATMaskResNet18.__init__-1330"><span class="linenos">1330</span></a>            <span class="n">building_block_nums</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
</span><span id="HATMaskResNet18.__init__-1331"><a href="#HATMaskResNet18.__init__-1331"><span class="linenos">1331</span></a>            <span class="n">building_block_preceding_output_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>
</span><span id="HATMaskResNet18.__init__-1332"><a href="#HATMaskResNet18.__init__-1332"><span class="linenos">1332</span></a>            <span class="n">building_block_input_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
</span><span id="HATMaskResNet18.__init__-1333"><a href="#HATMaskResNet18.__init__-1333"><span class="linenos">1333</span></a>            <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span>
</span><span id="HATMaskResNet18.__init__-1334"><a href="#HATMaskResNet18.__init__-1334"><span class="linenos">1334</span></a>            <span class="n">gate</span><span class="o">=</span><span class="n">gate</span><span class="p">,</span>
</span><span id="HATMaskResNet18.__init__-1335"><a href="#HATMaskResNet18.__init__-1335"><span class="linenos">1335</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="HATMaskResNet18.__init__-1336"><a href="#HATMaskResNet18.__init__-1336"><span class="linenos">1336</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="HATMaskResNet18.__init__-1337"><a href="#HATMaskResNet18.__init__-1337"><span class="linenos">1337</span></a>        <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Construct and initialise the ResNet-18 backbone network with task embedding. Note that batch normalisation is incompatible with HAT mechanism.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</li>
<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>
<li><strong>gate</strong> (<code>str</code>): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:
<ul>
<li><code>sigmoid</code>: the sigmoid function.</li>
</ul></li>
<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>
<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>.</li>
</ul>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt><a href="#HATMaskResNetBase">HATMaskResNetBase</a></dt>
                                <dd id="HATMaskResNet18.update_multiple_blocks_task_embedding" class="function"><a href="#HATMaskResNetBase.update_multiple_blocks_task_embedding">update_multiple_blocks_task_embedding</a></dd>
                <dd id="HATMaskResNet18.forward" class="function"><a href="#HATMaskResNetBase.forward">forward</a></dd>

            </div>
            <div><dt><a href="#ResNetBase">ResNetBase</a></dt>
                                <dd id="HATMaskResNet18.batch_normalisation" class="variable"><a href="#ResNetBase.batch_normalisation">batch_normalisation</a></dd>
                <dd id="HATMaskResNet18.activation" class="variable"><a href="#ResNetBase.activation">activation</a></dd>
                <dd id="HATMaskResNet18.conv1" class="variable"><a href="#ResNetBase.conv1">conv1</a></dd>
                <dd id="HATMaskResNet18.maxpool" class="variable"><a href="#ResNetBase.maxpool">maxpool</a></dd>
                <dd id="HATMaskResNet18.conv2x" class="variable"><a href="#ResNetBase.conv2x">conv2x</a></dd>
                <dd id="HATMaskResNet18.conv3x" class="variable"><a href="#ResNetBase.conv3x">conv3x</a></dd>
                <dd id="HATMaskResNet18.conv4x" class="variable"><a href="#ResNetBase.conv4x">conv4x</a></dd>
                <dd id="HATMaskResNet18.conv5x" class="variable"><a href="#ResNetBase.conv5x">conv5x</a></dd>
                <dd id="HATMaskResNet18.avepool" class="variable"><a href="#ResNetBase.avepool">avepool</a></dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="HATMaskResNet34">
                            <input id="HATMaskResNet34-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">HATMaskResNet34</span><wbr>(<span class="base"><a href="#HATMaskResNetBase">HATMaskResNetBase</a></span>):

                <label class="view-source-button" for="HATMaskResNet34-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HATMaskResNet34"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HATMaskResNet34-1340"><a href="#HATMaskResNet34-1340"><span class="linenos">1340</span></a><span class="k">class</span><span class="w"> </span><span class="nc">HATMaskResNet34</span><span class="p">(</span><span class="n">HATMaskResNetBase</span><span class="p">):</span>
</span><span id="HATMaskResNet34-1341"><a href="#HATMaskResNet34-1341"><span class="linenos">1341</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;HAT masked ResNet-34 backbone network.</span>
</span><span id="HATMaskResNet34-1342"><a href="#HATMaskResNet34-1342"><span class="linenos">1342</span></a>
</span><span id="HATMaskResNet34-1343"><a href="#HATMaskResNet34-1343"><span class="linenos">1343</span></a><span class="sd">    [HAT (Hard Attention to the Task, 2018)](http://proceedings.mlr.press/v80/serra18a) is an architecture-based continual learning approach that uses learnable hard attention masks to select the task-specific parameters.</span>
</span><span id="HATMaskResNet34-1344"><a href="#HATMaskResNet34-1344"><span class="linenos">1344</span></a>
</span><span id="HATMaskResNet34-1345"><a href="#HATMaskResNet34-1345"><span class="linenos">1345</span></a><span class="sd">    ResNet-34 is a smaller architecture proposed in the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html). It consists of 34 weight convolutional layers in total. See Table 1 in the paper for details.</span>
</span><span id="HATMaskResNet34-1346"><a href="#HATMaskResNet34-1346"><span class="linenos">1346</span></a>
</span><span id="HATMaskResNet34-1347"><a href="#HATMaskResNet34-1347"><span class="linenos">1347</span></a><span class="sd">    Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the unit-wise task embedding and gate function.</span>
</span><span id="HATMaskResNet34-1348"><a href="#HATMaskResNet34-1348"><span class="linenos">1348</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="HATMaskResNet34-1349"><a href="#HATMaskResNet34-1349"><span class="linenos">1349</span></a>
</span><span id="HATMaskResNet34-1350"><a href="#HATMaskResNet34-1350"><span class="linenos">1350</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="HATMaskResNet34-1351"><a href="#HATMaskResNet34-1351"><span class="linenos">1351</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="HATMaskResNet34-1352"><a href="#HATMaskResNet34-1352"><span class="linenos">1352</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HATMaskResNet34-1353"><a href="#HATMaskResNet34-1353"><span class="linenos">1353</span></a>        <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HATMaskResNet34-1354"><a href="#HATMaskResNet34-1354"><span class="linenos">1354</span></a>        <span class="n">gate</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="HATMaskResNet34-1355"><a href="#HATMaskResNet34-1355"><span class="linenos">1355</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="HATMaskResNet34-1356"><a href="#HATMaskResNet34-1356"><span class="linenos">1356</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="HATMaskResNet34-1357"><a href="#HATMaskResNet34-1357"><span class="linenos">1357</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HATMaskResNet34-1358"><a href="#HATMaskResNet34-1358"><span class="linenos">1358</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the ResNet-34 backbone network with task embedding. Note that batch normalisation is incompatible with HAT mechanism.</span>
</span><span id="HATMaskResNet34-1359"><a href="#HATMaskResNet34-1359"><span class="linenos">1359</span></a>
</span><span id="HATMaskResNet34-1360"><a href="#HATMaskResNet34-1360"><span class="linenos">1360</span></a><span class="sd">        **Args:**</span>
</span><span id="HATMaskResNet34-1361"><a href="#HATMaskResNet34-1361"><span class="linenos">1361</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</span>
</span><span id="HATMaskResNet34-1362"><a href="#HATMaskResNet34-1362"><span class="linenos">1362</span></a><span class="sd">        - **output_dim** (`int`): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</span>
</span><span id="HATMaskResNet34-1363"><a href="#HATMaskResNet34-1363"><span class="linenos">1363</span></a><span class="sd">        - **gate** (`str`): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:</span>
</span><span id="HATMaskResNet34-1364"><a href="#HATMaskResNet34-1364"><span class="linenos">1364</span></a><span class="sd">            - `sigmoid`: the sigmoid function.</span>
</span><span id="HATMaskResNet34-1365"><a href="#HATMaskResNet34-1365"><span class="linenos">1365</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="HATMaskResNet34-1366"><a href="#HATMaskResNet34-1366"><span class="linenos">1366</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`.</span>
</span><span id="HATMaskResNet34-1367"><a href="#HATMaskResNet34-1367"><span class="linenos">1367</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HATMaskResNet34-1368"><a href="#HATMaskResNet34-1368"><span class="linenos">1368</span></a>        <span class="n">HATMaskResNetBase</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="HATMaskResNet34-1369"><a href="#HATMaskResNet34-1369"><span class="linenos">1369</span></a>            <span class="bp">self</span><span class="p">,</span>
</span><span id="HATMaskResNet34-1370"><a href="#HATMaskResNet34-1370"><span class="linenos">1370</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
</span><span id="HATMaskResNet34-1371"><a href="#HATMaskResNet34-1371"><span class="linenos">1371</span></a>            <span class="n">building_block_type</span><span class="o">=</span><span class="n">HATMaskResNetBlockSmall</span><span class="p">,</span>  <span class="c1"># use the smaller building block for ResNet-34</span>
</span><span id="HATMaskResNet34-1372"><a href="#HATMaskResNet34-1372"><span class="linenos">1372</span></a>            <span class="n">building_block_nums</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
</span><span id="HATMaskResNet34-1373"><a href="#HATMaskResNet34-1373"><span class="linenos">1373</span></a>            <span class="n">building_block_preceding_output_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>
</span><span id="HATMaskResNet34-1374"><a href="#HATMaskResNet34-1374"><span class="linenos">1374</span></a>            <span class="n">building_block_input_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
</span><span id="HATMaskResNet34-1375"><a href="#HATMaskResNet34-1375"><span class="linenos">1375</span></a>            <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span>
</span><span id="HATMaskResNet34-1376"><a href="#HATMaskResNet34-1376"><span class="linenos">1376</span></a>            <span class="n">gate</span><span class="o">=</span><span class="n">gate</span><span class="p">,</span>
</span><span id="HATMaskResNet34-1377"><a href="#HATMaskResNet34-1377"><span class="linenos">1377</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="HATMaskResNet34-1378"><a href="#HATMaskResNet34-1378"><span class="linenos">1378</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="HATMaskResNet34-1379"><a href="#HATMaskResNet34-1379"><span class="linenos">1379</span></a>        <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>HAT masked ResNet-34 backbone network.</p>

<p><a href="http://proceedings.mlr.press/v80/serra18a">HAT (Hard Attention to the Task, 2018)</a> is an architecture-based continual learning approach that uses learnable hard attention masks to select the task-specific parameters.</p>

<p>ResNet-34 is a smaller architecture proposed in the <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">original ResNet paper</a>. It consists of 34 weight convolutional layers in total. See Table 1 in the paper for details.</p>

<p>Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the unit-wise task embedding and gate function.</p>
</div>


                            <div id="HATMaskResNet34.__init__" class="classattr">
                                        <input id="HATMaskResNet34.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">HATMaskResNet34</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">gate</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	activation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class="param">	<span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span></span>)</span>

                <label class="view-source-button" for="HATMaskResNet34.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HATMaskResNet34.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HATMaskResNet34.__init__-1350"><a href="#HATMaskResNet34.__init__-1350"><span class="linenos">1350</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="HATMaskResNet34.__init__-1351"><a href="#HATMaskResNet34.__init__-1351"><span class="linenos">1351</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="HATMaskResNet34.__init__-1352"><a href="#HATMaskResNet34.__init__-1352"><span class="linenos">1352</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HATMaskResNet34.__init__-1353"><a href="#HATMaskResNet34.__init__-1353"><span class="linenos">1353</span></a>        <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HATMaskResNet34.__init__-1354"><a href="#HATMaskResNet34.__init__-1354"><span class="linenos">1354</span></a>        <span class="n">gate</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="HATMaskResNet34.__init__-1355"><a href="#HATMaskResNet34.__init__-1355"><span class="linenos">1355</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="HATMaskResNet34.__init__-1356"><a href="#HATMaskResNet34.__init__-1356"><span class="linenos">1356</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="HATMaskResNet34.__init__-1357"><a href="#HATMaskResNet34.__init__-1357"><span class="linenos">1357</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HATMaskResNet34.__init__-1358"><a href="#HATMaskResNet34.__init__-1358"><span class="linenos">1358</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the ResNet-34 backbone network with task embedding. Note that batch normalisation is incompatible with HAT mechanism.</span>
</span><span id="HATMaskResNet34.__init__-1359"><a href="#HATMaskResNet34.__init__-1359"><span class="linenos">1359</span></a>
</span><span id="HATMaskResNet34.__init__-1360"><a href="#HATMaskResNet34.__init__-1360"><span class="linenos">1360</span></a><span class="sd">        **Args:**</span>
</span><span id="HATMaskResNet34.__init__-1361"><a href="#HATMaskResNet34.__init__-1361"><span class="linenos">1361</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</span>
</span><span id="HATMaskResNet34.__init__-1362"><a href="#HATMaskResNet34.__init__-1362"><span class="linenos">1362</span></a><span class="sd">        - **output_dim** (`int`): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</span>
</span><span id="HATMaskResNet34.__init__-1363"><a href="#HATMaskResNet34.__init__-1363"><span class="linenos">1363</span></a><span class="sd">        - **gate** (`str`): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:</span>
</span><span id="HATMaskResNet34.__init__-1364"><a href="#HATMaskResNet34.__init__-1364"><span class="linenos">1364</span></a><span class="sd">            - `sigmoid`: the sigmoid function.</span>
</span><span id="HATMaskResNet34.__init__-1365"><a href="#HATMaskResNet34.__init__-1365"><span class="linenos">1365</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="HATMaskResNet34.__init__-1366"><a href="#HATMaskResNet34.__init__-1366"><span class="linenos">1366</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`.</span>
</span><span id="HATMaskResNet34.__init__-1367"><a href="#HATMaskResNet34.__init__-1367"><span class="linenos">1367</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HATMaskResNet34.__init__-1368"><a href="#HATMaskResNet34.__init__-1368"><span class="linenos">1368</span></a>        <span class="n">HATMaskResNetBase</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="HATMaskResNet34.__init__-1369"><a href="#HATMaskResNet34.__init__-1369"><span class="linenos">1369</span></a>            <span class="bp">self</span><span class="p">,</span>
</span><span id="HATMaskResNet34.__init__-1370"><a href="#HATMaskResNet34.__init__-1370"><span class="linenos">1370</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
</span><span id="HATMaskResNet34.__init__-1371"><a href="#HATMaskResNet34.__init__-1371"><span class="linenos">1371</span></a>            <span class="n">building_block_type</span><span class="o">=</span><span class="n">HATMaskResNetBlockSmall</span><span class="p">,</span>  <span class="c1"># use the smaller building block for ResNet-34</span>
</span><span id="HATMaskResNet34.__init__-1372"><a href="#HATMaskResNet34.__init__-1372"><span class="linenos">1372</span></a>            <span class="n">building_block_nums</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
</span><span id="HATMaskResNet34.__init__-1373"><a href="#HATMaskResNet34.__init__-1373"><span class="linenos">1373</span></a>            <span class="n">building_block_preceding_output_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>
</span><span id="HATMaskResNet34.__init__-1374"><a href="#HATMaskResNet34.__init__-1374"><span class="linenos">1374</span></a>            <span class="n">building_block_input_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
</span><span id="HATMaskResNet34.__init__-1375"><a href="#HATMaskResNet34.__init__-1375"><span class="linenos">1375</span></a>            <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span>
</span><span id="HATMaskResNet34.__init__-1376"><a href="#HATMaskResNet34.__init__-1376"><span class="linenos">1376</span></a>            <span class="n">gate</span><span class="o">=</span><span class="n">gate</span><span class="p">,</span>
</span><span id="HATMaskResNet34.__init__-1377"><a href="#HATMaskResNet34.__init__-1377"><span class="linenos">1377</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="HATMaskResNet34.__init__-1378"><a href="#HATMaskResNet34.__init__-1378"><span class="linenos">1378</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="HATMaskResNet34.__init__-1379"><a href="#HATMaskResNet34.__init__-1379"><span class="linenos">1379</span></a>        <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Construct and initialise the ResNet-34 backbone network with task embedding. Note that batch normalisation is incompatible with HAT mechanism.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</li>
<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>
<li><strong>gate</strong> (<code>str</code>): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:
<ul>
<li><code>sigmoid</code>: the sigmoid function.</li>
</ul></li>
<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>
<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>.</li>
</ul>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt><a href="#HATMaskResNetBase">HATMaskResNetBase</a></dt>
                                <dd id="HATMaskResNet34.update_multiple_blocks_task_embedding" class="function"><a href="#HATMaskResNetBase.update_multiple_blocks_task_embedding">update_multiple_blocks_task_embedding</a></dd>
                <dd id="HATMaskResNet34.forward" class="function"><a href="#HATMaskResNetBase.forward">forward</a></dd>

            </div>
            <div><dt><a href="#ResNetBase">ResNetBase</a></dt>
                                <dd id="HATMaskResNet34.batch_normalisation" class="variable"><a href="#ResNetBase.batch_normalisation">batch_normalisation</a></dd>
                <dd id="HATMaskResNet34.activation" class="variable"><a href="#ResNetBase.activation">activation</a></dd>
                <dd id="HATMaskResNet34.conv1" class="variable"><a href="#ResNetBase.conv1">conv1</a></dd>
                <dd id="HATMaskResNet34.maxpool" class="variable"><a href="#ResNetBase.maxpool">maxpool</a></dd>
                <dd id="HATMaskResNet34.conv2x" class="variable"><a href="#ResNetBase.conv2x">conv2x</a></dd>
                <dd id="HATMaskResNet34.conv3x" class="variable"><a href="#ResNetBase.conv3x">conv3x</a></dd>
                <dd id="HATMaskResNet34.conv4x" class="variable"><a href="#ResNetBase.conv4x">conv4x</a></dd>
                <dd id="HATMaskResNet34.conv5x" class="variable"><a href="#ResNetBase.conv5x">conv5x</a></dd>
                <dd id="HATMaskResNet34.avepool" class="variable"><a href="#ResNetBase.avepool">avepool</a></dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="HATMaskResNet50">
                            <input id="HATMaskResNet50-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">HATMaskResNet50</span><wbr>(<span class="base"><a href="#HATMaskResNetBase">HATMaskResNetBase</a></span>):

                <label class="view-source-button" for="HATMaskResNet50-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HATMaskResNet50"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HATMaskResNet50-1382"><a href="#HATMaskResNet50-1382"><span class="linenos">1382</span></a><span class="k">class</span><span class="w"> </span><span class="nc">HATMaskResNet50</span><span class="p">(</span><span class="n">HATMaskResNetBase</span><span class="p">):</span>
</span><span id="HATMaskResNet50-1383"><a href="#HATMaskResNet50-1383"><span class="linenos">1383</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;HAT masked ResNet-50 backbone network.</span>
</span><span id="HATMaskResNet50-1384"><a href="#HATMaskResNet50-1384"><span class="linenos">1384</span></a>
</span><span id="HATMaskResNet50-1385"><a href="#HATMaskResNet50-1385"><span class="linenos">1385</span></a><span class="sd">    [HAT (Hard Attention to the Task, 2018)](http://proceedings.mlr.press/v80/serra18a) is an architecture-based continual learning approach that uses learnable hard attention masks to select the task-specific parameters.</span>
</span><span id="HATMaskResNet50-1386"><a href="#HATMaskResNet50-1386"><span class="linenos">1386</span></a>
</span><span id="HATMaskResNet50-1387"><a href="#HATMaskResNet50-1387"><span class="linenos">1387</span></a><span class="sd">    ResNet-50 is a larger architecture proposed in the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html). It consists of 50 weight convolutional layers in total. See Table 1 in the paper for details.</span>
</span><span id="HATMaskResNet50-1388"><a href="#HATMaskResNet50-1388"><span class="linenos">1388</span></a>
</span><span id="HATMaskResNet50-1389"><a href="#HATMaskResNet50-1389"><span class="linenos">1389</span></a><span class="sd">    Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the unit-wise task embedding and gate function.</span>
</span><span id="HATMaskResNet50-1390"><a href="#HATMaskResNet50-1390"><span class="linenos">1390</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="HATMaskResNet50-1391"><a href="#HATMaskResNet50-1391"><span class="linenos">1391</span></a>
</span><span id="HATMaskResNet50-1392"><a href="#HATMaskResNet50-1392"><span class="linenos">1392</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="HATMaskResNet50-1393"><a href="#HATMaskResNet50-1393"><span class="linenos">1393</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="HATMaskResNet50-1394"><a href="#HATMaskResNet50-1394"><span class="linenos">1394</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HATMaskResNet50-1395"><a href="#HATMaskResNet50-1395"><span class="linenos">1395</span></a>        <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HATMaskResNet50-1396"><a href="#HATMaskResNet50-1396"><span class="linenos">1396</span></a>        <span class="n">gate</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="HATMaskResNet50-1397"><a href="#HATMaskResNet50-1397"><span class="linenos">1397</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="HATMaskResNet50-1398"><a href="#HATMaskResNet50-1398"><span class="linenos">1398</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="HATMaskResNet50-1399"><a href="#HATMaskResNet50-1399"><span class="linenos">1399</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HATMaskResNet50-1400"><a href="#HATMaskResNet50-1400"><span class="linenos">1400</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the ResNet-50 backbone network with task embedding. Note that batch normalisation is incompatible with HAT mechanism.</span>
</span><span id="HATMaskResNet50-1401"><a href="#HATMaskResNet50-1401"><span class="linenos">1401</span></a>
</span><span id="HATMaskResNet50-1402"><a href="#HATMaskResNet50-1402"><span class="linenos">1402</span></a><span class="sd">        **Args:**</span>
</span><span id="HATMaskResNet50-1403"><a href="#HATMaskResNet50-1403"><span class="linenos">1403</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</span>
</span><span id="HATMaskResNet50-1404"><a href="#HATMaskResNet50-1404"><span class="linenos">1404</span></a><span class="sd">        - **output_dim** (`int`): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</span>
</span><span id="HATMaskResNet50-1405"><a href="#HATMaskResNet50-1405"><span class="linenos">1405</span></a><span class="sd">        - **gate** (`str`): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:</span>
</span><span id="HATMaskResNet50-1406"><a href="#HATMaskResNet50-1406"><span class="linenos">1406</span></a><span class="sd">            - `sigmoid`: the sigmoid function.</span>
</span><span id="HATMaskResNet50-1407"><a href="#HATMaskResNet50-1407"><span class="linenos">1407</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="HATMaskResNet50-1408"><a href="#HATMaskResNet50-1408"><span class="linenos">1408</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`.</span>
</span><span id="HATMaskResNet50-1409"><a href="#HATMaskResNet50-1409"><span class="linenos">1409</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HATMaskResNet50-1410"><a href="#HATMaskResNet50-1410"><span class="linenos">1410</span></a>        <span class="n">HATMaskResNetBase</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="HATMaskResNet50-1411"><a href="#HATMaskResNet50-1411"><span class="linenos">1411</span></a>            <span class="bp">self</span><span class="p">,</span>
</span><span id="HATMaskResNet50-1412"><a href="#HATMaskResNet50-1412"><span class="linenos">1412</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
</span><span id="HATMaskResNet50-1413"><a href="#HATMaskResNet50-1413"><span class="linenos">1413</span></a>            <span class="n">building_block_type</span><span class="o">=</span><span class="n">HATMaskResNetBlockLarge</span><span class="p">,</span>  <span class="c1"># use the smaller building block for ResNet-50</span>
</span><span id="HATMaskResNet50-1414"><a href="#HATMaskResNet50-1414"><span class="linenos">1414</span></a>            <span class="n">building_block_nums</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
</span><span id="HATMaskResNet50-1415"><a href="#HATMaskResNet50-1415"><span class="linenos">1415</span></a>            <span class="n">building_block_preceding_output_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span>
</span><span id="HATMaskResNet50-1416"><a href="#HATMaskResNet50-1416"><span class="linenos">1416</span></a>            <span class="n">building_block_input_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
</span><span id="HATMaskResNet50-1417"><a href="#HATMaskResNet50-1417"><span class="linenos">1417</span></a>            <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span>
</span><span id="HATMaskResNet50-1418"><a href="#HATMaskResNet50-1418"><span class="linenos">1418</span></a>            <span class="n">gate</span><span class="o">=</span><span class="n">gate</span><span class="p">,</span>
</span><span id="HATMaskResNet50-1419"><a href="#HATMaskResNet50-1419"><span class="linenos">1419</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="HATMaskResNet50-1420"><a href="#HATMaskResNet50-1420"><span class="linenos">1420</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="HATMaskResNet50-1421"><a href="#HATMaskResNet50-1421"><span class="linenos">1421</span></a>        <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>HAT masked ResNet-50 backbone network.</p>

<p><a href="http://proceedings.mlr.press/v80/serra18a">HAT (Hard Attention to the Task, 2018)</a> is an architecture-based continual learning approach that uses learnable hard attention masks to select the task-specific parameters.</p>

<p>ResNet-50 is a larger architecture proposed in the <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">original ResNet paper</a>. It consists of 50 weight convolutional layers in total. See Table 1 in the paper for details.</p>

<p>Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the unit-wise task embedding and gate function.</p>
</div>


                            <div id="HATMaskResNet50.__init__" class="classattr">
                                        <input id="HATMaskResNet50.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">HATMaskResNet50</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">gate</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	activation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class="param">	<span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span></span>)</span>

                <label class="view-source-button" for="HATMaskResNet50.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HATMaskResNet50.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HATMaskResNet50.__init__-1392"><a href="#HATMaskResNet50.__init__-1392"><span class="linenos">1392</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="HATMaskResNet50.__init__-1393"><a href="#HATMaskResNet50.__init__-1393"><span class="linenos">1393</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="HATMaskResNet50.__init__-1394"><a href="#HATMaskResNet50.__init__-1394"><span class="linenos">1394</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HATMaskResNet50.__init__-1395"><a href="#HATMaskResNet50.__init__-1395"><span class="linenos">1395</span></a>        <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HATMaskResNet50.__init__-1396"><a href="#HATMaskResNet50.__init__-1396"><span class="linenos">1396</span></a>        <span class="n">gate</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="HATMaskResNet50.__init__-1397"><a href="#HATMaskResNet50.__init__-1397"><span class="linenos">1397</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="HATMaskResNet50.__init__-1398"><a href="#HATMaskResNet50.__init__-1398"><span class="linenos">1398</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="HATMaskResNet50.__init__-1399"><a href="#HATMaskResNet50.__init__-1399"><span class="linenos">1399</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HATMaskResNet50.__init__-1400"><a href="#HATMaskResNet50.__init__-1400"><span class="linenos">1400</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the ResNet-50 backbone network with task embedding. Note that batch normalisation is incompatible with HAT mechanism.</span>
</span><span id="HATMaskResNet50.__init__-1401"><a href="#HATMaskResNet50.__init__-1401"><span class="linenos">1401</span></a>
</span><span id="HATMaskResNet50.__init__-1402"><a href="#HATMaskResNet50.__init__-1402"><span class="linenos">1402</span></a><span class="sd">        **Args:**</span>
</span><span id="HATMaskResNet50.__init__-1403"><a href="#HATMaskResNet50.__init__-1403"><span class="linenos">1403</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</span>
</span><span id="HATMaskResNet50.__init__-1404"><a href="#HATMaskResNet50.__init__-1404"><span class="linenos">1404</span></a><span class="sd">        - **output_dim** (`int`): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</span>
</span><span id="HATMaskResNet50.__init__-1405"><a href="#HATMaskResNet50.__init__-1405"><span class="linenos">1405</span></a><span class="sd">        - **gate** (`str`): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:</span>
</span><span id="HATMaskResNet50.__init__-1406"><a href="#HATMaskResNet50.__init__-1406"><span class="linenos">1406</span></a><span class="sd">            - `sigmoid`: the sigmoid function.</span>
</span><span id="HATMaskResNet50.__init__-1407"><a href="#HATMaskResNet50.__init__-1407"><span class="linenos">1407</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="HATMaskResNet50.__init__-1408"><a href="#HATMaskResNet50.__init__-1408"><span class="linenos">1408</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`.</span>
</span><span id="HATMaskResNet50.__init__-1409"><a href="#HATMaskResNet50.__init__-1409"><span class="linenos">1409</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HATMaskResNet50.__init__-1410"><a href="#HATMaskResNet50.__init__-1410"><span class="linenos">1410</span></a>        <span class="n">HATMaskResNetBase</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="HATMaskResNet50.__init__-1411"><a href="#HATMaskResNet50.__init__-1411"><span class="linenos">1411</span></a>            <span class="bp">self</span><span class="p">,</span>
</span><span id="HATMaskResNet50.__init__-1412"><a href="#HATMaskResNet50.__init__-1412"><span class="linenos">1412</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
</span><span id="HATMaskResNet50.__init__-1413"><a href="#HATMaskResNet50.__init__-1413"><span class="linenos">1413</span></a>            <span class="n">building_block_type</span><span class="o">=</span><span class="n">HATMaskResNetBlockLarge</span><span class="p">,</span>  <span class="c1"># use the smaller building block for ResNet-50</span>
</span><span id="HATMaskResNet50.__init__-1414"><a href="#HATMaskResNet50.__init__-1414"><span class="linenos">1414</span></a>            <span class="n">building_block_nums</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
</span><span id="HATMaskResNet50.__init__-1415"><a href="#HATMaskResNet50.__init__-1415"><span class="linenos">1415</span></a>            <span class="n">building_block_preceding_output_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span>
</span><span id="HATMaskResNet50.__init__-1416"><a href="#HATMaskResNet50.__init__-1416"><span class="linenos">1416</span></a>            <span class="n">building_block_input_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
</span><span id="HATMaskResNet50.__init__-1417"><a href="#HATMaskResNet50.__init__-1417"><span class="linenos">1417</span></a>            <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span>
</span><span id="HATMaskResNet50.__init__-1418"><a href="#HATMaskResNet50.__init__-1418"><span class="linenos">1418</span></a>            <span class="n">gate</span><span class="o">=</span><span class="n">gate</span><span class="p">,</span>
</span><span id="HATMaskResNet50.__init__-1419"><a href="#HATMaskResNet50.__init__-1419"><span class="linenos">1419</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="HATMaskResNet50.__init__-1420"><a href="#HATMaskResNet50.__init__-1420"><span class="linenos">1420</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="HATMaskResNet50.__init__-1421"><a href="#HATMaskResNet50.__init__-1421"><span class="linenos">1421</span></a>        <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Construct and initialise the ResNet-50 backbone network with task embedding. Note that batch normalisation is incompatible with HAT mechanism.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</li>
<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>
<li><strong>gate</strong> (<code>str</code>): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:
<ul>
<li><code>sigmoid</code>: the sigmoid function.</li>
</ul></li>
<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>
<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>.</li>
</ul>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt><a href="#HATMaskResNetBase">HATMaskResNetBase</a></dt>
                                <dd id="HATMaskResNet50.update_multiple_blocks_task_embedding" class="function"><a href="#HATMaskResNetBase.update_multiple_blocks_task_embedding">update_multiple_blocks_task_embedding</a></dd>
                <dd id="HATMaskResNet50.forward" class="function"><a href="#HATMaskResNetBase.forward">forward</a></dd>

            </div>
            <div><dt><a href="#ResNetBase">ResNetBase</a></dt>
                                <dd id="HATMaskResNet50.batch_normalisation" class="variable"><a href="#ResNetBase.batch_normalisation">batch_normalisation</a></dd>
                <dd id="HATMaskResNet50.activation" class="variable"><a href="#ResNetBase.activation">activation</a></dd>
                <dd id="HATMaskResNet50.conv1" class="variable"><a href="#ResNetBase.conv1">conv1</a></dd>
                <dd id="HATMaskResNet50.maxpool" class="variable"><a href="#ResNetBase.maxpool">maxpool</a></dd>
                <dd id="HATMaskResNet50.conv2x" class="variable"><a href="#ResNetBase.conv2x">conv2x</a></dd>
                <dd id="HATMaskResNet50.conv3x" class="variable"><a href="#ResNetBase.conv3x">conv3x</a></dd>
                <dd id="HATMaskResNet50.conv4x" class="variable"><a href="#ResNetBase.conv4x">conv4x</a></dd>
                <dd id="HATMaskResNet50.conv5x" class="variable"><a href="#ResNetBase.conv5x">conv5x</a></dd>
                <dd id="HATMaskResNet50.avepool" class="variable"><a href="#ResNetBase.avepool">avepool</a></dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="HATMaskResNet101">
                            <input id="HATMaskResNet101-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">HATMaskResNet101</span><wbr>(<span class="base"><a href="#HATMaskResNetBase">HATMaskResNetBase</a></span>):

                <label class="view-source-button" for="HATMaskResNet101-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HATMaskResNet101"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HATMaskResNet101-1424"><a href="#HATMaskResNet101-1424"><span class="linenos">1424</span></a><span class="k">class</span><span class="w"> </span><span class="nc">HATMaskResNet101</span><span class="p">(</span><span class="n">HATMaskResNetBase</span><span class="p">):</span>
</span><span id="HATMaskResNet101-1425"><a href="#HATMaskResNet101-1425"><span class="linenos">1425</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;HAT masked ResNet-101 backbone network.</span>
</span><span id="HATMaskResNet101-1426"><a href="#HATMaskResNet101-1426"><span class="linenos">1426</span></a>
</span><span id="HATMaskResNet101-1427"><a href="#HATMaskResNet101-1427"><span class="linenos">1427</span></a><span class="sd">    [HAT (Hard Attention to the Task, 2018)](http://proceedings.mlr.press/v80/serra18a) is an architecture-based continual learning approach that uses learnable hard attention masks to select the task-specific parameters.</span>
</span><span id="HATMaskResNet101-1428"><a href="#HATMaskResNet101-1428"><span class="linenos">1428</span></a>
</span><span id="HATMaskResNet101-1429"><a href="#HATMaskResNet101-1429"><span class="linenos">1429</span></a><span class="sd">    ResNet-101 is a larger architecture proposed in the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html). It consists of 101 weight convolutional layers in total. See Table 1 in the paper for details.</span>
</span><span id="HATMaskResNet101-1430"><a href="#HATMaskResNet101-1430"><span class="linenos">1430</span></a>
</span><span id="HATMaskResNet101-1431"><a href="#HATMaskResNet101-1431"><span class="linenos">1431</span></a><span class="sd">    Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the unit-wise task embedding and gate function.</span>
</span><span id="HATMaskResNet101-1432"><a href="#HATMaskResNet101-1432"><span class="linenos">1432</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="HATMaskResNet101-1433"><a href="#HATMaskResNet101-1433"><span class="linenos">1433</span></a>
</span><span id="HATMaskResNet101-1434"><a href="#HATMaskResNet101-1434"><span class="linenos">1434</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="HATMaskResNet101-1435"><a href="#HATMaskResNet101-1435"><span class="linenos">1435</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="HATMaskResNet101-1436"><a href="#HATMaskResNet101-1436"><span class="linenos">1436</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HATMaskResNet101-1437"><a href="#HATMaskResNet101-1437"><span class="linenos">1437</span></a>        <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HATMaskResNet101-1438"><a href="#HATMaskResNet101-1438"><span class="linenos">1438</span></a>        <span class="n">gate</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="HATMaskResNet101-1439"><a href="#HATMaskResNet101-1439"><span class="linenos">1439</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="HATMaskResNet101-1440"><a href="#HATMaskResNet101-1440"><span class="linenos">1440</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="HATMaskResNet101-1441"><a href="#HATMaskResNet101-1441"><span class="linenos">1441</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HATMaskResNet101-1442"><a href="#HATMaskResNet101-1442"><span class="linenos">1442</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the ResNet-101 backbone network with task embedding. Note that batch normalisation is incompatible with HAT mechanism.</span>
</span><span id="HATMaskResNet101-1443"><a href="#HATMaskResNet101-1443"><span class="linenos">1443</span></a>
</span><span id="HATMaskResNet101-1444"><a href="#HATMaskResNet101-1444"><span class="linenos">1444</span></a><span class="sd">        **Args:**</span>
</span><span id="HATMaskResNet101-1445"><a href="#HATMaskResNet101-1445"><span class="linenos">1445</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</span>
</span><span id="HATMaskResNet101-1446"><a href="#HATMaskResNet101-1446"><span class="linenos">1446</span></a><span class="sd">        - **output_dim** (`int`): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</span>
</span><span id="HATMaskResNet101-1447"><a href="#HATMaskResNet101-1447"><span class="linenos">1447</span></a><span class="sd">        - **gate** (`str`): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:</span>
</span><span id="HATMaskResNet101-1448"><a href="#HATMaskResNet101-1448"><span class="linenos">1448</span></a><span class="sd">            - `sigmoid`: the sigmoid function.</span>
</span><span id="HATMaskResNet101-1449"><a href="#HATMaskResNet101-1449"><span class="linenos">1449</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="HATMaskResNet101-1450"><a href="#HATMaskResNet101-1450"><span class="linenos">1450</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`.</span>
</span><span id="HATMaskResNet101-1451"><a href="#HATMaskResNet101-1451"><span class="linenos">1451</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HATMaskResNet101-1452"><a href="#HATMaskResNet101-1452"><span class="linenos">1452</span></a>        <span class="n">HATMaskResNetBase</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="HATMaskResNet101-1453"><a href="#HATMaskResNet101-1453"><span class="linenos">1453</span></a>            <span class="bp">self</span><span class="p">,</span>
</span><span id="HATMaskResNet101-1454"><a href="#HATMaskResNet101-1454"><span class="linenos">1454</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
</span><span id="HATMaskResNet101-1455"><a href="#HATMaskResNet101-1455"><span class="linenos">1455</span></a>            <span class="n">building_block_type</span><span class="o">=</span><span class="n">HATMaskResNetBlockLarge</span><span class="p">,</span>  <span class="c1"># use the smaller building block for ResNet-18</span>
</span><span id="HATMaskResNet101-1456"><a href="#HATMaskResNet101-1456"><span class="linenos">1456</span></a>            <span class="n">building_block_nums</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
</span><span id="HATMaskResNet101-1457"><a href="#HATMaskResNet101-1457"><span class="linenos">1457</span></a>            <span class="n">building_block_preceding_output_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span>
</span><span id="HATMaskResNet101-1458"><a href="#HATMaskResNet101-1458"><span class="linenos">1458</span></a>            <span class="n">building_block_input_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
</span><span id="HATMaskResNet101-1459"><a href="#HATMaskResNet101-1459"><span class="linenos">1459</span></a>            <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span>
</span><span id="HATMaskResNet101-1460"><a href="#HATMaskResNet101-1460"><span class="linenos">1460</span></a>            <span class="n">gate</span><span class="o">=</span><span class="n">gate</span><span class="p">,</span>
</span><span id="HATMaskResNet101-1461"><a href="#HATMaskResNet101-1461"><span class="linenos">1461</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="HATMaskResNet101-1462"><a href="#HATMaskResNet101-1462"><span class="linenos">1462</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="HATMaskResNet101-1463"><a href="#HATMaskResNet101-1463"><span class="linenos">1463</span></a>        <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>HAT masked ResNet-101 backbone network.</p>

<p><a href="http://proceedings.mlr.press/v80/serra18a">HAT (Hard Attention to the Task, 2018)</a> is an architecture-based continual learning approach that uses learnable hard attention masks to select the task-specific parameters.</p>

<p>ResNet-101 is a larger architecture proposed in the <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">original ResNet paper</a>. It consists of 101 weight convolutional layers in total. See Table 1 in the paper for details.</p>

<p>Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the unit-wise task embedding and gate function.</p>
</div>


                            <div id="HATMaskResNet101.__init__" class="classattr">
                                        <input id="HATMaskResNet101.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">HATMaskResNet101</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">gate</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	activation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class="param">	<span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span></span>)</span>

                <label class="view-source-button" for="HATMaskResNet101.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HATMaskResNet101.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HATMaskResNet101.__init__-1434"><a href="#HATMaskResNet101.__init__-1434"><span class="linenos">1434</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="HATMaskResNet101.__init__-1435"><a href="#HATMaskResNet101.__init__-1435"><span class="linenos">1435</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="HATMaskResNet101.__init__-1436"><a href="#HATMaskResNet101.__init__-1436"><span class="linenos">1436</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HATMaskResNet101.__init__-1437"><a href="#HATMaskResNet101.__init__-1437"><span class="linenos">1437</span></a>        <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HATMaskResNet101.__init__-1438"><a href="#HATMaskResNet101.__init__-1438"><span class="linenos">1438</span></a>        <span class="n">gate</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="HATMaskResNet101.__init__-1439"><a href="#HATMaskResNet101.__init__-1439"><span class="linenos">1439</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="HATMaskResNet101.__init__-1440"><a href="#HATMaskResNet101.__init__-1440"><span class="linenos">1440</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="HATMaskResNet101.__init__-1441"><a href="#HATMaskResNet101.__init__-1441"><span class="linenos">1441</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HATMaskResNet101.__init__-1442"><a href="#HATMaskResNet101.__init__-1442"><span class="linenos">1442</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the ResNet-101 backbone network with task embedding. Note that batch normalisation is incompatible with HAT mechanism.</span>
</span><span id="HATMaskResNet101.__init__-1443"><a href="#HATMaskResNet101.__init__-1443"><span class="linenos">1443</span></a>
</span><span id="HATMaskResNet101.__init__-1444"><a href="#HATMaskResNet101.__init__-1444"><span class="linenos">1444</span></a><span class="sd">        **Args:**</span>
</span><span id="HATMaskResNet101.__init__-1445"><a href="#HATMaskResNet101.__init__-1445"><span class="linenos">1445</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</span>
</span><span id="HATMaskResNet101.__init__-1446"><a href="#HATMaskResNet101.__init__-1446"><span class="linenos">1446</span></a><span class="sd">        - **output_dim** (`int`): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</span>
</span><span id="HATMaskResNet101.__init__-1447"><a href="#HATMaskResNet101.__init__-1447"><span class="linenos">1447</span></a><span class="sd">        - **gate** (`str`): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:</span>
</span><span id="HATMaskResNet101.__init__-1448"><a href="#HATMaskResNet101.__init__-1448"><span class="linenos">1448</span></a><span class="sd">            - `sigmoid`: the sigmoid function.</span>
</span><span id="HATMaskResNet101.__init__-1449"><a href="#HATMaskResNet101.__init__-1449"><span class="linenos">1449</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="HATMaskResNet101.__init__-1450"><a href="#HATMaskResNet101.__init__-1450"><span class="linenos">1450</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`.</span>
</span><span id="HATMaskResNet101.__init__-1451"><a href="#HATMaskResNet101.__init__-1451"><span class="linenos">1451</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HATMaskResNet101.__init__-1452"><a href="#HATMaskResNet101.__init__-1452"><span class="linenos">1452</span></a>        <span class="n">HATMaskResNetBase</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="HATMaskResNet101.__init__-1453"><a href="#HATMaskResNet101.__init__-1453"><span class="linenos">1453</span></a>            <span class="bp">self</span><span class="p">,</span>
</span><span id="HATMaskResNet101.__init__-1454"><a href="#HATMaskResNet101.__init__-1454"><span class="linenos">1454</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
</span><span id="HATMaskResNet101.__init__-1455"><a href="#HATMaskResNet101.__init__-1455"><span class="linenos">1455</span></a>            <span class="n">building_block_type</span><span class="o">=</span><span class="n">HATMaskResNetBlockLarge</span><span class="p">,</span>  <span class="c1"># use the smaller building block for ResNet-18</span>
</span><span id="HATMaskResNet101.__init__-1456"><a href="#HATMaskResNet101.__init__-1456"><span class="linenos">1456</span></a>            <span class="n">building_block_nums</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
</span><span id="HATMaskResNet101.__init__-1457"><a href="#HATMaskResNet101.__init__-1457"><span class="linenos">1457</span></a>            <span class="n">building_block_preceding_output_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span>
</span><span id="HATMaskResNet101.__init__-1458"><a href="#HATMaskResNet101.__init__-1458"><span class="linenos">1458</span></a>            <span class="n">building_block_input_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
</span><span id="HATMaskResNet101.__init__-1459"><a href="#HATMaskResNet101.__init__-1459"><span class="linenos">1459</span></a>            <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span>
</span><span id="HATMaskResNet101.__init__-1460"><a href="#HATMaskResNet101.__init__-1460"><span class="linenos">1460</span></a>            <span class="n">gate</span><span class="o">=</span><span class="n">gate</span><span class="p">,</span>
</span><span id="HATMaskResNet101.__init__-1461"><a href="#HATMaskResNet101.__init__-1461"><span class="linenos">1461</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="HATMaskResNet101.__init__-1462"><a href="#HATMaskResNet101.__init__-1462"><span class="linenos">1462</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="HATMaskResNet101.__init__-1463"><a href="#HATMaskResNet101.__init__-1463"><span class="linenos">1463</span></a>        <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Construct and initialise the ResNet-101 backbone network with task embedding. Note that batch normalisation is incompatible with HAT mechanism.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</li>
<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>
<li><strong>gate</strong> (<code>str</code>): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:
<ul>
<li><code>sigmoid</code>: the sigmoid function.</li>
</ul></li>
<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>
<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>.</li>
</ul>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt><a href="#HATMaskResNetBase">HATMaskResNetBase</a></dt>
                                <dd id="HATMaskResNet101.update_multiple_blocks_task_embedding" class="function"><a href="#HATMaskResNetBase.update_multiple_blocks_task_embedding">update_multiple_blocks_task_embedding</a></dd>
                <dd id="HATMaskResNet101.forward" class="function"><a href="#HATMaskResNetBase.forward">forward</a></dd>

            </div>
            <div><dt><a href="#ResNetBase">ResNetBase</a></dt>
                                <dd id="HATMaskResNet101.batch_normalisation" class="variable"><a href="#ResNetBase.batch_normalisation">batch_normalisation</a></dd>
                <dd id="HATMaskResNet101.activation" class="variable"><a href="#ResNetBase.activation">activation</a></dd>
                <dd id="HATMaskResNet101.conv1" class="variable"><a href="#ResNetBase.conv1">conv1</a></dd>
                <dd id="HATMaskResNet101.maxpool" class="variable"><a href="#ResNetBase.maxpool">maxpool</a></dd>
                <dd id="HATMaskResNet101.conv2x" class="variable"><a href="#ResNetBase.conv2x">conv2x</a></dd>
                <dd id="HATMaskResNet101.conv3x" class="variable"><a href="#ResNetBase.conv3x">conv3x</a></dd>
                <dd id="HATMaskResNet101.conv4x" class="variable"><a href="#ResNetBase.conv4x">conv4x</a></dd>
                <dd id="HATMaskResNet101.conv5x" class="variable"><a href="#ResNetBase.conv5x">conv5x</a></dd>
                <dd id="HATMaskResNet101.avepool" class="variable"><a href="#ResNetBase.avepool">avepool</a></dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="HATMaskResNet152">
                            <input id="HATMaskResNet152-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">HATMaskResNet152</span><wbr>(<span class="base"><a href="#HATMaskResNetBase">HATMaskResNetBase</a></span>):

                <label class="view-source-button" for="HATMaskResNet152-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HATMaskResNet152"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HATMaskResNet152-1466"><a href="#HATMaskResNet152-1466"><span class="linenos">1466</span></a><span class="k">class</span><span class="w"> </span><span class="nc">HATMaskResNet152</span><span class="p">(</span><span class="n">HATMaskResNetBase</span><span class="p">):</span>
</span><span id="HATMaskResNet152-1467"><a href="#HATMaskResNet152-1467"><span class="linenos">1467</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;HAT masked ResNet-152 backbone network.</span>
</span><span id="HATMaskResNet152-1468"><a href="#HATMaskResNet152-1468"><span class="linenos">1468</span></a>
</span><span id="HATMaskResNet152-1469"><a href="#HATMaskResNet152-1469"><span class="linenos">1469</span></a><span class="sd">    [HAT (Hard Attention to the Task, 2018)](http://proceedings.mlr.press/v80/serra18a) is an architecture-based continual learning approach that uses learnable hard attention masks to select the task-specific parameters.</span>
</span><span id="HATMaskResNet152-1470"><a href="#HATMaskResNet152-1470"><span class="linenos">1470</span></a>
</span><span id="HATMaskResNet152-1471"><a href="#HATMaskResNet152-1471"><span class="linenos">1471</span></a><span class="sd">    ResNet-152 is the largest architecture proposed in the [original ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html). It consists of 152 weight convolutional layers in total. See Table 1 in the paper for details.</span>
</span><span id="HATMaskResNet152-1472"><a href="#HATMaskResNet152-1472"><span class="linenos">1472</span></a>
</span><span id="HATMaskResNet152-1473"><a href="#HATMaskResNet152-1473"><span class="linenos">1473</span></a><span class="sd">    Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the unit-wise task embedding and gate function.</span>
</span><span id="HATMaskResNet152-1474"><a href="#HATMaskResNet152-1474"><span class="linenos">1474</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="HATMaskResNet152-1475"><a href="#HATMaskResNet152-1475"><span class="linenos">1475</span></a>
</span><span id="HATMaskResNet152-1476"><a href="#HATMaskResNet152-1476"><span class="linenos">1476</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="HATMaskResNet152-1477"><a href="#HATMaskResNet152-1477"><span class="linenos">1477</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="HATMaskResNet152-1478"><a href="#HATMaskResNet152-1478"><span class="linenos">1478</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HATMaskResNet152-1479"><a href="#HATMaskResNet152-1479"><span class="linenos">1479</span></a>        <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HATMaskResNet152-1480"><a href="#HATMaskResNet152-1480"><span class="linenos">1480</span></a>        <span class="n">gate</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="HATMaskResNet152-1481"><a href="#HATMaskResNet152-1481"><span class="linenos">1481</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="HATMaskResNet152-1482"><a href="#HATMaskResNet152-1482"><span class="linenos">1482</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="HATMaskResNet152-1483"><a href="#HATMaskResNet152-1483"><span class="linenos">1483</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HATMaskResNet152-1484"><a href="#HATMaskResNet152-1484"><span class="linenos">1484</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the ResNet-152 backbone network with task embedding. Note that batch normalisation is incompatible with HAT mechanism.</span>
</span><span id="HATMaskResNet152-1485"><a href="#HATMaskResNet152-1485"><span class="linenos">1485</span></a>
</span><span id="HATMaskResNet152-1486"><a href="#HATMaskResNet152-1486"><span class="linenos">1486</span></a><span class="sd">        **Args:**</span>
</span><span id="HATMaskResNet152-1487"><a href="#HATMaskResNet152-1487"><span class="linenos">1487</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</span>
</span><span id="HATMaskResNet152-1488"><a href="#HATMaskResNet152-1488"><span class="linenos">1488</span></a><span class="sd">        - **output_dim** (`int`): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</span>
</span><span id="HATMaskResNet152-1489"><a href="#HATMaskResNet152-1489"><span class="linenos">1489</span></a><span class="sd">        - **gate** (`str`): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:</span>
</span><span id="HATMaskResNet152-1490"><a href="#HATMaskResNet152-1490"><span class="linenos">1490</span></a><span class="sd">            - `sigmoid`: the sigmoid function.</span>
</span><span id="HATMaskResNet152-1491"><a href="#HATMaskResNet152-1491"><span class="linenos">1491</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="HATMaskResNet152-1492"><a href="#HATMaskResNet152-1492"><span class="linenos">1492</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`.</span>
</span><span id="HATMaskResNet152-1493"><a href="#HATMaskResNet152-1493"><span class="linenos">1493</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HATMaskResNet152-1494"><a href="#HATMaskResNet152-1494"><span class="linenos">1494</span></a>        <span class="n">HATMaskResNetBase</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="HATMaskResNet152-1495"><a href="#HATMaskResNet152-1495"><span class="linenos">1495</span></a>            <span class="bp">self</span><span class="p">,</span>
</span><span id="HATMaskResNet152-1496"><a href="#HATMaskResNet152-1496"><span class="linenos">1496</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
</span><span id="HATMaskResNet152-1497"><a href="#HATMaskResNet152-1497"><span class="linenos">1497</span></a>            <span class="n">building_block_type</span><span class="o">=</span><span class="n">HATMaskResNetBlockLarge</span><span class="p">,</span>  <span class="c1"># use the smaller building block for ResNet-152</span>
</span><span id="HATMaskResNet152-1498"><a href="#HATMaskResNet152-1498"><span class="linenos">1498</span></a>            <span class="n">building_block_nums</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">36</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
</span><span id="HATMaskResNet152-1499"><a href="#HATMaskResNet152-1499"><span class="linenos">1499</span></a>            <span class="n">building_block_preceding_output_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span>
</span><span id="HATMaskResNet152-1500"><a href="#HATMaskResNet152-1500"><span class="linenos">1500</span></a>            <span class="n">building_block_input_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
</span><span id="HATMaskResNet152-1501"><a href="#HATMaskResNet152-1501"><span class="linenos">1501</span></a>            <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span>
</span><span id="HATMaskResNet152-1502"><a href="#HATMaskResNet152-1502"><span class="linenos">1502</span></a>            <span class="n">gate</span><span class="o">=</span><span class="n">gate</span><span class="p">,</span>
</span><span id="HATMaskResNet152-1503"><a href="#HATMaskResNet152-1503"><span class="linenos">1503</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="HATMaskResNet152-1504"><a href="#HATMaskResNet152-1504"><span class="linenos">1504</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="HATMaskResNet152-1505"><a href="#HATMaskResNet152-1505"><span class="linenos">1505</span></a>        <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>HAT masked ResNet-152 backbone network.</p>

<p><a href="http://proceedings.mlr.press/v80/serra18a">HAT (Hard Attention to the Task, 2018)</a> is an architecture-based continual learning approach that uses learnable hard attention masks to select the task-specific parameters.</p>

<p>ResNet-152 is the largest architecture proposed in the <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">original ResNet paper</a>. It consists of 152 weight convolutional layers in total. See Table 1 in the paper for details.</p>

<p>Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the unit-wise task embedding and gate function.</p>
</div>


                            <div id="HATMaskResNet152.__init__" class="classattr">
                                        <input id="HATMaskResNet152.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">HATMaskResNet152</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">gate</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	activation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class="param">	<span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span></span>)</span>

                <label class="view-source-button" for="HATMaskResNet152.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HATMaskResNet152.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HATMaskResNet152.__init__-1476"><a href="#HATMaskResNet152.__init__-1476"><span class="linenos">1476</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="HATMaskResNet152.__init__-1477"><a href="#HATMaskResNet152.__init__-1477"><span class="linenos">1477</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="HATMaskResNet152.__init__-1478"><a href="#HATMaskResNet152.__init__-1478"><span class="linenos">1478</span></a>        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HATMaskResNet152.__init__-1479"><a href="#HATMaskResNet152.__init__-1479"><span class="linenos">1479</span></a>        <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="HATMaskResNet152.__init__-1480"><a href="#HATMaskResNet152.__init__-1480"><span class="linenos">1480</span></a>        <span class="n">gate</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="HATMaskResNet152.__init__-1481"><a href="#HATMaskResNet152.__init__-1481"><span class="linenos">1481</span></a>        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
</span><span id="HATMaskResNet152.__init__-1482"><a href="#HATMaskResNet152.__init__-1482"><span class="linenos">1482</span></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="HATMaskResNet152.__init__-1483"><a href="#HATMaskResNet152.__init__-1483"><span class="linenos">1483</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HATMaskResNet152.__init__-1484"><a href="#HATMaskResNet152.__init__-1484"><span class="linenos">1484</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct and initialise the ResNet-152 backbone network with task embedding. Note that batch normalisation is incompatible with HAT mechanism.</span>
</span><span id="HATMaskResNet152.__init__-1485"><a href="#HATMaskResNet152.__init__-1485"><span class="linenos">1485</span></a>
</span><span id="HATMaskResNet152.__init__-1486"><a href="#HATMaskResNet152.__init__-1486"><span class="linenos">1486</span></a><span class="sd">        **Args:**</span>
</span><span id="HATMaskResNet152.__init__-1487"><a href="#HATMaskResNet152.__init__-1487"><span class="linenos">1487</span></a><span class="sd">        - **input_channels** (`int`): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</span>
</span><span id="HATMaskResNet152.__init__-1488"><a href="#HATMaskResNet152.__init__-1488"><span class="linenos">1488</span></a><span class="sd">        - **output_dim** (`int`): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</span>
</span><span id="HATMaskResNet152.__init__-1489"><a href="#HATMaskResNet152.__init__-1489"><span class="linenos">1489</span></a><span class="sd">        - **gate** (`str`): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:</span>
</span><span id="HATMaskResNet152.__init__-1490"><a href="#HATMaskResNet152.__init__-1490"><span class="linenos">1490</span></a><span class="sd">            - `sigmoid`: the sigmoid function.</span>
</span><span id="HATMaskResNet152.__init__-1491"><a href="#HATMaskResNet152.__init__-1491"><span class="linenos">1491</span></a><span class="sd">        - **activation_layer** (`nn.Module`): activation function of each layer (if not `None`), if `None` this layer won&#39;t be used. Default `nn.ReLU`.</span>
</span><span id="HATMaskResNet152.__init__-1492"><a href="#HATMaskResNet152.__init__-1492"><span class="linenos">1492</span></a><span class="sd">        - **bias** (`bool`): whether to use bias in the convolutional layer. Default `False`.</span>
</span><span id="HATMaskResNet152.__init__-1493"><a href="#HATMaskResNet152.__init__-1493"><span class="linenos">1493</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HATMaskResNet152.__init__-1494"><a href="#HATMaskResNet152.__init__-1494"><span class="linenos">1494</span></a>        <span class="n">HATMaskResNetBase</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="HATMaskResNet152.__init__-1495"><a href="#HATMaskResNet152.__init__-1495"><span class="linenos">1495</span></a>            <span class="bp">self</span><span class="p">,</span>
</span><span id="HATMaskResNet152.__init__-1496"><a href="#HATMaskResNet152.__init__-1496"><span class="linenos">1496</span></a>            <span class="n">input_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
</span><span id="HATMaskResNet152.__init__-1497"><a href="#HATMaskResNet152.__init__-1497"><span class="linenos">1497</span></a>            <span class="n">building_block_type</span><span class="o">=</span><span class="n">HATMaskResNetBlockLarge</span><span class="p">,</span>  <span class="c1"># use the smaller building block for ResNet-152</span>
</span><span id="HATMaskResNet152.__init__-1498"><a href="#HATMaskResNet152.__init__-1498"><span class="linenos">1498</span></a>            <span class="n">building_block_nums</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">36</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
</span><span id="HATMaskResNet152.__init__-1499"><a href="#HATMaskResNet152.__init__-1499"><span class="linenos">1499</span></a>            <span class="n">building_block_preceding_output_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span>
</span><span id="HATMaskResNet152.__init__-1500"><a href="#HATMaskResNet152.__init__-1500"><span class="linenos">1500</span></a>            <span class="n">building_block_input_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
</span><span id="HATMaskResNet152.__init__-1501"><a href="#HATMaskResNet152.__init__-1501"><span class="linenos">1501</span></a>            <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span>
</span><span id="HATMaskResNet152.__init__-1502"><a href="#HATMaskResNet152.__init__-1502"><span class="linenos">1502</span></a>            <span class="n">gate</span><span class="o">=</span><span class="n">gate</span><span class="p">,</span>
</span><span id="HATMaskResNet152.__init__-1503"><a href="#HATMaskResNet152.__init__-1503"><span class="linenos">1503</span></a>            <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
</span><span id="HATMaskResNet152.__init__-1504"><a href="#HATMaskResNet152.__init__-1504"><span class="linenos">1504</span></a>            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
</span><span id="HATMaskResNet152.__init__-1505"><a href="#HATMaskResNet152.__init__-1505"><span class="linenos">1505</span></a>        <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Construct and initialise the ResNet-152 backbone network with task embedding. Note that batch normalisation is incompatible with HAT mechanism.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</li>
<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>
<li><strong>gate</strong> (<code>str</code>): the type of gate function turning the real value task embeddings into attention masks, should be one of the following:
<ul>
<li><code>sigmoid</code>: the sigmoid function.</li>
</ul></li>
<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>
<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>.</li>
</ul>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt><a href="#HATMaskResNetBase">HATMaskResNetBase</a></dt>
                                <dd id="HATMaskResNet152.update_multiple_blocks_task_embedding" class="function"><a href="#HATMaskResNetBase.update_multiple_blocks_task_embedding">update_multiple_blocks_task_embedding</a></dd>
                <dd id="HATMaskResNet152.forward" class="function"><a href="#HATMaskResNetBase.forward">forward</a></dd>

            </div>
            <div><dt><a href="#ResNetBase">ResNetBase</a></dt>
                                <dd id="HATMaskResNet152.batch_normalisation" class="variable"><a href="#ResNetBase.batch_normalisation">batch_normalisation</a></dd>
                <dd id="HATMaskResNet152.activation" class="variable"><a href="#ResNetBase.activation">activation</a></dd>
                <dd id="HATMaskResNet152.conv1" class="variable"><a href="#ResNetBase.conv1">conv1</a></dd>
                <dd id="HATMaskResNet152.maxpool" class="variable"><a href="#ResNetBase.maxpool">maxpool</a></dd>
                <dd id="HATMaskResNet152.conv2x" class="variable"><a href="#ResNetBase.conv2x">conv2x</a></dd>
                <dd id="HATMaskResNet152.conv3x" class="variable"><a href="#ResNetBase.conv3x">conv3x</a></dd>
                <dd id="HATMaskResNet152.conv4x" class="variable"><a href="#ResNetBase.conv4x">conv4x</a></dd>
                <dd id="HATMaskResNet152.conv5x" class="variable"><a href="#ResNetBase.conv5x">conv5x</a></dd>
                <dd id="HATMaskResNet152.avepool" class="variable"><a href="#ResNetBase.avepool">avepool</a></dd>

            </div>
                                </dl>
                            </div>
                </section>
    </main>
<script>
    function escapeHTML(html) {
        return document.createElement('div').appendChild(document.createTextNode(html)).parentNode.innerHTML;
    }

    const originalContent = document.querySelector("main.pdoc");
    let currentContent = originalContent;

    function setContent(innerHTML) {
        let elem;
        if (innerHTML) {
            elem = document.createElement("main");
            elem.classList.add("pdoc");
            elem.innerHTML = innerHTML;
        } else {
            elem = originalContent;
        }
        if (currentContent !== elem) {
            currentContent.replaceWith(elem);
            currentContent = elem;
        }
    }

    function getSearchTerm() {
        return (new URL(window.location)).searchParams.get("search");
    }

    const searchBox = document.querySelector(".pdoc input[type=search]");
    searchBox.addEventListener("input", function () {
        let url = new URL(window.location);
        if (searchBox.value.trim()) {
            url.hash = "";
            url.searchParams.set("search", searchBox.value);
        } else {
            url.searchParams.delete("search");
        }
        history.replaceState("", "", url.toString());
        onInput();
    });
    window.addEventListener("popstate", onInput);


    let search, searchErr;

    async function initialize() {
        try {
            search = await new Promise((resolve, reject) => {
                const script = document.createElement("script");
                script.type = "text/javascript";
                script.async = true;
                script.onload = () => resolve(window.pdocSearch);
                script.onerror = (e) => reject(e);
                script.src = "../../search.js";
                document.getElementsByTagName("head")[0].appendChild(script);
            });
        } catch (e) {
            console.error("Cannot fetch pdoc search index");
            searchErr = "Cannot fetch search index.";
        }
        onInput();

        document.querySelector("nav.pdoc").addEventListener("click", e => {
            if (e.target.hash) {
                searchBox.value = "";
                searchBox.dispatchEvent(new Event("input"));
            }
        });
    }

    function onInput() {
        setContent((() => {
            const term = getSearchTerm();
            if (!term) {
                return null
            }
            if (searchErr) {
                return `<h3>Error: ${searchErr}</h3>`
            }
            if (!search) {
                return "<h3>Searching...</h3>"
            }

            window.scrollTo({top: 0, left: 0, behavior: 'auto'});

            const results = search(term);

            let html;
            if (results.length === 0) {
                html = `No search results for '${escapeHTML(term)}'.`
            } else {
                html = `<h4>${results.length} search result${results.length > 1 ? "s" : ""} for '${escapeHTML(term)}'.</h4>`;
            }
            for (let result of results.slice(0, 10)) {
                let doc = result.doc;
                let url = `../../${doc.modulename.replaceAll(".", "/")}.html`;
                if (doc.qualname) {
                    url += `#${doc.qualname}`;
                }

                let heading;
                switch (result.doc.kind) {
                    case "function":
                        if (doc.fullname.endsWith(".__init__")) {
                            heading = `<span class="name">${doc.fullname.replace(/\.__init__$/, "")}</span>${doc.signature}`;
                        } else {
                            heading = `<span class="def">${doc.funcdef}</span> <span class="name">${doc.fullname}</span>${doc.signature}`;
                        }
                        break;
                    case "class":
                        heading = `<span class="def">class</span> <span class="name">${doc.fullname}</span>`;
                        if (doc.bases)
                            heading += `<wbr>(<span class="base">${doc.bases}</span>)`;
                        heading += `:`;
                        break;
                    case "variable":
                        heading = `<span class="name">${doc.fullname}</span>`;
                        if (doc.annotation)
                            heading += `<span class="annotation">${doc.annotation}</span>`;
                        if (doc.default_value)
                            heading += `<span class="default_value"> = ${doc.default_value}</span>`;
                        break;
                    default:
                        heading = `<span class="name">${doc.fullname}</span>`;
                        break;
                }
                html += `
                        <section class="search-result">
                        <a href="${url}" class="attr ${doc.kind}">${heading}</a>
                        <div class="docstring">${doc.doc}</div>
                        </section>
                    `;

            }
            return html;
        })());
    }

    if (getSearchTerm()) {
        initialize();
        searchBox.value = getSearchTerm();
        onInput();
    } else {
        searchBox.addEventListener("focus", initialize, {once: true});
    }

    searchBox.addEventListener("keydown", e => {
        if (["ArrowDown", "ArrowUp", "Enter"].includes(e.key)) {
            let focused = currentContent.querySelector(".search-result.focused");
            if (!focused) {
                currentContent.querySelector(".search-result").classList.add("focused");
            } else if (
                e.key === "ArrowDown"
                && focused.nextElementSibling
                && focused.nextElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.nextElementSibling.classList.add("focused");
                focused.nextElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "ArrowUp"
                && focused.previousElementSibling
                && focused.previousElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.previousElementSibling.classList.add("focused");
                focused.previousElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "Enter"
            ) {
                focused.querySelector("a").click();
            }
        }
    });
</script></body>
</html>