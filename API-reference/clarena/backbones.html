<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="pdoc 15.0.1"/>
    <title>clarena.backbones API documentation</title>

    <style>/*! * Bootstrap Reboot v5.0.0 (https://getbootstrap.com/) * Copyright 2011-2021 The Bootstrap Authors * Copyright 2011-2021 Twitter, Inc. * Licensed under MIT (https://github.com/twbs/bootstrap/blob/main/LICENSE) * Forked from Normalize.css, licensed MIT (https://github.com/necolas/normalize.css/blob/master/LICENSE.md) */*,::after,::before{box-sizing:border-box}@media (prefers-reduced-motion:no-preference){:root{scroll-behavior:smooth}}body{margin:0;font-family:system-ui,-apple-system,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans","Liberation Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";font-size:1rem;font-weight:400;line-height:1.5;color:#212529;background-color:#fff;-webkit-text-size-adjust:100%;-webkit-tap-highlight-color:transparent}hr{margin:1rem 0;color:inherit;background-color:currentColor;border:0;opacity:.25}hr:not([size]){height:1px}h1,h2,h3,h4,h5,h6{margin-top:0;margin-bottom:.5rem;font-weight:500;line-height:1.2}h1{font-size:calc(1.375rem + 1.5vw)}@media (min-width:1200px){h1{font-size:2.5rem}}h2{font-size:calc(1.325rem + .9vw)}@media (min-width:1200px){h2{font-size:2rem}}h3{font-size:calc(1.3rem + .6vw)}@media (min-width:1200px){h3{font-size:1.75rem}}h4{font-size:calc(1.275rem + .3vw)}@media (min-width:1200px){h4{font-size:1.5rem}}h5{font-size:1.25rem}h6{font-size:1rem}p{margin-top:0;margin-bottom:1rem}abbr[data-bs-original-title],abbr[title]{-webkit-text-decoration:underline dotted;text-decoration:underline dotted;cursor:help;-webkit-text-decoration-skip-ink:none;text-decoration-skip-ink:none}address{margin-bottom:1rem;font-style:normal;line-height:inherit}ol,ul{padding-left:2rem}dl,ol,ul{margin-top:0;margin-bottom:1rem}ol ol,ol ul,ul ol,ul ul{margin-bottom:0}dt{font-weight:700}dd{margin-bottom:.5rem;margin-left:0}blockquote{margin:0 0 1rem}b,strong{font-weight:bolder}small{font-size:.875em}mark{padding:.2em;background-color:#fcf8e3}sub,sup{position:relative;font-size:.75em;line-height:0;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}a{color:#0d6efd;text-decoration:underline}a:hover{color:#0a58ca}a:not([href]):not([class]),a:not([href]):not([class]):hover{color:inherit;text-decoration:none}code,kbd,pre,samp{font-family:SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;font-size:1em;direction:ltr;unicode-bidi:bidi-override}pre{display:block;margin-top:0;margin-bottom:1rem;overflow:auto;font-size:.875em}pre code{font-size:inherit;color:inherit;word-break:normal}code{font-size:.875em;color:#d63384;word-wrap:break-word}a>code{color:inherit}kbd{padding:.2rem .4rem;font-size:.875em;color:#fff;background-color:#212529;border-radius:.2rem}kbd kbd{padding:0;font-size:1em;font-weight:700}figure{margin:0 0 1rem}img,svg{vertical-align:middle}table{caption-side:bottom;border-collapse:collapse}caption{padding-top:.5rem;padding-bottom:.5rem;color:#6c757d;text-align:left}th{text-align:inherit;text-align:-webkit-match-parent}tbody,td,tfoot,th,thead,tr{border-color:inherit;border-style:solid;border-width:0}label{display:inline-block}button{border-radius:0}button:focus:not(:focus-visible){outline:0}button,input,optgroup,select,textarea{margin:0;font-family:inherit;font-size:inherit;line-height:inherit}button,select{text-transform:none}[role=button]{cursor:pointer}select{word-wrap:normal}select:disabled{opacity:1}[list]::-webkit-calendar-picker-indicator{display:none}[type=button],[type=reset],[type=submit],button{-webkit-appearance:button}[type=button]:not(:disabled),[type=reset]:not(:disabled),[type=submit]:not(:disabled),button:not(:disabled){cursor:pointer}::-moz-focus-inner{padding:0;border-style:none}textarea{resize:vertical}fieldset{min-width:0;padding:0;margin:0;border:0}legend{float:left;width:100%;padding:0;margin-bottom:.5rem;font-size:calc(1.275rem + .3vw);line-height:inherit}@media (min-width:1200px){legend{font-size:1.5rem}}legend+*{clear:left}::-webkit-datetime-edit-day-field,::-webkit-datetime-edit-fields-wrapper,::-webkit-datetime-edit-hour-field,::-webkit-datetime-edit-minute,::-webkit-datetime-edit-month-field,::-webkit-datetime-edit-text,::-webkit-datetime-edit-year-field{padding:0}::-webkit-inner-spin-button{height:auto}[type=search]{outline-offset:-2px;-webkit-appearance:textfield}::-webkit-search-decoration{-webkit-appearance:none}::-webkit-color-swatch-wrapper{padding:0}::file-selector-button{font:inherit}::-webkit-file-upload-button{font:inherit;-webkit-appearance:button}output{display:inline-block}iframe{border:0}summary{display:list-item;cursor:pointer}progress{vertical-align:baseline}[hidden]{display:none!important}</style>
    <style>/*! syntax-highlighting.css */pre{line-height:125%;}span.linenos{color:inherit; background-color:transparent; padding-left:5px; padding-right:20px;}.pdoc-code .hll{background-color:#ffffcc}.pdoc-code{background:#f8f8f8;}.pdoc-code .c{color:#3D7B7B; font-style:italic}.pdoc-code .err{border:1px solid #FF0000}.pdoc-code .k{color:#008000; font-weight:bold}.pdoc-code .o{color:#666666}.pdoc-code .ch{color:#3D7B7B; font-style:italic}.pdoc-code .cm{color:#3D7B7B; font-style:italic}.pdoc-code .cp{color:#9C6500}.pdoc-code .cpf{color:#3D7B7B; font-style:italic}.pdoc-code .c1{color:#3D7B7B; font-style:italic}.pdoc-code .cs{color:#3D7B7B; font-style:italic}.pdoc-code .gd{color:#A00000}.pdoc-code .ge{font-style:italic}.pdoc-code .gr{color:#E40000}.pdoc-code .gh{color:#000080; font-weight:bold}.pdoc-code .gi{color:#008400}.pdoc-code .go{color:#717171}.pdoc-code .gp{color:#000080; font-weight:bold}.pdoc-code .gs{font-weight:bold}.pdoc-code .gu{color:#800080; font-weight:bold}.pdoc-code .gt{color:#0044DD}.pdoc-code .kc{color:#008000; font-weight:bold}.pdoc-code .kd{color:#008000; font-weight:bold}.pdoc-code .kn{color:#008000; font-weight:bold}.pdoc-code .kp{color:#008000}.pdoc-code .kr{color:#008000; font-weight:bold}.pdoc-code .kt{color:#B00040}.pdoc-code .m{color:#666666}.pdoc-code .s{color:#BA2121}.pdoc-code .na{color:#687822}.pdoc-code .nb{color:#008000}.pdoc-code .nc{color:#0000FF; font-weight:bold}.pdoc-code .no{color:#880000}.pdoc-code .nd{color:#AA22FF}.pdoc-code .ni{color:#717171; font-weight:bold}.pdoc-code .ne{color:#CB3F38; font-weight:bold}.pdoc-code .nf{color:#0000FF}.pdoc-code .nl{color:#767600}.pdoc-code .nn{color:#0000FF; font-weight:bold}.pdoc-code .nt{color:#008000; font-weight:bold}.pdoc-code .nv{color:#19177C}.pdoc-code .ow{color:#AA22FF; font-weight:bold}.pdoc-code .w{color:#bbbbbb}.pdoc-code .mb{color:#666666}.pdoc-code .mf{color:#666666}.pdoc-code .mh{color:#666666}.pdoc-code .mi{color:#666666}.pdoc-code .mo{color:#666666}.pdoc-code .sa{color:#BA2121}.pdoc-code .sb{color:#BA2121}.pdoc-code .sc{color:#BA2121}.pdoc-code .dl{color:#BA2121}.pdoc-code .sd{color:#BA2121; font-style:italic}.pdoc-code .s2{color:#BA2121}.pdoc-code .se{color:#AA5D1F; font-weight:bold}.pdoc-code .sh{color:#BA2121}.pdoc-code .si{color:#A45A77; font-weight:bold}.pdoc-code .sx{color:#008000}.pdoc-code .sr{color:#A45A77}.pdoc-code .s1{color:#BA2121}.pdoc-code .ss{color:#19177C}.pdoc-code .bp{color:#008000}.pdoc-code .fm{color:#0000FF}.pdoc-code .vc{color:#19177C}.pdoc-code .vg{color:#19177C}.pdoc-code .vi{color:#19177C}.pdoc-code .vm{color:#19177C}.pdoc-code .il{color:#666666}</style>
    <style>/*! theme.css */:root{--pdoc-background:#fff;}.pdoc{--text:#212529;--muted:#6c757d;--link:#3660a5;--link-hover:#1659c5;--code:#f8f8f8;--active:#fff598;--accent:#eee;--accent2:#c1c1c1;--nav-hover:rgba(255, 255, 255, 0.5);--name:#0066BB;--def:#008800;--annotation:#007020;}</style>
    <style>/*! layout.css */html, body{width:100%;height:100%;}html, main{scroll-behavior:smooth;}body{background-color:var(--pdoc-background);}@media (max-width:769px){#navtoggle{cursor:pointer;position:absolute;width:50px;height:40px;top:1rem;right:1rem;border-color:var(--text);color:var(--text);display:flex;opacity:0.8;z-index:999;}#navtoggle:hover{opacity:1;}#togglestate + div{display:none;}#togglestate:checked + div{display:inherit;}main, header{padding:2rem 3vw;}header + main{margin-top:-3rem;}.git-button{display:none !important;}nav input[type="search"]{max-width:77%;}nav input[type="search"]:first-child{margin-top:-6px;}nav input[type="search"]:valid ~ *{display:none !important;}}@media (min-width:770px){:root{--sidebar-width:clamp(12.5rem, 28vw, 22rem);}nav{position:fixed;overflow:auto;height:100vh;width:var(--sidebar-width);}main, header{padding:3rem 2rem 3rem calc(var(--sidebar-width) + 3rem);width:calc(54rem + var(--sidebar-width));max-width:100%;}header + main{margin-top:-4rem;}#navtoggle{display:none;}}#togglestate{position:absolute;height:0;opacity:0;}nav.pdoc{--pad:clamp(0.5rem, 2vw, 1.75rem);--indent:1.5rem;background-color:var(--accent);border-right:1px solid var(--accent2);box-shadow:0 0 20px rgba(50, 50, 50, .2) inset;padding:0 0 0 var(--pad);overflow-wrap:anywhere;scrollbar-width:thin; scrollbar-color:var(--accent2) transparent; z-index:1}nav.pdoc::-webkit-scrollbar{width:.4rem; }nav.pdoc::-webkit-scrollbar-thumb{background-color:var(--accent2); }nav.pdoc > div{padding:var(--pad) 0;}nav.pdoc .module-list-button{display:inline-flex;align-items:center;color:var(--text);border-color:var(--muted);margin-bottom:1rem;}nav.pdoc .module-list-button:hover{border-color:var(--text);}nav.pdoc input[type=search]{display:block;outline-offset:0;width:calc(100% - var(--pad));}nav.pdoc .logo{max-width:calc(100% - var(--pad));max-height:35vh;display:block;margin:0 auto 1rem;transform:translate(calc(-.5 * var(--pad)), 0);}nav.pdoc ul{list-style:none;padding-left:0;}nav.pdoc > div > ul{margin-left:calc(0px - var(--pad));}nav.pdoc li a{padding:.2rem 0 .2rem calc(var(--pad) + var(--indent));}nav.pdoc > div > ul > li > a{padding-left:var(--pad);}nav.pdoc li{transition:all 100ms;}nav.pdoc li:hover{background-color:var(--nav-hover);}nav.pdoc a, nav.pdoc a:hover{color:var(--text);}nav.pdoc a{display:block;}nav.pdoc > h2:first-of-type{margin-top:1.5rem;}nav.pdoc .class:before{content:"class ";color:var(--muted);}nav.pdoc .function:after{content:"()";color:var(--muted);}nav.pdoc footer:before{content:"";display:block;width:calc(100% - var(--pad));border-top:solid var(--accent2) 1px;margin-top:1.5rem;padding-top:.5rem;}nav.pdoc footer{font-size:small;}</style>
    <style>/*! content.css */.pdoc{color:var(--text);box-sizing:border-box;line-height:1.5;background:none;}.pdoc .pdoc-button{cursor:pointer;display:inline-block;border:solid black 1px;border-radius:2px;font-size:.75rem;padding:calc(0.5em - 1px) 1em;transition:100ms all;}.pdoc .alert{padding:1rem 1rem 1rem calc(1.5rem + 24px);border:1px solid transparent;border-radius:.25rem;background-repeat:no-repeat;background-position:.75rem center;margin-bottom:1rem;}.pdoc .alert > em{display:none;}.pdoc .alert > *:last-child{margin-bottom:0;}.pdoc .alert.note {color:#084298;background-color:#cfe2ff;border-color:#b6d4fe;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23084298%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8%2016A8%208%200%201%200%208%200a8%208%200%200%200%200%2016zm.93-9.412-1%204.705c-.07.34.029.533.304.533.194%200%20.487-.07.686-.246l-.088.416c-.287.346-.92.598-1.465.598-.703%200-1.002-.422-.808-1.319l.738-3.468c.064-.293.006-.399-.287-.47l-.451-.081.082-.381%202.29-.287zM8%205.5a1%201%200%201%201%200-2%201%201%200%200%201%200%202z%22/%3E%3C/svg%3E");}.pdoc .alert.warning{color:#664d03;background-color:#fff3cd;border-color:#ffecb5;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23664d03%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8.982%201.566a1.13%201.13%200%200%200-1.96%200L.165%2013.233c-.457.778.091%201.767.98%201.767h13.713c.889%200%201.438-.99.98-1.767L8.982%201.566zM8%205c.535%200%20.954.462.9.995l-.35%203.507a.552.552%200%200%201-1.1%200L7.1%205.995A.905.905%200%200%201%208%205zm.002%206a1%201%200%201%201%200%202%201%201%200%200%201%200-2z%22/%3E%3C/svg%3E");}.pdoc .alert.danger{color:#842029;background-color:#f8d7da;border-color:#f5c2c7;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23842029%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M5.52.359A.5.5%200%200%201%206%200h4a.5.5%200%200%201%20.474.658L8.694%206H12.5a.5.5%200%200%201%20.395.807l-7%209a.5.5%200%200%201-.873-.454L6.823%209.5H3.5a.5.5%200%200%201-.48-.641l2.5-8.5z%22/%3E%3C/svg%3E");}.pdoc .visually-hidden{position:absolute !important;width:1px !important;height:1px !important;padding:0 !important;margin:-1px !important;overflow:hidden !important;clip:rect(0, 0, 0, 0) !important;white-space:nowrap !important;border:0 !important;}.pdoc h1, .pdoc h2, .pdoc h3{font-weight:300;margin:.3em 0;padding:.2em 0;}.pdoc > section:not(.module-info) h1{font-size:1.5rem;font-weight:500;}.pdoc > section:not(.module-info) h2{font-size:1.4rem;font-weight:500;}.pdoc > section:not(.module-info) h3{font-size:1.3rem;font-weight:500;}.pdoc > section:not(.module-info) h4{font-size:1.2rem;}.pdoc > section:not(.module-info) h5{font-size:1.1rem;}.pdoc a{text-decoration:none;color:var(--link);}.pdoc a:hover{color:var(--link-hover);}.pdoc blockquote{margin-left:2rem;}.pdoc pre{border-top:1px solid var(--accent2);border-bottom:1px solid var(--accent2);margin-top:0;margin-bottom:1em;padding:.5rem 0 .5rem .5rem;overflow-x:auto;background-color:var(--code);}.pdoc code{color:var(--text);padding:.2em .4em;margin:0;font-size:85%;background-color:var(--accent);border-radius:6px;}.pdoc a > code{color:inherit;}.pdoc pre > code{display:inline-block;font-size:inherit;background:none;border:none;padding:0;}.pdoc > section:not(.module-info){margin-bottom:1.5rem;}.pdoc .modulename{margin-top:0;font-weight:bold;}.pdoc .modulename a{color:var(--link);transition:100ms all;}.pdoc .git-button{float:right;border:solid var(--link) 1px;}.pdoc .git-button:hover{background-color:var(--link);color:var(--pdoc-background);}.view-source-toggle-state,.view-source-toggle-state ~ .pdoc-code{display:none;}.view-source-toggle-state:checked ~ .pdoc-code{display:block;}.view-source-button{display:inline-block;float:right;font-size:.75rem;line-height:1.5rem;color:var(--muted);padding:0 .4rem 0 1.3rem;cursor:pointer;text-indent:-2px;}.view-source-button > span{visibility:hidden;}.module-info .view-source-button{float:none;display:flex;justify-content:flex-end;margin:-1.2rem .4rem -.2rem 0;}.view-source-button::before{position:absolute;content:"View Source";display:list-item;list-style-type:disclosure-closed;}.view-source-toggle-state:checked ~ .attr .view-source-button::before,.view-source-toggle-state:checked ~ .view-source-button::before{list-style-type:disclosure-open;}.pdoc .docstring{margin-bottom:1.5rem;}.pdoc section:not(.module-info) .docstring{margin-left:clamp(0rem, 5vw - 2rem, 1rem);}.pdoc .docstring .pdoc-code{margin-left:1em;margin-right:1em;}.pdoc h1:target,.pdoc h2:target,.pdoc h3:target,.pdoc h4:target,.pdoc h5:target,.pdoc h6:target,.pdoc .pdoc-code > pre > span:target{background-color:var(--active);box-shadow:-1rem 0 0 0 var(--active);}.pdoc .pdoc-code > pre > span:target{display:block;}.pdoc div:target > .attr,.pdoc section:target > .attr,.pdoc dd:target > a{background-color:var(--active);}.pdoc *{scroll-margin:2rem;}.pdoc .pdoc-code .linenos{user-select:none;}.pdoc .attr:hover{filter:contrast(0.95);}.pdoc section, .pdoc .classattr{position:relative;}.pdoc .headerlink{--width:clamp(1rem, 3vw, 2rem);position:absolute;top:0;left:calc(0rem - var(--width));transition:all 100ms ease-in-out;opacity:0;}.pdoc .headerlink::before{content:"#";display:block;text-align:center;width:var(--width);height:2.3rem;line-height:2.3rem;font-size:1.5rem;}.pdoc .attr:hover ~ .headerlink,.pdoc *:target > .headerlink,.pdoc .headerlink:hover{opacity:1;}.pdoc .attr{display:block;margin:.5rem 0 .5rem;padding:.4rem .4rem .4rem 1rem;background-color:var(--accent);overflow-x:auto;}.pdoc .classattr{margin-left:2rem;}.pdoc .decorator-deprecated{color:#842029;}.pdoc .decorator-deprecated ~ span{filter:grayscale(1) opacity(0.8);}.pdoc .name{color:var(--name);font-weight:bold;}.pdoc .def{color:var(--def);font-weight:bold;}.pdoc .signature{background-color:transparent;}.pdoc .param, .pdoc .return-annotation{white-space:pre;}.pdoc .signature.multiline .param{display:block;}.pdoc .signature.condensed .param{display:inline-block;}.pdoc .annotation{color:var(--annotation);}.pdoc .view-value-toggle-state,.pdoc .view-value-toggle-state ~ .default_value{display:none;}.pdoc .view-value-toggle-state:checked ~ .default_value{display:inherit;}.pdoc .view-value-button{font-size:.5rem;vertical-align:middle;border-style:dashed;margin-top:-0.1rem;}.pdoc .view-value-button:hover{background:white;}.pdoc .view-value-button::before{content:"show";text-align:center;width:2.2em;display:inline-block;}.pdoc .view-value-toggle-state:checked ~ .view-value-button::before{content:"hide";}.pdoc .inherited{margin-left:2rem;}.pdoc .inherited dt{font-weight:700;}.pdoc .inherited dt, .pdoc .inherited dd{display:inline;margin-left:0;margin-bottom:.5rem;}.pdoc .inherited dd:not(:last-child):after{content:", ";}.pdoc .inherited .class:before{content:"class ";}.pdoc .inherited .function a:after{content:"()";}.pdoc .search-result .docstring{overflow:auto;max-height:25vh;}.pdoc .search-result.focused > .attr{background-color:var(--active);}.pdoc .attribution{margin-top:2rem;display:block;opacity:0.5;transition:all 200ms;filter:grayscale(100%);}.pdoc .attribution:hover{opacity:1;filter:grayscale(0%);}.pdoc .attribution img{margin-left:5px;height:35px;vertical-align:middle;width:70px;transition:all 200ms;}.pdoc table{display:block;width:max-content;max-width:100%;overflow:auto;margin-bottom:1rem;}.pdoc table th{font-weight:600;}.pdoc table th, .pdoc table td{padding:6px 13px;border:1px solid var(--accent2);}</style>
    <style>/*! custom.css */</style><script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']]
        }
    };
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>
    /* Re-invoke MathJax when DOM content changes, for example during search. */
    document.addEventListener("DOMContentLoaded", () => {
        new MutationObserver(() => MathJax.typeset()).observe(
            document.querySelector("main.pdoc").parentNode,
            {childList: true}
        );
    })
</script>
<style>
    mjx-container {
        overflow-x: auto;
        overflow-y: hidden;
    }
</style></head>
<body>
    <nav class="pdoc">
        <label id="navtoggle" for="togglestate" class="pdoc-button"><svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 30 30'><path stroke-linecap='round' stroke="currentColor" stroke-miterlimit='10' stroke-width='2' d='M4 7h22M4 15h22M4 23h22'/></svg></label>
        <input id="togglestate" type="checkbox" aria-hidden="true" tabindex="-1">
        <div>            <a class="pdoc-button module-list-button" href="../clarena.html">
<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-box-arrow-in-left" viewBox="0 0 16 16">
  <path fill-rule="evenodd" d="M10 3.5a.5.5 0 0 0-.5-.5h-8a.5.5 0 0 0-.5.5v9a.5.5 0 0 0 .5.5h8a.5.5 0 0 0 .5-.5v-2a.5.5 0 0 1 1 0v2A1.5 1.5 0 0 1 9.5 14h-8A1.5 1.5 0 0 1 0 12.5v-9A1.5 1.5 0 0 1 1.5 2h8A1.5 1.5 0 0 1 11 3.5v2a.5.5 0 0 1-1 0v-2z"/>
  <path fill-rule="evenodd" d="M4.146 8.354a.5.5 0 0 1 0-.708l3-3a.5.5 0 1 1 .708.708L5.707 7.5H14.5a.5.5 0 0 1 0 1H5.707l2.147 2.146a.5.5 0 0 1-.708.708l-3-3z"/>
</svg>                &nbsp;clarena</a>


            <input type="search" placeholder="Search..." role="searchbox" aria-label="search"
                   pattern=".+" required>

            <h2>Contents</h2>
            <ul>
  <li><a href="#backbone-networks">Backbone Networks</a></li>
</ul>


            <h2>Submodules</h2>
            <ul>
                    <li><a href="backbones/mlp.html">mlp</a></li>
                    <li><a href="backbones/resnet.html">resnet</a></li>
            </ul>

            <h2>API Documentation</h2>
                <ul class="memberlist">
            <li>
                    <a class="class" href="#Backbone">Backbone</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#Backbone.__init__">Backbone</a>
                        </li>
                        <li>
                                <a class="variable" href="#Backbone.output_dim">output_dim</a>
                        </li>
                        <li>
                                <a class="variable" href="#Backbone.weighted_layer_names">weighted_layer_names</a>
                        </li>
                        <li>
                                <a class="function" href="#Backbone.get_layer_by_name">get_layer_by_name</a>
                        </li>
                        <li>
                                <a class="function" href="#Backbone.preceding_layer_name">preceding_layer_name</a>
                        </li>
                        <li>
                                <a class="function" href="#Backbone.next_layer_name">next_layer_name</a>
                        </li>
                        <li>
                                <a class="function" href="#Backbone.forward">forward</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#CLBackbone">CLBackbone</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#CLBackbone.__init__">CLBackbone</a>
                        </li>
                        <li>
                                <a class="variable" href="#CLBackbone.task_id">task_id</a>
                        </li>
                        <li>
                                <a class="variable" href="#CLBackbone.processed_task_ids">processed_task_ids</a>
                        </li>
                        <li>
                                <a class="function" href="#CLBackbone.setup_task_id">setup_task_id</a>
                        </li>
                        <li>
                                <a class="function" href="#CLBackbone.forward">forward</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#HATMaskBackbone">HATMaskBackbone</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#HATMaskBackbone.__init__">HATMaskBackbone</a>
                        </li>
                        <li>
                                <a class="variable" href="#HATMaskBackbone.gate">gate</a>
                        </li>
                        <li>
                                <a class="variable" href="#HATMaskBackbone.task_embedding_t">task_embedding_t</a>
                        </li>
                        <li>
                                <a class="variable" href="#HATMaskBackbone.masks">masks</a>
                        </li>
                        <li>
                                <a class="function" href="#HATMaskBackbone.initialize_task_embedding">initialize_task_embedding</a>
                        </li>
                        <li>
                                <a class="function" href="#HATMaskBackbone.sanity_check">sanity_check</a>
                        </li>
                        <li>
                                <a class="function" href="#HATMaskBackbone.get_mask">get_mask</a>
                        </li>
                        <li>
                                <a class="function" href="#HATMaskBackbone.te_to_binary_mask">te_to_binary_mask</a>
                        </li>
                        <li>
                                <a class="function" href="#HATMaskBackbone.store_mask">store_mask</a>
                        </li>
                        <li>
                                <a class="function" href="#HATMaskBackbone.get_layer_measure_parameter_wise">get_layer_measure_parameter_wise</a>
                        </li>
                        <li>
                                <a class="function" href="#HATMaskBackbone.forward">forward</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#WSNMaskBackbone">WSNMaskBackbone</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#WSNMaskBackbone.__init__">WSNMaskBackbone</a>
                        </li>
                        <li>
                                <a class="variable" href="#WSNMaskBackbone.gate_fn">gate_fn</a>
                        </li>
                        <li>
                                <a class="variable" href="#WSNMaskBackbone.weight_score_t">weight_score_t</a>
                        </li>
                        <li>
                                <a class="variable" href="#WSNMaskBackbone.bias_score_t">bias_score_t</a>
                        </li>
                        <li>
                                <a class="function" href="#WSNMaskBackbone.sanity_check">sanity_check</a>
                        </li>
                        <li>
                                <a class="function" href="#WSNMaskBackbone.initialize_parameter_score">initialize_parameter_score</a>
                        </li>
                        <li>
                                <a class="function" href="#WSNMaskBackbone.get_mask">get_mask</a>
                        </li>
                        <li>
                                <a class="function" href="#WSNMaskBackbone.forward">forward</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#NISPAMaskBackbone">NISPAMaskBackbone</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#NISPAMaskBackbone.__init__">NISPAMaskBackbone</a>
                        </li>
                        <li>
                                <a class="variable" href="#NISPAMaskBackbone.weight_mask_t">weight_mask_t</a>
                        </li>
                        <li>
                                <a class="variable" href="#NISPAMaskBackbone.frozen_weight_mask_t">frozen_weight_mask_t</a>
                        </li>
                        <li>
                                <a class="variable" href="#NISPAMaskBackbone.bias_mask_t">bias_mask_t</a>
                        </li>
                        <li>
                                <a class="variable" href="#NISPAMaskBackbone.frozen_bias_mask_t">frozen_bias_mask_t</a>
                        </li>
                        <li>
                                <a class="function" href="#NISPAMaskBackbone.sanity_check">sanity_check</a>
                        </li>
                        <li>
                                <a class="function" href="#NISPAMaskBackbone.initialize_parameter_mask">initialize_parameter_mask</a>
                        </li>
                        <li>
                                <a class="function" href="#NISPAMaskBackbone.forward">forward</a>
                        </li>
                </ul>

            </li>
    </ul>



        <a class="attribution" title="pdoc: Python API documentation generator" href="https://pdoc.dev" target="_blank">
            built with <span class="visually-hidden">pdoc</span><img
                alt="pdoc logo"
                src="data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20role%3D%22img%22%20aria-label%3D%22pdoc%20logo%22%20width%3D%22300%22%20height%3D%22150%22%20viewBox%3D%22-1%200%2060%2030%22%3E%3Ctitle%3Epdoc%3C/title%3E%3Cpath%20d%3D%22M29.621%2021.293c-.011-.273-.214-.475-.511-.481a.5.5%200%200%200-.489.503l-.044%201.393c-.097.551-.695%201.215-1.566%201.704-.577.428-1.306.486-2.193.182-1.426-.617-2.467-1.654-3.304-2.487l-.173-.172a3.43%203.43%200%200%200-.365-.306.49.49%200%200%200-.286-.196c-1.718-1.06-4.931-1.47-7.353.191l-.219.15c-1.707%201.187-3.413%202.131-4.328%201.03-.02-.027-.49-.685-.141-1.763.233-.721.546-2.408.772-4.076.042-.09.067-.187.046-.288.166-1.347.277-2.625.241-3.351%201.378-1.008%202.271-2.586%202.271-4.362%200-.976-.272-1.935-.788-2.774-.057-.094-.122-.18-.184-.268.033-.167.052-.339.052-.516%200-1.477-1.202-2.679-2.679-2.679-.791%200-1.496.352-1.987.9a6.3%206.3%200%200%200-1.001.029c-.492-.564-1.207-.929-2.012-.929-1.477%200-2.679%201.202-2.679%202.679A2.65%202.65%200%200%200%20.97%206.554c-.383.747-.595%201.572-.595%202.41%200%202.311%201.507%204.29%203.635%205.107-.037.699-.147%202.27-.423%203.294l-.137.461c-.622%202.042-2.515%208.257%201.727%2010.643%201.614.908%203.06%201.248%204.317%201.248%202.665%200%204.492-1.524%205.322-2.401%201.476-1.559%202.886-1.854%206.491.82%201.877%201.393%203.514%201.753%204.861%201.068%202.223-1.713%202.811-3.867%203.399-6.374.077-.846.056-1.469.054-1.537zm-4.835%204.313c-.054.305-.156.586-.242.629-.034-.007-.131-.022-.307-.157-.145-.111-.314-.478-.456-.908.221.121.432.25.675.355.115.039.219.051.33.081zm-2.251-1.238c-.05.33-.158.648-.252.694-.022.001-.125-.018-.307-.157-.217-.166-.488-.906-.639-1.573.358.344.754.693%201.198%201.036zm-3.887-2.337c-.006-.116-.018-.231-.041-.342.635.145%201.189.368%201.599.625.097.231.166.481.174.642-.03.049-.055.101-.067.158-.046.013-.128.026-.298.004-.278-.037-.901-.57-1.367-1.087zm-1.127-.497c.116.306.176.625.12.71-.019.014-.117.045-.345.016-.206-.027-.604-.332-.986-.695.41-.051.816-.056%201.211-.031zm-4.535%201.535c.209.22.379.47.358.598-.006.041-.088.138-.351.234-.144.055-.539-.063-.979-.259a11.66%2011.66%200%200%200%20.972-.573zm.983-.664c.359-.237.738-.418%201.126-.554.25.237.479.548.457.694-.006.042-.087.138-.351.235-.174.064-.694-.105-1.232-.375zm-3.381%201.794c-.022.145-.061.29-.149.401-.133.166-.358.248-.69.251h-.002c-.133%200-.306-.26-.45-.621.417.091.854.07%201.291-.031zm-2.066-8.077a4.78%204.78%200%200%201-.775-.584c.172-.115.505-.254.88-.378l-.105.962zm-.331%202.302a10.32%2010.32%200%200%201-.828-.502c.202-.143.576-.328.984-.49l-.156.992zm-.45%202.157l-.701-.403c.214-.115.536-.249.891-.376a11.57%2011.57%200%200%201-.19.779zm-.181%201.716c.064.398.194.702.298.893-.194-.051-.435-.162-.736-.398.061-.119.224-.3.438-.495zM8.87%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zm-.735-.389a1.15%201.15%200%200%200-.314.783%201.16%201.16%200%200%200%201.162%201.162c.457%200%20.842-.27%201.032-.653.026.117.042.238.042.362a1.68%201.68%200%200%201-1.679%201.679%201.68%201.68%200%200%201-1.679-1.679c0-.843.626-1.535%201.436-1.654zM5.059%205.406A1.68%201.68%200%200%201%203.38%207.085a1.68%201.68%200%200%201-1.679-1.679c0-.037.009-.072.011-.109.21.3.541.508.935.508a1.16%201.16%200%200%200%201.162-1.162%201.14%201.14%200%200%200-.474-.912c.015%200%20.03-.005.045-.005.926.001%201.679.754%201.679%201.68zM3.198%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zM1.375%208.964c0-.52.103-1.035.288-1.52.466.394%201.06.64%201.717.64%201.144%200%202.116-.725%202.499-1.738.383%201.012%201.355%201.738%202.499%201.738.867%200%201.631-.421%202.121-1.062.307.605.478%201.267.478%201.942%200%202.486-2.153%204.51-4.801%204.51s-4.801-2.023-4.801-4.51zm24.342%2019.349c-.985.498-2.267.168-3.813-.979-3.073-2.281-5.453-3.199-7.813-.705-1.315%201.391-4.163%203.365-8.423.97-3.174-1.786-2.239-6.266-1.261-9.479l.146-.492c.276-1.02.395-2.457.444-3.268a6.11%206.11%200%200%200%201.18.115%206.01%206.01%200%200%200%202.536-.562l-.006.175c-.802.215-1.848.612-2.021%201.25-.079.295.021.601.274.837.219.203.415.364.598.501-.667.304-1.243.698-1.311%201.179-.02.144-.022.507.393.787.213.144.395.26.564.365-1.285.521-1.361.96-1.381%201.126-.018.142-.011.496.427.746l.854.489c-.473.389-.971.914-.999%201.429-.018.278.095.532.316.713.675.556%201.231.721%201.653.721.059%200%20.104-.014.158-.02.207.707.641%201.64%201.513%201.64h.013c.8-.008%201.236-.345%201.462-.626.173-.216.268-.457.325-.692.424.195.93.374%201.372.374.151%200%20.294-.021.423-.068.732-.27.944-.704.993-1.021.009-.061.003-.119.002-.179.266.086.538.147.789.147.15%200%20.294-.021.423-.069.542-.2.797-.489.914-.754.237.147.478.258.704.288.106.014.205.021.296.021.356%200%20.595-.101.767-.229.438.435%201.094.992%201.656%201.067.106.014.205.021.296.021a1.56%201.56%200%200%200%20.323-.035c.17.575.453%201.289.866%201.605.358.273.665.362.914.362a.99.99%200%200%200%20.421-.093%201.03%201.03%200%200%200%20.245-.164c.168.428.39.846.68%201.068.358.273.665.362.913.362a.99.99%200%200%200%20.421-.093c.317-.148.512-.448.639-.762.251.157.495.257.726.257.127%200%20.25-.024.37-.071.427-.17.706-.617.841-1.314.022-.015.047-.022.068-.038.067-.051.133-.104.196-.159-.443%201.486-1.107%202.761-2.086%203.257zM8.66%209.925a.5.5%200%201%200-1%200c0%20.653-.818%201.205-1.787%201.205s-1.787-.552-1.787-1.205a.5.5%200%201%200-1%200c0%201.216%201.25%202.205%202.787%202.205s2.787-.989%202.787-2.205zm4.4%2015.965l-.208.097c-2.661%201.258-4.708%201.436-6.086.527-1.542-1.017-1.88-3.19-1.844-4.198a.4.4%200%200%200-.385-.414c-.242-.029-.406.164-.414.385-.046%201.249.367%203.686%202.202%204.896.708.467%201.547.7%202.51.7%201.248%200%202.706-.392%204.362-1.174l.185-.086a.4.4%200%200%200%20.205-.527c-.089-.204-.326-.291-.527-.206zM9.547%202.292c.093.077.205.114.317.114a.5.5%200%200%200%20.318-.886L8.817.397a.5.5%200%200%200-.703.068.5.5%200%200%200%20.069.703l1.364%201.124zm-7.661-.065c.086%200%20.173-.022.253-.068l1.523-.893a.5.5%200%200%200-.506-.863l-1.523.892a.5.5%200%200%200-.179.685c.094.158.261.247.432.247z%22%20transform%3D%22matrix%28-1%200%200%201%2058%200%29%22%20fill%3D%22%233bb300%22/%3E%3Cpath%20d%3D%22M.3%2021.86V10.18q0-.46.02-.68.04-.22.18-.5.28-.54%201.34-.54%201.06%200%201.42.28.38.26.44.78.76-1.04%202.38-1.04%201.64%200%203.1%201.54%201.46%201.54%201.46%203.58%200%202.04-1.46%203.58-1.44%201.54-3.08%201.54-1.64%200-2.38-.92v4.04q0%20.46-.04.68-.02.22-.18.5-.14.3-.5.42-.36.12-.98.12-.62%200-1-.12-.36-.12-.52-.4-.14-.28-.18-.5-.02-.22-.02-.68zm3.96-9.42q-.46.54-.46%201.18%200%20.64.46%201.18.48.52%201.2.52.74%200%201.24-.52.52-.52.52-1.18%200-.66-.48-1.18-.48-.54-1.26-.54-.76%200-1.22.54zm14.741-8.36q.16-.3.54-.42.38-.12%201-.12.64%200%201.02.12.38.12.52.42.16.3.18.54.04.22.04.68v11.94q0%20.46-.04.7-.02.22-.18.5-.3.54-1.7.54-1.38%200-1.54-.98-.84.96-2.34.96-1.8%200-3.28-1.56-1.48-1.58-1.48-3.66%200-2.1%201.48-3.68%201.5-1.58%203.28-1.58%201.48%200%202.3%201v-4.2q0-.46.02-.68.04-.24.18-.52zm-3.24%2010.86q.52.54%201.26.54.74%200%201.22-.54.5-.54.5-1.18%200-.66-.48-1.22-.46-.56-1.26-.56-.8%200-1.28.56-.48.54-.48%201.2%200%20.66.52%201.2zm7.833-1.2q0-2.4%201.68-3.96%201.68-1.56%203.84-1.56%202.16%200%203.82%201.56%201.66%201.54%201.66%203.94%200%201.66-.86%202.96-.86%201.28-2.1%201.9-1.22.6-2.54.6-1.32%200-2.56-.64-1.24-.66-2.1-1.92-.84-1.28-.84-2.88zm4.18%201.44q.64.48%201.3.48.66%200%201.32-.5.66-.5.66-1.48%200-.98-.62-1.46-.62-.48-1.34-.48-.72%200-1.34.5-.62.5-.62%201.48%200%20.96.64%201.46zm11.412-1.44q0%20.84.56%201.32.56.46%201.18.46.64%200%201.18-.36.56-.38.9-.38.6%200%201.46%201.06.46.58.46%201.04%200%20.76-1.1%201.42-1.14.8-2.8.8-1.86%200-3.58-1.34-.82-.64-1.34-1.7-.52-1.08-.52-2.36%200-1.3.52-2.34.52-1.06%201.34-1.7%201.66-1.32%203.54-1.32.76%200%201.48.22.72.2%201.06.4l.32.2q.36.24.56.38.52.4.52.92%200%20.5-.42%201.14-.72%201.1-1.38%201.1-.38%200-1.08-.44-.36-.34-1.04-.34-.66%200-1.24.48-.58.48-.58%201.34z%22%20fill%3D%22green%22/%3E%3C/svg%3E"/>
        </a>
</div>
    </nav>
    <main class="pdoc">
            <section class="module-info">
                    <h1 class="modulename">
<a href="./../clarena.html">clarena</a><wbr>.backbones    </h1>

                        <div class="docstring"><h1 id="backbone-networks">Backbone Networks</h1>

<p>This submodule provides the <strong>backbone neural network architectures</strong> for various machine learning paradigms in CLArena.</p>

<p>Here are the base classes for backbone networks, which inherit from PyTorch <code>nn.Module</code>:</p>

<ul>
<li><code><a href="#Backbone">Backbone</a></code>: The base class for backbones.</li>
<li><code><a href="#CLBackbone">CLBackbone</a></code>: The base class for continual learning backbones.</li>
<li><code><a href="#HATMaskBackbone">HATMaskBackbone</a></code>: The base class for backbones used in <a href="http://proceedings.mlr.press/v80/serra18a">HAT (Hard Attention to the Task)</a> CL algorithm.</li>
<li><code><a href="#WSNMaskBackbone">WSNMaskBackbone</a></code>: The base class for backbones used in <a href="https://proceedings.mlr.press/v162/kang22b/kang22b.pdf">WSN (Winning Subnetworks)</a> CL algorithm.</li>
</ul>

<p>Please note that this is an API documentation. Please refer to the main documentation pages for more information about how to configure and implement backbone networks:</p>

<ul>
<li><a href="https://pengxiang-wang.com/projects/continual-learning-arena/docs/continual-learning/configure-main-experiment/backbone-network"><strong>Configure Backbone Network (CL Main)</strong></a></li>
<li><a href="https://pengxiang-wang.com/projects/continual-learning-arena/docs/multi-task-learning-arena/configure-experiment/backbone-network"><strong>Configure Backbone Network (MTL)</strong></a></li>
<li><a href="https://pengxiang-wang.com/projects/continual-learning-arena/docs/single-task-learning-arena/configure-experiment/backbone-network"><strong>Configure Backbone Network (STL)</strong></a></li>
<li><a href="https://pengxiang-wang.com/projects/continual-learning-arena/docs/custom-implementation/backbone-network"><strong>Implement Custom Backbone Network</strong></a></li>
</ul>
</div>

                        <input id="mod-backbones-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">

                        <label class="view-source-button" for="mod-backbones-view-source"><span>View Source</span></label>

                        <div class="pdoc-code codehilite"><pre><span></span><span id="L-1"><a href="#L-1"><span class="linenos"> 1</span></a><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-2"><a href="#L-2"><span class="linenos"> 2</span></a>
</span><span id="L-3"><a href="#L-3"><span class="linenos"> 3</span></a><span class="sd"># Backbone Networks</span>
</span><span id="L-4"><a href="#L-4"><span class="linenos"> 4</span></a>
</span><span id="L-5"><a href="#L-5"><span class="linenos"> 5</span></a><span class="sd">This submodule provides the **backbone neural network architectures** for various machine learning paradigms in CLArena.</span>
</span><span id="L-6"><a href="#L-6"><span class="linenos"> 6</span></a>
</span><span id="L-7"><a href="#L-7"><span class="linenos"> 7</span></a><span class="sd">Here are the base classes for backbone networks, which inherit from PyTorch `nn.Module`:</span>
</span><span id="L-8"><a href="#L-8"><span class="linenos"> 8</span></a>
</span><span id="L-9"><a href="#L-9"><span class="linenos"> 9</span></a><span class="sd">- `Backbone`: The base class for backbones.</span>
</span><span id="L-10"><a href="#L-10"><span class="linenos">10</span></a><span class="sd">- `CLBackbone`: The base class for continual learning backbones.</span>
</span><span id="L-11"><a href="#L-11"><span class="linenos">11</span></a><span class="sd">- `HATMaskBackbone`: The base class for backbones used in [HAT (Hard Attention to the Task)](http://proceedings.mlr.press/v80/serra18a) CL algorithm.</span>
</span><span id="L-12"><a href="#L-12"><span class="linenos">12</span></a><span class="sd">- `WSNMaskBackbone`: The base class for backbones used in [WSN (Winning Subnetworks)](https://proceedings.mlr.press/v162/kang22b/kang22b.pdf) CL algorithm.</span>
</span><span id="L-13"><a href="#L-13"><span class="linenos">13</span></a>
</span><span id="L-14"><a href="#L-14"><span class="linenos">14</span></a><span class="sd">Please note that this is an API documentation. Please refer to the main documentation pages for more information about how to configure and implement backbone networks:</span>
</span><span id="L-15"><a href="#L-15"><span class="linenos">15</span></a>
</span><span id="L-16"><a href="#L-16"><span class="linenos">16</span></a><span class="sd">- [**Configure Backbone Network (CL Main)**](https://pengxiang-wang.com/projects/continual-learning-arena/docs/continual-learning/configure-main-experiment/backbone-network)</span>
</span><span id="L-17"><a href="#L-17"><span class="linenos">17</span></a><span class="sd">- [**Configure Backbone Network (MTL)**](https://pengxiang-wang.com/projects/continual-learning-arena/docs/multi-task-learning-arena/configure-experiment/backbone-network)</span>
</span><span id="L-18"><a href="#L-18"><span class="linenos">18</span></a><span class="sd">- [**Configure Backbone Network (STL)**](https://pengxiang-wang.com/projects/continual-learning-arena/docs/single-task-learning-arena/configure-experiment/backbone-network)</span>
</span><span id="L-19"><a href="#L-19"><span class="linenos">19</span></a><span class="sd">- [**Implement Custom Backbone Network**](https://pengxiang-wang.com/projects/continual-learning-arena/docs/custom-implementation/backbone-network)</span>
</span><span id="L-20"><a href="#L-20"><span class="linenos">20</span></a>
</span><span id="L-21"><a href="#L-21"><span class="linenos">21</span></a>
</span><span id="L-22"><a href="#L-22"><span class="linenos">22</span></a><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-23"><a href="#L-23"><span class="linenos">23</span></a>
</span><span id="L-24"><a href="#L-24"><span class="linenos">24</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">.base</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
</span><span id="L-25"><a href="#L-25"><span class="linenos">25</span></a>    <span class="n">Backbone</span><span class="p">,</span>
</span><span id="L-26"><a href="#L-26"><span class="linenos">26</span></a>    <span class="n">CLBackbone</span><span class="p">,</span>
</span><span id="L-27"><a href="#L-27"><span class="linenos">27</span></a>    <span class="n">HATMaskBackbone</span><span class="p">,</span>
</span><span id="L-28"><a href="#L-28"><span class="linenos">28</span></a>    <span class="n">WSNMaskBackbone</span><span class="p">,</span>
</span><span id="L-29"><a href="#L-29"><span class="linenos">29</span></a>    <span class="n">NISPAMaskBackbone</span><span class="p">,</span>
</span><span id="L-30"><a href="#L-30"><span class="linenos">30</span></a><span class="p">)</span>
</span><span id="L-31"><a href="#L-31"><span class="linenos">31</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">.mlp</span><span class="w"> </span><span class="kn">import</span> <span class="n">MLP</span><span class="p">,</span> <span class="n">CLMLP</span><span class="p">,</span> <span class="n">HATMaskMLP</span><span class="p">,</span> <span class="n">WSNMaskMLP</span>
</span><span id="L-32"><a href="#L-32"><span class="linenos">32</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">.resnet</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
</span><span id="L-33"><a href="#L-33"><span class="linenos">33</span></a>    <span class="n">ResNet18</span><span class="p">,</span>
</span><span id="L-34"><a href="#L-34"><span class="linenos">34</span></a>    <span class="n">ResNet34</span><span class="p">,</span>
</span><span id="L-35"><a href="#L-35"><span class="linenos">35</span></a>    <span class="n">ResNet50</span><span class="p">,</span>
</span><span id="L-36"><a href="#L-36"><span class="linenos">36</span></a>    <span class="n">ResNet101</span><span class="p">,</span>
</span><span id="L-37"><a href="#L-37"><span class="linenos">37</span></a>    <span class="n">ResNet152</span><span class="p">,</span>
</span><span id="L-38"><a href="#L-38"><span class="linenos">38</span></a>    <span class="n">CLResNet18</span><span class="p">,</span>
</span><span id="L-39"><a href="#L-39"><span class="linenos">39</span></a>    <span class="n">CLResNet34</span><span class="p">,</span>
</span><span id="L-40"><a href="#L-40"><span class="linenos">40</span></a>    <span class="n">CLResNet50</span><span class="p">,</span>
</span><span id="L-41"><a href="#L-41"><span class="linenos">41</span></a>    <span class="n">CLResNet101</span><span class="p">,</span>
</span><span id="L-42"><a href="#L-42"><span class="linenos">42</span></a>    <span class="n">CLResNet152</span><span class="p">,</span>
</span><span id="L-43"><a href="#L-43"><span class="linenos">43</span></a>    <span class="n">HATMaskResNet18</span><span class="p">,</span>
</span><span id="L-44"><a href="#L-44"><span class="linenos">44</span></a>    <span class="n">HATMaskResNet34</span><span class="p">,</span>
</span><span id="L-45"><a href="#L-45"><span class="linenos">45</span></a>    <span class="n">HATMaskResNet50</span><span class="p">,</span>
</span><span id="L-46"><a href="#L-46"><span class="linenos">46</span></a>    <span class="n">HATMaskResNet101</span><span class="p">,</span>
</span><span id="L-47"><a href="#L-47"><span class="linenos">47</span></a>    <span class="n">HATMaskResNet152</span><span class="p">,</span>
</span><span id="L-48"><a href="#L-48"><span class="linenos">48</span></a><span class="p">)</span>
</span><span id="L-49"><a href="#L-49"><span class="linenos">49</span></a>
</span><span id="L-50"><a href="#L-50"><span class="linenos">50</span></a>
</span><span id="L-51"><a href="#L-51"><span class="linenos">51</span></a><span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
</span><span id="L-52"><a href="#L-52"><span class="linenos">52</span></a>    <span class="s2">&quot;Backbone&quot;</span><span class="p">,</span>
</span><span id="L-53"><a href="#L-53"><span class="linenos">53</span></a>    <span class="s2">&quot;CLBackbone&quot;</span><span class="p">,</span>
</span><span id="L-54"><a href="#L-54"><span class="linenos">54</span></a>    <span class="s2">&quot;HATMaskBackbone&quot;</span><span class="p">,</span>
</span><span id="L-55"><a href="#L-55"><span class="linenos">55</span></a>    <span class="s2">&quot;WSNMaskBackbone&quot;</span><span class="p">,</span>
</span><span id="L-56"><a href="#L-56"><span class="linenos">56</span></a>    <span class="s2">&quot;NISPAMaskBackbone&quot;</span><span class="p">,</span>
</span><span id="L-57"><a href="#L-57"><span class="linenos">57</span></a>    <span class="s2">&quot;mlp&quot;</span><span class="p">,</span>
</span><span id="L-58"><a href="#L-58"><span class="linenos">58</span></a>    <span class="s2">&quot;resnet&quot;</span><span class="p">,</span>
</span><span id="L-59"><a href="#L-59"><span class="linenos">59</span></a><span class="p">]</span>
</span></pre></div>


            </section>
                <section id="Backbone">
                            <input id="Backbone-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">Backbone</span><wbr>(<span class="base">torch.nn.modules.module.Module</span>):

                <label class="view-source-button" for="Backbone-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Backbone"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Backbone-25"><a href="#Backbone-25"><span class="linenos"> 25</span></a><span class="k">class</span><span class="w"> </span><span class="nc">Backbone</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="Backbone-26"><a href="#Backbone-26"><span class="linenos"> 26</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The base class for continual learning backbone networks; inherits from `nn.Module`.&quot;&quot;&quot;</span>
</span><span id="Backbone-27"><a href="#Backbone-27"><span class="linenos"> 27</span></a>
</span><span id="Backbone-28"><a href="#Backbone-28"><span class="linenos"> 28</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="Backbone-29"><a href="#Backbone-29"><span class="linenos"> 29</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialize the backbone network.</span>
</span><span id="Backbone-30"><a href="#Backbone-30"><span class="linenos"> 30</span></a>
</span><span id="Backbone-31"><a href="#Backbone-31"><span class="linenos"> 31</span></a><span class="sd">        **Args:**</span>
</span><span id="Backbone-32"><a href="#Backbone-32"><span class="linenos"> 32</span></a><span class="sd">        - **output_dim** (`int` | `None`): The output dimension that connects to output heads. The `input_dim` of output heads is expected to be the same as this `output_dim`. In some cases, this class is used as a block in the backbone network that doesn&#39;t have an output dimension. In this case, it can be `None`.</span>
</span><span id="Backbone-33"><a href="#Backbone-33"><span class="linenos"> 33</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="Backbone-34"><a href="#Backbone-34"><span class="linenos"> 34</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="Backbone-35"><a href="#Backbone-35"><span class="linenos"> 35</span></a>
</span><span id="Backbone-36"><a href="#Backbone-36"><span class="linenos"> 36</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">output_dim</span>
</span><span id="Backbone-37"><a href="#Backbone-37"><span class="linenos"> 37</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the output dimension of the backbone network.&quot;&quot;&quot;</span>
</span><span id="Backbone-38"><a href="#Backbone-38"><span class="linenos"> 38</span></a>
</span><span id="Backbone-39"><a href="#Backbone-39"><span class="linenos"> 39</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="Backbone-40"><a href="#Backbone-40"><span class="linenos"> 40</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Maintain a list of weighted layer names. A weighted layer has weights connecting to other weighted layers. They are the main part of neural networks. **It must be provided in subclasses.**</span>
</span><span id="Backbone-41"><a href="#Backbone-41"><span class="linenos"> 41</span></a><span class="sd">        </span>
</span><span id="Backbone-42"><a href="#Backbone-42"><span class="linenos"> 42</span></a><span class="sd">        The names follow the `nn.Module` internal naming mechanism. For example, if a layer is assigned to `self.conv1`, the name becomes `conv1`. If `nn.Sequential` is used, the name becomes the index of the layer in the sequence, such as `0`, `1`, etc. If a hierarchical structure is used (for example, a `nn.Module` is assigned to `self.block` which has `self.conv1`), the name becomes `block/conv1`. Note that it should be `block.conv1` according to `nn.Module`&#39;s internal mechanism, but we use &#39;/&#39; instead of &#39;.&#39; to avoid errors when using &#39;.&#39; as keys in a `ModuleDict`.</span>
</span><span id="Backbone-43"><a href="#Backbone-43"><span class="linenos"> 43</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="Backbone-44"><a href="#Backbone-44"><span class="linenos"> 44</span></a>
</span><span id="Backbone-45"><a href="#Backbone-45"><span class="linenos"> 45</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_layer_by_name</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="Backbone-46"><a href="#Backbone-46"><span class="linenos"> 46</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the layer by its name.</span>
</span><span id="Backbone-47"><a href="#Backbone-47"><span class="linenos"> 47</span></a>
</span><span id="Backbone-48"><a href="#Backbone-48"><span class="linenos"> 48</span></a><span class="sd">        **Args:**</span>
</span><span id="Backbone-49"><a href="#Backbone-49"><span class="linenos"> 49</span></a><span class="sd">        - **layer_name** (`str` | `None`): The layer name with &#39;.&#39; replaced by &#39;/&#39;, like `block/conv1`, rather than `block.conv1`. If `None`, return `None`.</span>
</span><span id="Backbone-50"><a href="#Backbone-50"><span class="linenos"> 50</span></a>
</span><span id="Backbone-51"><a href="#Backbone-51"><span class="linenos"> 51</span></a><span class="sd">        **Returns:**</span>
</span><span id="Backbone-52"><a href="#Backbone-52"><span class="linenos"> 52</span></a><span class="sd">        - **layer** (`nn.Module` | `None`): The layer.</span>
</span><span id="Backbone-53"><a href="#Backbone-53"><span class="linenos"> 53</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="Backbone-54"><a href="#Backbone-54"><span class="linenos"> 54</span></a>        <span class="k">if</span> <span class="n">layer_name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="Backbone-55"><a href="#Backbone-55"><span class="linenos"> 55</span></a>            <span class="k">return</span> <span class="kc">None</span>
</span><span id="Backbone-56"><a href="#Backbone-56"><span class="linenos"> 56</span></a>
</span><span id="Backbone-57"><a href="#Backbone-57"><span class="linenos"> 57</span></a>        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
</span><span id="Backbone-58"><a href="#Backbone-58"><span class="linenos"> 58</span></a>            <span class="k">if</span> <span class="n">name</span> <span class="o">==</span> <span class="n">layer_name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">,</span> <span class="s2">&quot;.&quot;</span><span class="p">):</span>
</span><span id="Backbone-59"><a href="#Backbone-59"><span class="linenos"> 59</span></a>                <span class="k">return</span> <span class="n">layer</span>
</span><span id="Backbone-60"><a href="#Backbone-60"><span class="linenos"> 60</span></a>
</span><span id="Backbone-61"><a href="#Backbone-61"><span class="linenos"> 61</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">preceding_layer_name</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
</span><span id="Backbone-62"><a href="#Backbone-62"><span class="linenos"> 62</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the name of the preceding layer of the given layer from the stored `self.masked_layer_order`. If the given layer is the first layer, return `None`.</span>
</span><span id="Backbone-63"><a href="#Backbone-63"><span class="linenos"> 63</span></a>
</span><span id="Backbone-64"><a href="#Backbone-64"><span class="linenos"> 64</span></a><span class="sd">        **Args:**</span>
</span><span id="Backbone-65"><a href="#Backbone-65"><span class="linenos"> 65</span></a><span class="sd">        - **layer_name** (`str`): The name of the layer.</span>
</span><span id="Backbone-66"><a href="#Backbone-66"><span class="linenos"> 66</span></a>
</span><span id="Backbone-67"><a href="#Backbone-67"><span class="linenos"> 67</span></a><span class="sd">        **Returns:**</span>
</span><span id="Backbone-68"><a href="#Backbone-68"><span class="linenos"> 68</span></a><span class="sd">        - **preceding_layer_name** (`str`): The name of the preceding layer.</span>
</span><span id="Backbone-69"><a href="#Backbone-69"><span class="linenos"> 69</span></a>
</span><span id="Backbone-70"><a href="#Backbone-70"><span class="linenos"> 70</span></a><span class="sd">        **Raises:**</span>
</span><span id="Backbone-71"><a href="#Backbone-71"><span class="linenos"> 71</span></a><span class="sd">        - **ValueError**: If `layer_name` is not in the weighted layer order.</span>
</span><span id="Backbone-72"><a href="#Backbone-72"><span class="linenos"> 72</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="Backbone-73"><a href="#Backbone-73"><span class="linenos"> 73</span></a>
</span><span id="Backbone-74"><a href="#Backbone-74"><span class="linenos"> 74</span></a>        <span class="k">if</span> <span class="n">layer_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="Backbone-75"><a href="#Backbone-75"><span class="linenos"> 75</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The layer name </span><span class="si">{</span><span class="n">layer_name</span><span class="si">}</span><span class="s2"> doesn&#39;t exist.&quot;</span><span class="p">)</span>
</span><span id="Backbone-76"><a href="#Backbone-76"><span class="linenos"> 76</span></a>
</span><span id="Backbone-77"><a href="#Backbone-77"><span class="linenos"> 77</span></a>        <span class="n">weighted_layer_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="Backbone-78"><a href="#Backbone-78"><span class="linenos"> 78</span></a>        <span class="k">if</span> <span class="n">weighted_layer_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="Backbone-79"><a href="#Backbone-79"><span class="linenos"> 79</span></a>            <span class="k">return</span> <span class="kc">None</span>
</span><span id="Backbone-80"><a href="#Backbone-80"><span class="linenos"> 80</span></a>        <span class="n">preceding_layer_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">[</span><span class="n">weighted_layer_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
</span><span id="Backbone-81"><a href="#Backbone-81"><span class="linenos"> 81</span></a>        <span class="k">return</span> <span class="n">preceding_layer_name</span>
</span><span id="Backbone-82"><a href="#Backbone-82"><span class="linenos"> 82</span></a>
</span><span id="Backbone-83"><a href="#Backbone-83"><span class="linenos"> 83</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">next_layer_name</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
</span><span id="Backbone-84"><a href="#Backbone-84"><span class="linenos"> 84</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the name of the next layer of the given layer from the stored `self.masked_layer_order`. If the given layer is the last layer of the BACKBONE, return `None`.</span>
</span><span id="Backbone-85"><a href="#Backbone-85"><span class="linenos"> 85</span></a>
</span><span id="Backbone-86"><a href="#Backbone-86"><span class="linenos"> 86</span></a><span class="sd">        **Args:**</span>
</span><span id="Backbone-87"><a href="#Backbone-87"><span class="linenos"> 87</span></a><span class="sd">        - **layer_name** (`str`): The name of the layer.</span>
</span><span id="Backbone-88"><a href="#Backbone-88"><span class="linenos"> 88</span></a>
</span><span id="Backbone-89"><a href="#Backbone-89"><span class="linenos"> 89</span></a><span class="sd">        **Returns:**</span>
</span><span id="Backbone-90"><a href="#Backbone-90"><span class="linenos"> 90</span></a><span class="sd">        - **next_layer_name** (`str`): The name of the next layer.</span>
</span><span id="Backbone-91"><a href="#Backbone-91"><span class="linenos"> 91</span></a>
</span><span id="Backbone-92"><a href="#Backbone-92"><span class="linenos"> 92</span></a><span class="sd">        **Raises:**</span>
</span><span id="Backbone-93"><a href="#Backbone-93"><span class="linenos"> 93</span></a><span class="sd">        - **ValueError**: If `layer_name` is not in the weighted layer order.</span>
</span><span id="Backbone-94"><a href="#Backbone-94"><span class="linenos"> 94</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="Backbone-95"><a href="#Backbone-95"><span class="linenos"> 95</span></a>
</span><span id="Backbone-96"><a href="#Backbone-96"><span class="linenos"> 96</span></a>        <span class="k">if</span> <span class="n">layer_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="Backbone-97"><a href="#Backbone-97"><span class="linenos"> 97</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The layer name </span><span class="si">{</span><span class="n">layer_name</span><span class="si">}</span><span class="s2"> doesn&#39;t exist.&quot;</span><span class="p">)</span>
</span><span id="Backbone-98"><a href="#Backbone-98"><span class="linenos"> 98</span></a>
</span><span id="Backbone-99"><a href="#Backbone-99"><span class="linenos"> 99</span></a>        <span class="n">weighted_layer_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="Backbone-100"><a href="#Backbone-100"><span class="linenos">100</span></a>        <span class="k">if</span> <span class="n">weighted_layer_idx</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="Backbone-101"><a href="#Backbone-101"><span class="linenos">101</span></a>            <span class="k">return</span> <span class="kc">None</span>
</span><span id="Backbone-102"><a href="#Backbone-102"><span class="linenos">102</span></a>        <span class="n">next_layer_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">[</span><span class="n">weighted_layer_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
</span><span id="Backbone-103"><a href="#Backbone-103"><span class="linenos">103</span></a>        <span class="k">return</span> <span class="n">next_layer_name</span>
</span><span id="Backbone-104"><a href="#Backbone-104"><span class="linenos">104</span></a>
</span><span id="Backbone-105"><a href="#Backbone-105"><span class="linenos">105</span></a>    <span class="nd">@override</span>  <span class="c1"># since `nn.Module` uses it</span>
</span><span id="Backbone-106"><a href="#Backbone-106"><span class="linenos">106</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="Backbone-107"><a href="#Backbone-107"><span class="linenos">107</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="Backbone-108"><a href="#Backbone-108"><span class="linenos">108</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="Backbone-109"><a href="#Backbone-109"><span class="linenos">109</span></a>        <span class="n">stage</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="Backbone-110"><a href="#Backbone-110"><span class="linenos">110</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
</span><span id="Backbone-111"><a href="#Backbone-111"><span class="linenos">111</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The forward pass. **It must be implemented by subclasses.**</span>
</span><span id="Backbone-112"><a href="#Backbone-112"><span class="linenos">112</span></a>
</span><span id="Backbone-113"><a href="#Backbone-113"><span class="linenos">113</span></a><span class="sd">        **Args:**</span>
</span><span id="Backbone-114"><a href="#Backbone-114"><span class="linenos">114</span></a><span class="sd">        - **input** (`Tensor`): The input tensor from data.</span>
</span><span id="Backbone-115"><a href="#Backbone-115"><span class="linenos">115</span></a><span class="sd">        - **stage** (`str`): The stage of the forward pass; one of:</span>
</span><span id="Backbone-116"><a href="#Backbone-116"><span class="linenos">116</span></a><span class="sd">            1. &#39;train&#39;: training stage.</span>
</span><span id="Backbone-117"><a href="#Backbone-117"><span class="linenos">117</span></a><span class="sd">            2. &#39;validation&#39;: validation stage.</span>
</span><span id="Backbone-118"><a href="#Backbone-118"><span class="linenos">118</span></a><span class="sd">            3. &#39;test&#39;: testing stage.</span>
</span><span id="Backbone-119"><a href="#Backbone-119"><span class="linenos">119</span></a>
</span><span id="Backbone-120"><a href="#Backbone-120"><span class="linenos">120</span></a><span class="sd">        **Returns:**</span>
</span><span id="Backbone-121"><a href="#Backbone-121"><span class="linenos">121</span></a><span class="sd">        - **output_feature** (`Tensor`): The output feature tensor to be passed into heads. This is the main target of backpropagation.</span>
</span><span id="Backbone-122"><a href="#Backbone-122"><span class="linenos">122</span></a><span class="sd">        - **activations** (`dict[str, Tensor]`): The hidden features (after activation) in each weighted layer. Key (`str`) is the weighted layer name, value (`Tensor`) is the hidden feature tensor. This is used for certain algorithms that need to use the hidden features for various purposes.</span>
</span><span id="Backbone-123"><a href="#Backbone-123"><span class="linenos">123</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span></pre></div>


            <div class="docstring"><p>The base class for continual learning backbone networks; inherits from <code>nn.Module</code>.</p>
</div>


                            <div id="Backbone.__init__" class="classattr">
                                        <input id="Backbone.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">Backbone</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span>, </span><span class="param"><span class="o">**</span><span class="n">kwargs</span></span>)</span>

                <label class="view-source-button" for="Backbone.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Backbone.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Backbone.__init__-28"><a href="#Backbone.__init__-28"><span class="linenos">28</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="Backbone.__init__-29"><a href="#Backbone.__init__-29"><span class="linenos">29</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialize the backbone network.</span>
</span><span id="Backbone.__init__-30"><a href="#Backbone.__init__-30"><span class="linenos">30</span></a>
</span><span id="Backbone.__init__-31"><a href="#Backbone.__init__-31"><span class="linenos">31</span></a><span class="sd">        **Args:**</span>
</span><span id="Backbone.__init__-32"><a href="#Backbone.__init__-32"><span class="linenos">32</span></a><span class="sd">        - **output_dim** (`int` | `None`): The output dimension that connects to output heads. The `input_dim` of output heads is expected to be the same as this `output_dim`. In some cases, this class is used as a block in the backbone network that doesn&#39;t have an output dimension. In this case, it can be `None`.</span>
</span><span id="Backbone.__init__-33"><a href="#Backbone.__init__-33"><span class="linenos">33</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="Backbone.__init__-34"><a href="#Backbone.__init__-34"><span class="linenos">34</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="Backbone.__init__-35"><a href="#Backbone.__init__-35"><span class="linenos">35</span></a>
</span><span id="Backbone.__init__-36"><a href="#Backbone.__init__-36"><span class="linenos">36</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">output_dim</span>
</span><span id="Backbone.__init__-37"><a href="#Backbone.__init__-37"><span class="linenos">37</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the output dimension of the backbone network.&quot;&quot;&quot;</span>
</span><span id="Backbone.__init__-38"><a href="#Backbone.__init__-38"><span class="linenos">38</span></a>
</span><span id="Backbone.__init__-39"><a href="#Backbone.__init__-39"><span class="linenos">39</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="Backbone.__init__-40"><a href="#Backbone.__init__-40"><span class="linenos">40</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Maintain a list of weighted layer names. A weighted layer has weights connecting to other weighted layers. They are the main part of neural networks. **It must be provided in subclasses.**</span>
</span><span id="Backbone.__init__-41"><a href="#Backbone.__init__-41"><span class="linenos">41</span></a><span class="sd">        </span>
</span><span id="Backbone.__init__-42"><a href="#Backbone.__init__-42"><span class="linenos">42</span></a><span class="sd">        The names follow the `nn.Module` internal naming mechanism. For example, if a layer is assigned to `self.conv1`, the name becomes `conv1`. If `nn.Sequential` is used, the name becomes the index of the layer in the sequence, such as `0`, `1`, etc. If a hierarchical structure is used (for example, a `nn.Module` is assigned to `self.block` which has `self.conv1`), the name becomes `block/conv1`. Note that it should be `block.conv1` according to `nn.Module`&#39;s internal mechanism, but we use &#39;/&#39; instead of &#39;.&#39; to avoid errors when using &#39;.&#39; as keys in a `ModuleDict`.</span>
</span><span id="Backbone.__init__-43"><a href="#Backbone.__init__-43"><span class="linenos">43</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span></pre></div>


            <div class="docstring"><p>Initialize the backbone network.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>output_dim</strong> (<code>int</code> | <code>None</code>): The output dimension that connects to output heads. The <code>input_dim</code> of output heads is expected to be the same as this <code><a href="#Backbone.output_dim">output_dim</a></code>. In some cases, this class is used as a block in the backbone network that doesn't have an output dimension. In this case, it can be <code>None</code>.</li>
</ul>
</div>


                            </div>
                            <div id="Backbone.output_dim" class="classattr">
                                <div class="attr variable">
            <span class="name">output_dim</span><span class="annotation">: int</span>

        
    </div>
    <a class="headerlink" href="#Backbone.output_dim"></a>
    
            <div class="docstring"><p>Store the output dimension of the backbone network.</p>
</div>


                            </div>
                            <div id="Backbone.weighted_layer_names" class="classattr">
                                <div class="attr variable">
            <span class="name">weighted_layer_names</span><span class="annotation">: list[str]</span>

        
    </div>
    <a class="headerlink" href="#Backbone.weighted_layer_names"></a>
    
            <div class="docstring"><p>Maintain a list of weighted layer names. A weighted layer has weights connecting to other weighted layers. They are the main part of neural networks. <strong>It must be provided in subclasses.</strong></p>

<p>The names follow the <code>nn.Module</code> internal naming mechanism. For example, if a layer is assigned to <code>self.conv1</code>, the name becomes <code>conv1</code>. If <code>nn.Sequential</code> is used, the name becomes the index of the layer in the sequence, such as <code>0</code>, <code>1</code>, etc. If a hierarchical structure is used (for example, a <code>nn.Module</code> is assigned to <code>self.block</code> which has <code>self.conv1</code>), the name becomes <code>block/conv1</code>. Note that it should be <code>block.conv1</code> according to <code>nn.Module</code>'s internal mechanism, but we use '/' instead of '.' to avoid errors when using '.' as keys in a <code>ModuleDict</code>.</p>
</div>


                            </div>
                            <div id="Backbone.get_layer_by_name" class="classattr">
                                        <input id="Backbone.get_layer_by_name-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">get_layer_by_name</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span>:</span></span>

                <label class="view-source-button" for="Backbone.get_layer_by_name-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Backbone.get_layer_by_name"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Backbone.get_layer_by_name-45"><a href="#Backbone.get_layer_by_name-45"><span class="linenos">45</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_layer_by_name</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">|</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="Backbone.get_layer_by_name-46"><a href="#Backbone.get_layer_by_name-46"><span class="linenos">46</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the layer by its name.</span>
</span><span id="Backbone.get_layer_by_name-47"><a href="#Backbone.get_layer_by_name-47"><span class="linenos">47</span></a>
</span><span id="Backbone.get_layer_by_name-48"><a href="#Backbone.get_layer_by_name-48"><span class="linenos">48</span></a><span class="sd">        **Args:**</span>
</span><span id="Backbone.get_layer_by_name-49"><a href="#Backbone.get_layer_by_name-49"><span class="linenos">49</span></a><span class="sd">        - **layer_name** (`str` | `None`): The layer name with &#39;.&#39; replaced by &#39;/&#39;, like `block/conv1`, rather than `block.conv1`. If `None`, return `None`.</span>
</span><span id="Backbone.get_layer_by_name-50"><a href="#Backbone.get_layer_by_name-50"><span class="linenos">50</span></a>
</span><span id="Backbone.get_layer_by_name-51"><a href="#Backbone.get_layer_by_name-51"><span class="linenos">51</span></a><span class="sd">        **Returns:**</span>
</span><span id="Backbone.get_layer_by_name-52"><a href="#Backbone.get_layer_by_name-52"><span class="linenos">52</span></a><span class="sd">        - **layer** (`nn.Module` | `None`): The layer.</span>
</span><span id="Backbone.get_layer_by_name-53"><a href="#Backbone.get_layer_by_name-53"><span class="linenos">53</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="Backbone.get_layer_by_name-54"><a href="#Backbone.get_layer_by_name-54"><span class="linenos">54</span></a>        <span class="k">if</span> <span class="n">layer_name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="Backbone.get_layer_by_name-55"><a href="#Backbone.get_layer_by_name-55"><span class="linenos">55</span></a>            <span class="k">return</span> <span class="kc">None</span>
</span><span id="Backbone.get_layer_by_name-56"><a href="#Backbone.get_layer_by_name-56"><span class="linenos">56</span></a>
</span><span id="Backbone.get_layer_by_name-57"><a href="#Backbone.get_layer_by_name-57"><span class="linenos">57</span></a>        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
</span><span id="Backbone.get_layer_by_name-58"><a href="#Backbone.get_layer_by_name-58"><span class="linenos">58</span></a>            <span class="k">if</span> <span class="n">name</span> <span class="o">==</span> <span class="n">layer_name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">,</span> <span class="s2">&quot;.&quot;</span><span class="p">):</span>
</span><span id="Backbone.get_layer_by_name-59"><a href="#Backbone.get_layer_by_name-59"><span class="linenos">59</span></a>                <span class="k">return</span> <span class="n">layer</span>
</span></pre></div>


            <div class="docstring"><p>Get the layer by its name.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>layer_name</strong> (<code>str</code> | <code>None</code>): The layer name with '.' replaced by '/', like <code>block/conv1</code>, rather than <code>block.conv1</code>. If <code>None</code>, return <code>None</code>.</li>
</ul>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>layer</strong> (<code>nn.Module</code> | <code>None</code>): The layer.</li>
</ul>
</div>


                            </div>
                            <div id="Backbone.preceding_layer_name" class="classattr">
                                        <input id="Backbone.preceding_layer_name-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">preceding_layer_name</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span></span><span class="return-annotation">) -> <span class="nb">str</span>:</span></span>

                <label class="view-source-button" for="Backbone.preceding_layer_name-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Backbone.preceding_layer_name"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Backbone.preceding_layer_name-61"><a href="#Backbone.preceding_layer_name-61"><span class="linenos">61</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">preceding_layer_name</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
</span><span id="Backbone.preceding_layer_name-62"><a href="#Backbone.preceding_layer_name-62"><span class="linenos">62</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the name of the preceding layer of the given layer from the stored `self.masked_layer_order`. If the given layer is the first layer, return `None`.</span>
</span><span id="Backbone.preceding_layer_name-63"><a href="#Backbone.preceding_layer_name-63"><span class="linenos">63</span></a>
</span><span id="Backbone.preceding_layer_name-64"><a href="#Backbone.preceding_layer_name-64"><span class="linenos">64</span></a><span class="sd">        **Args:**</span>
</span><span id="Backbone.preceding_layer_name-65"><a href="#Backbone.preceding_layer_name-65"><span class="linenos">65</span></a><span class="sd">        - **layer_name** (`str`): The name of the layer.</span>
</span><span id="Backbone.preceding_layer_name-66"><a href="#Backbone.preceding_layer_name-66"><span class="linenos">66</span></a>
</span><span id="Backbone.preceding_layer_name-67"><a href="#Backbone.preceding_layer_name-67"><span class="linenos">67</span></a><span class="sd">        **Returns:**</span>
</span><span id="Backbone.preceding_layer_name-68"><a href="#Backbone.preceding_layer_name-68"><span class="linenos">68</span></a><span class="sd">        - **preceding_layer_name** (`str`): The name of the preceding layer.</span>
</span><span id="Backbone.preceding_layer_name-69"><a href="#Backbone.preceding_layer_name-69"><span class="linenos">69</span></a>
</span><span id="Backbone.preceding_layer_name-70"><a href="#Backbone.preceding_layer_name-70"><span class="linenos">70</span></a><span class="sd">        **Raises:**</span>
</span><span id="Backbone.preceding_layer_name-71"><a href="#Backbone.preceding_layer_name-71"><span class="linenos">71</span></a><span class="sd">        - **ValueError**: If `layer_name` is not in the weighted layer order.</span>
</span><span id="Backbone.preceding_layer_name-72"><a href="#Backbone.preceding_layer_name-72"><span class="linenos">72</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="Backbone.preceding_layer_name-73"><a href="#Backbone.preceding_layer_name-73"><span class="linenos">73</span></a>
</span><span id="Backbone.preceding_layer_name-74"><a href="#Backbone.preceding_layer_name-74"><span class="linenos">74</span></a>        <span class="k">if</span> <span class="n">layer_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="Backbone.preceding_layer_name-75"><a href="#Backbone.preceding_layer_name-75"><span class="linenos">75</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The layer name </span><span class="si">{</span><span class="n">layer_name</span><span class="si">}</span><span class="s2"> doesn&#39;t exist.&quot;</span><span class="p">)</span>
</span><span id="Backbone.preceding_layer_name-76"><a href="#Backbone.preceding_layer_name-76"><span class="linenos">76</span></a>
</span><span id="Backbone.preceding_layer_name-77"><a href="#Backbone.preceding_layer_name-77"><span class="linenos">77</span></a>        <span class="n">weighted_layer_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="Backbone.preceding_layer_name-78"><a href="#Backbone.preceding_layer_name-78"><span class="linenos">78</span></a>        <span class="k">if</span> <span class="n">weighted_layer_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="Backbone.preceding_layer_name-79"><a href="#Backbone.preceding_layer_name-79"><span class="linenos">79</span></a>            <span class="k">return</span> <span class="kc">None</span>
</span><span id="Backbone.preceding_layer_name-80"><a href="#Backbone.preceding_layer_name-80"><span class="linenos">80</span></a>        <span class="n">preceding_layer_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">[</span><span class="n">weighted_layer_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
</span><span id="Backbone.preceding_layer_name-81"><a href="#Backbone.preceding_layer_name-81"><span class="linenos">81</span></a>        <span class="k">return</span> <span class="n">preceding_layer_name</span>
</span></pre></div>


            <div class="docstring"><p>Get the name of the preceding layer of the given layer from the stored <code>self.masked_layer_order</code>. If the given layer is the first layer, return <code>None</code>.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>layer_name</strong> (<code>str</code>): The name of the layer.</li>
</ul>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>preceding_layer_name</strong> (<code>str</code>): The name of the preceding layer.</li>
</ul>

<p><strong>Raises:</strong></p>

<ul>
<li><strong>ValueError</strong>: If <code>layer_name</code> is not in the weighted layer order.</li>
</ul>
</div>


                            </div>
                            <div id="Backbone.next_layer_name" class="classattr">
                                        <input id="Backbone.next_layer_name-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">next_layer_name</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span></span><span class="return-annotation">) -> <span class="nb">str</span>:</span></span>

                <label class="view-source-button" for="Backbone.next_layer_name-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Backbone.next_layer_name"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Backbone.next_layer_name-83"><a href="#Backbone.next_layer_name-83"><span class="linenos"> 83</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">next_layer_name</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
</span><span id="Backbone.next_layer_name-84"><a href="#Backbone.next_layer_name-84"><span class="linenos"> 84</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the name of the next layer of the given layer from the stored `self.masked_layer_order`. If the given layer is the last layer of the BACKBONE, return `None`.</span>
</span><span id="Backbone.next_layer_name-85"><a href="#Backbone.next_layer_name-85"><span class="linenos"> 85</span></a>
</span><span id="Backbone.next_layer_name-86"><a href="#Backbone.next_layer_name-86"><span class="linenos"> 86</span></a><span class="sd">        **Args:**</span>
</span><span id="Backbone.next_layer_name-87"><a href="#Backbone.next_layer_name-87"><span class="linenos"> 87</span></a><span class="sd">        - **layer_name** (`str`): The name of the layer.</span>
</span><span id="Backbone.next_layer_name-88"><a href="#Backbone.next_layer_name-88"><span class="linenos"> 88</span></a>
</span><span id="Backbone.next_layer_name-89"><a href="#Backbone.next_layer_name-89"><span class="linenos"> 89</span></a><span class="sd">        **Returns:**</span>
</span><span id="Backbone.next_layer_name-90"><a href="#Backbone.next_layer_name-90"><span class="linenos"> 90</span></a><span class="sd">        - **next_layer_name** (`str`): The name of the next layer.</span>
</span><span id="Backbone.next_layer_name-91"><a href="#Backbone.next_layer_name-91"><span class="linenos"> 91</span></a>
</span><span id="Backbone.next_layer_name-92"><a href="#Backbone.next_layer_name-92"><span class="linenos"> 92</span></a><span class="sd">        **Raises:**</span>
</span><span id="Backbone.next_layer_name-93"><a href="#Backbone.next_layer_name-93"><span class="linenos"> 93</span></a><span class="sd">        - **ValueError**: If `layer_name` is not in the weighted layer order.</span>
</span><span id="Backbone.next_layer_name-94"><a href="#Backbone.next_layer_name-94"><span class="linenos"> 94</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="Backbone.next_layer_name-95"><a href="#Backbone.next_layer_name-95"><span class="linenos"> 95</span></a>
</span><span id="Backbone.next_layer_name-96"><a href="#Backbone.next_layer_name-96"><span class="linenos"> 96</span></a>        <span class="k">if</span> <span class="n">layer_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="Backbone.next_layer_name-97"><a href="#Backbone.next_layer_name-97"><span class="linenos"> 97</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The layer name </span><span class="si">{</span><span class="n">layer_name</span><span class="si">}</span><span class="s2"> doesn&#39;t exist.&quot;</span><span class="p">)</span>
</span><span id="Backbone.next_layer_name-98"><a href="#Backbone.next_layer_name-98"><span class="linenos"> 98</span></a>
</span><span id="Backbone.next_layer_name-99"><a href="#Backbone.next_layer_name-99"><span class="linenos"> 99</span></a>        <span class="n">weighted_layer_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="Backbone.next_layer_name-100"><a href="#Backbone.next_layer_name-100"><span class="linenos">100</span></a>        <span class="k">if</span> <span class="n">weighted_layer_idx</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="Backbone.next_layer_name-101"><a href="#Backbone.next_layer_name-101"><span class="linenos">101</span></a>            <span class="k">return</span> <span class="kc">None</span>
</span><span id="Backbone.next_layer_name-102"><a href="#Backbone.next_layer_name-102"><span class="linenos">102</span></a>        <span class="n">next_layer_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">[</span><span class="n">weighted_layer_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
</span><span id="Backbone.next_layer_name-103"><a href="#Backbone.next_layer_name-103"><span class="linenos">103</span></a>        <span class="k">return</span> <span class="n">next_layer_name</span>
</span></pre></div>


            <div class="docstring"><p>Get the name of the next layer of the given layer from the stored <code>self.masked_layer_order</code>. If the given layer is the last layer of the BACKBONE, return <code>None</code>.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>layer_name</strong> (<code>str</code>): The name of the layer.</li>
</ul>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>next_layer_name</strong> (<code>str</code>): The name of the next layer.</li>
</ul>

<p><strong>Raises:</strong></p>

<ul>
<li><strong>ValueError</strong>: If <code>layer_name</code> is not in the weighted layer order.</li>
</ul>
</div>


                            </div>
                            <div id="Backbone.forward" class="classattr">
                                        <input id="Backbone.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
                    <div class="decorator decorator-override">@override</div>

        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">stage</span><span class="p">:</span> <span class="nb">str</span></span><span class="return-annotation">) -> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span>:</span></span>

                <label class="view-source-button" for="Backbone.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Backbone.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Backbone.forward-105"><a href="#Backbone.forward-105"><span class="linenos">105</span></a>    <span class="nd">@override</span>  <span class="c1"># since `nn.Module` uses it</span>
</span><span id="Backbone.forward-106"><a href="#Backbone.forward-106"><span class="linenos">106</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="Backbone.forward-107"><a href="#Backbone.forward-107"><span class="linenos">107</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="Backbone.forward-108"><a href="#Backbone.forward-108"><span class="linenos">108</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="Backbone.forward-109"><a href="#Backbone.forward-109"><span class="linenos">109</span></a>        <span class="n">stage</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="Backbone.forward-110"><a href="#Backbone.forward-110"><span class="linenos">110</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
</span><span id="Backbone.forward-111"><a href="#Backbone.forward-111"><span class="linenos">111</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The forward pass. **It must be implemented by subclasses.**</span>
</span><span id="Backbone.forward-112"><a href="#Backbone.forward-112"><span class="linenos">112</span></a>
</span><span id="Backbone.forward-113"><a href="#Backbone.forward-113"><span class="linenos">113</span></a><span class="sd">        **Args:**</span>
</span><span id="Backbone.forward-114"><a href="#Backbone.forward-114"><span class="linenos">114</span></a><span class="sd">        - **input** (`Tensor`): The input tensor from data.</span>
</span><span id="Backbone.forward-115"><a href="#Backbone.forward-115"><span class="linenos">115</span></a><span class="sd">        - **stage** (`str`): The stage of the forward pass; one of:</span>
</span><span id="Backbone.forward-116"><a href="#Backbone.forward-116"><span class="linenos">116</span></a><span class="sd">            1. &#39;train&#39;: training stage.</span>
</span><span id="Backbone.forward-117"><a href="#Backbone.forward-117"><span class="linenos">117</span></a><span class="sd">            2. &#39;validation&#39;: validation stage.</span>
</span><span id="Backbone.forward-118"><a href="#Backbone.forward-118"><span class="linenos">118</span></a><span class="sd">            3. &#39;test&#39;: testing stage.</span>
</span><span id="Backbone.forward-119"><a href="#Backbone.forward-119"><span class="linenos">119</span></a>
</span><span id="Backbone.forward-120"><a href="#Backbone.forward-120"><span class="linenos">120</span></a><span class="sd">        **Returns:**</span>
</span><span id="Backbone.forward-121"><a href="#Backbone.forward-121"><span class="linenos">121</span></a><span class="sd">        - **output_feature** (`Tensor`): The output feature tensor to be passed into heads. This is the main target of backpropagation.</span>
</span><span id="Backbone.forward-122"><a href="#Backbone.forward-122"><span class="linenos">122</span></a><span class="sd">        - **activations** (`dict[str, Tensor]`): The hidden features (after activation) in each weighted layer. Key (`str`) is the weighted layer name, value (`Tensor`) is the hidden feature tensor. This is used for certain algorithms that need to use the hidden features for various purposes.</span>
</span><span id="Backbone.forward-123"><a href="#Backbone.forward-123"><span class="linenos">123</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span></pre></div>


            <div class="docstring"><p>The forward pass. <strong>It must be implemented by subclasses.</strong></p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>input</strong> (<code>Tensor</code>): The input tensor from data.</li>
<li><strong>stage</strong> (<code>str</code>): The stage of the forward pass; one of:
<ol>
<li>'train': training stage.</li>
<li>'validation': validation stage.</li>
<li>'test': testing stage.</li>
</ol></li>
</ul>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>output_feature</strong> (<code>Tensor</code>): The output feature tensor to be passed into heads. This is the main target of backpropagation.</li>
<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): The hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name, value (<code>Tensor</code>) is the hidden feature tensor. This is used for certain algorithms that need to use the hidden features for various purposes.</li>
</ul>
</div>


                            </div>
                </section>
                <section id="CLBackbone">
                            <input id="CLBackbone-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">CLBackbone</span><wbr>(<span class="base"><a href="#Backbone">clarena.backbones.Backbone</a></span>):

                <label class="view-source-button" for="CLBackbone-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#CLBackbone"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="CLBackbone-126"><a href="#CLBackbone-126"><span class="linenos">126</span></a><span class="k">class</span><span class="w"> </span><span class="nc">CLBackbone</span><span class="p">(</span><span class="n">Backbone</span><span class="p">):</span>
</span><span id="CLBackbone-127"><a href="#CLBackbone-127"><span class="linenos">127</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The base class of continual learning backbone networks, inherited from `Backbone`.&quot;&quot;&quot;</span>
</span><span id="CLBackbone-128"><a href="#CLBackbone-128"><span class="linenos">128</span></a>
</span><span id="CLBackbone-129"><a href="#CLBackbone-129"><span class="linenos">129</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="CLBackbone-130"><a href="#CLBackbone-130"><span class="linenos">130</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialize the CL backbone network.</span>
</span><span id="CLBackbone-131"><a href="#CLBackbone-131"><span class="linenos">131</span></a>
</span><span id="CLBackbone-132"><a href="#CLBackbone-132"><span class="linenos">132</span></a><span class="sd">        **Args:**</span>
</span><span id="CLBackbone-133"><a href="#CLBackbone-133"><span class="linenos">133</span></a><span class="sd">        - **output_dim** (`int` | `None`): The output dimension that connects to CL output heads. The `input_dim` of output heads is expected to be the same as this `output_dim`. In some cases, this class is used as a block in the backbone network that doesn&#39;t have an output dimension. In this case, it can be `None`.</span>
</span><span id="CLBackbone-134"><a href="#CLBackbone-134"><span class="linenos">134</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="CLBackbone-135"><a href="#CLBackbone-135"><span class="linenos">135</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span id="CLBackbone-136"><a href="#CLBackbone-136"><span class="linenos">136</span></a>
</span><span id="CLBackbone-137"><a href="#CLBackbone-137"><span class="linenos">137</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">:</span> <span class="nb">int</span>
</span><span id="CLBackbone-138"><a href="#CLBackbone-138"><span class="linenos">138</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Task ID counter indicating which task is being processed. Self-updated during the task loop. Starting from 1.&quot;&quot;&quot;</span>
</span><span id="CLBackbone-139"><a href="#CLBackbone-139"><span class="linenos">139</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">processed_task_ids</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="CLBackbone-140"><a href="#CLBackbone-140"><span class="linenos">140</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Task IDs that have been processed in the experiment.&quot;&quot;&quot;</span>
</span><span id="CLBackbone-141"><a href="#CLBackbone-141"><span class="linenos">141</span></a>
</span><span id="CLBackbone-142"><a href="#CLBackbone-142"><span class="linenos">142</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">setup_task_id</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">task_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="CLBackbone-143"><a href="#CLBackbone-143"><span class="linenos">143</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Set up task `task_id`. This must be done before the `forward()` method is called.</span>
</span><span id="CLBackbone-144"><a href="#CLBackbone-144"><span class="linenos">144</span></a>
</span><span id="CLBackbone-145"><a href="#CLBackbone-145"><span class="linenos">145</span></a><span class="sd">        **Args:**</span>
</span><span id="CLBackbone-146"><a href="#CLBackbone-146"><span class="linenos">146</span></a><span class="sd">        - **task_id** (`int`): The target task ID.</span>
</span><span id="CLBackbone-147"><a href="#CLBackbone-147"><span class="linenos">147</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="CLBackbone-148"><a href="#CLBackbone-148"><span class="linenos">148</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">=</span> <span class="n">task_id</span>
</span><span id="CLBackbone-149"><a href="#CLBackbone-149"><span class="linenos">149</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">processed_task_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">task_id</span><span class="p">)</span>
</span><span id="CLBackbone-150"><a href="#CLBackbone-150"><span class="linenos">150</span></a>
</span><span id="CLBackbone-151"><a href="#CLBackbone-151"><span class="linenos">151</span></a>    <span class="nd">@override</span>  <span class="c1"># since `nn.Module` uses it</span>
</span><span id="CLBackbone-152"><a href="#CLBackbone-152"><span class="linenos">152</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="CLBackbone-153"><a href="#CLBackbone-153"><span class="linenos">153</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="CLBackbone-154"><a href="#CLBackbone-154"><span class="linenos">154</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="CLBackbone-155"><a href="#CLBackbone-155"><span class="linenos">155</span></a>        <span class="n">stage</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="CLBackbone-156"><a href="#CLBackbone-156"><span class="linenos">156</span></a>        <span class="n">task_id</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="CLBackbone-157"><a href="#CLBackbone-157"><span class="linenos">157</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
</span><span id="CLBackbone-158"><a href="#CLBackbone-158"><span class="linenos">158</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The forward pass for data from task `task_id`. In some backbones, the forward pass might be different for different tasks. **It must be implemented by subclasses.**</span>
</span><span id="CLBackbone-159"><a href="#CLBackbone-159"><span class="linenos">159</span></a>
</span><span id="CLBackbone-160"><a href="#CLBackbone-160"><span class="linenos">160</span></a><span class="sd">        **Args:**</span>
</span><span id="CLBackbone-161"><a href="#CLBackbone-161"><span class="linenos">161</span></a><span class="sd">        - **input** (`Tensor`): The input tensor from data.</span>
</span><span id="CLBackbone-162"><a href="#CLBackbone-162"><span class="linenos">162</span></a><span class="sd">        - **stage** (`str`): The stage of the forward pass; one of:</span>
</span><span id="CLBackbone-163"><a href="#CLBackbone-163"><span class="linenos">163</span></a><span class="sd">            1. &#39;train&#39;: training stage.</span>
</span><span id="CLBackbone-164"><a href="#CLBackbone-164"><span class="linenos">164</span></a><span class="sd">            2. &#39;validation&#39;: validation stage.</span>
</span><span id="CLBackbone-165"><a href="#CLBackbone-165"><span class="linenos">165</span></a><span class="sd">            3. &#39;test&#39;: testing stage.</span>
</span><span id="CLBackbone-166"><a href="#CLBackbone-166"><span class="linenos">166</span></a><span class="sd">        - **task_id** (`int` | `None`): The task ID where the data are from. If the stage is &#39;train&#39; or &#39;validation&#39;, it is usually the current task `self.task_id`. If the stage is &#39;test&#39;, it could be from any seen task. In TIL, the task IDs of test data are provided; thus this argument can be used. In CIL, they are not provided, so it is just a placeholder for API consistency and is not used. Best practice is not to provide this argument and leave it as the default value.</span>
</span><span id="CLBackbone-167"><a href="#CLBackbone-167"><span class="linenos">167</span></a>
</span><span id="CLBackbone-168"><a href="#CLBackbone-168"><span class="linenos">168</span></a><span class="sd">        **Returns:**</span>
</span><span id="CLBackbone-169"><a href="#CLBackbone-169"><span class="linenos">169</span></a><span class="sd">        - **output_feature** (`Tensor`): The output feature tensor to be passed into heads. This is the main target of backpropagation.</span>
</span><span id="CLBackbone-170"><a href="#CLBackbone-170"><span class="linenos">170</span></a><span class="sd">        - **activations** (`dict[str, Tensor]`): The hidden features (after activation) in each weighted layer. Key (`str`) is the weighted layer name, value (`Tensor`) is the hidden feature tensor. This is used for continual learning algorithms that need hidden features for various purposes.</span>
</span><span id="CLBackbone-171"><a href="#CLBackbone-171"><span class="linenos">171</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span></pre></div>


            <div class="docstring"><p>The base class of continual learning backbone networks, inherited from <code><a href="#Backbone">Backbone</a></code>.</p>
</div>


                            <div id="CLBackbone.__init__" class="classattr">
                                        <input id="CLBackbone.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">CLBackbone</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span>, </span><span class="param"><span class="o">**</span><span class="n">kwargs</span></span>)</span>

                <label class="view-source-button" for="CLBackbone.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#CLBackbone.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="CLBackbone.__init__-129"><a href="#CLBackbone.__init__-129"><span class="linenos">129</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="CLBackbone.__init__-130"><a href="#CLBackbone.__init__-130"><span class="linenos">130</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialize the CL backbone network.</span>
</span><span id="CLBackbone.__init__-131"><a href="#CLBackbone.__init__-131"><span class="linenos">131</span></a>
</span><span id="CLBackbone.__init__-132"><a href="#CLBackbone.__init__-132"><span class="linenos">132</span></a><span class="sd">        **Args:**</span>
</span><span id="CLBackbone.__init__-133"><a href="#CLBackbone.__init__-133"><span class="linenos">133</span></a><span class="sd">        - **output_dim** (`int` | `None`): The output dimension that connects to CL output heads. The `input_dim` of output heads is expected to be the same as this `output_dim`. In some cases, this class is used as a block in the backbone network that doesn&#39;t have an output dimension. In this case, it can be `None`.</span>
</span><span id="CLBackbone.__init__-134"><a href="#CLBackbone.__init__-134"><span class="linenos">134</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="CLBackbone.__init__-135"><a href="#CLBackbone.__init__-135"><span class="linenos">135</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span id="CLBackbone.__init__-136"><a href="#CLBackbone.__init__-136"><span class="linenos">136</span></a>
</span><span id="CLBackbone.__init__-137"><a href="#CLBackbone.__init__-137"><span class="linenos">137</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="p">:</span> <span class="nb">int</span>
</span><span id="CLBackbone.__init__-138"><a href="#CLBackbone.__init__-138"><span class="linenos">138</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Task ID counter indicating which task is being processed. Self-updated during the task loop. Starting from 1.&quot;&quot;&quot;</span>
</span><span id="CLBackbone.__init__-139"><a href="#CLBackbone.__init__-139"><span class="linenos">139</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">processed_task_ids</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="CLBackbone.__init__-140"><a href="#CLBackbone.__init__-140"><span class="linenos">140</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Task IDs that have been processed in the experiment.&quot;&quot;&quot;</span>
</span></pre></div>


            <div class="docstring"><p>Initialize the CL backbone network.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>output_dim</strong> (<code>int</code> | <code>None</code>): The output dimension that connects to CL output heads. The <code>input_dim</code> of output heads is expected to be the same as this <code><a href="#CLBackbone.output_dim">output_dim</a></code>. In some cases, this class is used as a block in the backbone network that doesn't have an output dimension. In this case, it can be <code>None</code>.</li>
</ul>
</div>


                            </div>
                            <div id="CLBackbone.task_id" class="classattr">
                                <div class="attr variable">
            <span class="name">task_id</span><span class="annotation">: int</span>

        
    </div>
    <a class="headerlink" href="#CLBackbone.task_id"></a>
    
            <div class="docstring"><p>Task ID counter indicating which task is being processed. Self-updated during the task loop. Starting from 1.</p>
</div>


                            </div>
                            <div id="CLBackbone.processed_task_ids" class="classattr">
                                <div class="attr variable">
            <span class="name">processed_task_ids</span><span class="annotation">: list[int]</span>

        
    </div>
    <a class="headerlink" href="#CLBackbone.processed_task_ids"></a>
    
            <div class="docstring"><p>Task IDs that have been processed in the experiment.</p>
</div>


                            </div>
                            <div id="CLBackbone.setup_task_id" class="classattr">
                                        <input id="CLBackbone.setup_task_id-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">setup_task_id</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">task_id</span><span class="p">:</span> <span class="nb">int</span></span><span class="return-annotation">) -> <span class="kc">None</span>:</span></span>

                <label class="view-source-button" for="CLBackbone.setup_task_id-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#CLBackbone.setup_task_id"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="CLBackbone.setup_task_id-142"><a href="#CLBackbone.setup_task_id-142"><span class="linenos">142</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">setup_task_id</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">task_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="CLBackbone.setup_task_id-143"><a href="#CLBackbone.setup_task_id-143"><span class="linenos">143</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Set up task `task_id`. This must be done before the `forward()` method is called.</span>
</span><span id="CLBackbone.setup_task_id-144"><a href="#CLBackbone.setup_task_id-144"><span class="linenos">144</span></a>
</span><span id="CLBackbone.setup_task_id-145"><a href="#CLBackbone.setup_task_id-145"><span class="linenos">145</span></a><span class="sd">        **Args:**</span>
</span><span id="CLBackbone.setup_task_id-146"><a href="#CLBackbone.setup_task_id-146"><span class="linenos">146</span></a><span class="sd">        - **task_id** (`int`): The target task ID.</span>
</span><span id="CLBackbone.setup_task_id-147"><a href="#CLBackbone.setup_task_id-147"><span class="linenos">147</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="CLBackbone.setup_task_id-148"><a href="#CLBackbone.setup_task_id-148"><span class="linenos">148</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">task_id</span> <span class="o">=</span> <span class="n">task_id</span>
</span><span id="CLBackbone.setup_task_id-149"><a href="#CLBackbone.setup_task_id-149"><span class="linenos">149</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">processed_task_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">task_id</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Set up task <code><a href="#CLBackbone.task_id">task_id</a></code>. This must be done before the <code><a href="#CLBackbone.forward">forward()</a></code> method is called.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>task_id</strong> (<code>int</code>): The target task ID.</li>
</ul>
</div>


                            </div>
                            <div id="CLBackbone.forward" class="classattr">
                                        <input id="CLBackbone.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
                    <div class="decorator decorator-override">@override</div>

        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">stage</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">task_id</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span></span><span class="return-annotation">) -> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span>:</span></span>

                <label class="view-source-button" for="CLBackbone.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#CLBackbone.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="CLBackbone.forward-151"><a href="#CLBackbone.forward-151"><span class="linenos">151</span></a>    <span class="nd">@override</span>  <span class="c1"># since `nn.Module` uses it</span>
</span><span id="CLBackbone.forward-152"><a href="#CLBackbone.forward-152"><span class="linenos">152</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="CLBackbone.forward-153"><a href="#CLBackbone.forward-153"><span class="linenos">153</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="CLBackbone.forward-154"><a href="#CLBackbone.forward-154"><span class="linenos">154</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="CLBackbone.forward-155"><a href="#CLBackbone.forward-155"><span class="linenos">155</span></a>        <span class="n">stage</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="CLBackbone.forward-156"><a href="#CLBackbone.forward-156"><span class="linenos">156</span></a>        <span class="n">task_id</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="CLBackbone.forward-157"><a href="#CLBackbone.forward-157"><span class="linenos">157</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
</span><span id="CLBackbone.forward-158"><a href="#CLBackbone.forward-158"><span class="linenos">158</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The forward pass for data from task `task_id`. In some backbones, the forward pass might be different for different tasks. **It must be implemented by subclasses.**</span>
</span><span id="CLBackbone.forward-159"><a href="#CLBackbone.forward-159"><span class="linenos">159</span></a>
</span><span id="CLBackbone.forward-160"><a href="#CLBackbone.forward-160"><span class="linenos">160</span></a><span class="sd">        **Args:**</span>
</span><span id="CLBackbone.forward-161"><a href="#CLBackbone.forward-161"><span class="linenos">161</span></a><span class="sd">        - **input** (`Tensor`): The input tensor from data.</span>
</span><span id="CLBackbone.forward-162"><a href="#CLBackbone.forward-162"><span class="linenos">162</span></a><span class="sd">        - **stage** (`str`): The stage of the forward pass; one of:</span>
</span><span id="CLBackbone.forward-163"><a href="#CLBackbone.forward-163"><span class="linenos">163</span></a><span class="sd">            1. &#39;train&#39;: training stage.</span>
</span><span id="CLBackbone.forward-164"><a href="#CLBackbone.forward-164"><span class="linenos">164</span></a><span class="sd">            2. &#39;validation&#39;: validation stage.</span>
</span><span id="CLBackbone.forward-165"><a href="#CLBackbone.forward-165"><span class="linenos">165</span></a><span class="sd">            3. &#39;test&#39;: testing stage.</span>
</span><span id="CLBackbone.forward-166"><a href="#CLBackbone.forward-166"><span class="linenos">166</span></a><span class="sd">        - **task_id** (`int` | `None`): The task ID where the data are from. If the stage is &#39;train&#39; or &#39;validation&#39;, it is usually the current task `self.task_id`. If the stage is &#39;test&#39;, it could be from any seen task. In TIL, the task IDs of test data are provided; thus this argument can be used. In CIL, they are not provided, so it is just a placeholder for API consistency and is not used. Best practice is not to provide this argument and leave it as the default value.</span>
</span><span id="CLBackbone.forward-167"><a href="#CLBackbone.forward-167"><span class="linenos">167</span></a>
</span><span id="CLBackbone.forward-168"><a href="#CLBackbone.forward-168"><span class="linenos">168</span></a><span class="sd">        **Returns:**</span>
</span><span id="CLBackbone.forward-169"><a href="#CLBackbone.forward-169"><span class="linenos">169</span></a><span class="sd">        - **output_feature** (`Tensor`): The output feature tensor to be passed into heads. This is the main target of backpropagation.</span>
</span><span id="CLBackbone.forward-170"><a href="#CLBackbone.forward-170"><span class="linenos">170</span></a><span class="sd">        - **activations** (`dict[str, Tensor]`): The hidden features (after activation) in each weighted layer. Key (`str`) is the weighted layer name, value (`Tensor`) is the hidden feature tensor. This is used for continual learning algorithms that need hidden features for various purposes.</span>
</span><span id="CLBackbone.forward-171"><a href="#CLBackbone.forward-171"><span class="linenos">171</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span></pre></div>


            <div class="docstring"><p>The forward pass for data from task <code><a href="#CLBackbone.task_id">task_id</a></code>. In some backbones, the forward pass might be different for different tasks. <strong>It must be implemented by subclasses.</strong></p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>input</strong> (<code>Tensor</code>): The input tensor from data.</li>
<li><strong>stage</strong> (<code>str</code>): The stage of the forward pass; one of:
<ol>
<li>'train': training stage.</li>
<li>'validation': validation stage.</li>
<li>'test': testing stage.</li>
</ol></li>
<li><strong>task_id</strong> (<code>int</code> | <code>None</code>): The task ID where the data are from. If the stage is 'train' or 'validation', it is usually the current task <code>self.task_id</code>. If the stage is 'test', it could be from any seen task. In TIL, the task IDs of test data are provided; thus this argument can be used. In CIL, they are not provided, so it is just a placeholder for API consistency and is not used. Best practice is not to provide this argument and leave it as the default value.</li>
</ul>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>output_feature</strong> (<code>Tensor</code>): The output feature tensor to be passed into heads. This is the main target of backpropagation.</li>
<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): The hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name, value (<code>Tensor</code>) is the hidden feature tensor. This is used for continual learning algorithms that need hidden features for various purposes.</li>
</ul>
</div>


                            </div>
                </section>
                <section id="HATMaskBackbone">
                            <input id="HATMaskBackbone-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">HATMaskBackbone</span><wbr>(<span class="base"><a href="#CLBackbone">clarena.backbones.CLBackbone</a></span>):

                <label class="view-source-button" for="HATMaskBackbone-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HATMaskBackbone"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HATMaskBackbone-174"><a href="#HATMaskBackbone-174"><span class="linenos">174</span></a><span class="k">class</span><span class="w"> </span><span class="nc">HATMaskBackbone</span><span class="p">(</span><span class="n">CLBackbone</span><span class="p">):</span>
</span><span id="HATMaskBackbone-175"><a href="#HATMaskBackbone-175"><span class="linenos">175</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The backbone network for HAT-based algorithms with learnable hard attention masks.</span>
</span><span id="HATMaskBackbone-176"><a href="#HATMaskBackbone-176"><span class="linenos">176</span></a>
</span><span id="HATMaskBackbone-177"><a href="#HATMaskBackbone-177"><span class="linenos">177</span></a><span class="sd">    HAT-based algorithms:</span>
</span><span id="HATMaskBackbone-178"><a href="#HATMaskBackbone-178"><span class="linenos">178</span></a>
</span><span id="HATMaskBackbone-179"><a href="#HATMaskBackbone-179"><span class="linenos">179</span></a><span class="sd">    - [**HAT (Hard Attention to the Task, 2018)**](http://proceedings.mlr.press/v80/serra18a) is an architecture-based continual learning approach that uses learnable hard attention masks to select task-specific parameters.</span>
</span><span id="HATMaskBackbone-180"><a href="#HATMaskBackbone-180"><span class="linenos">180</span></a><span class="sd">    - [**Adaptive HAT (Adaptive Hard Attention to the Task, 2024)**](https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9) is an architecture-based continual learning approach that improves HAT by introducing adaptive soft gradient clipping based on parameter importance and network sparsity.</span>
</span><span id="HATMaskBackbone-181"><a href="#HATMaskBackbone-181"><span class="linenos">181</span></a><span class="sd">    - **FG-AdaHAT** is an architecture-based continual learning approach that improves HAT by introducing fine-grained neuron-wise importance measures guiding the adaptive adjustment mechanism in AdaHAT.</span>
</span><span id="HATMaskBackbone-182"><a href="#HATMaskBackbone-182"><span class="linenos">182</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="HATMaskBackbone-183"><a href="#HATMaskBackbone-183"><span class="linenos">183</span></a>
</span><span id="HATMaskBackbone-184"><a href="#HATMaskBackbone-184"><span class="linenos">184</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span> <span class="n">gate</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HATMaskBackbone-185"><a href="#HATMaskBackbone-185"><span class="linenos">185</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialize the HAT mask backbone network with task embeddings and masks.</span>
</span><span id="HATMaskBackbone-186"><a href="#HATMaskBackbone-186"><span class="linenos">186</span></a>
</span><span id="HATMaskBackbone-187"><a href="#HATMaskBackbone-187"><span class="linenos">187</span></a><span class="sd">        **Args:**</span>
</span><span id="HATMaskBackbone-188"><a href="#HATMaskBackbone-188"><span class="linenos">188</span></a><span class="sd">        - **output_dim** (`int`): The output dimension that connects to CL output heads. The `input_dim` of output heads is expected to be the same as this `output_dim`. In some cases, this class is used as a block in the backbone network that doesn&#39;t have an output dimension. In this case, it can be `None`.</span>
</span><span id="HATMaskBackbone-189"><a href="#HATMaskBackbone-189"><span class="linenos">189</span></a><span class="sd">        - **gate** (`str`): The type of gate function turning the real value task embeddings into attention masks; one of:</span>
</span><span id="HATMaskBackbone-190"><a href="#HATMaskBackbone-190"><span class="linenos">190</span></a><span class="sd">            - `sigmoid`: the sigmoid function.</span>
</span><span id="HATMaskBackbone-191"><a href="#HATMaskBackbone-191"><span class="linenos">191</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HATMaskBackbone-192"><a href="#HATMaskBackbone-192"><span class="linenos">192</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span id="HATMaskBackbone-193"><a href="#HATMaskBackbone-193"><span class="linenos">193</span></a>
</span><span id="HATMaskBackbone-194"><a href="#HATMaskBackbone-194"><span class="linenos">194</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">gate</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">gate</span>
</span><span id="HATMaskBackbone-195"><a href="#HATMaskBackbone-195"><span class="linenos">195</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the type of gate function.&quot;&quot;&quot;</span>
</span><span id="HATMaskBackbone-196"><a href="#HATMaskBackbone-196"><span class="linenos">196</span></a>        <span class="k">if</span> <span class="n">gate</span> <span class="o">==</span> <span class="s2">&quot;sigmoid&quot;</span><span class="p">:</span>
</span><span id="HATMaskBackbone-197"><a href="#HATMaskBackbone-197"><span class="linenos">197</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">gate_fn</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
</span><span id="HATMaskBackbone-198"><a href="#HATMaskBackbone-198"><span class="linenos">198</span></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The gate function turning the real value task embeddings into attention masks.&quot;&quot;&quot;</span>
</span><span id="HATMaskBackbone-199"><a href="#HATMaskBackbone-199"><span class="linenos">199</span></a>
</span><span id="HATMaskBackbone-200"><a href="#HATMaskBackbone-200"><span class="linenos">200</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">()</span>
</span><span id="HATMaskBackbone-201"><a href="#HATMaskBackbone-201"><span class="linenos">201</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the task embedding for the current task. Keys are layer names and values are the task embedding `nn.Embedding` for the layer. Each task embedding has size (1, number of units).</span>
</span><span id="HATMaskBackbone-202"><a href="#HATMaskBackbone-202"><span class="linenos">202</span></a><span class="sd">        </span>
</span><span id="HATMaskBackbone-203"><a href="#HATMaskBackbone-203"><span class="linenos">203</span></a><span class="sd">        We use `ModuleDict` rather than `dict` to ensure `LightningModule` properly registers these model parameters for purposes such as automatic device transfer and model summaries.</span>
</span><span id="HATMaskBackbone-204"><a href="#HATMaskBackbone-204"><span class="linenos">204</span></a><span class="sd">        </span>
</span><span id="HATMaskBackbone-205"><a href="#HATMaskBackbone-205"><span class="linenos">205</span></a><span class="sd">        We use `nn.Embedding` rather than `nn.Parameter` to store the task embedding for each layer, which is a type of `nn.Module` and can be accepted by `nn.ModuleDict`. (`nn.Parameter` cannot be accepted by `nn.ModuleDict`.)</span>
</span><span id="HATMaskBackbone-206"><a href="#HATMaskBackbone-206"><span class="linenos">206</span></a><span class="sd">        </span>
</span><span id="HATMaskBackbone-207"><a href="#HATMaskBackbone-207"><span class="linenos">207</span></a><span class="sd">        **This must be defined to cover each weighted layer (as listed in `self.weighted_layer_names`) in the backbone network.** Otherwise, the uncovered parts will keep updating for all tasks and become a source of catastrophic forgetting.</span>
</span><span id="HATMaskBackbone-208"><a href="#HATMaskBackbone-208"><span class="linenos">208</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HATMaskBackbone-209"><a href="#HATMaskBackbone-209"><span class="linenos">209</span></a>
</span><span id="HATMaskBackbone-210"><a href="#HATMaskBackbone-210"><span class="linenos">210</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">masks</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="HATMaskBackbone-211"><a href="#HATMaskBackbone-211"><span class="linenos">211</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the binary attention mask of each previous task gated from the task embedding. Keys are task IDs and values are the corresponding mask. Each mask is a dict where keys are layer names and values are the binary mask tensor for the layer. The mask tensor has size (number of units, ). &quot;&quot;&quot;</span>
</span><span id="HATMaskBackbone-212"><a href="#HATMaskBackbone-212"><span class="linenos">212</span></a>
</span><span id="HATMaskBackbone-213"><a href="#HATMaskBackbone-213"><span class="linenos">213</span></a>        <span class="n">HATMaskBackbone</span><span class="o">.</span><span class="n">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</span><span id="HATMaskBackbone-214"><a href="#HATMaskBackbone-214"><span class="linenos">214</span></a>
</span><span id="HATMaskBackbone-215"><a href="#HATMaskBackbone-215"><span class="linenos">215</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">initialize_task_embedding</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HATMaskBackbone-216"><a href="#HATMaskBackbone-216"><span class="linenos">216</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialize the task embedding for the current task.</span>
</span><span id="HATMaskBackbone-217"><a href="#HATMaskBackbone-217"><span class="linenos">217</span></a>
</span><span id="HATMaskBackbone-218"><a href="#HATMaskBackbone-218"><span class="linenos">218</span></a><span class="sd">        **Args:**</span>
</span><span id="HATMaskBackbone-219"><a href="#HATMaskBackbone-219"><span class="linenos">219</span></a><span class="sd">        - **mode** (`str`): The initialization mode for task embeddings; one of:</span>
</span><span id="HATMaskBackbone-220"><a href="#HATMaskBackbone-220"><span class="linenos">220</span></a><span class="sd">            1. &#39;N01&#39; (default): standard normal distribution $N(0, 1)$.</span>
</span><span id="HATMaskBackbone-221"><a href="#HATMaskBackbone-221"><span class="linenos">221</span></a><span class="sd">            2. &#39;U-11&#39;: uniform distribution $U(-1, 1)$.</span>
</span><span id="HATMaskBackbone-222"><a href="#HATMaskBackbone-222"><span class="linenos">222</span></a><span class="sd">            3. &#39;U01&#39;: uniform distribution $U(0, 1)$.</span>
</span><span id="HATMaskBackbone-223"><a href="#HATMaskBackbone-223"><span class="linenos">223</span></a><span class="sd">            4. &#39;U-10&#39;: uniform distribution $U(-1, 0)$.</span>
</span><span id="HATMaskBackbone-224"><a href="#HATMaskBackbone-224"><span class="linenos">224</span></a><span class="sd">            5. &#39;last&#39;: inherit task embeddings from the last task.</span>
</span><span id="HATMaskBackbone-225"><a href="#HATMaskBackbone-225"><span class="linenos">225</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HATMaskBackbone-226"><a href="#HATMaskBackbone-226"><span class="linenos">226</span></a>        <span class="k">for</span> <span class="n">te</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
</span><span id="HATMaskBackbone-227"><a href="#HATMaskBackbone-227"><span class="linenos">227</span></a>            <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;N01&quot;</span><span class="p">:</span>
</span><span id="HATMaskBackbone-228"><a href="#HATMaskBackbone-228"><span class="linenos">228</span></a>                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">te</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="HATMaskBackbone-229"><a href="#HATMaskBackbone-229"><span class="linenos">229</span></a>            <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;U-11&quot;</span><span class="p">:</span>
</span><span id="HATMaskBackbone-230"><a href="#HATMaskBackbone-230"><span class="linenos">230</span></a>                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="n">te</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="HATMaskBackbone-231"><a href="#HATMaskBackbone-231"><span class="linenos">231</span></a>            <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;U01&quot;</span><span class="p">:</span>
</span><span id="HATMaskBackbone-232"><a href="#HATMaskBackbone-232"><span class="linenos">232</span></a>                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="n">te</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="HATMaskBackbone-233"><a href="#HATMaskBackbone-233"><span class="linenos">233</span></a>            <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;U-10&quot;</span><span class="p">:</span>
</span><span id="HATMaskBackbone-234"><a href="#HATMaskBackbone-234"><span class="linenos">234</span></a>                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="n">te</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span><span id="HATMaskBackbone-235"><a href="#HATMaskBackbone-235"><span class="linenos">235</span></a>            <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;last&quot;</span><span class="p">:</span>
</span><span id="HATMaskBackbone-236"><a href="#HATMaskBackbone-236"><span class="linenos">236</span></a>                <span class="k">pass</span>
</span><span id="HATMaskBackbone-237"><a href="#HATMaskBackbone-237"><span class="linenos">237</span></a>
</span><span id="HATMaskBackbone-238"><a href="#HATMaskBackbone-238"><span class="linenos">238</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HATMaskBackbone-239"><a href="#HATMaskBackbone-239"><span class="linenos">239</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Check the sanity of the arguments.&quot;&quot;&quot;</span>
</span><span id="HATMaskBackbone-240"><a href="#HATMaskBackbone-240"><span class="linenos">240</span></a>
</span><span id="HATMaskBackbone-241"><a href="#HATMaskBackbone-241"><span class="linenos">241</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gate</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;sigmoid&quot;</span><span class="p">]:</span>
</span><span id="HATMaskBackbone-242"><a href="#HATMaskBackbone-242"><span class="linenos">242</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The gate should be one of &#39;sigmoid&#39;.&quot;</span><span class="p">)</span>
</span><span id="HATMaskBackbone-243"><a href="#HATMaskBackbone-243"><span class="linenos">243</span></a>
</span><span id="HATMaskBackbone-244"><a href="#HATMaskBackbone-244"><span class="linenos">244</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_mask</span><span class="p">(</span>
</span><span id="HATMaskBackbone-245"><a href="#HATMaskBackbone-245"><span class="linenos">245</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="HATMaskBackbone-246"><a href="#HATMaskBackbone-246"><span class="linenos">246</span></a>        <span class="n">stage</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="HATMaskBackbone-247"><a href="#HATMaskBackbone-247"><span class="linenos">247</span></a>        <span class="n">s_max</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HATMaskBackbone-248"><a href="#HATMaskBackbone-248"><span class="linenos">248</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HATMaskBackbone-249"><a href="#HATMaskBackbone-249"><span class="linenos">249</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HATMaskBackbone-250"><a href="#HATMaskBackbone-250"><span class="linenos">250</span></a>        <span class="n">test_task_id</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HATMaskBackbone-251"><a href="#HATMaskBackbone-251"><span class="linenos">251</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
</span><span id="HATMaskBackbone-252"><a href="#HATMaskBackbone-252"><span class="linenos">252</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the hard attention mask used in the `forward()` method for different stages.</span>
</span><span id="HATMaskBackbone-253"><a href="#HATMaskBackbone-253"><span class="linenos">253</span></a>
</span><span id="HATMaskBackbone-254"><a href="#HATMaskBackbone-254"><span class="linenos">254</span></a><span class="sd">        **Args:**</span>
</span><span id="HATMaskBackbone-255"><a href="#HATMaskBackbone-255"><span class="linenos">255</span></a><span class="sd">        - **stage** (`str`): The stage when applying the conversion; one of:</span>
</span><span id="HATMaskBackbone-256"><a href="#HATMaskBackbone-256"><span class="linenos">256</span></a><span class="sd">            1. &#39;train&#39;: training stage. Get the mask from the current task embedding through the gate function, scaled by an annealed scalar. See 2.4 &quot;Hard Attention Training&quot; in the HAT paper.</span>
</span><span id="HATMaskBackbone-257"><a href="#HATMaskBackbone-257"><span class="linenos">257</span></a><span class="sd">            2. &#39;validation&#39;: validation stage. Get the mask from the current task embedding through the gate function, scaled by `s_max`, where large scaling makes masks nearly binary. (Note that in this stage, the binary mask hasn&#39;t been stored yet, as training is not over.)</span>
</span><span id="HATMaskBackbone-258"><a href="#HATMaskBackbone-258"><span class="linenos">258</span></a><span class="sd">            3. &#39;test&#39;: testing stage. Apply the test mask directly from the stored masks using `test_task_id`.</span>
</span><span id="HATMaskBackbone-259"><a href="#HATMaskBackbone-259"><span class="linenos">259</span></a><span class="sd">        - **s_max** (`float`): The maximum scaling factor in the gate function. Doesn&#39;t apply to the testing stage. See 2.4 in the HAT paper.</span>
</span><span id="HATMaskBackbone-260"><a href="#HATMaskBackbone-260"><span class="linenos">260</span></a><span class="sd">        - **batch_idx** (`int` | `None`): The current batch index. Applies only to the training stage. For other stages, it is `None`.</span>
</span><span id="HATMaskBackbone-261"><a href="#HATMaskBackbone-261"><span class="linenos">261</span></a><span class="sd">        - **num_batches** (`int` | `None`): The total number of batches. Applies only to the training stage. For other stages, it is `None`.</span>
</span><span id="HATMaskBackbone-262"><a href="#HATMaskBackbone-262"><span class="linenos">262</span></a><span class="sd">        - **test_task_id** (`int` | `None`): The test task ID. Applies only to the testing stage. For other stages, it is `None`.</span>
</span><span id="HATMaskBackbone-263"><a href="#HATMaskBackbone-263"><span class="linenos">263</span></a>
</span><span id="HATMaskBackbone-264"><a href="#HATMaskBackbone-264"><span class="linenos">264</span></a><span class="sd">        **Returns:**</span>
</span><span id="HATMaskBackbone-265"><a href="#HATMaskBackbone-265"><span class="linenos">265</span></a><span class="sd">        - **mask** (`dict[str, Tensor]`): The hard attention (with values 0 or 1) mask. Key (`str`) is the layer name; value (`Tensor`) is the mask tensor. The mask tensor has size (number of units,).</span>
</span><span id="HATMaskBackbone-266"><a href="#HATMaskBackbone-266"><span class="linenos">266</span></a>
</span><span id="HATMaskBackbone-267"><a href="#HATMaskBackbone-267"><span class="linenos">267</span></a><span class="sd">        **Raises:**</span>
</span><span id="HATMaskBackbone-268"><a href="#HATMaskBackbone-268"><span class="linenos">268</span></a><span class="sd">        - **ValueError**: If `batch_idx` and `batch_num` are not provided in the &#39;train&#39; stage; if `s_max` is not provided in the &#39;validation&#39; stage; if `task_id` is not provided in the &#39;test&#39; stage.</span>
</span><span id="HATMaskBackbone-269"><a href="#HATMaskBackbone-269"><span class="linenos">269</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HATMaskBackbone-270"><a href="#HATMaskBackbone-270"><span class="linenos">270</span></a>
</span><span id="HATMaskBackbone-271"><a href="#HATMaskBackbone-271"><span class="linenos">271</span></a>        <span class="c1"># sanity check</span>
</span><span id="HATMaskBackbone-272"><a href="#HATMaskBackbone-272"><span class="linenos">272</span></a>        <span class="k">if</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;train&quot;</span> <span class="ow">and</span> <span class="p">(</span>
</span><span id="HATMaskBackbone-273"><a href="#HATMaskBackbone-273"><span class="linenos">273</span></a>            <span class="n">s_max</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">batch_idx</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">num_batches</span> <span class="ow">is</span> <span class="kc">None</span>
</span><span id="HATMaskBackbone-274"><a href="#HATMaskBackbone-274"><span class="linenos">274</span></a>        <span class="p">):</span>
</span><span id="HATMaskBackbone-275"><a href="#HATMaskBackbone-275"><span class="linenos">275</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="HATMaskBackbone-276"><a href="#HATMaskBackbone-276"><span class="linenos">276</span></a>                <span class="s2">&quot;The `s_max`, `batch_idx` and `batch_num` should be provided at training stage, instead of the default value `None`.&quot;</span>
</span><span id="HATMaskBackbone-277"><a href="#HATMaskBackbone-277"><span class="linenos">277</span></a>            <span class="p">)</span>
</span><span id="HATMaskBackbone-278"><a href="#HATMaskBackbone-278"><span class="linenos">278</span></a>        <span class="k">if</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;validation&quot;</span> <span class="ow">and</span> <span class="p">(</span><span class="n">s_max</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
</span><span id="HATMaskBackbone-279"><a href="#HATMaskBackbone-279"><span class="linenos">279</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="HATMaskBackbone-280"><a href="#HATMaskBackbone-280"><span class="linenos">280</span></a>                <span class="s2">&quot;The `s_max` should be provided at validation stage, instead of the default value `None`.&quot;</span>
</span><span id="HATMaskBackbone-281"><a href="#HATMaskBackbone-281"><span class="linenos">281</span></a>            <span class="p">)</span>
</span><span id="HATMaskBackbone-282"><a href="#HATMaskBackbone-282"><span class="linenos">282</span></a>        <span class="k">if</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;test&quot;</span> <span class="ow">and</span> <span class="p">(</span><span class="n">test_task_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
</span><span id="HATMaskBackbone-283"><a href="#HATMaskBackbone-283"><span class="linenos">283</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="HATMaskBackbone-284"><a href="#HATMaskBackbone-284"><span class="linenos">284</span></a>                <span class="s2">&quot;The `task_mask` should be provided at testing stage, instead of the default value `None`.&quot;</span>
</span><span id="HATMaskBackbone-285"><a href="#HATMaskBackbone-285"><span class="linenos">285</span></a>            <span class="p">)</span>
</span><span id="HATMaskBackbone-286"><a href="#HATMaskBackbone-286"><span class="linenos">286</span></a>
</span><span id="HATMaskBackbone-287"><a href="#HATMaskBackbone-287"><span class="linenos">287</span></a>        <span class="n">mask</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="HATMaskBackbone-288"><a href="#HATMaskBackbone-288"><span class="linenos">288</span></a>        <span class="k">if</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;train&quot;</span><span class="p">:</span>
</span><span id="HATMaskBackbone-289"><a href="#HATMaskBackbone-289"><span class="linenos">289</span></a>            <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="HATMaskBackbone-290"><a href="#HATMaskBackbone-290"><span class="linenos">290</span></a>                <span class="n">anneal_scalar</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">s_max</span> <span class="o">+</span> <span class="p">(</span><span class="n">s_max</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">s_max</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">batch_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span>
</span><span id="HATMaskBackbone-291"><a href="#HATMaskBackbone-291"><span class="linenos">291</span></a>                    <span class="n">num_batches</span> <span class="o">-</span> <span class="mi">1</span>
</span><span id="HATMaskBackbone-292"><a href="#HATMaskBackbone-292"><span class="linenos">292</span></a>                <span class="p">)</span>  <span class="c1"># see equation (3) in chapter 2.4 &quot;Hard Attention Training&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="HATMaskBackbone-293"><a href="#HATMaskBackbone-293"><span class="linenos">293</span></a>                <span class="n">mask</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gate_fn</span><span class="p">(</span>
</span><span id="HATMaskBackbone-294"><a href="#HATMaskBackbone-294"><span class="linenos">294</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span> <span class="o">*</span> <span class="n">anneal_scalar</span>
</span><span id="HATMaskBackbone-295"><a href="#HATMaskBackbone-295"><span class="linenos">295</span></a>                <span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
</span><span id="HATMaskBackbone-296"><a href="#HATMaskBackbone-296"><span class="linenos">296</span></a>        <span class="k">elif</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;validation&quot;</span><span class="p">:</span>
</span><span id="HATMaskBackbone-297"><a href="#HATMaskBackbone-297"><span class="linenos">297</span></a>            <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="HATMaskBackbone-298"><a href="#HATMaskBackbone-298"><span class="linenos">298</span></a>                <span class="n">mask</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gate_fn</span><span class="p">(</span>
</span><span id="HATMaskBackbone-299"><a href="#HATMaskBackbone-299"><span class="linenos">299</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span> <span class="o">*</span> <span class="n">s_max</span>
</span><span id="HATMaskBackbone-300"><a href="#HATMaskBackbone-300"><span class="linenos">300</span></a>                <span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
</span><span id="HATMaskBackbone-301"><a href="#HATMaskBackbone-301"><span class="linenos">301</span></a>        <span class="k">elif</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;test&quot;</span><span class="p">:</span>
</span><span id="HATMaskBackbone-302"><a href="#HATMaskBackbone-302"><span class="linenos">302</span></a>            <span class="n">mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">masks</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">test_task_id</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span>
</span><span id="HATMaskBackbone-303"><a href="#HATMaskBackbone-303"><span class="linenos">303</span></a>
</span><span id="HATMaskBackbone-304"><a href="#HATMaskBackbone-304"><span class="linenos">304</span></a>        <span class="k">return</span> <span class="n">mask</span>
</span><span id="HATMaskBackbone-305"><a href="#HATMaskBackbone-305"><span class="linenos">305</span></a>
</span><span id="HATMaskBackbone-306"><a href="#HATMaskBackbone-306"><span class="linenos">306</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">te_to_binary_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
</span><span id="HATMaskBackbone-307"><a href="#HATMaskBackbone-307"><span class="linenos">307</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Convert the current task embedding to a binary mask.</span>
</span><span id="HATMaskBackbone-308"><a href="#HATMaskBackbone-308"><span class="linenos">308</span></a>
</span><span id="HATMaskBackbone-309"><a href="#HATMaskBackbone-309"><span class="linenos">309</span></a><span class="sd">        This method is used before the testing stage to convert the task embedding into a binary mask for each layer. The binary mask is used to select parameters for the current task.</span>
</span><span id="HATMaskBackbone-310"><a href="#HATMaskBackbone-310"><span class="linenos">310</span></a>
</span><span id="HATMaskBackbone-311"><a href="#HATMaskBackbone-311"><span class="linenos">311</span></a><span class="sd">        **Returns:**</span>
</span><span id="HATMaskBackbone-312"><a href="#HATMaskBackbone-312"><span class="linenos">312</span></a><span class="sd">        - **mask_t** (`dict[str, Tensor]`): The binary mask for the current task. Key (`str`) is the layer name; value (`Tensor`) is the mask tensor. The mask tensor has size (number of units,).</span>
</span><span id="HATMaskBackbone-313"><a href="#HATMaskBackbone-313"><span class="linenos">313</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HATMaskBackbone-314"><a href="#HATMaskBackbone-314"><span class="linenos">314</span></a>        <span class="c1"># get the mask for the current task</span>
</span><span id="HATMaskBackbone-315"><a href="#HATMaskBackbone-315"><span class="linenos">315</span></a>        <span class="n">mask_t</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="HATMaskBackbone-316"><a href="#HATMaskBackbone-316"><span class="linenos">316</span></a>            <span class="n">layer_name</span><span class="p">:</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>
</span><span id="HATMaskBackbone-317"><a href="#HATMaskBackbone-317"><span class="linenos">317</span></a>            <span class="o">.</span><span class="n">float</span><span class="p">()</span>
</span><span id="HATMaskBackbone-318"><a href="#HATMaskBackbone-318"><span class="linenos">318</span></a>            <span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
</span><span id="HATMaskBackbone-319"><a href="#HATMaskBackbone-319"><span class="linenos">319</span></a>            <span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="HATMaskBackbone-320"><a href="#HATMaskBackbone-320"><span class="linenos">320</span></a>            <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span>
</span><span id="HATMaskBackbone-321"><a href="#HATMaskBackbone-321"><span class="linenos">321</span></a>        <span class="p">}</span>
</span><span id="HATMaskBackbone-322"><a href="#HATMaskBackbone-322"><span class="linenos">322</span></a>
</span><span id="HATMaskBackbone-323"><a href="#HATMaskBackbone-323"><span class="linenos">323</span></a>        <span class="k">return</span> <span class="n">mask_t</span>
</span><span id="HATMaskBackbone-324"><a href="#HATMaskBackbone-324"><span class="linenos">324</span></a>
</span><span id="HATMaskBackbone-325"><a href="#HATMaskBackbone-325"><span class="linenos">325</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">store_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HATMaskBackbone-326"><a href="#HATMaskBackbone-326"><span class="linenos">326</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the mask for the current task.&quot;&quot;&quot;</span>
</span><span id="HATMaskBackbone-327"><a href="#HATMaskBackbone-327"><span class="linenos">327</span></a>        <span class="n">mask_t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">te_to_binary_mask</span><span class="p">()</span>
</span><span id="HATMaskBackbone-328"><a href="#HATMaskBackbone-328"><span class="linenos">328</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">masks</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mask_t</span>
</span><span id="HATMaskBackbone-329"><a href="#HATMaskBackbone-329"><span class="linenos">329</span></a>
</span><span id="HATMaskBackbone-330"><a href="#HATMaskBackbone-330"><span class="linenos">330</span></a>        <span class="k">return</span> <span class="n">mask_t</span>
</span><span id="HATMaskBackbone-331"><a href="#HATMaskBackbone-331"><span class="linenos">331</span></a>
</span><span id="HATMaskBackbone-332"><a href="#HATMaskBackbone-332"><span class="linenos">332</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_layer_measure_parameter_wise</span><span class="p">(</span>
</span><span id="HATMaskBackbone-333"><a href="#HATMaskBackbone-333"><span class="linenos">333</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="HATMaskBackbone-334"><a href="#HATMaskBackbone-334"><span class="linenos">334</span></a>        <span class="n">unit_wise_measure</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span>
</span><span id="HATMaskBackbone-335"><a href="#HATMaskBackbone-335"><span class="linenos">335</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="HATMaskBackbone-336"><a href="#HATMaskBackbone-336"><span class="linenos">336</span></a>        <span class="n">aggregation_mode</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="HATMaskBackbone-337"><a href="#HATMaskBackbone-337"><span class="linenos">337</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="HATMaskBackbone-338"><a href="#HATMaskBackbone-338"><span class="linenos">338</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the parameter-wise measure on the parameters right before the given layer.</span>
</span><span id="HATMaskBackbone-339"><a href="#HATMaskBackbone-339"><span class="linenos">339</span></a>
</span><span id="HATMaskBackbone-340"><a href="#HATMaskBackbone-340"><span class="linenos">340</span></a><span class="sd">        It is calculated from the given unit-wise measure. It aggregates two feature-sized vectors (corresponding to the given layer and the preceding layer) into a weight-wise matrix (corresponding to the weights in between) and a bias-wise vector (corresponding to the bias of the given layer), using the given aggregation method. For example, given two feature-sized measures $m_{l,i}$ and $m_{l-1,j}$ and &#39;min&#39; aggregation, the parameter-wise measure is $\min \left(a_{l,i}, a_{l-1,j}\right)$, a matrix with respect to $i, j$.</span>
</span><span id="HATMaskBackbone-341"><a href="#HATMaskBackbone-341"><span class="linenos">341</span></a>
</span><span id="HATMaskBackbone-342"><a href="#HATMaskBackbone-342"><span class="linenos">342</span></a><span class="sd">        Note that if the given layer is the first layer with no preceding layer, we will get the parameter-wise measure directly broadcast from the unit-wise measure of the given layer.</span>
</span><span id="HATMaskBackbone-343"><a href="#HATMaskBackbone-343"><span class="linenos">343</span></a>
</span><span id="HATMaskBackbone-344"><a href="#HATMaskBackbone-344"><span class="linenos">344</span></a><span class="sd">        This method is used to calculate parameter-wise measures in various HAT-based algorithms:</span>
</span><span id="HATMaskBackbone-345"><a href="#HATMaskBackbone-345"><span class="linenos">345</span></a>
</span><span id="HATMaskBackbone-346"><a href="#HATMaskBackbone-346"><span class="linenos">346</span></a><span class="sd">        - **HAT**: the parameter-wise measure is the binary mask for previous tasks from the unit-wise cumulative mask of previous tasks `self.cumulative_mask_for_previous_tasks`, which is $\min \left(a_{l,i}^{&lt;t}, a_{l-1,j}^{&lt;t}\right)$ in Eq. (2) in the HAT paper.</span>
</span><span id="HATMaskBackbone-347"><a href="#HATMaskBackbone-347"><span class="linenos">347</span></a><span class="sd">        - **AdaHAT**: the parameter-wise measure is the parameter importance for previous tasks from the unit-wise summative mask of previous tasks `self.summative_mask_for_previous_tasks`, which is $\min \left(m_{l,i}^{&lt;t,\text{sum}}, m_{l-1,j}^{&lt;t,\text{sum}}\right)$ in Eq. (9) in the AdaHAT paper.</span>
</span><span id="HATMaskBackbone-348"><a href="#HATMaskBackbone-348"><span class="linenos">348</span></a><span class="sd">        - **CBPHAT**: the parameter-wise measure is the parameter importance for previous tasks from the unit-wise importance of previous tasks `self.unit_importance_for_previous_tasks` based on contribution utility, which is $\min \left(I_{l,i}^{(t-1)}, I_{l-1,j}^{(t-1)}\right)$ in the adjustment rate formula in the paper draft.</span>
</span><span id="HATMaskBackbone-349"><a href="#HATMaskBackbone-349"><span class="linenos">349</span></a>
</span><span id="HATMaskBackbone-350"><a href="#HATMaskBackbone-350"><span class="linenos">350</span></a><span class="sd">        **Args:**</span>
</span><span id="HATMaskBackbone-351"><a href="#HATMaskBackbone-351"><span class="linenos">351</span></a><span class="sd">        - **unit_wise_measure** (`dict[str, Tensor]`): The unit-wise measure. Key is layer name; value is the unit-wise measure tensor. The tensor has size (number of units,).</span>
</span><span id="HATMaskBackbone-352"><a href="#HATMaskBackbone-352"><span class="linenos">352</span></a><span class="sd">        - **layer_name** (`str`): The name of the given layer.</span>
</span><span id="HATMaskBackbone-353"><a href="#HATMaskBackbone-353"><span class="linenos">353</span></a><span class="sd">        - **aggregation_mode** (`str`): The aggregation mode turning two feature-wise measures into a weight-wise matrix; one of:</span>
</span><span id="HATMaskBackbone-354"><a href="#HATMaskBackbone-354"><span class="linenos">354</span></a><span class="sd">            - &#39;min&#39;: takes the minimum of the two connected unit measures.</span>
</span><span id="HATMaskBackbone-355"><a href="#HATMaskBackbone-355"><span class="linenos">355</span></a><span class="sd">            - &#39;max&#39;: takes the maximum of the two connected unit measures.</span>
</span><span id="HATMaskBackbone-356"><a href="#HATMaskBackbone-356"><span class="linenos">356</span></a><span class="sd">            - &#39;mean&#39;: takes the mean of the two connected unit measures.</span>
</span><span id="HATMaskBackbone-357"><a href="#HATMaskBackbone-357"><span class="linenos">357</span></a><span class="sd">        **Returns:**</span>
</span><span id="HATMaskBackbone-358"><a href="#HATMaskBackbone-358"><span class="linenos">358</span></a><span class="sd">        - **weight_measure** (`Tensor`): The weight measure matrix, the same size as the corresponding weights.</span>
</span><span id="HATMaskBackbone-359"><a href="#HATMaskBackbone-359"><span class="linenos">359</span></a><span class="sd">        - **bias_measure** (`Tensor`): The bias measure vector, the same size as the corresponding bias.</span>
</span><span id="HATMaskBackbone-360"><a href="#HATMaskBackbone-360"><span class="linenos">360</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HATMaskBackbone-361"><a href="#HATMaskBackbone-361"><span class="linenos">361</span></a>
</span><span id="HATMaskBackbone-362"><a href="#HATMaskBackbone-362"><span class="linenos">362</span></a>        <span class="c1"># initialize the aggregation function</span>
</span><span id="HATMaskBackbone-363"><a href="#HATMaskBackbone-363"><span class="linenos">363</span></a>        <span class="k">if</span> <span class="n">aggregation_mode</span> <span class="o">==</span> <span class="s2">&quot;min&quot;</span><span class="p">:</span>
</span><span id="HATMaskBackbone-364"><a href="#HATMaskBackbone-364"><span class="linenos">364</span></a>            <span class="n">aggregation_func</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span>
</span><span id="HATMaskBackbone-365"><a href="#HATMaskBackbone-365"><span class="linenos">365</span></a>        <span class="k">elif</span> <span class="n">aggregation_mode</span> <span class="o">==</span> <span class="s2">&quot;max&quot;</span><span class="p">:</span>
</span><span id="HATMaskBackbone-366"><a href="#HATMaskBackbone-366"><span class="linenos">366</span></a>            <span class="n">aggregation_func</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span>
</span><span id="HATMaskBackbone-367"><a href="#HATMaskBackbone-367"><span class="linenos">367</span></a>        <span class="k">elif</span> <span class="n">aggregation_mode</span> <span class="o">==</span> <span class="s2">&quot;mean&quot;</span><span class="p">:</span>
</span><span id="HATMaskBackbone-368"><a href="#HATMaskBackbone-368"><span class="linenos">368</span></a>            <span class="n">aggregation_func</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span>
</span><span id="HATMaskBackbone-369"><a href="#HATMaskBackbone-369"><span class="linenos">369</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="HATMaskBackbone-370"><a href="#HATMaskBackbone-370"><span class="linenos">370</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="HATMaskBackbone-371"><a href="#HATMaskBackbone-371"><span class="linenos">371</span></a>                <span class="sa">f</span><span class="s2">&quot;The aggregation method </span><span class="si">{</span><span class="n">aggregation_mode</span><span class="si">}</span><span class="s2"> is not supported.&quot;</span>
</span><span id="HATMaskBackbone-372"><a href="#HATMaskBackbone-372"><span class="linenos">372</span></a>            <span class="p">)</span>
</span><span id="HATMaskBackbone-373"><a href="#HATMaskBackbone-373"><span class="linenos">373</span></a>
</span><span id="HATMaskBackbone-374"><a href="#HATMaskBackbone-374"><span class="linenos">374</span></a>        <span class="c1"># get the preceding layer</span>
</span><span id="HATMaskBackbone-375"><a href="#HATMaskBackbone-375"><span class="linenos">375</span></a>        <span class="n">preceding_layer_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">preceding_layer_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="HATMaskBackbone-376"><a href="#HATMaskBackbone-376"><span class="linenos">376</span></a>
</span><span id="HATMaskBackbone-377"><a href="#HATMaskBackbone-377"><span class="linenos">377</span></a>        <span class="c1"># get weight size for expanding the measures</span>
</span><span id="HATMaskBackbone-378"><a href="#HATMaskBackbone-378"><span class="linenos">378</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="HATMaskBackbone-379"><a href="#HATMaskBackbone-379"><span class="linenos">379</span></a>        <span class="n">weight_size</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</span><span id="HATMaskBackbone-380"><a href="#HATMaskBackbone-380"><span class="linenos">380</span></a>
</span><span id="HATMaskBackbone-381"><a href="#HATMaskBackbone-381"><span class="linenos">381</span></a>        <span class="c1"># construct the weight-wise measure</span>
</span><span id="HATMaskBackbone-382"><a href="#HATMaskBackbone-382"><span class="linenos">382</span></a>        <span class="n">layer_measure</span> <span class="o">=</span> <span class="n">unit_wise_measure</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="HATMaskBackbone-383"><a href="#HATMaskBackbone-383"><span class="linenos">383</span></a>        <span class="n">layer_measure_broadcast_size</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span>
</span><span id="HATMaskBackbone-384"><a href="#HATMaskBackbone-384"><span class="linenos">384</span></a>            <span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">weight_size</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="HATMaskBackbone-385"><a href="#HATMaskBackbone-385"><span class="linenos">385</span></a>        <span class="p">)</span>  <span class="c1"># since the size of mask tensor is (number of units, ), we extend it to (number of units, 1) and expand it to the weight size. The weight size has 2 dimensions in fully connected layers and 4 dimensions in convolutional layers</span>
</span><span id="HATMaskBackbone-386"><a href="#HATMaskBackbone-386"><span class="linenos">386</span></a>
</span><span id="HATMaskBackbone-387"><a href="#HATMaskBackbone-387"><span class="linenos">387</span></a>        <span class="n">layer_measure_broadcasted</span> <span class="o">=</span> <span class="n">layer_measure</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
</span><span id="HATMaskBackbone-388"><a href="#HATMaskBackbone-388"><span class="linenos">388</span></a>            <span class="o">*</span><span class="n">layer_measure_broadcast_size</span>
</span><span id="HATMaskBackbone-389"><a href="#HATMaskBackbone-389"><span class="linenos">389</span></a>        <span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span>
</span><span id="HATMaskBackbone-390"><a href="#HATMaskBackbone-390"><span class="linenos">390</span></a>            <span class="n">weight_size</span><span class="p">,</span>
</span><span id="HATMaskBackbone-391"><a href="#HATMaskBackbone-391"><span class="linenos">391</span></a>        <span class="p">)</span>  <span class="c1"># expand the given layer mask to the weight size and broadcast</span>
</span><span id="HATMaskBackbone-392"><a href="#HATMaskBackbone-392"><span class="linenos">392</span></a>
</span><span id="HATMaskBackbone-393"><a href="#HATMaskBackbone-393"><span class="linenos">393</span></a>        <span class="k">if</span> <span class="p">(</span>
</span><span id="HATMaskBackbone-394"><a href="#HATMaskBackbone-394"><span class="linenos">394</span></a>            <span class="n">preceding_layer_name</span>
</span><span id="HATMaskBackbone-395"><a href="#HATMaskBackbone-395"><span class="linenos">395</span></a>        <span class="p">):</span>  <span class="c1"># if the layer is not the first layer, where the preceding layer exists</span>
</span><span id="HATMaskBackbone-396"><a href="#HATMaskBackbone-396"><span class="linenos">396</span></a>
</span><span id="HATMaskBackbone-397"><a href="#HATMaskBackbone-397"><span class="linenos">397</span></a>            <span class="n">preceding_layer_measure_broadcast_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span>
</span><span id="HATMaskBackbone-398"><a href="#HATMaskBackbone-398"><span class="linenos">398</span></a>                <span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">weight_size</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="HATMaskBackbone-399"><a href="#HATMaskBackbone-399"><span class="linenos">399</span></a>            <span class="p">)</span>  <span class="c1"># since the size of mask tensor is (number of units, ), we extend it to (1, number of units) and expand it to the weight size. The weight size has 2 dimensions in fully connected layers and 4 dimensions in convolutional layers</span>
</span><span id="HATMaskBackbone-400"><a href="#HATMaskBackbone-400"><span class="linenos">400</span></a>            <span class="n">preceding_layer_measure</span> <span class="o">=</span> <span class="n">unit_wise_measure</span><span class="p">[</span><span class="n">preceding_layer_name</span><span class="p">]</span>
</span><span id="HATMaskBackbone-401"><a href="#HATMaskBackbone-401"><span class="linenos">401</span></a>            <span class="n">preceding_layer_measure_broadcasted</span> <span class="o">=</span> <span class="n">preceding_layer_measure</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
</span><span id="HATMaskBackbone-402"><a href="#HATMaskBackbone-402"><span class="linenos">402</span></a>                <span class="o">*</span><span class="n">preceding_layer_measure_broadcast_size</span>
</span><span id="HATMaskBackbone-403"><a href="#HATMaskBackbone-403"><span class="linenos">403</span></a>            <span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span>
</span><span id="HATMaskBackbone-404"><a href="#HATMaskBackbone-404"><span class="linenos">404</span></a>                <span class="n">weight_size</span>
</span><span id="HATMaskBackbone-405"><a href="#HATMaskBackbone-405"><span class="linenos">405</span></a>            <span class="p">)</span>  <span class="c1"># expand the preceding layer mask to the weight size and broadcast</span>
</span><span id="HATMaskBackbone-406"><a href="#HATMaskBackbone-406"><span class="linenos">406</span></a>            <span class="n">weight_measure</span> <span class="o">=</span> <span class="n">aggregation_func</span><span class="p">(</span>
</span><span id="HATMaskBackbone-407"><a href="#HATMaskBackbone-407"><span class="linenos">407</span></a>                <span class="n">layer_measure_broadcasted</span><span class="p">,</span> <span class="n">preceding_layer_measure_broadcasted</span>
</span><span id="HATMaskBackbone-408"><a href="#HATMaskBackbone-408"><span class="linenos">408</span></a>            <span class="p">)</span>  <span class="c1"># get the minimum of the two mask vectors, from expanded</span>
</span><span id="HATMaskBackbone-409"><a href="#HATMaskBackbone-409"><span class="linenos">409</span></a>        <span class="k">else</span><span class="p">:</span>  <span class="c1"># if the layer is the first layer</span>
</span><span id="HATMaskBackbone-410"><a href="#HATMaskBackbone-410"><span class="linenos">410</span></a>            <span class="n">weight_measure</span> <span class="o">=</span> <span class="n">layer_measure_broadcasted</span>
</span><span id="HATMaskBackbone-411"><a href="#HATMaskBackbone-411"><span class="linenos">411</span></a>
</span><span id="HATMaskBackbone-412"><a href="#HATMaskBackbone-412"><span class="linenos">412</span></a>        <span class="c1"># construct the bias-wise measure</span>
</span><span id="HATMaskBackbone-413"><a href="#HATMaskBackbone-413"><span class="linenos">413</span></a>        <span class="n">bias_measure</span> <span class="o">=</span> <span class="n">layer_measure</span>
</span><span id="HATMaskBackbone-414"><a href="#HATMaskBackbone-414"><span class="linenos">414</span></a>
</span><span id="HATMaskBackbone-415"><a href="#HATMaskBackbone-415"><span class="linenos">415</span></a>        <span class="k">return</span> <span class="n">weight_measure</span><span class="p">,</span> <span class="n">bias_measure</span>
</span><span id="HATMaskBackbone-416"><a href="#HATMaskBackbone-416"><span class="linenos">416</span></a>
</span><span id="HATMaskBackbone-417"><a href="#HATMaskBackbone-417"><span class="linenos">417</span></a>    <span class="nd">@override</span>
</span><span id="HATMaskBackbone-418"><a href="#HATMaskBackbone-418"><span class="linenos">418</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="HATMaskBackbone-419"><a href="#HATMaskBackbone-419"><span class="linenos">419</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="HATMaskBackbone-420"><a href="#HATMaskBackbone-420"><span class="linenos">420</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="HATMaskBackbone-421"><a href="#HATMaskBackbone-421"><span class="linenos">421</span></a>        <span class="n">stage</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="HATMaskBackbone-422"><a href="#HATMaskBackbone-422"><span class="linenos">422</span></a>        <span class="n">s_max</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HATMaskBackbone-423"><a href="#HATMaskBackbone-423"><span class="linenos">423</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HATMaskBackbone-424"><a href="#HATMaskBackbone-424"><span class="linenos">424</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HATMaskBackbone-425"><a href="#HATMaskBackbone-425"><span class="linenos">425</span></a>        <span class="n">test_task_id</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HATMaskBackbone-426"><a href="#HATMaskBackbone-426"><span class="linenos">426</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
</span><span id="HATMaskBackbone-427"><a href="#HATMaskBackbone-427"><span class="linenos">427</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The forward pass for data from task `task_id`. Task-specific masks for `task_id` are applied to the units in each layer.</span>
</span><span id="HATMaskBackbone-428"><a href="#HATMaskBackbone-428"><span class="linenos">428</span></a>
</span><span id="HATMaskBackbone-429"><a href="#HATMaskBackbone-429"><span class="linenos">429</span></a><span class="sd">        **Args:**</span>
</span><span id="HATMaskBackbone-430"><a href="#HATMaskBackbone-430"><span class="linenos">430</span></a><span class="sd">        - **input** (`Tensor`): The input tensor from data.</span>
</span><span id="HATMaskBackbone-431"><a href="#HATMaskBackbone-431"><span class="linenos">431</span></a><span class="sd">        - **stage** (`str`): The stage of the forward pass; one of:</span>
</span><span id="HATMaskBackbone-432"><a href="#HATMaskBackbone-432"><span class="linenos">432</span></a><span class="sd">            1. &#39;train&#39;: training stage.</span>
</span><span id="HATMaskBackbone-433"><a href="#HATMaskBackbone-433"><span class="linenos">433</span></a><span class="sd">            2. &#39;validation&#39;: validation stage.</span>
</span><span id="HATMaskBackbone-434"><a href="#HATMaskBackbone-434"><span class="linenos">434</span></a><span class="sd">            3. &#39;test&#39;: testing stage.</span>
</span><span id="HATMaskBackbone-435"><a href="#HATMaskBackbone-435"><span class="linenos">435</span></a><span class="sd">        - **s_max** (`float`): The maximum scaling factor in the gate function. See 2.4 &quot;Hard Attention Training&quot; in the HAT paper.</span>
</span><span id="HATMaskBackbone-436"><a href="#HATMaskBackbone-436"><span class="linenos">436</span></a><span class="sd">        - **batch_idx** (`int` | `None`): The current batch index. Applies only to the training stage. For other stages, it is `None`.</span>
</span><span id="HATMaskBackbone-437"><a href="#HATMaskBackbone-437"><span class="linenos">437</span></a><span class="sd">        - **num_batches** (`int` | `None`): The total number of batches. Applies only to the training stage. For other stages, it is `None`.</span>
</span><span id="HATMaskBackbone-438"><a href="#HATMaskBackbone-438"><span class="linenos">438</span></a><span class="sd">        - **test_task_id** (`int` | `None`): The test task ID. Applies only to the testing stage. For other stages, it is `None`.</span>
</span><span id="HATMaskBackbone-439"><a href="#HATMaskBackbone-439"><span class="linenos">439</span></a>
</span><span id="HATMaskBackbone-440"><a href="#HATMaskBackbone-440"><span class="linenos">440</span></a><span class="sd">        **Returns:**</span>
</span><span id="HATMaskBackbone-441"><a href="#HATMaskBackbone-441"><span class="linenos">441</span></a><span class="sd">        - **output_feature** (`Tensor`): The output feature tensor to be passed into heads. This is the main target of backpropagation.</span>
</span><span id="HATMaskBackbone-442"><a href="#HATMaskBackbone-442"><span class="linenos">442</span></a><span class="sd">        - **mask** (`dict[str, Tensor]`): The mask for the current task. Key (`str`) is layer name; value (`Tensor`) is the mask tensor. The mask tensor has size (number of units,).</span>
</span><span id="HATMaskBackbone-443"><a href="#HATMaskBackbone-443"><span class="linenos">443</span></a><span class="sd">        - **activations** (`dict[str, Tensor]`): The hidden features (after activation) in each weighted layer. Key (`str`) is the weighted layer name; value (`Tensor`) is the hidden feature tensor. This is used for continual learning algorithms that need hidden features. Although the HAT algorithm does not need this, it is still provided for API consistency for other HAT-based algorithms that inherit this `forward()` method of the `HAT` class.</span>
</span><span id="HATMaskBackbone-444"><a href="#HATMaskBackbone-444"><span class="linenos">444</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span></pre></div>


            <div class="docstring"><p>The backbone network for HAT-based algorithms with learnable hard attention masks.</p>

<p>HAT-based algorithms:</p>

<ul>
<li><a href="http://proceedings.mlr.press/v80/serra18a"><strong>HAT (Hard Attention to the Task, 2018)</strong></a> is an architecture-based continual learning approach that uses learnable hard attention masks to select task-specific parameters.</li>
<li><a href="https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9"><strong>Adaptive HAT (Adaptive Hard Attention to the Task, 2024)</strong></a> is an architecture-based continual learning approach that improves HAT by introducing adaptive soft gradient clipping based on parameter importance and network sparsity.</li>
<li><strong>FG-AdaHAT</strong> is an architecture-based continual learning approach that improves HAT by introducing fine-grained neuron-wise importance measures guiding the adaptive adjustment mechanism in AdaHAT.</li>
</ul>
</div>


                            <div id="HATMaskBackbone.__init__" class="classattr">
                                        <input id="HATMaskBackbone.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">HATMaskBackbone</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span>, </span><span class="param"><span class="n">gate</span><span class="p">:</span> <span class="nb">str</span>, </span><span class="param"><span class="o">**</span><span class="n">kwargs</span></span>)</span>

                <label class="view-source-button" for="HATMaskBackbone.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HATMaskBackbone.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HATMaskBackbone.__init__-184"><a href="#HATMaskBackbone.__init__-184"><span class="linenos">184</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span> <span class="n">gate</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HATMaskBackbone.__init__-185"><a href="#HATMaskBackbone.__init__-185"><span class="linenos">185</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialize the HAT mask backbone network with task embeddings and masks.</span>
</span><span id="HATMaskBackbone.__init__-186"><a href="#HATMaskBackbone.__init__-186"><span class="linenos">186</span></a>
</span><span id="HATMaskBackbone.__init__-187"><a href="#HATMaskBackbone.__init__-187"><span class="linenos">187</span></a><span class="sd">        **Args:**</span>
</span><span id="HATMaskBackbone.__init__-188"><a href="#HATMaskBackbone.__init__-188"><span class="linenos">188</span></a><span class="sd">        - **output_dim** (`int`): The output dimension that connects to CL output heads. The `input_dim` of output heads is expected to be the same as this `output_dim`. In some cases, this class is used as a block in the backbone network that doesn&#39;t have an output dimension. In this case, it can be `None`.</span>
</span><span id="HATMaskBackbone.__init__-189"><a href="#HATMaskBackbone.__init__-189"><span class="linenos">189</span></a><span class="sd">        - **gate** (`str`): The type of gate function turning the real value task embeddings into attention masks; one of:</span>
</span><span id="HATMaskBackbone.__init__-190"><a href="#HATMaskBackbone.__init__-190"><span class="linenos">190</span></a><span class="sd">            - `sigmoid`: the sigmoid function.</span>
</span><span id="HATMaskBackbone.__init__-191"><a href="#HATMaskBackbone.__init__-191"><span class="linenos">191</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HATMaskBackbone.__init__-192"><a href="#HATMaskBackbone.__init__-192"><span class="linenos">192</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span id="HATMaskBackbone.__init__-193"><a href="#HATMaskBackbone.__init__-193"><span class="linenos">193</span></a>
</span><span id="HATMaskBackbone.__init__-194"><a href="#HATMaskBackbone.__init__-194"><span class="linenos">194</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">gate</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">gate</span>
</span><span id="HATMaskBackbone.__init__-195"><a href="#HATMaskBackbone.__init__-195"><span class="linenos">195</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the type of gate function.&quot;&quot;&quot;</span>
</span><span id="HATMaskBackbone.__init__-196"><a href="#HATMaskBackbone.__init__-196"><span class="linenos">196</span></a>        <span class="k">if</span> <span class="n">gate</span> <span class="o">==</span> <span class="s2">&quot;sigmoid&quot;</span><span class="p">:</span>
</span><span id="HATMaskBackbone.__init__-197"><a href="#HATMaskBackbone.__init__-197"><span class="linenos">197</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">gate_fn</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
</span><span id="HATMaskBackbone.__init__-198"><a href="#HATMaskBackbone.__init__-198"><span class="linenos">198</span></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The gate function turning the real value task embeddings into attention masks.&quot;&quot;&quot;</span>
</span><span id="HATMaskBackbone.__init__-199"><a href="#HATMaskBackbone.__init__-199"><span class="linenos">199</span></a>
</span><span id="HATMaskBackbone.__init__-200"><a href="#HATMaskBackbone.__init__-200"><span class="linenos">200</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">()</span>
</span><span id="HATMaskBackbone.__init__-201"><a href="#HATMaskBackbone.__init__-201"><span class="linenos">201</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the task embedding for the current task. Keys are layer names and values are the task embedding `nn.Embedding` for the layer. Each task embedding has size (1, number of units).</span>
</span><span id="HATMaskBackbone.__init__-202"><a href="#HATMaskBackbone.__init__-202"><span class="linenos">202</span></a><span class="sd">        </span>
</span><span id="HATMaskBackbone.__init__-203"><a href="#HATMaskBackbone.__init__-203"><span class="linenos">203</span></a><span class="sd">        We use `ModuleDict` rather than `dict` to ensure `LightningModule` properly registers these model parameters for purposes such as automatic device transfer and model summaries.</span>
</span><span id="HATMaskBackbone.__init__-204"><a href="#HATMaskBackbone.__init__-204"><span class="linenos">204</span></a><span class="sd">        </span>
</span><span id="HATMaskBackbone.__init__-205"><a href="#HATMaskBackbone.__init__-205"><span class="linenos">205</span></a><span class="sd">        We use `nn.Embedding` rather than `nn.Parameter` to store the task embedding for each layer, which is a type of `nn.Module` and can be accepted by `nn.ModuleDict`. (`nn.Parameter` cannot be accepted by `nn.ModuleDict`.)</span>
</span><span id="HATMaskBackbone.__init__-206"><a href="#HATMaskBackbone.__init__-206"><span class="linenos">206</span></a><span class="sd">        </span>
</span><span id="HATMaskBackbone.__init__-207"><a href="#HATMaskBackbone.__init__-207"><span class="linenos">207</span></a><span class="sd">        **This must be defined to cover each weighted layer (as listed in `self.weighted_layer_names`) in the backbone network.** Otherwise, the uncovered parts will keep updating for all tasks and become a source of catastrophic forgetting.</span>
</span><span id="HATMaskBackbone.__init__-208"><a href="#HATMaskBackbone.__init__-208"><span class="linenos">208</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HATMaskBackbone.__init__-209"><a href="#HATMaskBackbone.__init__-209"><span class="linenos">209</span></a>
</span><span id="HATMaskBackbone.__init__-210"><a href="#HATMaskBackbone.__init__-210"><span class="linenos">210</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">masks</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="HATMaskBackbone.__init__-211"><a href="#HATMaskBackbone.__init__-211"><span class="linenos">211</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the binary attention mask of each previous task gated from the task embedding. Keys are task IDs and values are the corresponding mask. Each mask is a dict where keys are layer names and values are the binary mask tensor for the layer. The mask tensor has size (number of units, ). &quot;&quot;&quot;</span>
</span><span id="HATMaskBackbone.__init__-212"><a href="#HATMaskBackbone.__init__-212"><span class="linenos">212</span></a>
</span><span id="HATMaskBackbone.__init__-213"><a href="#HATMaskBackbone.__init__-213"><span class="linenos">213</span></a>        <span class="n">HATMaskBackbone</span><span class="o">.</span><span class="n">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Initialize the HAT mask backbone network with task embeddings and masks.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>output_dim</strong> (<code>int</code>): The output dimension that connects to CL output heads. The <code>input_dim</code> of output heads is expected to be the same as this <code><a href="#HATMaskBackbone.output_dim">output_dim</a></code>. In some cases, this class is used as a block in the backbone network that doesn't have an output dimension. In this case, it can be <code>None</code>.</li>
<li><strong>gate</strong> (<code>str</code>): The type of gate function turning the real value task embeddings into attention masks; one of:
<ul>
<li><code>sigmoid</code>: the sigmoid function.</li>
</ul></li>
</ul>
</div>


                            </div>
                            <div id="HATMaskBackbone.gate" class="classattr">
                                <div class="attr variable">
            <span class="name">gate</span><span class="annotation">: str</span>

        
    </div>
    <a class="headerlink" href="#HATMaskBackbone.gate"></a>
    
            <div class="docstring"><p>Store the type of gate function.</p>
</div>


                            </div>
                            <div id="HATMaskBackbone.task_embedding_t" class="classattr">
                                <div class="attr variable">
            <span class="name">task_embedding_t</span><span class="annotation">: torch.nn.modules.container.ModuleDict</span>

        
    </div>
    <a class="headerlink" href="#HATMaskBackbone.task_embedding_t"></a>
    
            <div class="docstring"><p>Store the task embedding for the current task. Keys are layer names and values are the task embedding <code>nn.Embedding</code> for the layer. Each task embedding has size (1, number of units).</p>

<p>We use <code>ModuleDict</code> rather than <code>dict</code> to ensure <code>LightningModule</code> properly registers these model parameters for purposes such as automatic device transfer and model summaries.</p>

<p>We use <code>nn.Embedding</code> rather than <code>nn.Parameter</code> to store the task embedding for each layer, which is a type of <code>nn.Module</code> and can be accepted by <code>nn.ModuleDict</code>. (<code>nn.Parameter</code> cannot be accepted by <code>nn.ModuleDict</code>.)</p>

<p><strong>This must be defined to cover each weighted layer (as listed in <code>self.weighted_layer_names</code>) in the backbone network.</strong> Otherwise, the uncovered parts will keep updating for all tasks and become a source of catastrophic forgetting.</p>
</div>


                            </div>
                            <div id="HATMaskBackbone.masks" class="classattr">
                                <div class="attr variable">
            <span class="name">masks</span><span class="annotation">: dict[str, dict[str, torch.Tensor]]</span>

        
    </div>
    <a class="headerlink" href="#HATMaskBackbone.masks"></a>
    
            <div class="docstring"><p>Store the binary attention mask of each previous task gated from the task embedding. Keys are task IDs and values are the corresponding mask. Each mask is a dict where keys are layer names and values are the binary mask tensor for the layer. The mask tensor has size (number of units, ).</p>
</div>


                            </div>
                            <div id="HATMaskBackbone.initialize_task_embedding" class="classattr">
                                        <input id="HATMaskBackbone.initialize_task_embedding-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">initialize_task_embedding</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">mode</span><span class="p">:</span> <span class="nb">str</span></span><span class="return-annotation">) -> <span class="kc">None</span>:</span></span>

                <label class="view-source-button" for="HATMaskBackbone.initialize_task_embedding-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HATMaskBackbone.initialize_task_embedding"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HATMaskBackbone.initialize_task_embedding-215"><a href="#HATMaskBackbone.initialize_task_embedding-215"><span class="linenos">215</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">initialize_task_embedding</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HATMaskBackbone.initialize_task_embedding-216"><a href="#HATMaskBackbone.initialize_task_embedding-216"><span class="linenos">216</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialize the task embedding for the current task.</span>
</span><span id="HATMaskBackbone.initialize_task_embedding-217"><a href="#HATMaskBackbone.initialize_task_embedding-217"><span class="linenos">217</span></a>
</span><span id="HATMaskBackbone.initialize_task_embedding-218"><a href="#HATMaskBackbone.initialize_task_embedding-218"><span class="linenos">218</span></a><span class="sd">        **Args:**</span>
</span><span id="HATMaskBackbone.initialize_task_embedding-219"><a href="#HATMaskBackbone.initialize_task_embedding-219"><span class="linenos">219</span></a><span class="sd">        - **mode** (`str`): The initialization mode for task embeddings; one of:</span>
</span><span id="HATMaskBackbone.initialize_task_embedding-220"><a href="#HATMaskBackbone.initialize_task_embedding-220"><span class="linenos">220</span></a><span class="sd">            1. &#39;N01&#39; (default): standard normal distribution $N(0, 1)$.</span>
</span><span id="HATMaskBackbone.initialize_task_embedding-221"><a href="#HATMaskBackbone.initialize_task_embedding-221"><span class="linenos">221</span></a><span class="sd">            2. &#39;U-11&#39;: uniform distribution $U(-1, 1)$.</span>
</span><span id="HATMaskBackbone.initialize_task_embedding-222"><a href="#HATMaskBackbone.initialize_task_embedding-222"><span class="linenos">222</span></a><span class="sd">            3. &#39;U01&#39;: uniform distribution $U(0, 1)$.</span>
</span><span id="HATMaskBackbone.initialize_task_embedding-223"><a href="#HATMaskBackbone.initialize_task_embedding-223"><span class="linenos">223</span></a><span class="sd">            4. &#39;U-10&#39;: uniform distribution $U(-1, 0)$.</span>
</span><span id="HATMaskBackbone.initialize_task_embedding-224"><a href="#HATMaskBackbone.initialize_task_embedding-224"><span class="linenos">224</span></a><span class="sd">            5. &#39;last&#39;: inherit task embeddings from the last task.</span>
</span><span id="HATMaskBackbone.initialize_task_embedding-225"><a href="#HATMaskBackbone.initialize_task_embedding-225"><span class="linenos">225</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HATMaskBackbone.initialize_task_embedding-226"><a href="#HATMaskBackbone.initialize_task_embedding-226"><span class="linenos">226</span></a>        <span class="k">for</span> <span class="n">te</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
</span><span id="HATMaskBackbone.initialize_task_embedding-227"><a href="#HATMaskBackbone.initialize_task_embedding-227"><span class="linenos">227</span></a>            <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;N01&quot;</span><span class="p">:</span>
</span><span id="HATMaskBackbone.initialize_task_embedding-228"><a href="#HATMaskBackbone.initialize_task_embedding-228"><span class="linenos">228</span></a>                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">te</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="HATMaskBackbone.initialize_task_embedding-229"><a href="#HATMaskBackbone.initialize_task_embedding-229"><span class="linenos">229</span></a>            <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;U-11&quot;</span><span class="p">:</span>
</span><span id="HATMaskBackbone.initialize_task_embedding-230"><a href="#HATMaskBackbone.initialize_task_embedding-230"><span class="linenos">230</span></a>                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="n">te</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="HATMaskBackbone.initialize_task_embedding-231"><a href="#HATMaskBackbone.initialize_task_embedding-231"><span class="linenos">231</span></a>            <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;U01&quot;</span><span class="p">:</span>
</span><span id="HATMaskBackbone.initialize_task_embedding-232"><a href="#HATMaskBackbone.initialize_task_embedding-232"><span class="linenos">232</span></a>                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="n">te</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="HATMaskBackbone.initialize_task_embedding-233"><a href="#HATMaskBackbone.initialize_task_embedding-233"><span class="linenos">233</span></a>            <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;U-10&quot;</span><span class="p">:</span>
</span><span id="HATMaskBackbone.initialize_task_embedding-234"><a href="#HATMaskBackbone.initialize_task_embedding-234"><span class="linenos">234</span></a>                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="n">te</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span><span id="HATMaskBackbone.initialize_task_embedding-235"><a href="#HATMaskBackbone.initialize_task_embedding-235"><span class="linenos">235</span></a>            <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;last&quot;</span><span class="p">:</span>
</span><span id="HATMaskBackbone.initialize_task_embedding-236"><a href="#HATMaskBackbone.initialize_task_embedding-236"><span class="linenos">236</span></a>                <span class="k">pass</span>
</span></pre></div>


            <div class="docstring"><p>Initialize the task embedding for the current task.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>mode</strong> (<code>str</code>): The initialization mode for task embeddings; one of:
<ol>
<li>'N01' (default): standard normal distribution $N(0, 1)$.</li>
<li>'U-11': uniform distribution $U(-1, 1)$.</li>
<li>'U01': uniform distribution $U(0, 1)$.</li>
<li>'U-10': uniform distribution $U(-1, 0)$.</li>
<li>'last': inherit task embeddings from the last task.</li>
</ol></li>
</ul>
</div>


                            </div>
                            <div id="HATMaskBackbone.sanity_check" class="classattr">
                                        <input id="HATMaskBackbone.sanity_check-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">sanity_check</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span></span><span class="return-annotation">) -> <span class="kc">None</span>:</span></span>

                <label class="view-source-button" for="HATMaskBackbone.sanity_check-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HATMaskBackbone.sanity_check"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HATMaskBackbone.sanity_check-238"><a href="#HATMaskBackbone.sanity_check-238"><span class="linenos">238</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HATMaskBackbone.sanity_check-239"><a href="#HATMaskBackbone.sanity_check-239"><span class="linenos">239</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Check the sanity of the arguments.&quot;&quot;&quot;</span>
</span><span id="HATMaskBackbone.sanity_check-240"><a href="#HATMaskBackbone.sanity_check-240"><span class="linenos">240</span></a>
</span><span id="HATMaskBackbone.sanity_check-241"><a href="#HATMaskBackbone.sanity_check-241"><span class="linenos">241</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gate</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;sigmoid&quot;</span><span class="p">]:</span>
</span><span id="HATMaskBackbone.sanity_check-242"><a href="#HATMaskBackbone.sanity_check-242"><span class="linenos">242</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The gate should be one of &#39;sigmoid&#39;.&quot;</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Check the sanity of the arguments.</p>
</div>


                            </div>
                            <div id="HATMaskBackbone.get_mask" class="classattr">
                                        <input id="HATMaskBackbone.get_mask-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">get_mask</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">stage</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">s_max</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">test_task_id</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span></span><span class="return-annotation">) -> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>:</span></span>

                <label class="view-source-button" for="HATMaskBackbone.get_mask-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HATMaskBackbone.get_mask"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HATMaskBackbone.get_mask-244"><a href="#HATMaskBackbone.get_mask-244"><span class="linenos">244</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_mask</span><span class="p">(</span>
</span><span id="HATMaskBackbone.get_mask-245"><a href="#HATMaskBackbone.get_mask-245"><span class="linenos">245</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="HATMaskBackbone.get_mask-246"><a href="#HATMaskBackbone.get_mask-246"><span class="linenos">246</span></a>        <span class="n">stage</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="HATMaskBackbone.get_mask-247"><a href="#HATMaskBackbone.get_mask-247"><span class="linenos">247</span></a>        <span class="n">s_max</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HATMaskBackbone.get_mask-248"><a href="#HATMaskBackbone.get_mask-248"><span class="linenos">248</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HATMaskBackbone.get_mask-249"><a href="#HATMaskBackbone.get_mask-249"><span class="linenos">249</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HATMaskBackbone.get_mask-250"><a href="#HATMaskBackbone.get_mask-250"><span class="linenos">250</span></a>        <span class="n">test_task_id</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HATMaskBackbone.get_mask-251"><a href="#HATMaskBackbone.get_mask-251"><span class="linenos">251</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
</span><span id="HATMaskBackbone.get_mask-252"><a href="#HATMaskBackbone.get_mask-252"><span class="linenos">252</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the hard attention mask used in the `forward()` method for different stages.</span>
</span><span id="HATMaskBackbone.get_mask-253"><a href="#HATMaskBackbone.get_mask-253"><span class="linenos">253</span></a>
</span><span id="HATMaskBackbone.get_mask-254"><a href="#HATMaskBackbone.get_mask-254"><span class="linenos">254</span></a><span class="sd">        **Args:**</span>
</span><span id="HATMaskBackbone.get_mask-255"><a href="#HATMaskBackbone.get_mask-255"><span class="linenos">255</span></a><span class="sd">        - **stage** (`str`): The stage when applying the conversion; one of:</span>
</span><span id="HATMaskBackbone.get_mask-256"><a href="#HATMaskBackbone.get_mask-256"><span class="linenos">256</span></a><span class="sd">            1. &#39;train&#39;: training stage. Get the mask from the current task embedding through the gate function, scaled by an annealed scalar. See 2.4 &quot;Hard Attention Training&quot; in the HAT paper.</span>
</span><span id="HATMaskBackbone.get_mask-257"><a href="#HATMaskBackbone.get_mask-257"><span class="linenos">257</span></a><span class="sd">            2. &#39;validation&#39;: validation stage. Get the mask from the current task embedding through the gate function, scaled by `s_max`, where large scaling makes masks nearly binary. (Note that in this stage, the binary mask hasn&#39;t been stored yet, as training is not over.)</span>
</span><span id="HATMaskBackbone.get_mask-258"><a href="#HATMaskBackbone.get_mask-258"><span class="linenos">258</span></a><span class="sd">            3. &#39;test&#39;: testing stage. Apply the test mask directly from the stored masks using `test_task_id`.</span>
</span><span id="HATMaskBackbone.get_mask-259"><a href="#HATMaskBackbone.get_mask-259"><span class="linenos">259</span></a><span class="sd">        - **s_max** (`float`): The maximum scaling factor in the gate function. Doesn&#39;t apply to the testing stage. See 2.4 in the HAT paper.</span>
</span><span id="HATMaskBackbone.get_mask-260"><a href="#HATMaskBackbone.get_mask-260"><span class="linenos">260</span></a><span class="sd">        - **batch_idx** (`int` | `None`): The current batch index. Applies only to the training stage. For other stages, it is `None`.</span>
</span><span id="HATMaskBackbone.get_mask-261"><a href="#HATMaskBackbone.get_mask-261"><span class="linenos">261</span></a><span class="sd">        - **num_batches** (`int` | `None`): The total number of batches. Applies only to the training stage. For other stages, it is `None`.</span>
</span><span id="HATMaskBackbone.get_mask-262"><a href="#HATMaskBackbone.get_mask-262"><span class="linenos">262</span></a><span class="sd">        - **test_task_id** (`int` | `None`): The test task ID. Applies only to the testing stage. For other stages, it is `None`.</span>
</span><span id="HATMaskBackbone.get_mask-263"><a href="#HATMaskBackbone.get_mask-263"><span class="linenos">263</span></a>
</span><span id="HATMaskBackbone.get_mask-264"><a href="#HATMaskBackbone.get_mask-264"><span class="linenos">264</span></a><span class="sd">        **Returns:**</span>
</span><span id="HATMaskBackbone.get_mask-265"><a href="#HATMaskBackbone.get_mask-265"><span class="linenos">265</span></a><span class="sd">        - **mask** (`dict[str, Tensor]`): The hard attention (with values 0 or 1) mask. Key (`str`) is the layer name; value (`Tensor`) is the mask tensor. The mask tensor has size (number of units,).</span>
</span><span id="HATMaskBackbone.get_mask-266"><a href="#HATMaskBackbone.get_mask-266"><span class="linenos">266</span></a>
</span><span id="HATMaskBackbone.get_mask-267"><a href="#HATMaskBackbone.get_mask-267"><span class="linenos">267</span></a><span class="sd">        **Raises:**</span>
</span><span id="HATMaskBackbone.get_mask-268"><a href="#HATMaskBackbone.get_mask-268"><span class="linenos">268</span></a><span class="sd">        - **ValueError**: If `batch_idx` and `batch_num` are not provided in the &#39;train&#39; stage; if `s_max` is not provided in the &#39;validation&#39; stage; if `task_id` is not provided in the &#39;test&#39; stage.</span>
</span><span id="HATMaskBackbone.get_mask-269"><a href="#HATMaskBackbone.get_mask-269"><span class="linenos">269</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HATMaskBackbone.get_mask-270"><a href="#HATMaskBackbone.get_mask-270"><span class="linenos">270</span></a>
</span><span id="HATMaskBackbone.get_mask-271"><a href="#HATMaskBackbone.get_mask-271"><span class="linenos">271</span></a>        <span class="c1"># sanity check</span>
</span><span id="HATMaskBackbone.get_mask-272"><a href="#HATMaskBackbone.get_mask-272"><span class="linenos">272</span></a>        <span class="k">if</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;train&quot;</span> <span class="ow">and</span> <span class="p">(</span>
</span><span id="HATMaskBackbone.get_mask-273"><a href="#HATMaskBackbone.get_mask-273"><span class="linenos">273</span></a>            <span class="n">s_max</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">batch_idx</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">num_batches</span> <span class="ow">is</span> <span class="kc">None</span>
</span><span id="HATMaskBackbone.get_mask-274"><a href="#HATMaskBackbone.get_mask-274"><span class="linenos">274</span></a>        <span class="p">):</span>
</span><span id="HATMaskBackbone.get_mask-275"><a href="#HATMaskBackbone.get_mask-275"><span class="linenos">275</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="HATMaskBackbone.get_mask-276"><a href="#HATMaskBackbone.get_mask-276"><span class="linenos">276</span></a>                <span class="s2">&quot;The `s_max`, `batch_idx` and `batch_num` should be provided at training stage, instead of the default value `None`.&quot;</span>
</span><span id="HATMaskBackbone.get_mask-277"><a href="#HATMaskBackbone.get_mask-277"><span class="linenos">277</span></a>            <span class="p">)</span>
</span><span id="HATMaskBackbone.get_mask-278"><a href="#HATMaskBackbone.get_mask-278"><span class="linenos">278</span></a>        <span class="k">if</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;validation&quot;</span> <span class="ow">and</span> <span class="p">(</span><span class="n">s_max</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
</span><span id="HATMaskBackbone.get_mask-279"><a href="#HATMaskBackbone.get_mask-279"><span class="linenos">279</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="HATMaskBackbone.get_mask-280"><a href="#HATMaskBackbone.get_mask-280"><span class="linenos">280</span></a>                <span class="s2">&quot;The `s_max` should be provided at validation stage, instead of the default value `None`.&quot;</span>
</span><span id="HATMaskBackbone.get_mask-281"><a href="#HATMaskBackbone.get_mask-281"><span class="linenos">281</span></a>            <span class="p">)</span>
</span><span id="HATMaskBackbone.get_mask-282"><a href="#HATMaskBackbone.get_mask-282"><span class="linenos">282</span></a>        <span class="k">if</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;test&quot;</span> <span class="ow">and</span> <span class="p">(</span><span class="n">test_task_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
</span><span id="HATMaskBackbone.get_mask-283"><a href="#HATMaskBackbone.get_mask-283"><span class="linenos">283</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="HATMaskBackbone.get_mask-284"><a href="#HATMaskBackbone.get_mask-284"><span class="linenos">284</span></a>                <span class="s2">&quot;The `task_mask` should be provided at testing stage, instead of the default value `None`.&quot;</span>
</span><span id="HATMaskBackbone.get_mask-285"><a href="#HATMaskBackbone.get_mask-285"><span class="linenos">285</span></a>            <span class="p">)</span>
</span><span id="HATMaskBackbone.get_mask-286"><a href="#HATMaskBackbone.get_mask-286"><span class="linenos">286</span></a>
</span><span id="HATMaskBackbone.get_mask-287"><a href="#HATMaskBackbone.get_mask-287"><span class="linenos">287</span></a>        <span class="n">mask</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="HATMaskBackbone.get_mask-288"><a href="#HATMaskBackbone.get_mask-288"><span class="linenos">288</span></a>        <span class="k">if</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;train&quot;</span><span class="p">:</span>
</span><span id="HATMaskBackbone.get_mask-289"><a href="#HATMaskBackbone.get_mask-289"><span class="linenos">289</span></a>            <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="HATMaskBackbone.get_mask-290"><a href="#HATMaskBackbone.get_mask-290"><span class="linenos">290</span></a>                <span class="n">anneal_scalar</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">s_max</span> <span class="o">+</span> <span class="p">(</span><span class="n">s_max</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">s_max</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">batch_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span>
</span><span id="HATMaskBackbone.get_mask-291"><a href="#HATMaskBackbone.get_mask-291"><span class="linenos">291</span></a>                    <span class="n">num_batches</span> <span class="o">-</span> <span class="mi">1</span>
</span><span id="HATMaskBackbone.get_mask-292"><a href="#HATMaskBackbone.get_mask-292"><span class="linenos">292</span></a>                <span class="p">)</span>  <span class="c1"># see equation (3) in chapter 2.4 &quot;Hard Attention Training&quot; in [HAT paper](http://proceedings.mlr.press/v80/serra18a).</span>
</span><span id="HATMaskBackbone.get_mask-293"><a href="#HATMaskBackbone.get_mask-293"><span class="linenos">293</span></a>                <span class="n">mask</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gate_fn</span><span class="p">(</span>
</span><span id="HATMaskBackbone.get_mask-294"><a href="#HATMaskBackbone.get_mask-294"><span class="linenos">294</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span> <span class="o">*</span> <span class="n">anneal_scalar</span>
</span><span id="HATMaskBackbone.get_mask-295"><a href="#HATMaskBackbone.get_mask-295"><span class="linenos">295</span></a>                <span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
</span><span id="HATMaskBackbone.get_mask-296"><a href="#HATMaskBackbone.get_mask-296"><span class="linenos">296</span></a>        <span class="k">elif</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;validation&quot;</span><span class="p">:</span>
</span><span id="HATMaskBackbone.get_mask-297"><a href="#HATMaskBackbone.get_mask-297"><span class="linenos">297</span></a>            <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="HATMaskBackbone.get_mask-298"><a href="#HATMaskBackbone.get_mask-298"><span class="linenos">298</span></a>                <span class="n">mask</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gate_fn</span><span class="p">(</span>
</span><span id="HATMaskBackbone.get_mask-299"><a href="#HATMaskBackbone.get_mask-299"><span class="linenos">299</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span> <span class="o">*</span> <span class="n">s_max</span>
</span><span id="HATMaskBackbone.get_mask-300"><a href="#HATMaskBackbone.get_mask-300"><span class="linenos">300</span></a>                <span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
</span><span id="HATMaskBackbone.get_mask-301"><a href="#HATMaskBackbone.get_mask-301"><span class="linenos">301</span></a>        <span class="k">elif</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;test&quot;</span><span class="p">:</span>
</span><span id="HATMaskBackbone.get_mask-302"><a href="#HATMaskBackbone.get_mask-302"><span class="linenos">302</span></a>            <span class="n">mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">masks</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">test_task_id</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span>
</span><span id="HATMaskBackbone.get_mask-303"><a href="#HATMaskBackbone.get_mask-303"><span class="linenos">303</span></a>
</span><span id="HATMaskBackbone.get_mask-304"><a href="#HATMaskBackbone.get_mask-304"><span class="linenos">304</span></a>        <span class="k">return</span> <span class="n">mask</span>
</span></pre></div>


            <div class="docstring"><p>Get the hard attention mask used in the <code><a href="#HATMaskBackbone.forward">forward()</a></code> method for different stages.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>stage</strong> (<code>str</code>): The stage when applying the conversion; one of:
<ol>
<li>'train': training stage. Get the mask from the current task embedding through the gate function, scaled by an annealed scalar. See 2.4 "Hard Attention Training" in the HAT paper.</li>
<li>'validation': validation stage. Get the mask from the current task embedding through the gate function, scaled by <code>s_max</code>, where large scaling makes masks nearly binary. (Note that in this stage, the binary mask hasn't been stored yet, as training is not over.)</li>
<li>'test': testing stage. Apply the test mask directly from the stored masks using <code>test_task_id</code>.</li>
</ol></li>
<li><strong>s_max</strong> (<code><a href="#HATMaskBackbone.float">float</a></code>): The maximum scaling factor in the gate function. Doesn't apply to the testing stage. See 2.4 in the HAT paper.</li>
<li><strong>batch_idx</strong> (<code>int</code> | <code>None</code>): The current batch index. Applies only to the training stage. For other stages, it is <code>None</code>.</li>
<li><strong>num_batches</strong> (<code>int</code> | <code>None</code>): The total number of batches. Applies only to the training stage. For other stages, it is <code>None</code>.</li>
<li><strong>test_task_id</strong> (<code>int</code> | <code>None</code>): The test task ID. Applies only to the testing stage. For other stages, it is <code>None</code>.</li>
</ul>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>mask</strong> (<code>dict[str, Tensor]</code>): The hard attention (with values 0 or 1) mask. Key (<code>str</code>) is the layer name; value (<code>Tensor</code>) is the mask tensor. The mask tensor has size (number of units,).</li>
</ul>

<p><strong>Raises:</strong></p>

<ul>
<li><strong>ValueError</strong>: If <code>batch_idx</code> and <code>batch_num</code> are not provided in the 'train' stage; if <code>s_max</code> is not provided in the 'validation' stage; if <code><a href="#HATMaskBackbone.task_id">task_id</a></code> is not provided in the 'test' stage.</li>
</ul>
</div>


                            </div>
                            <div id="HATMaskBackbone.te_to_binary_mask" class="classattr">
                                        <input id="HATMaskBackbone.te_to_binary_mask-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">te_to_binary_mask</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span></span><span class="return-annotation">) -> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>:</span></span>

                <label class="view-source-button" for="HATMaskBackbone.te_to_binary_mask-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HATMaskBackbone.te_to_binary_mask"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HATMaskBackbone.te_to_binary_mask-306"><a href="#HATMaskBackbone.te_to_binary_mask-306"><span class="linenos">306</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">te_to_binary_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
</span><span id="HATMaskBackbone.te_to_binary_mask-307"><a href="#HATMaskBackbone.te_to_binary_mask-307"><span class="linenos">307</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Convert the current task embedding to a binary mask.</span>
</span><span id="HATMaskBackbone.te_to_binary_mask-308"><a href="#HATMaskBackbone.te_to_binary_mask-308"><span class="linenos">308</span></a>
</span><span id="HATMaskBackbone.te_to_binary_mask-309"><a href="#HATMaskBackbone.te_to_binary_mask-309"><span class="linenos">309</span></a><span class="sd">        This method is used before the testing stage to convert the task embedding into a binary mask for each layer. The binary mask is used to select parameters for the current task.</span>
</span><span id="HATMaskBackbone.te_to_binary_mask-310"><a href="#HATMaskBackbone.te_to_binary_mask-310"><span class="linenos">310</span></a>
</span><span id="HATMaskBackbone.te_to_binary_mask-311"><a href="#HATMaskBackbone.te_to_binary_mask-311"><span class="linenos">311</span></a><span class="sd">        **Returns:**</span>
</span><span id="HATMaskBackbone.te_to_binary_mask-312"><a href="#HATMaskBackbone.te_to_binary_mask-312"><span class="linenos">312</span></a><span class="sd">        - **mask_t** (`dict[str, Tensor]`): The binary mask for the current task. Key (`str`) is the layer name; value (`Tensor`) is the mask tensor. The mask tensor has size (number of units,).</span>
</span><span id="HATMaskBackbone.te_to_binary_mask-313"><a href="#HATMaskBackbone.te_to_binary_mask-313"><span class="linenos">313</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HATMaskBackbone.te_to_binary_mask-314"><a href="#HATMaskBackbone.te_to_binary_mask-314"><span class="linenos">314</span></a>        <span class="c1"># get the mask for the current task</span>
</span><span id="HATMaskBackbone.te_to_binary_mask-315"><a href="#HATMaskBackbone.te_to_binary_mask-315"><span class="linenos">315</span></a>        <span class="n">mask_t</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="HATMaskBackbone.te_to_binary_mask-316"><a href="#HATMaskBackbone.te_to_binary_mask-316"><span class="linenos">316</span></a>            <span class="n">layer_name</span><span class="p">:</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">task_embedding_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>
</span><span id="HATMaskBackbone.te_to_binary_mask-317"><a href="#HATMaskBackbone.te_to_binary_mask-317"><span class="linenos">317</span></a>            <span class="o">.</span><span class="n">float</span><span class="p">()</span>
</span><span id="HATMaskBackbone.te_to_binary_mask-318"><a href="#HATMaskBackbone.te_to_binary_mask-318"><span class="linenos">318</span></a>            <span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
</span><span id="HATMaskBackbone.te_to_binary_mask-319"><a href="#HATMaskBackbone.te_to_binary_mask-319"><span class="linenos">319</span></a>            <span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="HATMaskBackbone.te_to_binary_mask-320"><a href="#HATMaskBackbone.te_to_binary_mask-320"><span class="linenos">320</span></a>            <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span>
</span><span id="HATMaskBackbone.te_to_binary_mask-321"><a href="#HATMaskBackbone.te_to_binary_mask-321"><span class="linenos">321</span></a>        <span class="p">}</span>
</span><span id="HATMaskBackbone.te_to_binary_mask-322"><a href="#HATMaskBackbone.te_to_binary_mask-322"><span class="linenos">322</span></a>
</span><span id="HATMaskBackbone.te_to_binary_mask-323"><a href="#HATMaskBackbone.te_to_binary_mask-323"><span class="linenos">323</span></a>        <span class="k">return</span> <span class="n">mask_t</span>
</span></pre></div>


            <div class="docstring"><p>Convert the current task embedding to a binary mask.</p>

<p>This method is used before the testing stage to convert the task embedding into a binary mask for each layer. The binary mask is used to select parameters for the current task.</p>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>mask_t</strong> (<code>dict[str, Tensor]</code>): The binary mask for the current task. Key (<code>str</code>) is the layer name; value (<code>Tensor</code>) is the mask tensor. The mask tensor has size (number of units,).</li>
</ul>
</div>


                            </div>
                            <div id="HATMaskBackbone.store_mask" class="classattr">
                                        <input id="HATMaskBackbone.store_mask-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">store_mask</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span></span><span class="return-annotation">) -> <span class="kc">None</span>:</span></span>

                <label class="view-source-button" for="HATMaskBackbone.store_mask-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HATMaskBackbone.store_mask"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HATMaskBackbone.store_mask-325"><a href="#HATMaskBackbone.store_mask-325"><span class="linenos">325</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">store_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="HATMaskBackbone.store_mask-326"><a href="#HATMaskBackbone.store_mask-326"><span class="linenos">326</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the mask for the current task.&quot;&quot;&quot;</span>
</span><span id="HATMaskBackbone.store_mask-327"><a href="#HATMaskBackbone.store_mask-327"><span class="linenos">327</span></a>        <span class="n">mask_t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">te_to_binary_mask</span><span class="p">()</span>
</span><span id="HATMaskBackbone.store_mask-328"><a href="#HATMaskBackbone.store_mask-328"><span class="linenos">328</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">masks</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">task_id</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mask_t</span>
</span><span id="HATMaskBackbone.store_mask-329"><a href="#HATMaskBackbone.store_mask-329"><span class="linenos">329</span></a>
</span><span id="HATMaskBackbone.store_mask-330"><a href="#HATMaskBackbone.store_mask-330"><span class="linenos">330</span></a>        <span class="k">return</span> <span class="n">mask_t</span>
</span></pre></div>


            <div class="docstring"><p>Store the mask for the current task.</p>
</div>


                            </div>
                            <div id="HATMaskBackbone.get_layer_measure_parameter_wise" class="classattr">
                                        <input id="HATMaskBackbone.get_layer_measure_parameter_wise-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">get_layer_measure_parameter_wise</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">unit_wise_measure</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>,</span><span class="param">	<span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">aggregation_mode</span><span class="p">:</span> <span class="nb">str</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="HATMaskBackbone.get_layer_measure_parameter_wise-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HATMaskBackbone.get_layer_measure_parameter_wise"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-332"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-332"><span class="linenos">332</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_layer_measure_parameter_wise</span><span class="p">(</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-333"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-333"><span class="linenos">333</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-334"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-334"><span class="linenos">334</span></a>        <span class="n">unit_wise_measure</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-335"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-335"><span class="linenos">335</span></a>        <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-336"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-336"><span class="linenos">336</span></a>        <span class="n">aggregation_mode</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-337"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-337"><span class="linenos">337</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-338"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-338"><span class="linenos">338</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the parameter-wise measure on the parameters right before the given layer.</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-339"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-339"><span class="linenos">339</span></a>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-340"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-340"><span class="linenos">340</span></a><span class="sd">        It is calculated from the given unit-wise measure. It aggregates two feature-sized vectors (corresponding to the given layer and the preceding layer) into a weight-wise matrix (corresponding to the weights in between) and a bias-wise vector (corresponding to the bias of the given layer), using the given aggregation method. For example, given two feature-sized measures $m_{l,i}$ and $m_{l-1,j}$ and &#39;min&#39; aggregation, the parameter-wise measure is $\min \left(a_{l,i}, a_{l-1,j}\right)$, a matrix with respect to $i, j$.</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-341"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-341"><span class="linenos">341</span></a>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-342"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-342"><span class="linenos">342</span></a><span class="sd">        Note that if the given layer is the first layer with no preceding layer, we will get the parameter-wise measure directly broadcast from the unit-wise measure of the given layer.</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-343"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-343"><span class="linenos">343</span></a>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-344"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-344"><span class="linenos">344</span></a><span class="sd">        This method is used to calculate parameter-wise measures in various HAT-based algorithms:</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-345"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-345"><span class="linenos">345</span></a>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-346"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-346"><span class="linenos">346</span></a><span class="sd">        - **HAT**: the parameter-wise measure is the binary mask for previous tasks from the unit-wise cumulative mask of previous tasks `self.cumulative_mask_for_previous_tasks`, which is $\min \left(a_{l,i}^{&lt;t}, a_{l-1,j}^{&lt;t}\right)$ in Eq. (2) in the HAT paper.</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-347"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-347"><span class="linenos">347</span></a><span class="sd">        - **AdaHAT**: the parameter-wise measure is the parameter importance for previous tasks from the unit-wise summative mask of previous tasks `self.summative_mask_for_previous_tasks`, which is $\min \left(m_{l,i}^{&lt;t,\text{sum}}, m_{l-1,j}^{&lt;t,\text{sum}}\right)$ in Eq. (9) in the AdaHAT paper.</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-348"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-348"><span class="linenos">348</span></a><span class="sd">        - **CBPHAT**: the parameter-wise measure is the parameter importance for previous tasks from the unit-wise importance of previous tasks `self.unit_importance_for_previous_tasks` based on contribution utility, which is $\min \left(I_{l,i}^{(t-1)}, I_{l-1,j}^{(t-1)}\right)$ in the adjustment rate formula in the paper draft.</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-349"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-349"><span class="linenos">349</span></a>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-350"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-350"><span class="linenos">350</span></a><span class="sd">        **Args:**</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-351"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-351"><span class="linenos">351</span></a><span class="sd">        - **unit_wise_measure** (`dict[str, Tensor]`): The unit-wise measure. Key is layer name; value is the unit-wise measure tensor. The tensor has size (number of units,).</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-352"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-352"><span class="linenos">352</span></a><span class="sd">        - **layer_name** (`str`): The name of the given layer.</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-353"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-353"><span class="linenos">353</span></a><span class="sd">        - **aggregation_mode** (`str`): The aggregation mode turning two feature-wise measures into a weight-wise matrix; one of:</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-354"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-354"><span class="linenos">354</span></a><span class="sd">            - &#39;min&#39;: takes the minimum of the two connected unit measures.</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-355"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-355"><span class="linenos">355</span></a><span class="sd">            - &#39;max&#39;: takes the maximum of the two connected unit measures.</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-356"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-356"><span class="linenos">356</span></a><span class="sd">            - &#39;mean&#39;: takes the mean of the two connected unit measures.</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-357"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-357"><span class="linenos">357</span></a><span class="sd">        **Returns:**</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-358"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-358"><span class="linenos">358</span></a><span class="sd">        - **weight_measure** (`Tensor`): The weight measure matrix, the same size as the corresponding weights.</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-359"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-359"><span class="linenos">359</span></a><span class="sd">        - **bias_measure** (`Tensor`): The bias measure vector, the same size as the corresponding bias.</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-360"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-360"><span class="linenos">360</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-361"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-361"><span class="linenos">361</span></a>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-362"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-362"><span class="linenos">362</span></a>        <span class="c1"># initialize the aggregation function</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-363"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-363"><span class="linenos">363</span></a>        <span class="k">if</span> <span class="n">aggregation_mode</span> <span class="o">==</span> <span class="s2">&quot;min&quot;</span><span class="p">:</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-364"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-364"><span class="linenos">364</span></a>            <span class="n">aggregation_func</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-365"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-365"><span class="linenos">365</span></a>        <span class="k">elif</span> <span class="n">aggregation_mode</span> <span class="o">==</span> <span class="s2">&quot;max&quot;</span><span class="p">:</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-366"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-366"><span class="linenos">366</span></a>            <span class="n">aggregation_func</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-367"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-367"><span class="linenos">367</span></a>        <span class="k">elif</span> <span class="n">aggregation_mode</span> <span class="o">==</span> <span class="s2">&quot;mean&quot;</span><span class="p">:</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-368"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-368"><span class="linenos">368</span></a>            <span class="n">aggregation_func</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-369"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-369"><span class="linenos">369</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-370"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-370"><span class="linenos">370</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-371"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-371"><span class="linenos">371</span></a>                <span class="sa">f</span><span class="s2">&quot;The aggregation method </span><span class="si">{</span><span class="n">aggregation_mode</span><span class="si">}</span><span class="s2"> is not supported.&quot;</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-372"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-372"><span class="linenos">372</span></a>            <span class="p">)</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-373"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-373"><span class="linenos">373</span></a>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-374"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-374"><span class="linenos">374</span></a>        <span class="c1"># get the preceding layer</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-375"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-375"><span class="linenos">375</span></a>        <span class="n">preceding_layer_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">preceding_layer_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-376"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-376"><span class="linenos">376</span></a>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-377"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-377"><span class="linenos">377</span></a>        <span class="c1"># get weight size for expanding the measures</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-378"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-378"><span class="linenos">378</span></a>        <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-379"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-379"><span class="linenos">379</span></a>        <span class="n">weight_size</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-380"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-380"><span class="linenos">380</span></a>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-381"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-381"><span class="linenos">381</span></a>        <span class="c1"># construct the weight-wise measure</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-382"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-382"><span class="linenos">382</span></a>        <span class="n">layer_measure</span> <span class="o">=</span> <span class="n">unit_wise_measure</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-383"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-383"><span class="linenos">383</span></a>        <span class="n">layer_measure_broadcast_size</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-384"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-384"><span class="linenos">384</span></a>            <span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">weight_size</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-385"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-385"><span class="linenos">385</span></a>        <span class="p">)</span>  <span class="c1"># since the size of mask tensor is (number of units, ), we extend it to (number of units, 1) and expand it to the weight size. The weight size has 2 dimensions in fully connected layers and 4 dimensions in convolutional layers</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-386"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-386"><span class="linenos">386</span></a>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-387"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-387"><span class="linenos">387</span></a>        <span class="n">layer_measure_broadcasted</span> <span class="o">=</span> <span class="n">layer_measure</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-388"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-388"><span class="linenos">388</span></a>            <span class="o">*</span><span class="n">layer_measure_broadcast_size</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-389"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-389"><span class="linenos">389</span></a>        <span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-390"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-390"><span class="linenos">390</span></a>            <span class="n">weight_size</span><span class="p">,</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-391"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-391"><span class="linenos">391</span></a>        <span class="p">)</span>  <span class="c1"># expand the given layer mask to the weight size and broadcast</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-392"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-392"><span class="linenos">392</span></a>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-393"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-393"><span class="linenos">393</span></a>        <span class="k">if</span> <span class="p">(</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-394"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-394"><span class="linenos">394</span></a>            <span class="n">preceding_layer_name</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-395"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-395"><span class="linenos">395</span></a>        <span class="p">):</span>  <span class="c1"># if the layer is not the first layer, where the preceding layer exists</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-396"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-396"><span class="linenos">396</span></a>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-397"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-397"><span class="linenos">397</span></a>            <span class="n">preceding_layer_measure_broadcast_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-398"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-398"><span class="linenos">398</span></a>                <span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">weight_size</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-399"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-399"><span class="linenos">399</span></a>            <span class="p">)</span>  <span class="c1"># since the size of mask tensor is (number of units, ), we extend it to (1, number of units) and expand it to the weight size. The weight size has 2 dimensions in fully connected layers and 4 dimensions in convolutional layers</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-400"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-400"><span class="linenos">400</span></a>            <span class="n">preceding_layer_measure</span> <span class="o">=</span> <span class="n">unit_wise_measure</span><span class="p">[</span><span class="n">preceding_layer_name</span><span class="p">]</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-401"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-401"><span class="linenos">401</span></a>            <span class="n">preceding_layer_measure_broadcasted</span> <span class="o">=</span> <span class="n">preceding_layer_measure</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-402"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-402"><span class="linenos">402</span></a>                <span class="o">*</span><span class="n">preceding_layer_measure_broadcast_size</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-403"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-403"><span class="linenos">403</span></a>            <span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-404"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-404"><span class="linenos">404</span></a>                <span class="n">weight_size</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-405"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-405"><span class="linenos">405</span></a>            <span class="p">)</span>  <span class="c1"># expand the preceding layer mask to the weight size and broadcast</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-406"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-406"><span class="linenos">406</span></a>            <span class="n">weight_measure</span> <span class="o">=</span> <span class="n">aggregation_func</span><span class="p">(</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-407"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-407"><span class="linenos">407</span></a>                <span class="n">layer_measure_broadcasted</span><span class="p">,</span> <span class="n">preceding_layer_measure_broadcasted</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-408"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-408"><span class="linenos">408</span></a>            <span class="p">)</span>  <span class="c1"># get the minimum of the two mask vectors, from expanded</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-409"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-409"><span class="linenos">409</span></a>        <span class="k">else</span><span class="p">:</span>  <span class="c1"># if the layer is the first layer</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-410"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-410"><span class="linenos">410</span></a>            <span class="n">weight_measure</span> <span class="o">=</span> <span class="n">layer_measure_broadcasted</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-411"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-411"><span class="linenos">411</span></a>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-412"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-412"><span class="linenos">412</span></a>        <span class="c1"># construct the bias-wise measure</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-413"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-413"><span class="linenos">413</span></a>        <span class="n">bias_measure</span> <span class="o">=</span> <span class="n">layer_measure</span>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-414"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-414"><span class="linenos">414</span></a>
</span><span id="HATMaskBackbone.get_layer_measure_parameter_wise-415"><a href="#HATMaskBackbone.get_layer_measure_parameter_wise-415"><span class="linenos">415</span></a>        <span class="k">return</span> <span class="n">weight_measure</span><span class="p">,</span> <span class="n">bias_measure</span>
</span></pre></div>


            <div class="docstring"><p>Get the parameter-wise measure on the parameters right before the given layer.</p>

<p>It is calculated from the given unit-wise measure. It aggregates two feature-sized vectors (corresponding to the given layer and the preceding layer) into a weight-wise matrix (corresponding to the weights in between) and a bias-wise vector (corresponding to the bias of the given layer), using the given aggregation method. For example, given two feature-sized measures $m_{l,i}$ and $m_{l-1,j}$ and 'min' aggregation, the parameter-wise measure is $\min \left(a_{l,i}, a_{l-1,j}\right)$, a matrix with respect to $i, j$.</p>

<p>Note that if the given layer is the first layer with no preceding layer, we will get the parameter-wise measure directly broadcast from the unit-wise measure of the given layer.</p>

<p>This method is used to calculate parameter-wise measures in various HAT-based algorithms:</p>

<ul>
<li><strong>HAT</strong>: the parameter-wise measure is the binary mask for previous tasks from the unit-wise cumulative mask of previous tasks <code>self.cumulative_mask_for_previous_tasks</code>, which is $\min \left(a_{l,i}^{<t}, a_{l-1,j}^{<t}\right)$ in Eq. (2) in the HAT paper.</li>
<li><strong>AdaHAT</strong>: the parameter-wise measure is the parameter importance for previous tasks from the unit-wise summative mask of previous tasks <code>self.summative_mask_for_previous_tasks</code>, which is $\min \left(m_{l,i}^{<t,\text{sum}}, m_{l-1,j}^{<t,\text{sum}}\right)$ in Eq. (9) in the AdaHAT paper.</li>
<li><strong>CBPHAT</strong>: the parameter-wise measure is the parameter importance for previous tasks from the unit-wise importance of previous tasks <code>self.unit_importance_for_previous_tasks</code> based on contribution utility, which is $\min \left(I_{l,i}^{(t-1)}, I_{l-1,j}^{(t-1)}\right)$ in the adjustment rate formula in the paper draft.</li>
</ul>

<p><strong>Args:</strong></p>

<ul>
<li><strong>unit_wise_measure</strong> (<code>dict[str, Tensor]</code>): The unit-wise measure. Key is layer name; value is the unit-wise measure tensor. The tensor has size (number of units,).</li>
<li><strong>layer_name</strong> (<code>str</code>): The name of the given layer.</li>
<li><strong>aggregation_mode</strong> (<code>str</code>): The aggregation mode turning two feature-wise measures into a weight-wise matrix; one of:
<ul>
<li>'min': takes the minimum of the two connected unit measures.</li>
<li>'max': takes the maximum of the two connected unit measures.</li>
<li>'mean': takes the mean of the two connected unit measures.
<strong>Returns:</strong></li>
</ul></li>
<li><strong>weight_measure</strong> (<code>Tensor</code>): The weight measure matrix, the same size as the corresponding weights.</li>
<li><strong>bias_measure</strong> (<code>Tensor</code>): The bias measure vector, the same size as the corresponding bias.</li>
</ul>
</div>


                            </div>
                            <div id="HATMaskBackbone.forward" class="classattr">
                                        <input id="HATMaskBackbone.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
                    <div class="decorator decorator-override">@override</div>

        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">stage</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">s_max</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">test_task_id</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span></span><span class="return-annotation">) -> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span>:</span></span>

                <label class="view-source-button" for="HATMaskBackbone.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#HATMaskBackbone.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="HATMaskBackbone.forward-417"><a href="#HATMaskBackbone.forward-417"><span class="linenos">417</span></a>    <span class="nd">@override</span>
</span><span id="HATMaskBackbone.forward-418"><a href="#HATMaskBackbone.forward-418"><span class="linenos">418</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="HATMaskBackbone.forward-419"><a href="#HATMaskBackbone.forward-419"><span class="linenos">419</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="HATMaskBackbone.forward-420"><a href="#HATMaskBackbone.forward-420"><span class="linenos">420</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="HATMaskBackbone.forward-421"><a href="#HATMaskBackbone.forward-421"><span class="linenos">421</span></a>        <span class="n">stage</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="HATMaskBackbone.forward-422"><a href="#HATMaskBackbone.forward-422"><span class="linenos">422</span></a>        <span class="n">s_max</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HATMaskBackbone.forward-423"><a href="#HATMaskBackbone.forward-423"><span class="linenos">423</span></a>        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HATMaskBackbone.forward-424"><a href="#HATMaskBackbone.forward-424"><span class="linenos">424</span></a>        <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HATMaskBackbone.forward-425"><a href="#HATMaskBackbone.forward-425"><span class="linenos">425</span></a>        <span class="n">test_task_id</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="HATMaskBackbone.forward-426"><a href="#HATMaskBackbone.forward-426"><span class="linenos">426</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
</span><span id="HATMaskBackbone.forward-427"><a href="#HATMaskBackbone.forward-427"><span class="linenos">427</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The forward pass for data from task `task_id`. Task-specific masks for `task_id` are applied to the units in each layer.</span>
</span><span id="HATMaskBackbone.forward-428"><a href="#HATMaskBackbone.forward-428"><span class="linenos">428</span></a>
</span><span id="HATMaskBackbone.forward-429"><a href="#HATMaskBackbone.forward-429"><span class="linenos">429</span></a><span class="sd">        **Args:**</span>
</span><span id="HATMaskBackbone.forward-430"><a href="#HATMaskBackbone.forward-430"><span class="linenos">430</span></a><span class="sd">        - **input** (`Tensor`): The input tensor from data.</span>
</span><span id="HATMaskBackbone.forward-431"><a href="#HATMaskBackbone.forward-431"><span class="linenos">431</span></a><span class="sd">        - **stage** (`str`): The stage of the forward pass; one of:</span>
</span><span id="HATMaskBackbone.forward-432"><a href="#HATMaskBackbone.forward-432"><span class="linenos">432</span></a><span class="sd">            1. &#39;train&#39;: training stage.</span>
</span><span id="HATMaskBackbone.forward-433"><a href="#HATMaskBackbone.forward-433"><span class="linenos">433</span></a><span class="sd">            2. &#39;validation&#39;: validation stage.</span>
</span><span id="HATMaskBackbone.forward-434"><a href="#HATMaskBackbone.forward-434"><span class="linenos">434</span></a><span class="sd">            3. &#39;test&#39;: testing stage.</span>
</span><span id="HATMaskBackbone.forward-435"><a href="#HATMaskBackbone.forward-435"><span class="linenos">435</span></a><span class="sd">        - **s_max** (`float`): The maximum scaling factor in the gate function. See 2.4 &quot;Hard Attention Training&quot; in the HAT paper.</span>
</span><span id="HATMaskBackbone.forward-436"><a href="#HATMaskBackbone.forward-436"><span class="linenos">436</span></a><span class="sd">        - **batch_idx** (`int` | `None`): The current batch index. Applies only to the training stage. For other stages, it is `None`.</span>
</span><span id="HATMaskBackbone.forward-437"><a href="#HATMaskBackbone.forward-437"><span class="linenos">437</span></a><span class="sd">        - **num_batches** (`int` | `None`): The total number of batches. Applies only to the training stage. For other stages, it is `None`.</span>
</span><span id="HATMaskBackbone.forward-438"><a href="#HATMaskBackbone.forward-438"><span class="linenos">438</span></a><span class="sd">        - **test_task_id** (`int` | `None`): The test task ID. Applies only to the testing stage. For other stages, it is `None`.</span>
</span><span id="HATMaskBackbone.forward-439"><a href="#HATMaskBackbone.forward-439"><span class="linenos">439</span></a>
</span><span id="HATMaskBackbone.forward-440"><a href="#HATMaskBackbone.forward-440"><span class="linenos">440</span></a><span class="sd">        **Returns:**</span>
</span><span id="HATMaskBackbone.forward-441"><a href="#HATMaskBackbone.forward-441"><span class="linenos">441</span></a><span class="sd">        - **output_feature** (`Tensor`): The output feature tensor to be passed into heads. This is the main target of backpropagation.</span>
</span><span id="HATMaskBackbone.forward-442"><a href="#HATMaskBackbone.forward-442"><span class="linenos">442</span></a><span class="sd">        - **mask** (`dict[str, Tensor]`): The mask for the current task. Key (`str`) is layer name; value (`Tensor`) is the mask tensor. The mask tensor has size (number of units,).</span>
</span><span id="HATMaskBackbone.forward-443"><a href="#HATMaskBackbone.forward-443"><span class="linenos">443</span></a><span class="sd">        - **activations** (`dict[str, Tensor]`): The hidden features (after activation) in each weighted layer. Key (`str`) is the weighted layer name; value (`Tensor`) is the hidden feature tensor. This is used for continual learning algorithms that need hidden features. Although the HAT algorithm does not need this, it is still provided for API consistency for other HAT-based algorithms that inherit this `forward()` method of the `HAT` class.</span>
</span><span id="HATMaskBackbone.forward-444"><a href="#HATMaskBackbone.forward-444"><span class="linenos">444</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span></pre></div>


            <div class="docstring"><p>The forward pass for data from task <code><a href="#HATMaskBackbone.task_id">task_id</a></code>. Task-specific masks for <code><a href="#HATMaskBackbone.task_id">task_id</a></code> are applied to the units in each layer.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>input</strong> (<code>Tensor</code>): The input tensor from data.</li>
<li><strong>stage</strong> (<code>str</code>): The stage of the forward pass; one of:
<ol>
<li>'train': training stage.</li>
<li>'validation': validation stage.</li>
<li>'test': testing stage.</li>
</ol></li>
<li><strong>s_max</strong> (<code><a href="#HATMaskBackbone.float">float</a></code>): The maximum scaling factor in the gate function. See 2.4 "Hard Attention Training" in the HAT paper.</li>
<li><strong>batch_idx</strong> (<code>int</code> | <code>None</code>): The current batch index. Applies only to the training stage. For other stages, it is <code>None</code>.</li>
<li><strong>num_batches</strong> (<code>int</code> | <code>None</code>): The total number of batches. Applies only to the training stage. For other stages, it is <code>None</code>.</li>
<li><strong>test_task_id</strong> (<code>int</code> | <code>None</code>): The test task ID. Applies only to the testing stage. For other stages, it is <code>None</code>.</li>
</ul>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>output_feature</strong> (<code>Tensor</code>): The output feature tensor to be passed into heads. This is the main target of backpropagation.</li>
<li><strong>mask</strong> (<code>dict[str, Tensor]</code>): The mask for the current task. Key (<code>str</code>) is layer name; value (<code>Tensor</code>) is the mask tensor. The mask tensor has size (number of units,).</li>
<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): The hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name; value (<code>Tensor</code>) is the hidden feature tensor. This is used for continual learning algorithms that need hidden features. Although the HAT algorithm does not need this, it is still provided for API consistency for other HAT-based algorithms that inherit this <code><a href="#HATMaskBackbone.forward">forward()</a></code> method of the <code>HAT</code> class.</li>
</ul>
</div>


                            </div>
                </section>
                <section id="WSNMaskBackbone">
                            <input id="WSNMaskBackbone-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">WSNMaskBackbone</span><wbr>(<span class="base"><a href="#CLBackbone">clarena.backbones.CLBackbone</a></span>):

                <label class="view-source-button" for="WSNMaskBackbone-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#WSNMaskBackbone"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="WSNMaskBackbone-447"><a href="#WSNMaskBackbone-447"><span class="linenos">447</span></a><span class="k">class</span><span class="w"> </span><span class="nc">WSNMaskBackbone</span><span class="p">(</span><span class="n">CLBackbone</span><span class="p">):</span>
</span><span id="WSNMaskBackbone-448"><a href="#WSNMaskBackbone-448"><span class="linenos">448</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The backbone network for the WSN algorithm with learnable parameter masks.</span>
</span><span id="WSNMaskBackbone-449"><a href="#WSNMaskBackbone-449"><span class="linenos">449</span></a>
</span><span id="WSNMaskBackbone-450"><a href="#WSNMaskBackbone-450"><span class="linenos">450</span></a><span class="sd">    [WSN (Winning Subnetworks, 2022)](https://proceedings.mlr.press/v162/kang22b/kang22b.pdf) is an architecture-based continual learning algorithm. It trains learnable parameter-wise scores and selects the most scored $c\%$ of the network parameters to be used for each task.</span>
</span><span id="WSNMaskBackbone-451"><a href="#WSNMaskBackbone-451"><span class="linenos">451</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="WSNMaskBackbone-452"><a href="#WSNMaskBackbone-452"><span class="linenos">452</span></a>
</span><span id="WSNMaskBackbone-453"><a href="#WSNMaskBackbone-453"><span class="linenos">453</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="WSNMaskBackbone-454"><a href="#WSNMaskBackbone-454"><span class="linenos">454</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialize the WSN mask backbone network with task embeddings and masks.</span>
</span><span id="WSNMaskBackbone-455"><a href="#WSNMaskBackbone-455"><span class="linenos">455</span></a>
</span><span id="WSNMaskBackbone-456"><a href="#WSNMaskBackbone-456"><span class="linenos">456</span></a><span class="sd">        **Args:**</span>
</span><span id="WSNMaskBackbone-457"><a href="#WSNMaskBackbone-457"><span class="linenos">457</span></a><span class="sd">        - **output_dim** (`int`): The output dimension that connects to CL output heads. The `input_dim` of output heads is expected to be the same as this `output_dim`. In some cases, this class is used as a block in the backbone network that doesn&#39;t have an output dimension. In this case, it can be `None`.</span>
</span><span id="WSNMaskBackbone-458"><a href="#WSNMaskBackbone-458"><span class="linenos">458</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="WSNMaskBackbone-459"><a href="#WSNMaskBackbone-459"><span class="linenos">459</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span id="WSNMaskBackbone-460"><a href="#WSNMaskBackbone-460"><span class="linenos">460</span></a>
</span><span id="WSNMaskBackbone-461"><a href="#WSNMaskBackbone-461"><span class="linenos">461</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">gate_fn</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span> <span class="o">=</span> <span class="n">PercentileLayerParameterMaskingByScore</span>
</span><span id="WSNMaskBackbone-462"><a href="#WSNMaskBackbone-462"><span class="linenos">462</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The gate function turning the real-value parameter score into binary parameter masks. It is a custom autograd function that applies percentile parameter masking by score.&quot;&quot;&quot;</span>
</span><span id="WSNMaskBackbone-463"><a href="#WSNMaskBackbone-463"><span class="linenos">463</span></a>
</span><span id="WSNMaskBackbone-464"><a href="#WSNMaskBackbone-464"><span class="linenos">464</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">weight_score_t</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">()</span>
</span><span id="WSNMaskBackbone-465"><a href="#WSNMaskBackbone-465"><span class="linenos">465</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the weight score for the current task. Keys are the layer names and values are the task embedding `nn.Embedding` for the layer. Each task embedding has the same size (output features, input features) as the weight.</span>
</span><span id="WSNMaskBackbone-466"><a href="#WSNMaskBackbone-466"><span class="linenos">466</span></a><span class="sd">        </span>
</span><span id="WSNMaskBackbone-467"><a href="#WSNMaskBackbone-467"><span class="linenos">467</span></a><span class="sd">        We use `ModuleDict` rather than `dict` to ensure `LightningModule` properly registers these model parameters for purposes such as automatic device transfer and model summaries.</span>
</span><span id="WSNMaskBackbone-468"><a href="#WSNMaskBackbone-468"><span class="linenos">468</span></a><span class="sd">        </span>
</span><span id="WSNMaskBackbone-469"><a href="#WSNMaskBackbone-469"><span class="linenos">469</span></a><span class="sd">        We use `nn.Embedding` rather than `nn.Parameter` to store the task embedding for each layer, which is a type of `nn.Module` and can be accepted by `nn.ModuleDict`. (`nn.Parameter` cannot be accepted by `nn.ModuleDict`.)</span>
</span><span id="WSNMaskBackbone-470"><a href="#WSNMaskBackbone-470"><span class="linenos">470</span></a><span class="sd">        </span>
</span><span id="WSNMaskBackbone-471"><a href="#WSNMaskBackbone-471"><span class="linenos">471</span></a><span class="sd">        **This must be defined to cover each weighted layer (as listed in `self.weighted_layer_names`) in the backbone network.** Otherwise, the uncovered parts will keep updating for all tasks and become a source of catastrophic forgetting.</span>
</span><span id="WSNMaskBackbone-472"><a href="#WSNMaskBackbone-472"><span class="linenos">472</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="WSNMaskBackbone-473"><a href="#WSNMaskBackbone-473"><span class="linenos">473</span></a>
</span><span id="WSNMaskBackbone-474"><a href="#WSNMaskBackbone-474"><span class="linenos">474</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">bias_score_t</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">()</span>
</span><span id="WSNMaskBackbone-475"><a href="#WSNMaskBackbone-475"><span class="linenos">475</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the bias score for the current task. Keys are the layer names and values are the task embedding `nn.Embedding` for the layer. Each task embedding has the same size (1, output features) as the bias. If the layer doesn&#39;t have a bias, it is `None`.</span>
</span><span id="WSNMaskBackbone-476"><a href="#WSNMaskBackbone-476"><span class="linenos">476</span></a><span class="sd">        </span>
</span><span id="WSNMaskBackbone-477"><a href="#WSNMaskBackbone-477"><span class="linenos">477</span></a><span class="sd">        We use `ModuleDict` rather than `dict` to ensure `LightningModule` properly registers these model parameters for purposes such as automatic device transfer and model summaries.</span>
</span><span id="WSNMaskBackbone-478"><a href="#WSNMaskBackbone-478"><span class="linenos">478</span></a><span class="sd">        </span>
</span><span id="WSNMaskBackbone-479"><a href="#WSNMaskBackbone-479"><span class="linenos">479</span></a><span class="sd">        We use `nn.Embedding` rather than `nn.Parameter` to store the task embedding for each layer, which is a type of `nn.Module` and can be accepted by `nn.ModuleDict`. (`nn.Parameter` cannot be accepted by `nn.ModuleDict`.)</span>
</span><span id="WSNMaskBackbone-480"><a href="#WSNMaskBackbone-480"><span class="linenos">480</span></a><span class="sd">        </span>
</span><span id="WSNMaskBackbone-481"><a href="#WSNMaskBackbone-481"><span class="linenos">481</span></a><span class="sd">        **This must be defined to cover each weighted layer (as listed in `self.weighted_layer_names`) in the backbone network.** Otherwise, the uncovered parts will keep updating for all tasks and become a source of catastrophic forgetting.</span>
</span><span id="WSNMaskBackbone-482"><a href="#WSNMaskBackbone-482"><span class="linenos">482</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="WSNMaskBackbone-483"><a href="#WSNMaskBackbone-483"><span class="linenos">483</span></a>
</span><span id="WSNMaskBackbone-484"><a href="#WSNMaskBackbone-484"><span class="linenos">484</span></a>        <span class="n">WSNMaskBackbone</span><span class="o">.</span><span class="n">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</span><span id="WSNMaskBackbone-485"><a href="#WSNMaskBackbone-485"><span class="linenos">485</span></a>
</span><span id="WSNMaskBackbone-486"><a href="#WSNMaskBackbone-486"><span class="linenos">486</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="WSNMaskBackbone-487"><a href="#WSNMaskBackbone-487"><span class="linenos">487</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Check the sanity of the arguments.&quot;&quot;&quot;</span>
</span><span id="WSNMaskBackbone-488"><a href="#WSNMaskBackbone-488"><span class="linenos">488</span></a>
</span><span id="WSNMaskBackbone-489"><a href="#WSNMaskBackbone-489"><span class="linenos">489</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">initialize_parameter_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="WSNMaskBackbone-490"><a href="#WSNMaskBackbone-490"><span class="linenos">490</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialize the parameter score for the current task.</span>
</span><span id="WSNMaskBackbone-491"><a href="#WSNMaskBackbone-491"><span class="linenos">491</span></a>
</span><span id="WSNMaskBackbone-492"><a href="#WSNMaskBackbone-492"><span class="linenos">492</span></a><span class="sd">        **Args:**</span>
</span><span id="WSNMaskBackbone-493"><a href="#WSNMaskBackbone-493"><span class="linenos">493</span></a><span class="sd">        - **mode** (`str`): The initialization mode for parameter scores; one of:</span>
</span><span id="WSNMaskBackbone-494"><a href="#WSNMaskBackbone-494"><span class="linenos">494</span></a><span class="sd">            1. &#39;default&#39;: the default initialization mode in the original WSN code.</span>
</span><span id="WSNMaskBackbone-495"><a href="#WSNMaskBackbone-495"><span class="linenos">495</span></a><span class="sd">            2. &#39;N01&#39;: standard normal distribution $N(0, 1)$.</span>
</span><span id="WSNMaskBackbone-496"><a href="#WSNMaskBackbone-496"><span class="linenos">496</span></a><span class="sd">            3. &#39;U01&#39;: uniform distribution $U(0, 1)$.</span>
</span><span id="WSNMaskBackbone-497"><a href="#WSNMaskBackbone-497"><span class="linenos">497</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="WSNMaskBackbone-498"><a href="#WSNMaskBackbone-498"><span class="linenos">498</span></a>
</span><span id="WSNMaskBackbone-499"><a href="#WSNMaskBackbone-499"><span class="linenos">499</span></a>        <span class="k">for</span> <span class="n">layer_name</span><span class="p">,</span> <span class="n">weight_score</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_score_t</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span><span id="WSNMaskBackbone-500"><a href="#WSNMaskBackbone-500"><span class="linenos">500</span></a>            <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;default&quot;</span><span class="p">:</span>
</span><span id="WSNMaskBackbone-501"><a href="#WSNMaskBackbone-501"><span class="linenos">501</span></a>                <span class="c1"># Kaiming Uniform Initialization for weight score</span>
</span><span id="WSNMaskBackbone-502"><a href="#WSNMaskBackbone-502"><span class="linenos">502</span></a>                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="n">weight_score</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
</span><span id="WSNMaskBackbone-503"><a href="#WSNMaskBackbone-503"><span class="linenos">503</span></a>
</span><span id="WSNMaskBackbone-504"><a href="#WSNMaskBackbone-504"><span class="linenos">504</span></a>                <span class="k">for</span> <span class="n">layer_name</span><span class="p">,</span> <span class="n">bias_score</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_score_t</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span><span id="WSNMaskBackbone-505"><a href="#WSNMaskBackbone-505"><span class="linenos">505</span></a>                    <span class="k">if</span> <span class="n">bias_score</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="WSNMaskBackbone-506"><a href="#WSNMaskBackbone-506"><span class="linenos">506</span></a>                        <span class="c1"># For bias, follow the standard bias initialization using fan_in</span>
</span><span id="WSNMaskBackbone-507"><a href="#WSNMaskBackbone-507"><span class="linenos">507</span></a>                        <span class="n">weight_score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_score_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="WSNMaskBackbone-508"><a href="#WSNMaskBackbone-508"><span class="linenos">508</span></a>                        <span class="n">fan_in</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">_calculate_fan_in_and_fan_out</span><span class="p">(</span>
</span><span id="WSNMaskBackbone-509"><a href="#WSNMaskBackbone-509"><span class="linenos">509</span></a>                            <span class="n">weight_score</span><span class="o">.</span><span class="n">weight</span>
</span><span id="WSNMaskBackbone-510"><a href="#WSNMaskBackbone-510"><span class="linenos">510</span></a>                        <span class="p">)</span>
</span><span id="WSNMaskBackbone-511"><a href="#WSNMaskBackbone-511"><span class="linenos">511</span></a>                        <span class="n">bound</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fan_in</span><span class="p">)</span> <span class="k">if</span> <span class="n">fan_in</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>
</span><span id="WSNMaskBackbone-512"><a href="#WSNMaskBackbone-512"><span class="linenos">512</span></a>                        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="n">bias_score</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="o">-</span><span class="n">bound</span><span class="p">,</span> <span class="n">bound</span><span class="p">)</span>
</span><span id="WSNMaskBackbone-513"><a href="#WSNMaskBackbone-513"><span class="linenos">513</span></a>            <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;N01&quot;</span><span class="p">:</span>
</span><span id="WSNMaskBackbone-514"><a href="#WSNMaskBackbone-514"><span class="linenos">514</span></a>                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">weight_score</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="WSNMaskBackbone-515"><a href="#WSNMaskBackbone-515"><span class="linenos">515</span></a>                <span class="k">for</span> <span class="n">layer_name</span><span class="p">,</span> <span class="n">bias_score</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_score_t</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span><span id="WSNMaskBackbone-516"><a href="#WSNMaskBackbone-516"><span class="linenos">516</span></a>                    <span class="k">if</span> <span class="n">bias_score</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="WSNMaskBackbone-517"><a href="#WSNMaskBackbone-517"><span class="linenos">517</span></a>                        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">bias_score</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="WSNMaskBackbone-518"><a href="#WSNMaskBackbone-518"><span class="linenos">518</span></a>            <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;U01&quot;</span><span class="p">:</span>
</span><span id="WSNMaskBackbone-519"><a href="#WSNMaskBackbone-519"><span class="linenos">519</span></a>                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="n">weight_score</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="WSNMaskBackbone-520"><a href="#WSNMaskBackbone-520"><span class="linenos">520</span></a>                <span class="k">for</span> <span class="n">layer_name</span><span class="p">,</span> <span class="n">bias_score</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_score_t</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span><span id="WSNMaskBackbone-521"><a href="#WSNMaskBackbone-521"><span class="linenos">521</span></a>                    <span class="k">if</span> <span class="n">bias_score</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="WSNMaskBackbone-522"><a href="#WSNMaskBackbone-522"><span class="linenos">522</span></a>                        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="n">bias_score</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="WSNMaskBackbone-523"><a href="#WSNMaskBackbone-523"><span class="linenos">523</span></a>
</span><span id="WSNMaskBackbone-524"><a href="#WSNMaskBackbone-524"><span class="linenos">524</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_mask</span><span class="p">(</span>
</span><span id="WSNMaskBackbone-525"><a href="#WSNMaskBackbone-525"><span class="linenos">525</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="WSNMaskBackbone-526"><a href="#WSNMaskBackbone-526"><span class="linenos">526</span></a>        <span class="n">stage</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="WSNMaskBackbone-527"><a href="#WSNMaskBackbone-527"><span class="linenos">527</span></a>        <span class="n">mask_percentage</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="WSNMaskBackbone-528"><a href="#WSNMaskBackbone-528"><span class="linenos">528</span></a>        <span class="n">test_mask</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="WSNMaskBackbone-529"><a href="#WSNMaskBackbone-529"><span class="linenos">529</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
</span><span id="WSNMaskBackbone-530"><a href="#WSNMaskBackbone-530"><span class="linenos">530</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the binary parameter mask used in the `forward()` method for different stages.</span>
</span><span id="WSNMaskBackbone-531"><a href="#WSNMaskBackbone-531"><span class="linenos">531</span></a>
</span><span id="WSNMaskBackbone-532"><a href="#WSNMaskBackbone-532"><span class="linenos">532</span></a><span class="sd">        **Args:**</span>
</span><span id="WSNMaskBackbone-533"><a href="#WSNMaskBackbone-533"><span class="linenos">533</span></a><span class="sd">        - **stage** (`str`): The stage when applying the conversion; one of:</span>
</span><span id="WSNMaskBackbone-534"><a href="#WSNMaskBackbone-534"><span class="linenos">534</span></a><span class="sd">            1. &#39;train&#39;: training stage. Get the mask from the parameter score of the current task through the gate function that masks the top $c\%$ largest scored parameters. See 3.1 &quot;Winning Subnetworks&quot; in the WSN paper.</span>
</span><span id="WSNMaskBackbone-535"><a href="#WSNMaskBackbone-535"><span class="linenos">535</span></a><span class="sd">            2. &#39;validation&#39;: validation stage. Same as &#39;train&#39;. (Note that in this stage, the binary mask hasn&#39;t been stored yet, as training is not over.)</span>
</span><span id="WSNMaskBackbone-536"><a href="#WSNMaskBackbone-536"><span class="linenos">536</span></a><span class="sd">            3. &#39;test&#39;: testing stage. Apply the test mask directly from the argument `test_mask`.</span>
</span><span id="WSNMaskBackbone-537"><a href="#WSNMaskBackbone-537"><span class="linenos">537</span></a><span class="sd">        - **test_mask** (`tuple[dict[str, Tensor], dict[str, Tensor]]` | `None`): The binary weight and bias masks used for testing. Applies only to the testing stage. For other stages, it is `None`.</span>
</span><span id="WSNMaskBackbone-538"><a href="#WSNMaskBackbone-538"><span class="linenos">538</span></a>
</span><span id="WSNMaskBackbone-539"><a href="#WSNMaskBackbone-539"><span class="linenos">539</span></a><span class="sd">        **Returns:**</span>
</span><span id="WSNMaskBackbone-540"><a href="#WSNMaskBackbone-540"><span class="linenos">540</span></a><span class="sd">        - **weight_mask** (`dict[str, Tensor]`): The binary mask on weights. Key (`str`) is the layer name; value (`Tensor`) is the mask tensor. The mask tensor has the same size (output features, input features) as the weight.</span>
</span><span id="WSNMaskBackbone-541"><a href="#WSNMaskBackbone-541"><span class="linenos">541</span></a><span class="sd">        - **bias_mask** (`dict[str, Tensor]`): The binary mask on biases. Key (`str`) is the layer name; value (`Tensor`) is the mask tensor. The mask tensor has the same size (output features,) as the bias. If the layer doesn&#39;t have a bias, it is `None`.</span>
</span><span id="WSNMaskBackbone-542"><a href="#WSNMaskBackbone-542"><span class="linenos">542</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="WSNMaskBackbone-543"><a href="#WSNMaskBackbone-543"><span class="linenos">543</span></a>        <span class="n">weight_mask</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="WSNMaskBackbone-544"><a href="#WSNMaskBackbone-544"><span class="linenos">544</span></a>        <span class="n">bias_mask</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="WSNMaskBackbone-545"><a href="#WSNMaskBackbone-545"><span class="linenos">545</span></a>        <span class="k">if</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;train&quot;</span> <span class="ow">or</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;validation&quot;</span><span class="p">:</span>
</span><span id="WSNMaskBackbone-546"><a href="#WSNMaskBackbone-546"><span class="linenos">546</span></a>            <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="WSNMaskBackbone-547"><a href="#WSNMaskBackbone-547"><span class="linenos">547</span></a>                <span class="n">weight_mask</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gate_fn</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
</span><span id="WSNMaskBackbone-548"><a href="#WSNMaskBackbone-548"><span class="linenos">548</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">weight_score_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">mask_percentage</span>
</span><span id="WSNMaskBackbone-549"><a href="#WSNMaskBackbone-549"><span class="linenos">549</span></a>                <span class="p">)</span>
</span><span id="WSNMaskBackbone-550"><a href="#WSNMaskBackbone-550"><span class="linenos">550</span></a>                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_score_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="WSNMaskBackbone-551"><a href="#WSNMaskBackbone-551"><span class="linenos">551</span></a>                    <span class="n">bias_mask</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gate_fn</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
</span><span id="WSNMaskBackbone-552"><a href="#WSNMaskBackbone-552"><span class="linenos">552</span></a>                        <span class="bp">self</span><span class="o">.</span><span class="n">bias_score_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span>
</span><span id="WSNMaskBackbone-553"><a href="#WSNMaskBackbone-553"><span class="linenos">553</span></a>                            <span class="mi">0</span>
</span><span id="WSNMaskBackbone-554"><a href="#WSNMaskBackbone-554"><span class="linenos">554</span></a>                        <span class="p">),</span>  <span class="c1"># from (1, output_dim) to (output_dim, )</span>
</span><span id="WSNMaskBackbone-555"><a href="#WSNMaskBackbone-555"><span class="linenos">555</span></a>                        <span class="n">mask_percentage</span><span class="p">,</span>
</span><span id="WSNMaskBackbone-556"><a href="#WSNMaskBackbone-556"><span class="linenos">556</span></a>                    <span class="p">)</span>
</span><span id="WSNMaskBackbone-557"><a href="#WSNMaskBackbone-557"><span class="linenos">557</span></a>                <span class="k">else</span><span class="p">:</span>
</span><span id="WSNMaskBackbone-558"><a href="#WSNMaskBackbone-558"><span class="linenos">558</span></a>                    <span class="n">bias_mask</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="WSNMaskBackbone-559"><a href="#WSNMaskBackbone-559"><span class="linenos">559</span></a>        <span class="k">elif</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;test&quot;</span><span class="p">:</span>
</span><span id="WSNMaskBackbone-560"><a href="#WSNMaskBackbone-560"><span class="linenos">560</span></a>            <span class="n">weight_mask</span><span class="p">,</span> <span class="n">bias_mask</span> <span class="o">=</span> <span class="n">test_mask</span>
</span><span id="WSNMaskBackbone-561"><a href="#WSNMaskBackbone-561"><span class="linenos">561</span></a>
</span><span id="WSNMaskBackbone-562"><a href="#WSNMaskBackbone-562"><span class="linenos">562</span></a>        <span class="k">return</span> <span class="n">weight_mask</span><span class="p">,</span> <span class="n">bias_mask</span>
</span><span id="WSNMaskBackbone-563"><a href="#WSNMaskBackbone-563"><span class="linenos">563</span></a>
</span><span id="WSNMaskBackbone-564"><a href="#WSNMaskBackbone-564"><span class="linenos">564</span></a>    <span class="nd">@override</span>
</span><span id="WSNMaskBackbone-565"><a href="#WSNMaskBackbone-565"><span class="linenos">565</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="WSNMaskBackbone-566"><a href="#WSNMaskBackbone-566"><span class="linenos">566</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="WSNMaskBackbone-567"><a href="#WSNMaskBackbone-567"><span class="linenos">567</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="WSNMaskBackbone-568"><a href="#WSNMaskBackbone-568"><span class="linenos">568</span></a>        <span class="n">stage</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="WSNMaskBackbone-569"><a href="#WSNMaskBackbone-569"><span class="linenos">569</span></a>        <span class="n">mask_percentage</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="WSNMaskBackbone-570"><a href="#WSNMaskBackbone-570"><span class="linenos">570</span></a>        <span class="n">test_mask</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="WSNMaskBackbone-571"><a href="#WSNMaskBackbone-571"><span class="linenos">571</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
</span><span id="WSNMaskBackbone-572"><a href="#WSNMaskBackbone-572"><span class="linenos">572</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The forward pass for data from task `task_id`. Task-specific mask for `task_id` are applied to the units in each layer.</span>
</span><span id="WSNMaskBackbone-573"><a href="#WSNMaskBackbone-573"><span class="linenos">573</span></a>
</span><span id="WSNMaskBackbone-574"><a href="#WSNMaskBackbone-574"><span class="linenos">574</span></a><span class="sd">        **Args:**</span>
</span><span id="WSNMaskBackbone-575"><a href="#WSNMaskBackbone-575"><span class="linenos">575</span></a><span class="sd">        - **input** (`Tensor`): The input tensor from data.</span>
</span><span id="WSNMaskBackbone-576"><a href="#WSNMaskBackbone-576"><span class="linenos">576</span></a><span class="sd">        - **stage** (`str`): The stage of the forward pass; one of:</span>
</span><span id="WSNMaskBackbone-577"><a href="#WSNMaskBackbone-577"><span class="linenos">577</span></a><span class="sd">            1. &#39;train&#39;: training stage.</span>
</span><span id="WSNMaskBackbone-578"><a href="#WSNMaskBackbone-578"><span class="linenos">578</span></a><span class="sd">            2. &#39;validation&#39;: validation stage.</span>
</span><span id="WSNMaskBackbone-579"><a href="#WSNMaskBackbone-579"><span class="linenos">579</span></a><span class="sd">            3. &#39;test&#39;: testing stage.</span>
</span><span id="WSNMaskBackbone-580"><a href="#WSNMaskBackbone-580"><span class="linenos">580</span></a><span class="sd">        - **mask_percentage** (`float`): The percentage of parameters to be masked. The value should be between 0 and 1.</span>
</span><span id="WSNMaskBackbone-581"><a href="#WSNMaskBackbone-581"><span class="linenos">581</span></a><span class="sd">        - **test_mask** (`tuple[dict[str, Tensor], dict[str, Tensor]]` | `None`): The binary weight and bias mask used for test. Applies only to the testing stage. For other stages, it is `None`.</span>
</span><span id="WSNMaskBackbone-582"><a href="#WSNMaskBackbone-582"><span class="linenos">582</span></a>
</span><span id="WSNMaskBackbone-583"><a href="#WSNMaskBackbone-583"><span class="linenos">583</span></a><span class="sd">        **Returns:**</span>
</span><span id="WSNMaskBackbone-584"><a href="#WSNMaskBackbone-584"><span class="linenos">584</span></a><span class="sd">        - **output_feature** (`Tensor`): The output feature tensor to be passed into heads. This is the main target of backpropagation.</span>
</span><span id="WSNMaskBackbone-585"><a href="#WSNMaskBackbone-585"><span class="linenos">585</span></a><span class="sd">        - **weight_mask** (`dict[str, Tensor]`): The weight mask for the current task. Key (`str`) is layer name; value (`Tensor`) is the mask tensor. The mask tensor has same (output features, input features) as the weight.</span>
</span><span id="WSNMaskBackbone-586"><a href="#WSNMaskBackbone-586"><span class="linenos">586</span></a><span class="sd">        - **bias_mask** (`dict[str, Tensor]`): The bias mask for the current task. Key (`str`) is layer name; value (`Tensor`) is the mask tensor. The mask tensor has same (output features, ) as the bias. If the layer doesn&#39;t have a bias, it is `None`.</span>
</span><span id="WSNMaskBackbone-587"><a href="#WSNMaskBackbone-587"><span class="linenos">587</span></a><span class="sd">        - **activations** (`dict[str, Tensor]`): The hidden features (after activation) in each weighted layer. Key (`str`) is the weighted layer name; value (`Tensor`) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes.</span>
</span><span id="WSNMaskBackbone-588"><a href="#WSNMaskBackbone-588"><span class="linenos">588</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span></pre></div>


            <div class="docstring"><p>The backbone network for the WSN algorithm with learnable parameter masks.</p>

<p><a href="https://proceedings.mlr.press/v162/kang22b/kang22b.pdf">WSN (Winning Subnetworks, 2022)</a> is an architecture-based continual learning algorithm. It trains learnable parameter-wise scores and selects the most scored $c\%$ of the network parameters to be used for each task.</p>
</div>


                            <div id="WSNMaskBackbone.__init__" class="classattr">
                                        <input id="WSNMaskBackbone.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">WSNMaskBackbone</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span>, </span><span class="param"><span class="o">**</span><span class="n">kwargs</span></span>)</span>

                <label class="view-source-button" for="WSNMaskBackbone.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#WSNMaskBackbone.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="WSNMaskBackbone.__init__-453"><a href="#WSNMaskBackbone.__init__-453"><span class="linenos">453</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="WSNMaskBackbone.__init__-454"><a href="#WSNMaskBackbone.__init__-454"><span class="linenos">454</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialize the WSN mask backbone network with task embeddings and masks.</span>
</span><span id="WSNMaskBackbone.__init__-455"><a href="#WSNMaskBackbone.__init__-455"><span class="linenos">455</span></a>
</span><span id="WSNMaskBackbone.__init__-456"><a href="#WSNMaskBackbone.__init__-456"><span class="linenos">456</span></a><span class="sd">        **Args:**</span>
</span><span id="WSNMaskBackbone.__init__-457"><a href="#WSNMaskBackbone.__init__-457"><span class="linenos">457</span></a><span class="sd">        - **output_dim** (`int`): The output dimension that connects to CL output heads. The `input_dim` of output heads is expected to be the same as this `output_dim`. In some cases, this class is used as a block in the backbone network that doesn&#39;t have an output dimension. In this case, it can be `None`.</span>
</span><span id="WSNMaskBackbone.__init__-458"><a href="#WSNMaskBackbone.__init__-458"><span class="linenos">458</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="WSNMaskBackbone.__init__-459"><a href="#WSNMaskBackbone.__init__-459"><span class="linenos">459</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span id="WSNMaskBackbone.__init__-460"><a href="#WSNMaskBackbone.__init__-460"><span class="linenos">460</span></a>
</span><span id="WSNMaskBackbone.__init__-461"><a href="#WSNMaskBackbone.__init__-461"><span class="linenos">461</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">gate_fn</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span> <span class="o">=</span> <span class="n">PercentileLayerParameterMaskingByScore</span>
</span><span id="WSNMaskBackbone.__init__-462"><a href="#WSNMaskBackbone.__init__-462"><span class="linenos">462</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The gate function turning the real-value parameter score into binary parameter masks. It is a custom autograd function that applies percentile parameter masking by score.&quot;&quot;&quot;</span>
</span><span id="WSNMaskBackbone.__init__-463"><a href="#WSNMaskBackbone.__init__-463"><span class="linenos">463</span></a>
</span><span id="WSNMaskBackbone.__init__-464"><a href="#WSNMaskBackbone.__init__-464"><span class="linenos">464</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">weight_score_t</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">()</span>
</span><span id="WSNMaskBackbone.__init__-465"><a href="#WSNMaskBackbone.__init__-465"><span class="linenos">465</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the weight score for the current task. Keys are the layer names and values are the task embedding `nn.Embedding` for the layer. Each task embedding has the same size (output features, input features) as the weight.</span>
</span><span id="WSNMaskBackbone.__init__-466"><a href="#WSNMaskBackbone.__init__-466"><span class="linenos">466</span></a><span class="sd">        </span>
</span><span id="WSNMaskBackbone.__init__-467"><a href="#WSNMaskBackbone.__init__-467"><span class="linenos">467</span></a><span class="sd">        We use `ModuleDict` rather than `dict` to ensure `LightningModule` properly registers these model parameters for purposes such as automatic device transfer and model summaries.</span>
</span><span id="WSNMaskBackbone.__init__-468"><a href="#WSNMaskBackbone.__init__-468"><span class="linenos">468</span></a><span class="sd">        </span>
</span><span id="WSNMaskBackbone.__init__-469"><a href="#WSNMaskBackbone.__init__-469"><span class="linenos">469</span></a><span class="sd">        We use `nn.Embedding` rather than `nn.Parameter` to store the task embedding for each layer, which is a type of `nn.Module` and can be accepted by `nn.ModuleDict`. (`nn.Parameter` cannot be accepted by `nn.ModuleDict`.)</span>
</span><span id="WSNMaskBackbone.__init__-470"><a href="#WSNMaskBackbone.__init__-470"><span class="linenos">470</span></a><span class="sd">        </span>
</span><span id="WSNMaskBackbone.__init__-471"><a href="#WSNMaskBackbone.__init__-471"><span class="linenos">471</span></a><span class="sd">        **This must be defined to cover each weighted layer (as listed in `self.weighted_layer_names`) in the backbone network.** Otherwise, the uncovered parts will keep updating for all tasks and become a source of catastrophic forgetting.</span>
</span><span id="WSNMaskBackbone.__init__-472"><a href="#WSNMaskBackbone.__init__-472"><span class="linenos">472</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="WSNMaskBackbone.__init__-473"><a href="#WSNMaskBackbone.__init__-473"><span class="linenos">473</span></a>
</span><span id="WSNMaskBackbone.__init__-474"><a href="#WSNMaskBackbone.__init__-474"><span class="linenos">474</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">bias_score_t</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">()</span>
</span><span id="WSNMaskBackbone.__init__-475"><a href="#WSNMaskBackbone.__init__-475"><span class="linenos">475</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the bias score for the current task. Keys are the layer names and values are the task embedding `nn.Embedding` for the layer. Each task embedding has the same size (1, output features) as the bias. If the layer doesn&#39;t have a bias, it is `None`.</span>
</span><span id="WSNMaskBackbone.__init__-476"><a href="#WSNMaskBackbone.__init__-476"><span class="linenos">476</span></a><span class="sd">        </span>
</span><span id="WSNMaskBackbone.__init__-477"><a href="#WSNMaskBackbone.__init__-477"><span class="linenos">477</span></a><span class="sd">        We use `ModuleDict` rather than `dict` to ensure `LightningModule` properly registers these model parameters for purposes such as automatic device transfer and model summaries.</span>
</span><span id="WSNMaskBackbone.__init__-478"><a href="#WSNMaskBackbone.__init__-478"><span class="linenos">478</span></a><span class="sd">        </span>
</span><span id="WSNMaskBackbone.__init__-479"><a href="#WSNMaskBackbone.__init__-479"><span class="linenos">479</span></a><span class="sd">        We use `nn.Embedding` rather than `nn.Parameter` to store the task embedding for each layer, which is a type of `nn.Module` and can be accepted by `nn.ModuleDict`. (`nn.Parameter` cannot be accepted by `nn.ModuleDict`.)</span>
</span><span id="WSNMaskBackbone.__init__-480"><a href="#WSNMaskBackbone.__init__-480"><span class="linenos">480</span></a><span class="sd">        </span>
</span><span id="WSNMaskBackbone.__init__-481"><a href="#WSNMaskBackbone.__init__-481"><span class="linenos">481</span></a><span class="sd">        **This must be defined to cover each weighted layer (as listed in `self.weighted_layer_names`) in the backbone network.** Otherwise, the uncovered parts will keep updating for all tasks and become a source of catastrophic forgetting.</span>
</span><span id="WSNMaskBackbone.__init__-482"><a href="#WSNMaskBackbone.__init__-482"><span class="linenos">482</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="WSNMaskBackbone.__init__-483"><a href="#WSNMaskBackbone.__init__-483"><span class="linenos">483</span></a>
</span><span id="WSNMaskBackbone.__init__-484"><a href="#WSNMaskBackbone.__init__-484"><span class="linenos">484</span></a>        <span class="n">WSNMaskBackbone</span><span class="o">.</span><span class="n">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Initialize the WSN mask backbone network with task embeddings and masks.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>output_dim</strong> (<code>int</code>): The output dimension that connects to CL output heads. The <code>input_dim</code> of output heads is expected to be the same as this <code><a href="#WSNMaskBackbone.output_dim">output_dim</a></code>. In some cases, this class is used as a block in the backbone network that doesn't have an output dimension. In this case, it can be <code>None</code>.</li>
</ul>
</div>


                            </div>
                            <div id="WSNMaskBackbone.gate_fn" class="classattr">
                                <div class="attr variable">
            <span class="name">gate_fn</span><span class="annotation">: torch.autograd.function.Function</span>

        
    </div>
    <a class="headerlink" href="#WSNMaskBackbone.gate_fn"></a>
    
            <div class="docstring"><p>The gate function turning the real-value parameter score into binary parameter masks. It is a custom autograd function that applies percentile parameter masking by score.</p>
</div>


                            </div>
                            <div id="WSNMaskBackbone.weight_score_t" class="classattr">
                                <div class="attr variable">
            <span class="name">weight_score_t</span><span class="annotation">: torch.nn.modules.container.ModuleDict</span>

        
    </div>
    <a class="headerlink" href="#WSNMaskBackbone.weight_score_t"></a>
    
            <div class="docstring"><p>Store the weight score for the current task. Keys are the layer names and values are the task embedding <code>nn.Embedding</code> for the layer. Each task embedding has the same size (output features, input features) as the weight.</p>

<p>We use <code>ModuleDict</code> rather than <code>dict</code> to ensure <code>LightningModule</code> properly registers these model parameters for purposes such as automatic device transfer and model summaries.</p>

<p>We use <code>nn.Embedding</code> rather than <code>nn.Parameter</code> to store the task embedding for each layer, which is a type of <code>nn.Module</code> and can be accepted by <code>nn.ModuleDict</code>. (<code>nn.Parameter</code> cannot be accepted by <code>nn.ModuleDict</code>.)</p>

<p><strong>This must be defined to cover each weighted layer (as listed in <code>self.weighted_layer_names</code>) in the backbone network.</strong> Otherwise, the uncovered parts will keep updating for all tasks and become a source of catastrophic forgetting.</p>
</div>


                            </div>
                            <div id="WSNMaskBackbone.bias_score_t" class="classattr">
                                <div class="attr variable">
            <span class="name">bias_score_t</span><span class="annotation">: torch.nn.modules.container.ModuleDict</span>

        
    </div>
    <a class="headerlink" href="#WSNMaskBackbone.bias_score_t"></a>
    
            <div class="docstring"><p>Store the bias score for the current task. Keys are the layer names and values are the task embedding <code>nn.Embedding</code> for the layer. Each task embedding has the same size (1, output features) as the bias. If the layer doesn't have a bias, it is <code>None</code>.</p>

<p>We use <code>ModuleDict</code> rather than <code>dict</code> to ensure <code>LightningModule</code> properly registers these model parameters for purposes such as automatic device transfer and model summaries.</p>

<p>We use <code>nn.Embedding</code> rather than <code>nn.Parameter</code> to store the task embedding for each layer, which is a type of <code>nn.Module</code> and can be accepted by <code>nn.ModuleDict</code>. (<code>nn.Parameter</code> cannot be accepted by <code>nn.ModuleDict</code>.)</p>

<p><strong>This must be defined to cover each weighted layer (as listed in <code>self.weighted_layer_names</code>) in the backbone network.</strong> Otherwise, the uncovered parts will keep updating for all tasks and become a source of catastrophic forgetting.</p>
</div>


                            </div>
                            <div id="WSNMaskBackbone.sanity_check" class="classattr">
                                        <input id="WSNMaskBackbone.sanity_check-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">sanity_check</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span></span><span class="return-annotation">) -> <span class="kc">None</span>:</span></span>

                <label class="view-source-button" for="WSNMaskBackbone.sanity_check-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#WSNMaskBackbone.sanity_check"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="WSNMaskBackbone.sanity_check-486"><a href="#WSNMaskBackbone.sanity_check-486"><span class="linenos">486</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="WSNMaskBackbone.sanity_check-487"><a href="#WSNMaskBackbone.sanity_check-487"><span class="linenos">487</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Check the sanity of the arguments.&quot;&quot;&quot;</span>
</span></pre></div>


            <div class="docstring"><p>Check the sanity of the arguments.</p>
</div>


                            </div>
                            <div id="WSNMaskBackbone.initialize_parameter_score" class="classattr">
                                        <input id="WSNMaskBackbone.initialize_parameter_score-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">initialize_parameter_score</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">mode</span><span class="p">:</span> <span class="nb">str</span></span><span class="return-annotation">) -> <span class="kc">None</span>:</span></span>

                <label class="view-source-button" for="WSNMaskBackbone.initialize_parameter_score-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#WSNMaskBackbone.initialize_parameter_score"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="WSNMaskBackbone.initialize_parameter_score-489"><a href="#WSNMaskBackbone.initialize_parameter_score-489"><span class="linenos">489</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">initialize_parameter_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="WSNMaskBackbone.initialize_parameter_score-490"><a href="#WSNMaskBackbone.initialize_parameter_score-490"><span class="linenos">490</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialize the parameter score for the current task.</span>
</span><span id="WSNMaskBackbone.initialize_parameter_score-491"><a href="#WSNMaskBackbone.initialize_parameter_score-491"><span class="linenos">491</span></a>
</span><span id="WSNMaskBackbone.initialize_parameter_score-492"><a href="#WSNMaskBackbone.initialize_parameter_score-492"><span class="linenos">492</span></a><span class="sd">        **Args:**</span>
</span><span id="WSNMaskBackbone.initialize_parameter_score-493"><a href="#WSNMaskBackbone.initialize_parameter_score-493"><span class="linenos">493</span></a><span class="sd">        - **mode** (`str`): The initialization mode for parameter scores; one of:</span>
</span><span id="WSNMaskBackbone.initialize_parameter_score-494"><a href="#WSNMaskBackbone.initialize_parameter_score-494"><span class="linenos">494</span></a><span class="sd">            1. &#39;default&#39;: the default initialization mode in the original WSN code.</span>
</span><span id="WSNMaskBackbone.initialize_parameter_score-495"><a href="#WSNMaskBackbone.initialize_parameter_score-495"><span class="linenos">495</span></a><span class="sd">            2. &#39;N01&#39;: standard normal distribution $N(0, 1)$.</span>
</span><span id="WSNMaskBackbone.initialize_parameter_score-496"><a href="#WSNMaskBackbone.initialize_parameter_score-496"><span class="linenos">496</span></a><span class="sd">            3. &#39;U01&#39;: uniform distribution $U(0, 1)$.</span>
</span><span id="WSNMaskBackbone.initialize_parameter_score-497"><a href="#WSNMaskBackbone.initialize_parameter_score-497"><span class="linenos">497</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="WSNMaskBackbone.initialize_parameter_score-498"><a href="#WSNMaskBackbone.initialize_parameter_score-498"><span class="linenos">498</span></a>
</span><span id="WSNMaskBackbone.initialize_parameter_score-499"><a href="#WSNMaskBackbone.initialize_parameter_score-499"><span class="linenos">499</span></a>        <span class="k">for</span> <span class="n">layer_name</span><span class="p">,</span> <span class="n">weight_score</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_score_t</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span><span id="WSNMaskBackbone.initialize_parameter_score-500"><a href="#WSNMaskBackbone.initialize_parameter_score-500"><span class="linenos">500</span></a>            <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;default&quot;</span><span class="p">:</span>
</span><span id="WSNMaskBackbone.initialize_parameter_score-501"><a href="#WSNMaskBackbone.initialize_parameter_score-501"><span class="linenos">501</span></a>                <span class="c1"># Kaiming Uniform Initialization for weight score</span>
</span><span id="WSNMaskBackbone.initialize_parameter_score-502"><a href="#WSNMaskBackbone.initialize_parameter_score-502"><span class="linenos">502</span></a>                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="n">weight_score</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
</span><span id="WSNMaskBackbone.initialize_parameter_score-503"><a href="#WSNMaskBackbone.initialize_parameter_score-503"><span class="linenos">503</span></a>
</span><span id="WSNMaskBackbone.initialize_parameter_score-504"><a href="#WSNMaskBackbone.initialize_parameter_score-504"><span class="linenos">504</span></a>                <span class="k">for</span> <span class="n">layer_name</span><span class="p">,</span> <span class="n">bias_score</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_score_t</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span><span id="WSNMaskBackbone.initialize_parameter_score-505"><a href="#WSNMaskBackbone.initialize_parameter_score-505"><span class="linenos">505</span></a>                    <span class="k">if</span> <span class="n">bias_score</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="WSNMaskBackbone.initialize_parameter_score-506"><a href="#WSNMaskBackbone.initialize_parameter_score-506"><span class="linenos">506</span></a>                        <span class="c1"># For bias, follow the standard bias initialization using fan_in</span>
</span><span id="WSNMaskBackbone.initialize_parameter_score-507"><a href="#WSNMaskBackbone.initialize_parameter_score-507"><span class="linenos">507</span></a>                        <span class="n">weight_score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_score_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
</span><span id="WSNMaskBackbone.initialize_parameter_score-508"><a href="#WSNMaskBackbone.initialize_parameter_score-508"><span class="linenos">508</span></a>                        <span class="n">fan_in</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">_calculate_fan_in_and_fan_out</span><span class="p">(</span>
</span><span id="WSNMaskBackbone.initialize_parameter_score-509"><a href="#WSNMaskBackbone.initialize_parameter_score-509"><span class="linenos">509</span></a>                            <span class="n">weight_score</span><span class="o">.</span><span class="n">weight</span>
</span><span id="WSNMaskBackbone.initialize_parameter_score-510"><a href="#WSNMaskBackbone.initialize_parameter_score-510"><span class="linenos">510</span></a>                        <span class="p">)</span>
</span><span id="WSNMaskBackbone.initialize_parameter_score-511"><a href="#WSNMaskBackbone.initialize_parameter_score-511"><span class="linenos">511</span></a>                        <span class="n">bound</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fan_in</span><span class="p">)</span> <span class="k">if</span> <span class="n">fan_in</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>
</span><span id="WSNMaskBackbone.initialize_parameter_score-512"><a href="#WSNMaskBackbone.initialize_parameter_score-512"><span class="linenos">512</span></a>                        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="n">bias_score</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="o">-</span><span class="n">bound</span><span class="p">,</span> <span class="n">bound</span><span class="p">)</span>
</span><span id="WSNMaskBackbone.initialize_parameter_score-513"><a href="#WSNMaskBackbone.initialize_parameter_score-513"><span class="linenos">513</span></a>            <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;N01&quot;</span><span class="p">:</span>
</span><span id="WSNMaskBackbone.initialize_parameter_score-514"><a href="#WSNMaskBackbone.initialize_parameter_score-514"><span class="linenos">514</span></a>                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">weight_score</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="WSNMaskBackbone.initialize_parameter_score-515"><a href="#WSNMaskBackbone.initialize_parameter_score-515"><span class="linenos">515</span></a>                <span class="k">for</span> <span class="n">layer_name</span><span class="p">,</span> <span class="n">bias_score</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_score_t</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span><span id="WSNMaskBackbone.initialize_parameter_score-516"><a href="#WSNMaskBackbone.initialize_parameter_score-516"><span class="linenos">516</span></a>                    <span class="k">if</span> <span class="n">bias_score</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="WSNMaskBackbone.initialize_parameter_score-517"><a href="#WSNMaskBackbone.initialize_parameter_score-517"><span class="linenos">517</span></a>                        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">bias_score</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="WSNMaskBackbone.initialize_parameter_score-518"><a href="#WSNMaskBackbone.initialize_parameter_score-518"><span class="linenos">518</span></a>            <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;U01&quot;</span><span class="p">:</span>
</span><span id="WSNMaskBackbone.initialize_parameter_score-519"><a href="#WSNMaskBackbone.initialize_parameter_score-519"><span class="linenos">519</span></a>                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="n">weight_score</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="WSNMaskBackbone.initialize_parameter_score-520"><a href="#WSNMaskBackbone.initialize_parameter_score-520"><span class="linenos">520</span></a>                <span class="k">for</span> <span class="n">layer_name</span><span class="p">,</span> <span class="n">bias_score</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_score_t</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span><span id="WSNMaskBackbone.initialize_parameter_score-521"><a href="#WSNMaskBackbone.initialize_parameter_score-521"><span class="linenos">521</span></a>                    <span class="k">if</span> <span class="n">bias_score</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="WSNMaskBackbone.initialize_parameter_score-522"><a href="#WSNMaskBackbone.initialize_parameter_score-522"><span class="linenos">522</span></a>                        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="n">bias_score</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Initialize the parameter score for the current task.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>mode</strong> (<code>str</code>): The initialization mode for parameter scores; one of:
<ol>
<li>'default': the default initialization mode in the original WSN code.</li>
<li>'N01': standard normal distribution $N(0, 1)$.</li>
<li>'U01': uniform distribution $U(0, 1)$.</li>
</ol></li>
</ul>
</div>


                            </div>
                            <div id="WSNMaskBackbone.get_mask" class="classattr">
                                        <input id="WSNMaskBackbone.get_mask-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">get_mask</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">stage</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">mask_percentage</span><span class="p">:</span> <span class="nb">float</span>,</span><span class="param">	<span class="n">test_mask</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span></span><span class="return-annotation">) -> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>:</span></span>

                <label class="view-source-button" for="WSNMaskBackbone.get_mask-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#WSNMaskBackbone.get_mask"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="WSNMaskBackbone.get_mask-524"><a href="#WSNMaskBackbone.get_mask-524"><span class="linenos">524</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_mask</span><span class="p">(</span>
</span><span id="WSNMaskBackbone.get_mask-525"><a href="#WSNMaskBackbone.get_mask-525"><span class="linenos">525</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="WSNMaskBackbone.get_mask-526"><a href="#WSNMaskBackbone.get_mask-526"><span class="linenos">526</span></a>        <span class="n">stage</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="WSNMaskBackbone.get_mask-527"><a href="#WSNMaskBackbone.get_mask-527"><span class="linenos">527</span></a>        <span class="n">mask_percentage</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="WSNMaskBackbone.get_mask-528"><a href="#WSNMaskBackbone.get_mask-528"><span class="linenos">528</span></a>        <span class="n">test_mask</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="WSNMaskBackbone.get_mask-529"><a href="#WSNMaskBackbone.get_mask-529"><span class="linenos">529</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
</span><span id="WSNMaskBackbone.get_mask-530"><a href="#WSNMaskBackbone.get_mask-530"><span class="linenos">530</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the binary parameter mask used in the `forward()` method for different stages.</span>
</span><span id="WSNMaskBackbone.get_mask-531"><a href="#WSNMaskBackbone.get_mask-531"><span class="linenos">531</span></a>
</span><span id="WSNMaskBackbone.get_mask-532"><a href="#WSNMaskBackbone.get_mask-532"><span class="linenos">532</span></a><span class="sd">        **Args:**</span>
</span><span id="WSNMaskBackbone.get_mask-533"><a href="#WSNMaskBackbone.get_mask-533"><span class="linenos">533</span></a><span class="sd">        - **stage** (`str`): The stage when applying the conversion; one of:</span>
</span><span id="WSNMaskBackbone.get_mask-534"><a href="#WSNMaskBackbone.get_mask-534"><span class="linenos">534</span></a><span class="sd">            1. &#39;train&#39;: training stage. Get the mask from the parameter score of the current task through the gate function that masks the top $c\%$ largest scored parameters. See 3.1 &quot;Winning Subnetworks&quot; in the WSN paper.</span>
</span><span id="WSNMaskBackbone.get_mask-535"><a href="#WSNMaskBackbone.get_mask-535"><span class="linenos">535</span></a><span class="sd">            2. &#39;validation&#39;: validation stage. Same as &#39;train&#39;. (Note that in this stage, the binary mask hasn&#39;t been stored yet, as training is not over.)</span>
</span><span id="WSNMaskBackbone.get_mask-536"><a href="#WSNMaskBackbone.get_mask-536"><span class="linenos">536</span></a><span class="sd">            3. &#39;test&#39;: testing stage. Apply the test mask directly from the argument `test_mask`.</span>
</span><span id="WSNMaskBackbone.get_mask-537"><a href="#WSNMaskBackbone.get_mask-537"><span class="linenos">537</span></a><span class="sd">        - **test_mask** (`tuple[dict[str, Tensor], dict[str, Tensor]]` | `None`): The binary weight and bias masks used for testing. Applies only to the testing stage. For other stages, it is `None`.</span>
</span><span id="WSNMaskBackbone.get_mask-538"><a href="#WSNMaskBackbone.get_mask-538"><span class="linenos">538</span></a>
</span><span id="WSNMaskBackbone.get_mask-539"><a href="#WSNMaskBackbone.get_mask-539"><span class="linenos">539</span></a><span class="sd">        **Returns:**</span>
</span><span id="WSNMaskBackbone.get_mask-540"><a href="#WSNMaskBackbone.get_mask-540"><span class="linenos">540</span></a><span class="sd">        - **weight_mask** (`dict[str, Tensor]`): The binary mask on weights. Key (`str`) is the layer name; value (`Tensor`) is the mask tensor. The mask tensor has the same size (output features, input features) as the weight.</span>
</span><span id="WSNMaskBackbone.get_mask-541"><a href="#WSNMaskBackbone.get_mask-541"><span class="linenos">541</span></a><span class="sd">        - **bias_mask** (`dict[str, Tensor]`): The binary mask on biases. Key (`str`) is the layer name; value (`Tensor`) is the mask tensor. The mask tensor has the same size (output features,) as the bias. If the layer doesn&#39;t have a bias, it is `None`.</span>
</span><span id="WSNMaskBackbone.get_mask-542"><a href="#WSNMaskBackbone.get_mask-542"><span class="linenos">542</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="WSNMaskBackbone.get_mask-543"><a href="#WSNMaskBackbone.get_mask-543"><span class="linenos">543</span></a>        <span class="n">weight_mask</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="WSNMaskBackbone.get_mask-544"><a href="#WSNMaskBackbone.get_mask-544"><span class="linenos">544</span></a>        <span class="n">bias_mask</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="WSNMaskBackbone.get_mask-545"><a href="#WSNMaskBackbone.get_mask-545"><span class="linenos">545</span></a>        <span class="k">if</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;train&quot;</span> <span class="ow">or</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;validation&quot;</span><span class="p">:</span>
</span><span id="WSNMaskBackbone.get_mask-546"><a href="#WSNMaskBackbone.get_mask-546"><span class="linenos">546</span></a>            <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="WSNMaskBackbone.get_mask-547"><a href="#WSNMaskBackbone.get_mask-547"><span class="linenos">547</span></a>                <span class="n">weight_mask</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gate_fn</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
</span><span id="WSNMaskBackbone.get_mask-548"><a href="#WSNMaskBackbone.get_mask-548"><span class="linenos">548</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">weight_score_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">mask_percentage</span>
</span><span id="WSNMaskBackbone.get_mask-549"><a href="#WSNMaskBackbone.get_mask-549"><span class="linenos">549</span></a>                <span class="p">)</span>
</span><span id="WSNMaskBackbone.get_mask-550"><a href="#WSNMaskBackbone.get_mask-550"><span class="linenos">550</span></a>                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_score_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="WSNMaskBackbone.get_mask-551"><a href="#WSNMaskBackbone.get_mask-551"><span class="linenos">551</span></a>                    <span class="n">bias_mask</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gate_fn</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
</span><span id="WSNMaskBackbone.get_mask-552"><a href="#WSNMaskBackbone.get_mask-552"><span class="linenos">552</span></a>                        <span class="bp">self</span><span class="o">.</span><span class="n">bias_score_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span>
</span><span id="WSNMaskBackbone.get_mask-553"><a href="#WSNMaskBackbone.get_mask-553"><span class="linenos">553</span></a>                            <span class="mi">0</span>
</span><span id="WSNMaskBackbone.get_mask-554"><a href="#WSNMaskBackbone.get_mask-554"><span class="linenos">554</span></a>                        <span class="p">),</span>  <span class="c1"># from (1, output_dim) to (output_dim, )</span>
</span><span id="WSNMaskBackbone.get_mask-555"><a href="#WSNMaskBackbone.get_mask-555"><span class="linenos">555</span></a>                        <span class="n">mask_percentage</span><span class="p">,</span>
</span><span id="WSNMaskBackbone.get_mask-556"><a href="#WSNMaskBackbone.get_mask-556"><span class="linenos">556</span></a>                    <span class="p">)</span>
</span><span id="WSNMaskBackbone.get_mask-557"><a href="#WSNMaskBackbone.get_mask-557"><span class="linenos">557</span></a>                <span class="k">else</span><span class="p">:</span>
</span><span id="WSNMaskBackbone.get_mask-558"><a href="#WSNMaskBackbone.get_mask-558"><span class="linenos">558</span></a>                    <span class="n">bias_mask</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="WSNMaskBackbone.get_mask-559"><a href="#WSNMaskBackbone.get_mask-559"><span class="linenos">559</span></a>        <span class="k">elif</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;test&quot;</span><span class="p">:</span>
</span><span id="WSNMaskBackbone.get_mask-560"><a href="#WSNMaskBackbone.get_mask-560"><span class="linenos">560</span></a>            <span class="n">weight_mask</span><span class="p">,</span> <span class="n">bias_mask</span> <span class="o">=</span> <span class="n">test_mask</span>
</span><span id="WSNMaskBackbone.get_mask-561"><a href="#WSNMaskBackbone.get_mask-561"><span class="linenos">561</span></a>
</span><span id="WSNMaskBackbone.get_mask-562"><a href="#WSNMaskBackbone.get_mask-562"><span class="linenos">562</span></a>        <span class="k">return</span> <span class="n">weight_mask</span><span class="p">,</span> <span class="n">bias_mask</span>
</span></pre></div>


            <div class="docstring"><p>Get the binary parameter mask used in the <code><a href="#WSNMaskBackbone.forward">forward()</a></code> method for different stages.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>stage</strong> (<code>str</code>): The stage when applying the conversion; one of:
<ol>
<li>'train': training stage. Get the mask from the parameter score of the current task through the gate function that masks the top $c\%$ largest scored parameters. See 3.1 "Winning Subnetworks" in the WSN paper.</li>
<li>'validation': validation stage. Same as 'train'. (Note that in this stage, the binary mask hasn't been stored yet, as training is not over.)</li>
<li>'test': testing stage. Apply the test mask directly from the argument <code>test_mask</code>.</li>
</ol></li>
<li><strong>test_mask</strong> (<code>tuple[dict[str, Tensor], dict[str, Tensor]]</code> | <code>None</code>): The binary weight and bias masks used for testing. Applies only to the testing stage. For other stages, it is <code>None</code>.</li>
</ul>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>weight_mask</strong> (<code>dict[str, Tensor]</code>): The binary mask on weights. Key (<code>str</code>) is the layer name; value (<code>Tensor</code>) is the mask tensor. The mask tensor has the same size (output features, input features) as the weight.</li>
<li><strong>bias_mask</strong> (<code>dict[str, Tensor]</code>): The binary mask on biases. Key (<code>str</code>) is the layer name; value (<code>Tensor</code>) is the mask tensor. The mask tensor has the same size (output features,) as the bias. If the layer doesn't have a bias, it is <code>None</code>.</li>
</ul>
</div>


                            </div>
                            <div id="WSNMaskBackbone.forward" class="classattr">
                                        <input id="WSNMaskBackbone.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
                    <div class="decorator decorator-override">@override</div>

        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">stage</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">mask_percentage</span><span class="p">:</span> <span class="nb">float</span>,</span><span class="param">	<span class="n">test_mask</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span></span><span class="return-annotation">) -> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span>:</span></span>

                <label class="view-source-button" for="WSNMaskBackbone.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#WSNMaskBackbone.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="WSNMaskBackbone.forward-564"><a href="#WSNMaskBackbone.forward-564"><span class="linenos">564</span></a>    <span class="nd">@override</span>
</span><span id="WSNMaskBackbone.forward-565"><a href="#WSNMaskBackbone.forward-565"><span class="linenos">565</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="WSNMaskBackbone.forward-566"><a href="#WSNMaskBackbone.forward-566"><span class="linenos">566</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="WSNMaskBackbone.forward-567"><a href="#WSNMaskBackbone.forward-567"><span class="linenos">567</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="WSNMaskBackbone.forward-568"><a href="#WSNMaskBackbone.forward-568"><span class="linenos">568</span></a>        <span class="n">stage</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="WSNMaskBackbone.forward-569"><a href="#WSNMaskBackbone.forward-569"><span class="linenos">569</span></a>        <span class="n">mask_percentage</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="WSNMaskBackbone.forward-570"><a href="#WSNMaskBackbone.forward-570"><span class="linenos">570</span></a>        <span class="n">test_mask</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="WSNMaskBackbone.forward-571"><a href="#WSNMaskBackbone.forward-571"><span class="linenos">571</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
</span><span id="WSNMaskBackbone.forward-572"><a href="#WSNMaskBackbone.forward-572"><span class="linenos">572</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The forward pass for data from task `task_id`. Task-specific mask for `task_id` are applied to the units in each layer.</span>
</span><span id="WSNMaskBackbone.forward-573"><a href="#WSNMaskBackbone.forward-573"><span class="linenos">573</span></a>
</span><span id="WSNMaskBackbone.forward-574"><a href="#WSNMaskBackbone.forward-574"><span class="linenos">574</span></a><span class="sd">        **Args:**</span>
</span><span id="WSNMaskBackbone.forward-575"><a href="#WSNMaskBackbone.forward-575"><span class="linenos">575</span></a><span class="sd">        - **input** (`Tensor`): The input tensor from data.</span>
</span><span id="WSNMaskBackbone.forward-576"><a href="#WSNMaskBackbone.forward-576"><span class="linenos">576</span></a><span class="sd">        - **stage** (`str`): The stage of the forward pass; one of:</span>
</span><span id="WSNMaskBackbone.forward-577"><a href="#WSNMaskBackbone.forward-577"><span class="linenos">577</span></a><span class="sd">            1. &#39;train&#39;: training stage.</span>
</span><span id="WSNMaskBackbone.forward-578"><a href="#WSNMaskBackbone.forward-578"><span class="linenos">578</span></a><span class="sd">            2. &#39;validation&#39;: validation stage.</span>
</span><span id="WSNMaskBackbone.forward-579"><a href="#WSNMaskBackbone.forward-579"><span class="linenos">579</span></a><span class="sd">            3. &#39;test&#39;: testing stage.</span>
</span><span id="WSNMaskBackbone.forward-580"><a href="#WSNMaskBackbone.forward-580"><span class="linenos">580</span></a><span class="sd">        - **mask_percentage** (`float`): The percentage of parameters to be masked. The value should be between 0 and 1.</span>
</span><span id="WSNMaskBackbone.forward-581"><a href="#WSNMaskBackbone.forward-581"><span class="linenos">581</span></a><span class="sd">        - **test_mask** (`tuple[dict[str, Tensor], dict[str, Tensor]]` | `None`): The binary weight and bias mask used for test. Applies only to the testing stage. For other stages, it is `None`.</span>
</span><span id="WSNMaskBackbone.forward-582"><a href="#WSNMaskBackbone.forward-582"><span class="linenos">582</span></a>
</span><span id="WSNMaskBackbone.forward-583"><a href="#WSNMaskBackbone.forward-583"><span class="linenos">583</span></a><span class="sd">        **Returns:**</span>
</span><span id="WSNMaskBackbone.forward-584"><a href="#WSNMaskBackbone.forward-584"><span class="linenos">584</span></a><span class="sd">        - **output_feature** (`Tensor`): The output feature tensor to be passed into heads. This is the main target of backpropagation.</span>
</span><span id="WSNMaskBackbone.forward-585"><a href="#WSNMaskBackbone.forward-585"><span class="linenos">585</span></a><span class="sd">        - **weight_mask** (`dict[str, Tensor]`): The weight mask for the current task. Key (`str`) is layer name; value (`Tensor`) is the mask tensor. The mask tensor has same (output features, input features) as the weight.</span>
</span><span id="WSNMaskBackbone.forward-586"><a href="#WSNMaskBackbone.forward-586"><span class="linenos">586</span></a><span class="sd">        - **bias_mask** (`dict[str, Tensor]`): The bias mask for the current task. Key (`str`) is layer name; value (`Tensor`) is the mask tensor. The mask tensor has same (output features, ) as the bias. If the layer doesn&#39;t have a bias, it is `None`.</span>
</span><span id="WSNMaskBackbone.forward-587"><a href="#WSNMaskBackbone.forward-587"><span class="linenos">587</span></a><span class="sd">        - **activations** (`dict[str, Tensor]`): The hidden features (after activation) in each weighted layer. Key (`str`) is the weighted layer name; value (`Tensor`) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes.</span>
</span><span id="WSNMaskBackbone.forward-588"><a href="#WSNMaskBackbone.forward-588"><span class="linenos">588</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span></pre></div>


            <div class="docstring"><p>The forward pass for data from task <code><a href="#WSNMaskBackbone.task_id">task_id</a></code>. Task-specific mask for <code><a href="#WSNMaskBackbone.task_id">task_id</a></code> are applied to the units in each layer.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>input</strong> (<code>Tensor</code>): The input tensor from data.</li>
<li><strong>stage</strong> (<code>str</code>): The stage of the forward pass; one of:
<ol>
<li>'train': training stage.</li>
<li>'validation': validation stage.</li>
<li>'test': testing stage.</li>
</ol></li>
<li><strong>mask_percentage</strong> (<code><a href="#WSNMaskBackbone.float">float</a></code>): The percentage of parameters to be masked. The value should be between 0 and 1.</li>
<li><strong>test_mask</strong> (<code>tuple[dict[str, Tensor], dict[str, Tensor]]</code> | <code>None</code>): The binary weight and bias mask used for test. Applies only to the testing stage. For other stages, it is <code>None</code>.</li>
</ul>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>output_feature</strong> (<code>Tensor</code>): The output feature tensor to be passed into heads. This is the main target of backpropagation.</li>
<li><strong>weight_mask</strong> (<code>dict[str, Tensor]</code>): The weight mask for the current task. Key (<code>str</code>) is layer name; value (<code>Tensor</code>) is the mask tensor. The mask tensor has same (output features, input features) as the weight.</li>
<li><strong>bias_mask</strong> (<code>dict[str, Tensor]</code>): The bias mask for the current task. Key (<code>str</code>) is layer name; value (<code>Tensor</code>) is the mask tensor. The mask tensor has same (output features, ) as the bias. If the layer doesn't have a bias, it is <code>None</code>.</li>
<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): The hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name; value (<code>Tensor</code>) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes.</li>
</ul>
</div>


                            </div>
                </section>
                <section id="NISPAMaskBackbone">
                            <input id="NISPAMaskBackbone-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">NISPAMaskBackbone</span><wbr>(<span class="base"><a href="#CLBackbone">clarena.backbones.CLBackbone</a></span>):

                <label class="view-source-button" for="NISPAMaskBackbone-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#NISPAMaskBackbone"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="NISPAMaskBackbone-632"><a href="#NISPAMaskBackbone-632"><span class="linenos">632</span></a><span class="k">class</span><span class="w"> </span><span class="nc">NISPAMaskBackbone</span><span class="p">(</span><span class="n">CLBackbone</span><span class="p">):</span>
</span><span id="NISPAMaskBackbone-633"><a href="#NISPAMaskBackbone-633"><span class="linenos">633</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The backbone network for the NISPA algorithm with neuron masks.</span>
</span><span id="NISPAMaskBackbone-634"><a href="#NISPAMaskBackbone-634"><span class="linenos">634</span></a>
</span><span id="NISPAMaskBackbone-635"><a href="#NISPAMaskBackbone-635"><span class="linenos">635</span></a><span class="sd">    [NISPA (Neuro-Inspired Stability-Plasticity Adaptation)](https://proceedings.mlr.press/v162/gurbuz22a/gurbuz22a.pdf) is an architecture-based continual learning algorithm.</span>
</span><span id="NISPAMaskBackbone-636"><a href="#NISPAMaskBackbone-636"><span class="linenos">636</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="NISPAMaskBackbone-637"><a href="#NISPAMaskBackbone-637"><span class="linenos">637</span></a>
</span><span id="NISPAMaskBackbone-638"><a href="#NISPAMaskBackbone-638"><span class="linenos">638</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="NISPAMaskBackbone-639"><a href="#NISPAMaskBackbone-639"><span class="linenos">639</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialize the NISPA mask backbone network with masks.</span>
</span><span id="NISPAMaskBackbone-640"><a href="#NISPAMaskBackbone-640"><span class="linenos">640</span></a>
</span><span id="NISPAMaskBackbone-641"><a href="#NISPAMaskBackbone-641"><span class="linenos">641</span></a><span class="sd">        **Args:**</span>
</span><span id="NISPAMaskBackbone-642"><a href="#NISPAMaskBackbone-642"><span class="linenos">642</span></a><span class="sd">        - **output_dim** (`int`): The output dimension that connects to CL output heads. The `input_dim` of output heads is expected to be the same as this `output_dim`. In some cases, this class is used as a block in the backbone network that doesn&#39;t have an output dimension. In this case, it can be `None`.</span>
</span><span id="NISPAMaskBackbone-643"><a href="#NISPAMaskBackbone-643"><span class="linenos">643</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="NISPAMaskBackbone-644"><a href="#NISPAMaskBackbone-644"><span class="linenos">644</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span id="NISPAMaskBackbone-645"><a href="#NISPAMaskBackbone-645"><span class="linenos">645</span></a>
</span><span id="NISPAMaskBackbone-646"><a href="#NISPAMaskBackbone-646"><span class="linenos">646</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">weight_mask_t</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="NISPAMaskBackbone-647"><a href="#NISPAMaskBackbone-647"><span class="linenos">647</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the weight mask for each layer. Key (`str`) is layer name, value (`Tensor`) is the mask tensor. The mask tensor has the same size (output features, input features) as weight.&quot;&quot;&quot;</span>
</span><span id="NISPAMaskBackbone-648"><a href="#NISPAMaskBackbone-648"><span class="linenos">648</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">frozen_weight_mask_t</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="NISPAMaskBackbone-649"><a href="#NISPAMaskBackbone-649"><span class="linenos">649</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the frozen weight mask for each layer. Key (`str`) is layer name, value (`Tensor`) is the mask tensor. The mask tensor has the same size (output features, input features) as weight.&quot;&quot;&quot;</span>
</span><span id="NISPAMaskBackbone-650"><a href="#NISPAMaskBackbone-650"><span class="linenos">650</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">bias_mask_t</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="NISPAMaskBackbone-651"><a href="#NISPAMaskBackbone-651"><span class="linenos">651</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the bias mask for each layer. Key (`str`) is layer name, value (`Tensor`) is the mask tensor. The mask tensor has the same size (output features, ) as bias. If the layer doesn&#39;t have bias, it is `None`.&quot;&quot;&quot;</span>
</span><span id="NISPAMaskBackbone-652"><a href="#NISPAMaskBackbone-652"><span class="linenos">652</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">frozen_bias_mask_t</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="NISPAMaskBackbone-653"><a href="#NISPAMaskBackbone-653"><span class="linenos">653</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the frozen bias mask for each layer. Key (`str`) is layer name, value (`Tensor`) is the mask tensor. The mask tensor has the same size (output features, ) as bias. If the layer doesn&#39;t have bias, it is `None`.&quot;&quot;&quot;</span>
</span><span id="NISPAMaskBackbone-654"><a href="#NISPAMaskBackbone-654"><span class="linenos">654</span></a>
</span><span id="NISPAMaskBackbone-655"><a href="#NISPAMaskBackbone-655"><span class="linenos">655</span></a>        <span class="n">NISPAMaskBackbone</span><span class="o">.</span><span class="n">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</span><span id="NISPAMaskBackbone-656"><a href="#NISPAMaskBackbone-656"><span class="linenos">656</span></a>
</span><span id="NISPAMaskBackbone-657"><a href="#NISPAMaskBackbone-657"><span class="linenos">657</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="NISPAMaskBackbone-658"><a href="#NISPAMaskBackbone-658"><span class="linenos">658</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Check the sanity of the arguments.&quot;&quot;&quot;</span>
</span><span id="NISPAMaskBackbone-659"><a href="#NISPAMaskBackbone-659"><span class="linenos">659</span></a>        <span class="k">pass</span>
</span><span id="NISPAMaskBackbone-660"><a href="#NISPAMaskBackbone-660"><span class="linenos">660</span></a>
</span><span id="NISPAMaskBackbone-661"><a href="#NISPAMaskBackbone-661"><span class="linenos">661</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">initialize_parameter_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="NISPAMaskBackbone-662"><a href="#NISPAMaskBackbone-662"><span class="linenos">662</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialize the parameter masks as zeros.&quot;&quot;&quot;</span>
</span><span id="NISPAMaskBackbone-663"><a href="#NISPAMaskBackbone-663"><span class="linenos">663</span></a>        <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="NISPAMaskBackbone-664"><a href="#NISPAMaskBackbone-664"><span class="linenos">664</span></a>            <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>  <span class="c1"># get the layer by its name</span>
</span><span id="NISPAMaskBackbone-665"><a href="#NISPAMaskBackbone-665"><span class="linenos">665</span></a>
</span><span id="NISPAMaskBackbone-666"><a href="#NISPAMaskBackbone-666"><span class="linenos">666</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">weight_mask_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
</span><span id="NISPAMaskBackbone-667"><a href="#NISPAMaskBackbone-667"><span class="linenos">667</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
</span><span id="NISPAMaskBackbone-668"><a href="#NISPAMaskBackbone-668"><span class="linenos">668</span></a>            <span class="p">)</span>
</span><span id="NISPAMaskBackbone-669"><a href="#NISPAMaskBackbone-669"><span class="linenos">669</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">frozen_weight_mask_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
</span><span id="NISPAMaskBackbone-670"><a href="#NISPAMaskBackbone-670"><span class="linenos">670</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
</span><span id="NISPAMaskBackbone-671"><a href="#NISPAMaskBackbone-671"><span class="linenos">671</span></a>            <span class="p">)</span>
</span><span id="NISPAMaskBackbone-672"><a href="#NISPAMaskBackbone-672"><span class="linenos">672</span></a>
</span><span id="NISPAMaskBackbone-673"><a href="#NISPAMaskBackbone-673"><span class="linenos">673</span></a>            <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="NISPAMaskBackbone-674"><a href="#NISPAMaskBackbone-674"><span class="linenos">674</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">bias_mask_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
</span><span id="NISPAMaskBackbone-675"><a href="#NISPAMaskBackbone-675"><span class="linenos">675</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
</span><span id="NISPAMaskBackbone-676"><a href="#NISPAMaskBackbone-676"><span class="linenos">676</span></a>                <span class="p">)</span>
</span><span id="NISPAMaskBackbone-677"><a href="#NISPAMaskBackbone-677"><span class="linenos">677</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">frozen_bias_mask_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
</span><span id="NISPAMaskBackbone-678"><a href="#NISPAMaskBackbone-678"><span class="linenos">678</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
</span><span id="NISPAMaskBackbone-679"><a href="#NISPAMaskBackbone-679"><span class="linenos">679</span></a>                <span class="p">)</span>
</span><span id="NISPAMaskBackbone-680"><a href="#NISPAMaskBackbone-680"><span class="linenos">680</span></a>
</span><span id="NISPAMaskBackbone-681"><a href="#NISPAMaskBackbone-681"><span class="linenos">681</span></a>    <span class="nd">@override</span>
</span><span id="NISPAMaskBackbone-682"><a href="#NISPAMaskBackbone-682"><span class="linenos">682</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="NISPAMaskBackbone-683"><a href="#NISPAMaskBackbone-683"><span class="linenos">683</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="NISPAMaskBackbone-684"><a href="#NISPAMaskBackbone-684"><span class="linenos">684</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="NISPAMaskBackbone-685"><a href="#NISPAMaskBackbone-685"><span class="linenos">685</span></a>        <span class="n">stage</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="NISPAMaskBackbone-686"><a href="#NISPAMaskBackbone-686"><span class="linenos">686</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
</span><span id="NISPAMaskBackbone-687"><a href="#NISPAMaskBackbone-687"><span class="linenos">687</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The forward pass for data from task `task_id`. The parameter mask is applied to the parameters in each layer.</span>
</span><span id="NISPAMaskBackbone-688"><a href="#NISPAMaskBackbone-688"><span class="linenos">688</span></a>
</span><span id="NISPAMaskBackbone-689"><a href="#NISPAMaskBackbone-689"><span class="linenos">689</span></a><span class="sd">        **Args:**</span>
</span><span id="NISPAMaskBackbone-690"><a href="#NISPAMaskBackbone-690"><span class="linenos">690</span></a><span class="sd">        - **input** (`Tensor`): The input tensor from data.</span>
</span><span id="NISPAMaskBackbone-691"><a href="#NISPAMaskBackbone-691"><span class="linenos">691</span></a><span class="sd">        - **stage** (`str`): The stage of the forward pass; one of:</span>
</span><span id="NISPAMaskBackbone-692"><a href="#NISPAMaskBackbone-692"><span class="linenos">692</span></a><span class="sd">            1. &#39;train&#39;: training stage.</span>
</span><span id="NISPAMaskBackbone-693"><a href="#NISPAMaskBackbone-693"><span class="linenos">693</span></a><span class="sd">            2. &#39;validation&#39;: validation stage.</span>
</span><span id="NISPAMaskBackbone-694"><a href="#NISPAMaskBackbone-694"><span class="linenos">694</span></a><span class="sd">            3. &#39;test&#39;: testing stage.</span>
</span><span id="NISPAMaskBackbone-695"><a href="#NISPAMaskBackbone-695"><span class="linenos">695</span></a>
</span><span id="NISPAMaskBackbone-696"><a href="#NISPAMaskBackbone-696"><span class="linenos">696</span></a><span class="sd">        **Returns:**</span>
</span><span id="NISPAMaskBackbone-697"><a href="#NISPAMaskBackbone-697"><span class="linenos">697</span></a><span class="sd">        - **output_feature** (`Tensor`): The output feature tensor to be passed into heads. This is the main target of backpropagation.</span>
</span><span id="NISPAMaskBackbone-698"><a href="#NISPAMaskBackbone-698"><span class="linenos">698</span></a><span class="sd">        - **weight_mask** (`dict[str, Tensor]`): The weight mask. Key (`str`) is layer name; value (`Tensor`) is the mask tensor. The mask tensor has the same size (output features, input features) as the weight.</span>
</span><span id="NISPAMaskBackbone-699"><a href="#NISPAMaskBackbone-699"><span class="linenos">699</span></a><span class="sd">        - **bias_mask** (`dict[str, Tensor]`): The bias mask. Key (`str`) is layer name; value (`Tensor`) is the mask tensor. The mask tensor has the same size (output features, ) as the bias. If the layer doesn&#39;t have a bias, it is `None`.</span>
</span><span id="NISPAMaskBackbone-700"><a href="#NISPAMaskBackbone-700"><span class="linenos">700</span></a><span class="sd">        - **activations** (`dict[str, Tensor]`): The hidden features (after activation) in each weighted layer. Key (`str`) is the weighted layer name; value (`Tensor`) is the hidden feature tensor. This is used for continual learning algorithms that need hidden features for various purposes.</span>
</span><span id="NISPAMaskBackbone-701"><a href="#NISPAMaskBackbone-701"><span class="linenos">701</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span></pre></div>


            <div class="docstring"><p>The backbone network for the NISPA algorithm with neuron masks.</p>

<p><a href="https://proceedings.mlr.press/v162/gurbuz22a/gurbuz22a.pdf">NISPA (Neuro-Inspired Stability-Plasticity Adaptation)</a> is an architecture-based continual learning algorithm.</p>
</div>


                            <div id="NISPAMaskBackbone.__init__" class="classattr">
                                        <input id="NISPAMaskBackbone.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">NISPAMaskBackbone</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span>, </span><span class="param"><span class="o">**</span><span class="n">kwargs</span></span>)</span>

                <label class="view-source-button" for="NISPAMaskBackbone.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#NISPAMaskBackbone.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="NISPAMaskBackbone.__init__-638"><a href="#NISPAMaskBackbone.__init__-638"><span class="linenos">638</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="NISPAMaskBackbone.__init__-639"><a href="#NISPAMaskBackbone.__init__-639"><span class="linenos">639</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialize the NISPA mask backbone network with masks.</span>
</span><span id="NISPAMaskBackbone.__init__-640"><a href="#NISPAMaskBackbone.__init__-640"><span class="linenos">640</span></a>
</span><span id="NISPAMaskBackbone.__init__-641"><a href="#NISPAMaskBackbone.__init__-641"><span class="linenos">641</span></a><span class="sd">        **Args:**</span>
</span><span id="NISPAMaskBackbone.__init__-642"><a href="#NISPAMaskBackbone.__init__-642"><span class="linenos">642</span></a><span class="sd">        - **output_dim** (`int`): The output dimension that connects to CL output heads. The `input_dim` of output heads is expected to be the same as this `output_dim`. In some cases, this class is used as a block in the backbone network that doesn&#39;t have an output dimension. In this case, it can be `None`.</span>
</span><span id="NISPAMaskBackbone.__init__-643"><a href="#NISPAMaskBackbone.__init__-643"><span class="linenos">643</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="NISPAMaskBackbone.__init__-644"><a href="#NISPAMaskBackbone.__init__-644"><span class="linenos">644</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span id="NISPAMaskBackbone.__init__-645"><a href="#NISPAMaskBackbone.__init__-645"><span class="linenos">645</span></a>
</span><span id="NISPAMaskBackbone.__init__-646"><a href="#NISPAMaskBackbone.__init__-646"><span class="linenos">646</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">weight_mask_t</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="NISPAMaskBackbone.__init__-647"><a href="#NISPAMaskBackbone.__init__-647"><span class="linenos">647</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the weight mask for each layer. Key (`str`) is layer name, value (`Tensor`) is the mask tensor. The mask tensor has the same size (output features, input features) as weight.&quot;&quot;&quot;</span>
</span><span id="NISPAMaskBackbone.__init__-648"><a href="#NISPAMaskBackbone.__init__-648"><span class="linenos">648</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">frozen_weight_mask_t</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="NISPAMaskBackbone.__init__-649"><a href="#NISPAMaskBackbone.__init__-649"><span class="linenos">649</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the frozen weight mask for each layer. Key (`str`) is layer name, value (`Tensor`) is the mask tensor. The mask tensor has the same size (output features, input features) as weight.&quot;&quot;&quot;</span>
</span><span id="NISPAMaskBackbone.__init__-650"><a href="#NISPAMaskBackbone.__init__-650"><span class="linenos">650</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">bias_mask_t</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="NISPAMaskBackbone.__init__-651"><a href="#NISPAMaskBackbone.__init__-651"><span class="linenos">651</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the bias mask for each layer. Key (`str`) is layer name, value (`Tensor`) is the mask tensor. The mask tensor has the same size (output features, ) as bias. If the layer doesn&#39;t have bias, it is `None`.&quot;&quot;&quot;</span>
</span><span id="NISPAMaskBackbone.__init__-652"><a href="#NISPAMaskBackbone.__init__-652"><span class="linenos">652</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">frozen_bias_mask_t</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="NISPAMaskBackbone.__init__-653"><a href="#NISPAMaskBackbone.__init__-653"><span class="linenos">653</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store the frozen bias mask for each layer. Key (`str`) is layer name, value (`Tensor`) is the mask tensor. The mask tensor has the same size (output features, ) as bias. If the layer doesn&#39;t have bias, it is `None`.&quot;&quot;&quot;</span>
</span><span id="NISPAMaskBackbone.__init__-654"><a href="#NISPAMaskBackbone.__init__-654"><span class="linenos">654</span></a>
</span><span id="NISPAMaskBackbone.__init__-655"><a href="#NISPAMaskBackbone.__init__-655"><span class="linenos">655</span></a>        <span class="n">NISPAMaskBackbone</span><span class="o">.</span><span class="n">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Initialize the NISPA mask backbone network with masks.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>output_dim</strong> (<code>int</code>): The output dimension that connects to CL output heads. The <code>input_dim</code> of output heads is expected to be the same as this <code><a href="#NISPAMaskBackbone.output_dim">output_dim</a></code>. In some cases, this class is used as a block in the backbone network that doesn't have an output dimension. In this case, it can be <code>None</code>.</li>
</ul>
</div>


                            </div>
                            <div id="NISPAMaskBackbone.weight_mask_t" class="classattr">
                                <div class="attr variable">
            <span class="name">weight_mask_t</span><span class="annotation">: dict[str, torch.Tensor]</span>

        
    </div>
    <a class="headerlink" href="#NISPAMaskBackbone.weight_mask_t"></a>
    
            <div class="docstring"><p>Store the weight mask for each layer. Key (<code>str</code>) is layer name, value (<code>Tensor</code>) is the mask tensor. The mask tensor has the same size (output features, input features) as weight.</p>
</div>


                            </div>
                            <div id="NISPAMaskBackbone.frozen_weight_mask_t" class="classattr">
                                <div class="attr variable">
            <span class="name">frozen_weight_mask_t</span><span class="annotation">: dict[str, torch.Tensor]</span>

        
    </div>
    <a class="headerlink" href="#NISPAMaskBackbone.frozen_weight_mask_t"></a>
    
            <div class="docstring"><p>Store the frozen weight mask for each layer. Key (<code>str</code>) is layer name, value (<code>Tensor</code>) is the mask tensor. The mask tensor has the same size (output features, input features) as weight.</p>
</div>


                            </div>
                            <div id="NISPAMaskBackbone.bias_mask_t" class="classattr">
                                <div class="attr variable">
            <span class="name">bias_mask_t</span><span class="annotation">: dict[str, torch.Tensor]</span>

        
    </div>
    <a class="headerlink" href="#NISPAMaskBackbone.bias_mask_t"></a>
    
            <div class="docstring"><p>Store the bias mask for each layer. Key (<code>str</code>) is layer name, value (<code>Tensor</code>) is the mask tensor. The mask tensor has the same size (output features, ) as bias. If the layer doesn't have bias, it is <code>None</code>.</p>
</div>


                            </div>
                            <div id="NISPAMaskBackbone.frozen_bias_mask_t" class="classattr">
                                <div class="attr variable">
            <span class="name">frozen_bias_mask_t</span><span class="annotation">: dict[str, torch.Tensor]</span>

        
    </div>
    <a class="headerlink" href="#NISPAMaskBackbone.frozen_bias_mask_t"></a>
    
            <div class="docstring"><p>Store the frozen bias mask for each layer. Key (<code>str</code>) is layer name, value (<code>Tensor</code>) is the mask tensor. The mask tensor has the same size (output features, ) as bias. If the layer doesn't have bias, it is <code>None</code>.</p>
</div>


                            </div>
                            <div id="NISPAMaskBackbone.sanity_check" class="classattr">
                                        <input id="NISPAMaskBackbone.sanity_check-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">sanity_check</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span></span><span class="return-annotation">) -> <span class="kc">None</span>:</span></span>

                <label class="view-source-button" for="NISPAMaskBackbone.sanity_check-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#NISPAMaskBackbone.sanity_check"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="NISPAMaskBackbone.sanity_check-657"><a href="#NISPAMaskBackbone.sanity_check-657"><span class="linenos">657</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">sanity_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="NISPAMaskBackbone.sanity_check-658"><a href="#NISPAMaskBackbone.sanity_check-658"><span class="linenos">658</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Check the sanity of the arguments.&quot;&quot;&quot;</span>
</span><span id="NISPAMaskBackbone.sanity_check-659"><a href="#NISPAMaskBackbone.sanity_check-659"><span class="linenos">659</span></a>        <span class="k">pass</span>
</span></pre></div>


            <div class="docstring"><p>Check the sanity of the arguments.</p>
</div>


                            </div>
                            <div id="NISPAMaskBackbone.initialize_parameter_mask" class="classattr">
                                        <input id="NISPAMaskBackbone.initialize_parameter_mask-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">initialize_parameter_mask</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span></span><span class="return-annotation">) -> <span class="kc">None</span>:</span></span>

                <label class="view-source-button" for="NISPAMaskBackbone.initialize_parameter_mask-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#NISPAMaskBackbone.initialize_parameter_mask"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="NISPAMaskBackbone.initialize_parameter_mask-661"><a href="#NISPAMaskBackbone.initialize_parameter_mask-661"><span class="linenos">661</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">initialize_parameter_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="NISPAMaskBackbone.initialize_parameter_mask-662"><a href="#NISPAMaskBackbone.initialize_parameter_mask-662"><span class="linenos">662</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialize the parameter masks as zeros.&quot;&quot;&quot;</span>
</span><span id="NISPAMaskBackbone.initialize_parameter_mask-663"><a href="#NISPAMaskBackbone.initialize_parameter_mask-663"><span class="linenos">663</span></a>        <span class="k">for</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">weighted_layer_names</span><span class="p">:</span>
</span><span id="NISPAMaskBackbone.initialize_parameter_mask-664"><a href="#NISPAMaskBackbone.initialize_parameter_mask-664"><span class="linenos">664</span></a>            <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_layer_by_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>  <span class="c1"># get the layer by its name</span>
</span><span id="NISPAMaskBackbone.initialize_parameter_mask-665"><a href="#NISPAMaskBackbone.initialize_parameter_mask-665"><span class="linenos">665</span></a>
</span><span id="NISPAMaskBackbone.initialize_parameter_mask-666"><a href="#NISPAMaskBackbone.initialize_parameter_mask-666"><span class="linenos">666</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">weight_mask_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
</span><span id="NISPAMaskBackbone.initialize_parameter_mask-667"><a href="#NISPAMaskBackbone.initialize_parameter_mask-667"><span class="linenos">667</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
</span><span id="NISPAMaskBackbone.initialize_parameter_mask-668"><a href="#NISPAMaskBackbone.initialize_parameter_mask-668"><span class="linenos">668</span></a>            <span class="p">)</span>
</span><span id="NISPAMaskBackbone.initialize_parameter_mask-669"><a href="#NISPAMaskBackbone.initialize_parameter_mask-669"><span class="linenos">669</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">frozen_weight_mask_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
</span><span id="NISPAMaskBackbone.initialize_parameter_mask-670"><a href="#NISPAMaskBackbone.initialize_parameter_mask-670"><span class="linenos">670</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
</span><span id="NISPAMaskBackbone.initialize_parameter_mask-671"><a href="#NISPAMaskBackbone.initialize_parameter_mask-671"><span class="linenos">671</span></a>            <span class="p">)</span>
</span><span id="NISPAMaskBackbone.initialize_parameter_mask-672"><a href="#NISPAMaskBackbone.initialize_parameter_mask-672"><span class="linenos">672</span></a>
</span><span id="NISPAMaskBackbone.initialize_parameter_mask-673"><a href="#NISPAMaskBackbone.initialize_parameter_mask-673"><span class="linenos">673</span></a>            <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="NISPAMaskBackbone.initialize_parameter_mask-674"><a href="#NISPAMaskBackbone.initialize_parameter_mask-674"><span class="linenos">674</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">bias_mask_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
</span><span id="NISPAMaskBackbone.initialize_parameter_mask-675"><a href="#NISPAMaskBackbone.initialize_parameter_mask-675"><span class="linenos">675</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
</span><span id="NISPAMaskBackbone.initialize_parameter_mask-676"><a href="#NISPAMaskBackbone.initialize_parameter_mask-676"><span class="linenos">676</span></a>                <span class="p">)</span>
</span><span id="NISPAMaskBackbone.initialize_parameter_mask-677"><a href="#NISPAMaskBackbone.initialize_parameter_mask-677"><span class="linenos">677</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">frozen_bias_mask_t</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
</span><span id="NISPAMaskBackbone.initialize_parameter_mask-678"><a href="#NISPAMaskBackbone.initialize_parameter_mask-678"><span class="linenos">678</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
</span><span id="NISPAMaskBackbone.initialize_parameter_mask-679"><a href="#NISPAMaskBackbone.initialize_parameter_mask-679"><span class="linenos">679</span></a>                <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Initialize the parameter masks as zeros.</p>
</div>


                            </div>
                            <div id="NISPAMaskBackbone.forward" class="classattr">
                                        <input id="NISPAMaskBackbone.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
                    <div class="decorator decorator-override">@override</div>

        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">stage</span><span class="p">:</span> <span class="nb">str</span></span><span class="return-annotation">) -> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span>:</span></span>

                <label class="view-source-button" for="NISPAMaskBackbone.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#NISPAMaskBackbone.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="NISPAMaskBackbone.forward-681"><a href="#NISPAMaskBackbone.forward-681"><span class="linenos">681</span></a>    <span class="nd">@override</span>
</span><span id="NISPAMaskBackbone.forward-682"><a href="#NISPAMaskBackbone.forward-682"><span class="linenos">682</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="NISPAMaskBackbone.forward-683"><a href="#NISPAMaskBackbone.forward-683"><span class="linenos">683</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="NISPAMaskBackbone.forward-684"><a href="#NISPAMaskBackbone.forward-684"><span class="linenos">684</span></a>        <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="NISPAMaskBackbone.forward-685"><a href="#NISPAMaskBackbone.forward-685"><span class="linenos">685</span></a>        <span class="n">stage</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="NISPAMaskBackbone.forward-686"><a href="#NISPAMaskBackbone.forward-686"><span class="linenos">686</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
</span><span id="NISPAMaskBackbone.forward-687"><a href="#NISPAMaskBackbone.forward-687"><span class="linenos">687</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The forward pass for data from task `task_id`. The parameter mask is applied to the parameters in each layer.</span>
</span><span id="NISPAMaskBackbone.forward-688"><a href="#NISPAMaskBackbone.forward-688"><span class="linenos">688</span></a>
</span><span id="NISPAMaskBackbone.forward-689"><a href="#NISPAMaskBackbone.forward-689"><span class="linenos">689</span></a><span class="sd">        **Args:**</span>
</span><span id="NISPAMaskBackbone.forward-690"><a href="#NISPAMaskBackbone.forward-690"><span class="linenos">690</span></a><span class="sd">        - **input** (`Tensor`): The input tensor from data.</span>
</span><span id="NISPAMaskBackbone.forward-691"><a href="#NISPAMaskBackbone.forward-691"><span class="linenos">691</span></a><span class="sd">        - **stage** (`str`): The stage of the forward pass; one of:</span>
</span><span id="NISPAMaskBackbone.forward-692"><a href="#NISPAMaskBackbone.forward-692"><span class="linenos">692</span></a><span class="sd">            1. &#39;train&#39;: training stage.</span>
</span><span id="NISPAMaskBackbone.forward-693"><a href="#NISPAMaskBackbone.forward-693"><span class="linenos">693</span></a><span class="sd">            2. &#39;validation&#39;: validation stage.</span>
</span><span id="NISPAMaskBackbone.forward-694"><a href="#NISPAMaskBackbone.forward-694"><span class="linenos">694</span></a><span class="sd">            3. &#39;test&#39;: testing stage.</span>
</span><span id="NISPAMaskBackbone.forward-695"><a href="#NISPAMaskBackbone.forward-695"><span class="linenos">695</span></a>
</span><span id="NISPAMaskBackbone.forward-696"><a href="#NISPAMaskBackbone.forward-696"><span class="linenos">696</span></a><span class="sd">        **Returns:**</span>
</span><span id="NISPAMaskBackbone.forward-697"><a href="#NISPAMaskBackbone.forward-697"><span class="linenos">697</span></a><span class="sd">        - **output_feature** (`Tensor`): The output feature tensor to be passed into heads. This is the main target of backpropagation.</span>
</span><span id="NISPAMaskBackbone.forward-698"><a href="#NISPAMaskBackbone.forward-698"><span class="linenos">698</span></a><span class="sd">        - **weight_mask** (`dict[str, Tensor]`): The weight mask. Key (`str`) is layer name; value (`Tensor`) is the mask tensor. The mask tensor has the same size (output features, input features) as the weight.</span>
</span><span id="NISPAMaskBackbone.forward-699"><a href="#NISPAMaskBackbone.forward-699"><span class="linenos">699</span></a><span class="sd">        - **bias_mask** (`dict[str, Tensor]`): The bias mask. Key (`str`) is layer name; value (`Tensor`) is the mask tensor. The mask tensor has the same size (output features, ) as the bias. If the layer doesn&#39;t have a bias, it is `None`.</span>
</span><span id="NISPAMaskBackbone.forward-700"><a href="#NISPAMaskBackbone.forward-700"><span class="linenos">700</span></a><span class="sd">        - **activations** (`dict[str, Tensor]`): The hidden features (after activation) in each weighted layer. Key (`str`) is the weighted layer name; value (`Tensor`) is the hidden feature tensor. This is used for continual learning algorithms that need hidden features for various purposes.</span>
</span><span id="NISPAMaskBackbone.forward-701"><a href="#NISPAMaskBackbone.forward-701"><span class="linenos">701</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span></pre></div>


            <div class="docstring"><p>The forward pass for data from task <code><a href="#NISPAMaskBackbone.task_id">task_id</a></code>. The parameter mask is applied to the parameters in each layer.</p>

<p><strong>Args:</strong></p>

<ul>
<li><strong>input</strong> (<code>Tensor</code>): The input tensor from data.</li>
<li><strong>stage</strong> (<code>str</code>): The stage of the forward pass; one of:
<ol>
<li>'train': training stage.</li>
<li>'validation': validation stage.</li>
<li>'test': testing stage.</li>
</ol></li>
</ul>

<p><strong>Returns:</strong></p>

<ul>
<li><strong>output_feature</strong> (<code>Tensor</code>): The output feature tensor to be passed into heads. This is the main target of backpropagation.</li>
<li><strong>weight_mask</strong> (<code>dict[str, Tensor]</code>): The weight mask. Key (<code>str</code>) is layer name; value (<code>Tensor</code>) is the mask tensor. The mask tensor has the same size (output features, input features) as the weight.</li>
<li><strong>bias_mask</strong> (<code>dict[str, Tensor]</code>): The bias mask. Key (<code>str</code>) is layer name; value (<code>Tensor</code>) is the mask tensor. The mask tensor has the same size (output features, ) as the bias. If the layer doesn't have a bias, it is <code>None</code>.</li>
<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): The hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name; value (<code>Tensor</code>) is the hidden feature tensor. This is used for continual learning algorithms that need hidden features for various purposes.</li>
</ul>
</div>


                            </div>
                </section>
    </main>
<script>
    function escapeHTML(html) {
        return document.createElement('div').appendChild(document.createTextNode(html)).parentNode.innerHTML;
    }

    const originalContent = document.querySelector("main.pdoc");
    let currentContent = originalContent;

    function setContent(innerHTML) {
        let elem;
        if (innerHTML) {
            elem = document.createElement("main");
            elem.classList.add("pdoc");
            elem.innerHTML = innerHTML;
        } else {
            elem = originalContent;
        }
        if (currentContent !== elem) {
            currentContent.replaceWith(elem);
            currentContent = elem;
        }
    }

    function getSearchTerm() {
        return (new URL(window.location)).searchParams.get("search");
    }

    const searchBox = document.querySelector(".pdoc input[type=search]");
    searchBox.addEventListener("input", function () {
        let url = new URL(window.location);
        if (searchBox.value.trim()) {
            url.hash = "";
            url.searchParams.set("search", searchBox.value);
        } else {
            url.searchParams.delete("search");
        }
        history.replaceState("", "", url.toString());
        onInput();
    });
    window.addEventListener("popstate", onInput);


    let search, searchErr;

    async function initialize() {
        try {
            search = await new Promise((resolve, reject) => {
                const script = document.createElement("script");
                script.type = "text/javascript";
                script.async = true;
                script.onload = () => resolve(window.pdocSearch);
                script.onerror = (e) => reject(e);
                script.src = "../search.js";
                document.getElementsByTagName("head")[0].appendChild(script);
            });
        } catch (e) {
            console.error("Cannot fetch pdoc search index");
            searchErr = "Cannot fetch search index.";
        }
        onInput();

        document.querySelector("nav.pdoc").addEventListener("click", e => {
            if (e.target.hash) {
                searchBox.value = "";
                searchBox.dispatchEvent(new Event("input"));
            }
        });
    }

    function onInput() {
        setContent((() => {
            const term = getSearchTerm();
            if (!term) {
                return null
            }
            if (searchErr) {
                return `<h3>Error: ${searchErr}</h3>`
            }
            if (!search) {
                return "<h3>Searching...</h3>"
            }

            window.scrollTo({top: 0, left: 0, behavior: 'auto'});

            const results = search(term);

            let html;
            if (results.length === 0) {
                html = `No search results for '${escapeHTML(term)}'.`
            } else {
                html = `<h4>${results.length} search result${results.length > 1 ? "s" : ""} for '${escapeHTML(term)}'.</h4>`;
            }
            for (let result of results.slice(0, 10)) {
                let doc = result.doc;
                let url = `../${doc.modulename.replaceAll(".", "/")}.html`;
                if (doc.qualname) {
                    url += `#${doc.qualname}`;
                }

                let heading;
                switch (result.doc.kind) {
                    case "function":
                        if (doc.fullname.endsWith(".__init__")) {
                            heading = `<span class="name">${doc.fullname.replace(/\.__init__$/, "")}</span>${doc.signature}`;
                        } else {
                            heading = `<span class="def">${doc.funcdef}</span> <span class="name">${doc.fullname}</span>${doc.signature}`;
                        }
                        break;
                    case "class":
                        heading = `<span class="def">class</span> <span class="name">${doc.fullname}</span>`;
                        if (doc.bases)
                            heading += `<wbr>(<span class="base">${doc.bases}</span>)`;
                        heading += `:`;
                        break;
                    case "variable":
                        heading = `<span class="name">${doc.fullname}</span>`;
                        if (doc.annotation)
                            heading += `<span class="annotation">${doc.annotation}</span>`;
                        if (doc.default_value)
                            heading += `<span class="default_value"> = ${doc.default_value}</span>`;
                        break;
                    default:
                        heading = `<span class="name">${doc.fullname}</span>`;
                        break;
                }
                html += `
                        <section class="search-result">
                        <a href="${url}" class="attr ${doc.kind}">${heading}</a>
                        <div class="docstring">${doc.doc}</div>
                        </section>
                    `;

            }
            return html;
        })());
    }

    if (getSearchTerm()) {
        initialize();
        searchBox.value = getSearchTerm();
        onInput();
    } else {
        searchBox.addEventListener("focus", initialize, {once: true});
    }

    searchBox.addEventListener("keydown", e => {
        if (["ArrowDown", "ArrowUp", "Enter"].includes(e.key)) {
            let focused = currentContent.querySelector(".search-result.focused");
            if (!focused) {
                currentContent.querySelector(".search-result").classList.add("focused");
            } else if (
                e.key === "ArrowDown"
                && focused.nextElementSibling
                && focused.nextElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.nextElementSibling.classList.add("focused");
                focused.nextElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "ArrowUp"
                && focused.previousElementSibling
                && focused.previousElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.previousElementSibling.classList.add("focused");
                focused.previousElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "Enter"
            ) {
                focused.querySelector("a").click();
            }
        }
    });
</script></body>
</html>